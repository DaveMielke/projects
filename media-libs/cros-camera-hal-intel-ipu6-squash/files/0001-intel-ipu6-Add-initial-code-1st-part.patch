From 2f4305a0c633ce074bf6ca502455be474a25616c Mon Sep 17 00:00:00 2001
From: Tianshu Qiu <tian.shu.qiu@intel.com>
Date: Wed, 30 Oct 2019 10:03:39 +0800
Subject: [PATCH 1/2] intel:ipu6: Add initial code (1st part)

Add initial code in src/platform2/camera/hal/intel/ipu6
1.HAL code
2.3a header file
3.ia_xxx files are not included in this patch.

Change-Id: Ie8fbf5df647e72c2733a67c07d82f9d565b17d1a
Signed-off-by: Tianshu Qiu <tian.shu.qiu@intel.com>
---
 camera/hal/intel/ipu6/BUILD.gn                |  482 +++
 .../hal/intel/ipu6/aal/Camera3AMetadata.cpp   |  132 +
 camera/hal/intel/ipu6/aal/Camera3AMetadata.h  |   51 +
 camera/hal/intel/ipu6/aal/Camera3HAL.cpp      |  249 ++
 camera/hal/intel/ipu6/aal/Camera3HAL.h        |   68 +
 .../hal/intel/ipu6/aal/Camera3HALModule.cpp   |  299 ++
 camera/hal/intel/ipu6/aal/Camera3Stream.cpp   |  653 ++++
 camera/hal/intel/ipu6/aal/Camera3Stream.h     |  177 ++
 camera/hal/intel/ipu6/aal/HALv3Interface.h    |   32 +
 camera/hal/intel/ipu6/aal/HALv3Utils.cpp      |  161 +
 camera/hal/intel/ipu6/aal/HALv3Utils.h        |   78 +
 .../intel/ipu6/aal/IntelAEStateMachine.cpp    |  285 ++
 .../hal/intel/ipu6/aal/IntelAEStateMachine.h  |  136 +
 .../intel/ipu6/aal/IntelAFStateMachine.cpp    |  415 +++
 .../hal/intel/ipu6/aal/IntelAFStateMachine.h  |  142 +
 .../intel/ipu6/aal/IntelAWBStateMachine.cpp   |  214 ++
 .../hal/intel/ipu6/aal/IntelAWBStateMachine.h |  134 +
 camera/hal/intel/ipu6/aal/MetadataConvert.cpp | 2247 ++++++++++++++
 camera/hal/intel/ipu6/aal/MetadataConvert.h   |  142 +
 camera/hal/intel/ipu6/aal/PostProcessor.cpp   |  193 ++
 camera/hal/intel/ipu6/aal/PostProcessor.h     |   60 +
 camera/hal/intel/ipu6/aal/RequestManager.cpp  |  788 +++++
 camera/hal/intel/ipu6/aal/RequestManager.h    |  115 +
 camera/hal/intel/ipu6/aal/ResultProcessor.cpp |  512 ++++
 camera/hal/intel/ipu6/aal/ResultProcessor.h   |  196 ++
 .../hal/intel/ipu6/aal/chrome/HALv3Header.h   |   20 +
 camera/hal/intel/ipu6/include/api/ICamera.h   |  578 ++++
 .../hal/intel/ipu6/include/api/Parameters.h   | 2630 ++++++++++++++++
 camera/hal/intel/ipu6/include/cameranvm.h     |  112 +
 .../hal/intel/ipu6/include/linux/ipu-isys.h   |   51 +
 .../intel/ipu6/include/utils/ScopedAtrace.h   |   73 +
 .../ipu6/modules/algowrapper/IntelAiq.cpp     |  131 +
 .../intel/ipu6/modules/algowrapper/IntelAiq.h |   49 +
 .../ipu6/modules/algowrapper/IntelCmc.cpp     |   62 +
 .../intel/ipu6/modules/algowrapper/IntelCmc.h |   38 +
 .../ipu6/modules/algowrapper/IntelDvs.cpp     |  186 ++
 .../intel/ipu6/modules/algowrapper/IntelDvs.h |   55 +
 .../algowrapper/IntelFaceDetection.cpp        |  141 +
 .../modules/algowrapper/IntelFaceDetection.h  |   44 +
 .../algowrapper/IntelIspParamAdaptor.cpp      |  140 +
 .../algowrapper/IntelIspParamAdaptor.h        |   44 +
 .../ipu6/modules/algowrapper/IntelLard.cpp    |   71 +
 .../ipu6/modules/algowrapper/IntelLard.h      |   34 +
 .../ipu6/modules/algowrapper/IntelLtm.cpp     |   64 +
 .../intel/ipu6/modules/algowrapper/IntelLtm.h |   32 +
 .../ipu6/modules/algowrapper/IntelMkn.cpp     |   69 +
 .../intel/ipu6/modules/algowrapper/IntelMkn.h |   35 +
 .../ipu6/modules/algowrapper/IntelPGParam.cpp | 1196 ++++++++
 .../ipu6/modules/algowrapper/IntelPGParam.h   |  237 ++
 .../ipu6/modules/algowrapper/IntelTNR7US.cpp  |  184 ++
 .../ipu6/modules/algowrapper/IntelTNR7US.h    |  108 +
 .../ipu6/modules/algowrapper/StatsTypes.h     |   46 +
 .../algowrapper/graph/GraphConfigImpl.cpp     |  859 ++++++
 .../algowrapper/graph/GraphConfigImpl.h       |  183 ++
 .../algowrapper/graph/GraphConfigPipe.cpp     | 2008 ++++++++++++
 .../algowrapper/graph/GraphConfigPipe.h       |  313 ++
 .../ipu6/modules/memory/Camera3BufferPool.cpp |  127 +
 .../ipu6/modules/memory/Camera3BufferPool.h   |   53 +
 .../modules/memory/chrome/Camera3Buffer.cpp   |  527 ++++
 .../modules/memory/chrome/Camera3Buffer.h     |  159 +
 .../ipu6/modules/sandboxing/IPCCommon.cpp     |  116 +
 .../intel/ipu6/modules/sandboxing/IPCCommon.h |  125 +
 .../modules/sandboxing/IPCGraphConfig.cpp     |  490 +++
 .../ipu6/modules/sandboxing/IPCGraphConfig.h  |  171 ++
 .../ipu6/modules/sandboxing/IPCIntelAiq.cpp   | 1121 +++++++
 .../ipu6/modules/sandboxing/IPCIntelAiq.h     |  351 +++
 .../ipu6/modules/sandboxing/IPCIntelCmc.cpp   |  258 ++
 .../ipu6/modules/sandboxing/IPCIntelCmc.h     |   95 +
 .../ipu6/modules/sandboxing/IPCIntelDvs.cpp   |  704 +++++
 .../ipu6/modules/sandboxing/IPCIntelDvs.h     |  239 ++
 .../ipu6/modules/sandboxing/IPCIntelFD.cpp    |   63 +
 .../ipu6/modules/sandboxing/IPCIntelFD.h      |   32 +
 .../ipu6/modules/sandboxing/IPCIntelLard.cpp  |  298 ++
 .../ipu6/modules/sandboxing/IPCIntelLard.h    |  104 +
 .../ipu6/modules/sandboxing/IPCIntelLtm.cpp   |  331 ++
 .../ipu6/modules/sandboxing/IPCIntelLtm.h     |  117 +
 .../ipu6/modules/sandboxing/IPCIntelMkn.cpp   |   79 +
 .../ipu6/modules/sandboxing/IPCIntelMkn.h     |   62 +
 .../modules/sandboxing/IPCIntelPGParam.cpp    |  543 ++++
 .../ipu6/modules/sandboxing/IPCIntelPGParam.h |  180 ++
 .../modules/sandboxing/IPCIspParamAdaptor.cpp |  554 ++++
 .../modules/sandboxing/IPCIspParamAdaptor.h   |  170 ++
 .../sandboxing/client/GraphConfigImpl.cpp     |  186 ++
 .../sandboxing/client/GraphConfigImpl.h       |   75 +
 .../modules/sandboxing/client/IntelAiq.cpp    |  319 ++
 .../ipu6/modules/sandboxing/client/IntelAiq.h |   70 +
 .../sandboxing/client/IntelAlgoClient.cpp     |  432 +++
 .../sandboxing/client/IntelAlgoClient.h       |  134 +
 .../sandboxing/client/IntelAlgoCommon.cpp     |  131 +
 .../sandboxing/client/IntelAlgoCommon.h       |   63 +
 .../modules/sandboxing/client/IntelCmc.cpp    |  107 +
 .../ipu6/modules/sandboxing/client/IntelCmc.h |   58 +
 .../modules/sandboxing/client/IntelDvs.cpp    |  336 ++
 .../ipu6/modules/sandboxing/client/IntelDvs.h |   78 +
 .../sandboxing/client/IntelFaceDetection.cpp  |  121 +
 .../sandboxing/client/IntelFaceDetection.h    |   48 +
 .../client/IntelIspParamAdaptor.cpp           |  196 ++
 .../sandboxing/client/IntelIspParamAdaptor.h  |   59 +
 .../modules/sandboxing/client/IntelLard.cpp   |  137 +
 .../modules/sandboxing/client/IntelLard.h     |   52 +
 .../modules/sandboxing/client/IntelLtm.cpp    |  133 +
 .../ipu6/modules/sandboxing/client/IntelLtm.h |   53 +
 .../modules/sandboxing/client/IntelMkn.cpp    |  119 +
 .../ipu6/modules/sandboxing/client/IntelMkn.h |   54 +
 .../sandboxing/client/IntelPGParam.cpp        |  294 ++
 .../modules/sandboxing/client/IntelPGParam.h  |   71 +
 .../modules/sandboxing/client/IntelTNR7US.cpp |  161 +
 .../modules/sandboxing/client/IntelTNR7US.h   |   54 +
 .../sandboxing/server/GraphConfigServer.cpp   |  164 +
 .../sandboxing/server/GraphConfigServer.h     |   44 +
 .../sandboxing/server/IntelAiqServer.cpp      |  327 ++
 .../sandboxing/server/IntelAiqServer.h        |   49 +
 .../sandboxing/server/IntelAlgoServer.cpp     |  241 ++
 .../sandboxing/server/IntelAlgoServer.h       |  106 +
 .../sandboxing/server/IntelCPUAlgoServer.cpp  |  307 ++
 .../sandboxing/server/IntelCPUAlgoServer.h    |   63 +
 .../sandboxing/server/IntelCmcServer.cpp      |   91 +
 .../sandboxing/server/IntelCmcServer.h        |   40 +
 .../sandboxing/server/IntelDvsServer.cpp      |  312 ++
 .../sandboxing/server/IntelDvsServer.h        |   54 +
 .../sandboxing/server/IntelFDServer.cpp       |   63 +
 .../modules/sandboxing/server/IntelFDServer.h |   39 +
 .../sandboxing/server/IntelGPUAlgoServer.cpp  |  110 +
 .../sandboxing/server/IntelGPUAlgoServer.h    |   50 +
 .../sandboxing/server/IntelLardServer.cpp     |  112 +
 .../sandboxing/server/IntelLardServer.h       |   39 +
 .../sandboxing/server/IntelLtmServer.cpp      |  105 +
 .../sandboxing/server/IntelLtmServer.h        |   40 +
 .../sandboxing/server/IntelMknServer.cpp      |  124 +
 .../sandboxing/server/IntelMknServer.h        |   45 +
 .../sandboxing/server/IntelPGParamServer.cpp  |  229 ++
 .../sandboxing/server/IntelPGParamServer.h    |   56 +
 .../sandboxing/server/IntelTNRServer.cpp      |   71 +
 .../sandboxing/server/IntelTNRServer.h        |   41 +
 .../server/IspParamAdaptorServer.cpp          |  155 +
 .../sandboxing/server/IspParamAdaptorServer.h |   44 +
 camera/hal/intel/ipu6/src/3a/AiqCore.cpp      |  961 ++++++
 camera/hal/intel/ipu6/src/3a/AiqCore.h        |  214 ++
 camera/hal/intel/ipu6/src/3a/AiqEngine.cpp    |  522 ++++
 camera/hal/intel/ipu6/src/3a/AiqEngine.h      |  149 +
 camera/hal/intel/ipu6/src/3a/AiqResult.cpp    |  182 ++
 camera/hal/intel/ipu6/src/3a/AiqResult.h      |  109 +
 .../intel/ipu6/src/3a/AiqResultStorage.cpp    |  286 ++
 .../hal/intel/ipu6/src/3a/AiqResultStorage.h  |  246 ++
 camera/hal/intel/ipu6/src/3a/AiqSetting.cpp   |  379 +++
 camera/hal/intel/ipu6/src/3a/AiqSetting.h     |  155 +
 .../hal/intel/ipu6/src/3a/AiqStatistics.cpp   |  147 +
 camera/hal/intel/ipu6/src/3a/AiqStatistics.h  |   58 +
 camera/hal/intel/ipu6/src/3a/AiqUnit.cpp      |  257 ++
 camera/hal/intel/ipu6/src/3a/AiqUnit.h        |  155 +
 camera/hal/intel/ipu6/src/3a/AiqUtils.cpp     |  660 ++++
 camera/hal/intel/ipu6/src/3a/AiqUtils.h       |  185 ++
 camera/hal/intel/ipu6/src/3a/Dvs.cpp          |  616 ++++
 camera/hal/intel/ipu6/src/3a/Dvs.h            |  119 +
 camera/hal/intel/ipu6/src/3a/DvsResult.cpp    |  105 +
 camera/hal/intel/ipu6/src/3a/DvsResult.h      |   48 +
 .../intel/ipu6/src/3a/I3AControlFactory.cpp   |   35 +
 .../hal/intel/ipu6/src/3a/I3AControlFactory.h |   44 +
 camera/hal/intel/ipu6/src/3a/ImagingControl.h |   46 +
 camera/hal/intel/ipu6/src/3a/LensManager.cpp  |  101 +
 camera/hal/intel/ipu6/src/3a/LensManager.h    |   77 +
 camera/hal/intel/ipu6/src/3a/Ltm.cpp          |  545 ++++
 camera/hal/intel/ipu6/src/3a/Ltm.h            |  185 ++
 camera/hal/intel/ipu6/src/3a/MakerNote.cpp    |  126 +
 camera/hal/intel/ipu6/src/3a/MakerNote.h      |  116 +
 .../hal/intel/ipu6/src/3a/SensorManager.cpp   |  327 ++
 camera/hal/intel/ipu6/src/3a/SensorManager.h  |  111 +
 .../ipu6/src/3a/intel3a/Intel3AParameter.cpp  |  714 +++++
 .../ipu6/src/3a/intel3a/Intel3AParameter.h    |  104 +
 .../hal/intel/ipu6/src/core/BufferQueue.cpp   |  286 ++
 camera/hal/intel/ipu6/src/core/BufferQueue.h  |  206 ++
 .../hal/intel/ipu6/src/core/CameraBuffer.cpp  |  407 +++
 camera/hal/intel/ipu6/src/core/CameraBuffer.h |  170 ++
 .../hal/intel/ipu6/src/core/CameraDevice.cpp  | 1182 ++++++++
 camera/hal/intel/ipu6/src/core/CameraDevice.h |  284 ++
 .../hal/intel/ipu6/src/core/CameraEvent.cpp   |   84 +
 camera/hal/intel/ipu6/src/core/CameraEvent.h  |   50 +
 .../hal/intel/ipu6/src/core/CameraEventType.h |  111 +
 .../hal/intel/ipu6/src/core/CameraStream.cpp  |  239 ++
 camera/hal/intel/ipu6/src/core/CameraStream.h |  118 +
 .../hal/intel/ipu6/src/core/CaptureUnit.cpp   |  477 +++
 camera/hal/intel/ipu6/src/core/CaptureUnit.h  |  194 ++
 camera/hal/intel/ipu6/src/core/DeviceBase.cpp |  442 +++
 camera/hal/intel/ipu6/src/core/DeviceBase.h   |  171 ++
 .../intel/ipu6/src/core/IspParamAdaptor.cpp   |  964 ++++++
 .../hal/intel/ipu6/src/core/IspParamAdaptor.h |  153 +
 camera/hal/intel/ipu6/src/core/IspSettings.h  |   46 +
 camera/hal/intel/ipu6/src/core/LensHw.cpp     |  159 +
 camera/hal/intel/ipu6/src/core/LensHw.h       |   63 +
 .../hal/intel/ipu6/src/core/PSysProcessor.cpp |  903 ++++++
 .../hal/intel/ipu6/src/core/PSysProcessor.h   |  133 +
 .../intel/ipu6/src/core/ProcessorManager.cpp  |  125 +
 .../intel/ipu6/src/core/ProcessorManager.h    |   71 +
 .../hal/intel/ipu6/src/core/RequestThread.cpp |  542 ++++
 .../hal/intel/ipu6/src/core/RequestThread.h   |  154 +
 .../hal/intel/ipu6/src/core/SensorHwCtrl.cpp  |  331 ++
 camera/hal/intel/ipu6/src/core/SensorHwCtrl.h |   99 +
 camera/hal/intel/ipu6/src/core/SofSource.cpp  |  201 ++
 camera/hal/intel/ipu6/src/core/SofSource.h    |   63 +
 camera/hal/intel/ipu6/src/core/StreamSource.h |   48 +
 .../intel/ipu6/src/core/SwImageProcessor.cpp  |  158 +
 .../intel/ipu6/src/core/SwImageProcessor.h    |   46 +
 .../hal/intel/ipu6/src/core/SyncManager.cpp   |  198 ++
 camera/hal/intel/ipu6/src/core/SyncManager.h  |   58 +
 camera/hal/intel/ipu6/src/core/TNRCommon.h    |   45 +
 .../src/core/psysprocessor/GPUExecutor.cpp    |  482 +++
 .../ipu6/src/core/psysprocessor/GPUExecutor.h |   60 +
 .../ipu6/src/core/psysprocessor/PGCommon.cpp  | 1235 ++++++++
 .../ipu6/src/core/psysprocessor/PGCommon.h    |  240 ++
 .../ipu6/src/core/psysprocessor/PGUtils.cpp   |  194 ++
 .../ipu6/src/core/psysprocessor/PGUtils.h     |   86 +
 .../ipu6/src/core/psysprocessor/PSysDAG.cpp   |  759 +++++
 .../ipu6/src/core/psysprocessor/PSysDAG.h     |  155 +
 .../core/psysprocessor/PipeLiteExecutor.cpp   | 1127 +++++++
 .../src/core/psysprocessor/PipeLiteExecutor.h |  191 ++
 .../src/core/psysprocessor/PolicyManager.cpp  |  168 +
 .../src/core/psysprocessor/PolicyManager.h    |   79 +
 camera/hal/intel/ipu6/src/fd/FaceBase.h       |   81 +
 .../hal/intel/ipu6/src/fd/FaceDetection.cpp   |  530 ++++
 camera/hal/intel/ipu6/src/fd/FaceDetection.h  |  120 +
 camera/hal/intel/ipu6/src/hal/CameraHal.cpp   |  268 ++
 camera/hal/intel/ipu6/src/hal/CameraHal.h     |   85 +
 camera/hal/intel/ipu6/src/hal/ICamera.cpp     |  371 +++
 .../ipu6/src/image_process/IImageProcessor.h  |   46 +
 .../ipu6/src/image_process/ImageConverter.cpp |  799 +++++
 .../ipu6/src/image_process/ImageConverter.h   |   59 +
 .../src/image_process/ImageScalerCore.cpp     |  959 ++++++
 .../ipu6/src/image_process/ImageScalerCore.h  |   78 +
 .../src/image_process/PostProcessorBase.cpp   |  318 ++
 .../src/image_process/PostProcessorBase.h     |  118 +
 .../src/image_process/PostProcessorCore.cpp   |  141 +
 .../src/image_process/PostProcessorCore.h     |   68 +
 .../ipu6/src/image_process/ProcessType.h      |   30 +
 .../chrome/ImageProcessorCore.cpp             |  234 ++
 .../image_process/chrome/ImageProcessorCore.h |   44 +
 .../hal/intel/ipu6/src/iutils/CameraDump.cpp  |  409 +++
 camera/hal/intel/ipu6/src/iutils/CameraDump.h |  170 ++
 .../hal/intel/ipu6/src/iutils/CameraLog.cpp   |  270 ++
 camera/hal/intel/ipu6/src/iutils/CameraLog.h  |  206 ++
 camera/hal/intel/ipu6/src/iutils/Errors.h     |   65 +
 camera/hal/intel/ipu6/src/iutils/RWLock.h     |   98 +
 .../intel/ipu6/src/iutils/ScopedAtrace.cpp    |   63 +
 .../ipu6/src/iutils/SwImageConverter.cpp      |  406 +++
 .../intel/ipu6/src/iutils/SwImageConverter.h  |   42 +
 camera/hal/intel/ipu6/src/iutils/Thread.cpp   |  222 ++
 camera/hal/intel/ipu6/src/iutils/Thread.h     |  198 ++
 camera/hal/intel/ipu6/src/iutils/Trace.cpp    |   54 +
 camera/hal/intel/ipu6/src/iutils/Trace.h      |  241 ++
 camera/hal/intel/ipu6/src/iutils/Utils.cpp    |  740 +++++
 camera/hal/intel/ipu6/src/iutils/Utils.h      |  277 ++
 camera/hal/intel/ipu6/src/jpeg/EXIFMaker.cpp  |  620 ++++
 camera/hal/intel/ipu6/src/jpeg/EXIFMaker.h    |   72 +
 .../hal/intel/ipu6/src/jpeg/EXIFMetaData.cpp  |   57 +
 camera/hal/intel/ipu6/src/jpeg/EXIFMetaData.h |   68 +
 camera/hal/intel/ipu6/src/jpeg/Exif.h         |  343 +++
 .../hal/intel/ipu6/src/jpeg/ExifCreater.cpp   |  542 ++++
 camera/hal/intel/ipu6/src/jpeg/ExifCreater.h  |  114 +
 camera/hal/intel/ipu6/src/jpeg/IJpegEncoder.h |   78 +
 camera/hal/intel/ipu6/src/jpeg/JpegMaker.cpp  |  222 ++
 camera/hal/intel/ipu6/src/jpeg/JpegMaker.h    |   56 +
 .../ipu6/src/jpeg/chrome/JpegEncoderCore.cpp  |   74 +
 .../ipu6/src/jpeg/chrome/JpegEncoderCore.h    |   41 +
 .../intel/ipu6/src/jpeg/sw/SWJpegEncoder.cpp  |  740 +++++
 .../intel/ipu6/src/jpeg/sw/SWJpegEncoder.h    |  186 ++
 .../ipu6/src/metadata/CameraMetadata.cpp      |  435 +++
 .../intel/ipu6/src/metadata/CameraMetadata.h  |  200 ++
 .../ipu6/src/metadata/ParameterGenerator.cpp  |  378 +++
 .../ipu6/src/metadata/ParameterGenerator.h    |  115 +
 .../ipu6/src/metadata/ParameterHelper.cpp     |   84 +
 .../intel/ipu6/src/metadata/ParameterHelper.h |  142 +
 .../intel/ipu6/src/metadata/Parameters.cpp    | 2026 +++++++++++++
 .../src/metadata/icamera_metadata_base.cpp    |  976 ++++++
 .../ipu6/src/metadata/icamera_metadata_base.h |  445 +++
 .../src/metadata/icamera_metadata_tag_info.c  | 2698 +++++++++++++++++
 .../ipu6/src/metadata/icamera_metadata_tags.h | 1019 +++++++
 .../ipu6/src/platformdata/AiqInitData.cpp     |  583 ++++
 .../intel/ipu6/src/platformdata/AiqInitData.h |  235 ++
 .../ipu6/src/platformdata/CameraParser.cpp    | 1903 ++++++++++++
 .../ipu6/src/platformdata/CameraParser.h      |  127 +
 .../intel/ipu6/src/platformdata/CameraTypes.h |  228 ++
 .../ipu6/src/platformdata/ParserBase.cpp      |  165 +
 .../intel/ipu6/src/platformdata/ParserBase.h  |  102 +
 .../ipu6/src/platformdata/PlatformData.cpp    | 1369 +++++++++
 .../ipu6/src/platformdata/PlatformData.h      | 1159 +++++++
 .../ipu6/src/platformdata/PolicyParser.cpp    |  226 ++
 .../ipu6/src/platformdata/PolicyParser.h      |   73 +
 .../ipu6/src/platformdata/gc/FormatUtils.cpp  |  338 +++
 .../ipu6/src/platformdata/gc/FormatUtils.h    |   34 +
 .../ipu6/src/platformdata/gc/GraphConfig.cpp  |  199 ++
 .../ipu6/src/platformdata/gc/GraphConfig.h    |   88 +
 .../platformdata/gc/GraphConfigManager.cpp    |  179 ++
 .../src/platformdata/gc/GraphConfigManager.h  |   85 +
 .../ipu6/src/platformdata/gc/GraphUtils.cpp   |  112 +
 .../ipu6/src/platformdata/gc/GraphUtils.h     |   27 +
 .../ipu6/src/platformdata/gc/HalStream.h      |   74 +
 .../ipu6/src/platformdata/gc/IGraphConfig.h   |  203 ++
 .../src/platformdata/gc/IGraphConfigManager.h |   43 +
 .../src/platformdata/gc/custom_gcss_keys.h    |   50 +
 .../hal/intel/ipu6/src/v4l2/MediaControl.cpp  | 1116 +++++++
 camera/hal/intel/ipu6/src/v4l2/MediaControl.h |  302 ++
 camera/hal/intel/ipu6/src/v4l2/NodeInfo.cpp   |   53 +
 camera/hal/intel/ipu6/src/v4l2/NodeInfo.h     |   50 +
 camera/hal/intel/ipu6/src/v4l2/SysCall.cpp    |  212 ++
 camera/hal/intel/ipu6/src/v4l2/SysCall.h      |   87 +
 .../intel/ipu6/src/v4l2/V4l2DeviceFactory.cpp |  126 +
 .../intel/ipu6/src/v4l2/V4l2DeviceFactory.h   |   61 +
 306 files changed, 82476 insertions(+)
 create mode 100644 camera/hal/intel/ipu6/BUILD.gn
 create mode 100644 camera/hal/intel/ipu6/aal/Camera3AMetadata.cpp
 create mode 100644 camera/hal/intel/ipu6/aal/Camera3AMetadata.h
 create mode 100644 camera/hal/intel/ipu6/aal/Camera3HAL.cpp
 create mode 100644 camera/hal/intel/ipu6/aal/Camera3HAL.h
 create mode 100644 camera/hal/intel/ipu6/aal/Camera3HALModule.cpp
 create mode 100644 camera/hal/intel/ipu6/aal/Camera3Stream.cpp
 create mode 100644 camera/hal/intel/ipu6/aal/Camera3Stream.h
 create mode 100644 camera/hal/intel/ipu6/aal/HALv3Interface.h
 create mode 100644 camera/hal/intel/ipu6/aal/HALv3Utils.cpp
 create mode 100644 camera/hal/intel/ipu6/aal/HALv3Utils.h
 create mode 100644 camera/hal/intel/ipu6/aal/IntelAEStateMachine.cpp
 create mode 100644 camera/hal/intel/ipu6/aal/IntelAEStateMachine.h
 create mode 100644 camera/hal/intel/ipu6/aal/IntelAFStateMachine.cpp
 create mode 100644 camera/hal/intel/ipu6/aal/IntelAFStateMachine.h
 create mode 100644 camera/hal/intel/ipu6/aal/IntelAWBStateMachine.cpp
 create mode 100644 camera/hal/intel/ipu6/aal/IntelAWBStateMachine.h
 create mode 100644 camera/hal/intel/ipu6/aal/MetadataConvert.cpp
 create mode 100644 camera/hal/intel/ipu6/aal/MetadataConvert.h
 create mode 100644 camera/hal/intel/ipu6/aal/PostProcessor.cpp
 create mode 100644 camera/hal/intel/ipu6/aal/PostProcessor.h
 create mode 100644 camera/hal/intel/ipu6/aal/RequestManager.cpp
 create mode 100644 camera/hal/intel/ipu6/aal/RequestManager.h
 create mode 100644 camera/hal/intel/ipu6/aal/ResultProcessor.cpp
 create mode 100644 camera/hal/intel/ipu6/aal/ResultProcessor.h
 create mode 100644 camera/hal/intel/ipu6/aal/chrome/HALv3Header.h
 create mode 100644 camera/hal/intel/ipu6/include/api/ICamera.h
 create mode 100644 camera/hal/intel/ipu6/include/api/Parameters.h
 create mode 100644 camera/hal/intel/ipu6/include/cameranvm.h
 create mode 100644 camera/hal/intel/ipu6/include/linux/ipu-isys.h
 create mode 100644 camera/hal/intel/ipu6/include/utils/ScopedAtrace.h
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelAiq.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelAiq.h
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelCmc.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelCmc.h
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelDvs.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelDvs.h
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelFaceDetection.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelFaceDetection.h
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelIspParamAdaptor.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelIspParamAdaptor.h
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelLard.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelLard.h
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelLtm.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelLtm.h
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelMkn.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelMkn.h
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelPGParam.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelPGParam.h
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelTNR7US.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/IntelTNR7US.h
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/StatsTypes.h
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/graph/GraphConfigImpl.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/graph/GraphConfigImpl.h
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/graph/GraphConfigPipe.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/algowrapper/graph/GraphConfigPipe.h
 create mode 100644 camera/hal/intel/ipu6/modules/memory/Camera3BufferPool.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/memory/Camera3BufferPool.h
 create mode 100644 camera/hal/intel/ipu6/modules/memory/chrome/Camera3Buffer.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/memory/chrome/Camera3Buffer.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCCommon.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCCommon.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCGraphConfig.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCGraphConfig.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIntelAiq.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIntelAiq.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIntelCmc.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIntelCmc.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIntelDvs.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIntelDvs.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIntelFD.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIntelFD.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIntelLard.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIntelLard.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIntelLtm.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIntelLtm.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIntelMkn.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIntelMkn.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIntelPGParam.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIntelPGParam.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIspParamAdaptor.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/IPCIspParamAdaptor.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/GraphConfigImpl.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/GraphConfigImpl.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelAiq.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelAiq.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelAlgoClient.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelAlgoClient.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelAlgoCommon.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelAlgoCommon.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelCmc.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelCmc.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelDvs.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelDvs.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelFaceDetection.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelFaceDetection.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelIspParamAdaptor.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelIspParamAdaptor.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelLard.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelLard.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelLtm.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelLtm.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelMkn.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelMkn.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelPGParam.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelPGParam.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelTNR7US.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/client/IntelTNR7US.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/GraphConfigServer.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/GraphConfigServer.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelAiqServer.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelAiqServer.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelAlgoServer.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelAlgoServer.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelCPUAlgoServer.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelCPUAlgoServer.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelCmcServer.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelCmcServer.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelDvsServer.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelDvsServer.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelFDServer.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelFDServer.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelGPUAlgoServer.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelGPUAlgoServer.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelLardServer.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelLardServer.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelLtmServer.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelLtmServer.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelMknServer.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelMknServer.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelPGParamServer.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelPGParamServer.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelTNRServer.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IntelTNRServer.h
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IspParamAdaptorServer.cpp
 create mode 100644 camera/hal/intel/ipu6/modules/sandboxing/server/IspParamAdaptorServer.h
 create mode 100644 camera/hal/intel/ipu6/src/3a/AiqCore.cpp
 create mode 100644 camera/hal/intel/ipu6/src/3a/AiqCore.h
 create mode 100644 camera/hal/intel/ipu6/src/3a/AiqEngine.cpp
 create mode 100644 camera/hal/intel/ipu6/src/3a/AiqEngine.h
 create mode 100644 camera/hal/intel/ipu6/src/3a/AiqResult.cpp
 create mode 100644 camera/hal/intel/ipu6/src/3a/AiqResult.h
 create mode 100644 camera/hal/intel/ipu6/src/3a/AiqResultStorage.cpp
 create mode 100644 camera/hal/intel/ipu6/src/3a/AiqResultStorage.h
 create mode 100644 camera/hal/intel/ipu6/src/3a/AiqSetting.cpp
 create mode 100644 camera/hal/intel/ipu6/src/3a/AiqSetting.h
 create mode 100644 camera/hal/intel/ipu6/src/3a/AiqStatistics.cpp
 create mode 100644 camera/hal/intel/ipu6/src/3a/AiqStatistics.h
 create mode 100644 camera/hal/intel/ipu6/src/3a/AiqUnit.cpp
 create mode 100644 camera/hal/intel/ipu6/src/3a/AiqUnit.h
 create mode 100644 camera/hal/intel/ipu6/src/3a/AiqUtils.cpp
 create mode 100644 camera/hal/intel/ipu6/src/3a/AiqUtils.h
 create mode 100644 camera/hal/intel/ipu6/src/3a/Dvs.cpp
 create mode 100644 camera/hal/intel/ipu6/src/3a/Dvs.h
 create mode 100644 camera/hal/intel/ipu6/src/3a/DvsResult.cpp
 create mode 100644 camera/hal/intel/ipu6/src/3a/DvsResult.h
 create mode 100644 camera/hal/intel/ipu6/src/3a/I3AControlFactory.cpp
 create mode 100644 camera/hal/intel/ipu6/src/3a/I3AControlFactory.h
 create mode 100644 camera/hal/intel/ipu6/src/3a/ImagingControl.h
 create mode 100644 camera/hal/intel/ipu6/src/3a/LensManager.cpp
 create mode 100644 camera/hal/intel/ipu6/src/3a/LensManager.h
 create mode 100644 camera/hal/intel/ipu6/src/3a/Ltm.cpp
 create mode 100644 camera/hal/intel/ipu6/src/3a/Ltm.h
 create mode 100644 camera/hal/intel/ipu6/src/3a/MakerNote.cpp
 create mode 100644 camera/hal/intel/ipu6/src/3a/MakerNote.h
 create mode 100644 camera/hal/intel/ipu6/src/3a/SensorManager.cpp
 create mode 100644 camera/hal/intel/ipu6/src/3a/SensorManager.h
 create mode 100644 camera/hal/intel/ipu6/src/3a/intel3a/Intel3AParameter.cpp
 create mode 100644 camera/hal/intel/ipu6/src/3a/intel3a/Intel3AParameter.h
 create mode 100644 camera/hal/intel/ipu6/src/core/BufferQueue.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/BufferQueue.h
 create mode 100644 camera/hal/intel/ipu6/src/core/CameraBuffer.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/CameraBuffer.h
 create mode 100644 camera/hal/intel/ipu6/src/core/CameraDevice.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/CameraDevice.h
 create mode 100644 camera/hal/intel/ipu6/src/core/CameraEvent.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/CameraEvent.h
 create mode 100644 camera/hal/intel/ipu6/src/core/CameraEventType.h
 create mode 100644 camera/hal/intel/ipu6/src/core/CameraStream.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/CameraStream.h
 create mode 100644 camera/hal/intel/ipu6/src/core/CaptureUnit.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/CaptureUnit.h
 create mode 100644 camera/hal/intel/ipu6/src/core/DeviceBase.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/DeviceBase.h
 create mode 100644 camera/hal/intel/ipu6/src/core/IspParamAdaptor.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/IspParamAdaptor.h
 create mode 100644 camera/hal/intel/ipu6/src/core/IspSettings.h
 create mode 100644 camera/hal/intel/ipu6/src/core/LensHw.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/LensHw.h
 create mode 100644 camera/hal/intel/ipu6/src/core/PSysProcessor.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/PSysProcessor.h
 create mode 100644 camera/hal/intel/ipu6/src/core/ProcessorManager.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/ProcessorManager.h
 create mode 100644 camera/hal/intel/ipu6/src/core/RequestThread.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/RequestThread.h
 create mode 100644 camera/hal/intel/ipu6/src/core/SensorHwCtrl.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/SensorHwCtrl.h
 create mode 100644 camera/hal/intel/ipu6/src/core/SofSource.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/SofSource.h
 create mode 100644 camera/hal/intel/ipu6/src/core/StreamSource.h
 create mode 100644 camera/hal/intel/ipu6/src/core/SwImageProcessor.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/SwImageProcessor.h
 create mode 100644 camera/hal/intel/ipu6/src/core/SyncManager.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/SyncManager.h
 create mode 100644 camera/hal/intel/ipu6/src/core/TNRCommon.h
 create mode 100644 camera/hal/intel/ipu6/src/core/psysprocessor/GPUExecutor.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/psysprocessor/GPUExecutor.h
 create mode 100644 camera/hal/intel/ipu6/src/core/psysprocessor/PGCommon.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/psysprocessor/PGCommon.h
 create mode 100644 camera/hal/intel/ipu6/src/core/psysprocessor/PGUtils.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/psysprocessor/PGUtils.h
 create mode 100644 camera/hal/intel/ipu6/src/core/psysprocessor/PSysDAG.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/psysprocessor/PSysDAG.h
 create mode 100644 camera/hal/intel/ipu6/src/core/psysprocessor/PipeLiteExecutor.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/psysprocessor/PipeLiteExecutor.h
 create mode 100644 camera/hal/intel/ipu6/src/core/psysprocessor/PolicyManager.cpp
 create mode 100644 camera/hal/intel/ipu6/src/core/psysprocessor/PolicyManager.h
 create mode 100644 camera/hal/intel/ipu6/src/fd/FaceBase.h
 create mode 100644 camera/hal/intel/ipu6/src/fd/FaceDetection.cpp
 create mode 100644 camera/hal/intel/ipu6/src/fd/FaceDetection.h
 create mode 100644 camera/hal/intel/ipu6/src/hal/CameraHal.cpp
 create mode 100644 camera/hal/intel/ipu6/src/hal/CameraHal.h
 create mode 100644 camera/hal/intel/ipu6/src/hal/ICamera.cpp
 create mode 100644 camera/hal/intel/ipu6/src/image_process/IImageProcessor.h
 create mode 100644 camera/hal/intel/ipu6/src/image_process/ImageConverter.cpp
 create mode 100644 camera/hal/intel/ipu6/src/image_process/ImageConverter.h
 create mode 100644 camera/hal/intel/ipu6/src/image_process/ImageScalerCore.cpp
 create mode 100644 camera/hal/intel/ipu6/src/image_process/ImageScalerCore.h
 create mode 100644 camera/hal/intel/ipu6/src/image_process/PostProcessorBase.cpp
 create mode 100644 camera/hal/intel/ipu6/src/image_process/PostProcessorBase.h
 create mode 100644 camera/hal/intel/ipu6/src/image_process/PostProcessorCore.cpp
 create mode 100644 camera/hal/intel/ipu6/src/image_process/PostProcessorCore.h
 create mode 100644 camera/hal/intel/ipu6/src/image_process/ProcessType.h
 create mode 100644 camera/hal/intel/ipu6/src/image_process/chrome/ImageProcessorCore.cpp
 create mode 100644 camera/hal/intel/ipu6/src/image_process/chrome/ImageProcessorCore.h
 create mode 100644 camera/hal/intel/ipu6/src/iutils/CameraDump.cpp
 create mode 100644 camera/hal/intel/ipu6/src/iutils/CameraDump.h
 create mode 100644 camera/hal/intel/ipu6/src/iutils/CameraLog.cpp
 create mode 100644 camera/hal/intel/ipu6/src/iutils/CameraLog.h
 create mode 100644 camera/hal/intel/ipu6/src/iutils/Errors.h
 create mode 100644 camera/hal/intel/ipu6/src/iutils/RWLock.h
 create mode 100644 camera/hal/intel/ipu6/src/iutils/ScopedAtrace.cpp
 create mode 100644 camera/hal/intel/ipu6/src/iutils/SwImageConverter.cpp
 create mode 100644 camera/hal/intel/ipu6/src/iutils/SwImageConverter.h
 create mode 100644 camera/hal/intel/ipu6/src/iutils/Thread.cpp
 create mode 100644 camera/hal/intel/ipu6/src/iutils/Thread.h
 create mode 100644 camera/hal/intel/ipu6/src/iutils/Trace.cpp
 create mode 100644 camera/hal/intel/ipu6/src/iutils/Trace.h
 create mode 100644 camera/hal/intel/ipu6/src/iutils/Utils.cpp
 create mode 100644 camera/hal/intel/ipu6/src/iutils/Utils.h
 create mode 100644 camera/hal/intel/ipu6/src/jpeg/EXIFMaker.cpp
 create mode 100644 camera/hal/intel/ipu6/src/jpeg/EXIFMaker.h
 create mode 100644 camera/hal/intel/ipu6/src/jpeg/EXIFMetaData.cpp
 create mode 100644 camera/hal/intel/ipu6/src/jpeg/EXIFMetaData.h
 create mode 100644 camera/hal/intel/ipu6/src/jpeg/Exif.h
 create mode 100644 camera/hal/intel/ipu6/src/jpeg/ExifCreater.cpp
 create mode 100644 camera/hal/intel/ipu6/src/jpeg/ExifCreater.h
 create mode 100644 camera/hal/intel/ipu6/src/jpeg/IJpegEncoder.h
 create mode 100644 camera/hal/intel/ipu6/src/jpeg/JpegMaker.cpp
 create mode 100644 camera/hal/intel/ipu6/src/jpeg/JpegMaker.h
 create mode 100644 camera/hal/intel/ipu6/src/jpeg/chrome/JpegEncoderCore.cpp
 create mode 100644 camera/hal/intel/ipu6/src/jpeg/chrome/JpegEncoderCore.h
 create mode 100644 camera/hal/intel/ipu6/src/jpeg/sw/SWJpegEncoder.cpp
 create mode 100644 camera/hal/intel/ipu6/src/jpeg/sw/SWJpegEncoder.h
 create mode 100644 camera/hal/intel/ipu6/src/metadata/CameraMetadata.cpp
 create mode 100644 camera/hal/intel/ipu6/src/metadata/CameraMetadata.h
 create mode 100644 camera/hal/intel/ipu6/src/metadata/ParameterGenerator.cpp
 create mode 100644 camera/hal/intel/ipu6/src/metadata/ParameterGenerator.h
 create mode 100644 camera/hal/intel/ipu6/src/metadata/ParameterHelper.cpp
 create mode 100644 camera/hal/intel/ipu6/src/metadata/ParameterHelper.h
 create mode 100644 camera/hal/intel/ipu6/src/metadata/Parameters.cpp
 create mode 100644 camera/hal/intel/ipu6/src/metadata/icamera_metadata_base.cpp
 create mode 100644 camera/hal/intel/ipu6/src/metadata/icamera_metadata_base.h
 create mode 100644 camera/hal/intel/ipu6/src/metadata/icamera_metadata_tag_info.c
 create mode 100644 camera/hal/intel/ipu6/src/metadata/icamera_metadata_tags.h
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/AiqInitData.cpp
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/AiqInitData.h
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/CameraParser.cpp
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/CameraParser.h
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/CameraTypes.h
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/ParserBase.cpp
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/ParserBase.h
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/PlatformData.cpp
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/PlatformData.h
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/PolicyParser.cpp
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/PolicyParser.h
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/gc/FormatUtils.cpp
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/gc/FormatUtils.h
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/gc/GraphConfig.cpp
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/gc/GraphConfig.h
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/gc/GraphConfigManager.cpp
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/gc/GraphConfigManager.h
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/gc/GraphUtils.cpp
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/gc/GraphUtils.h
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/gc/HalStream.h
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/gc/IGraphConfig.h
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/gc/IGraphConfigManager.h
 create mode 100644 camera/hal/intel/ipu6/src/platformdata/gc/custom_gcss_keys.h
 create mode 100644 camera/hal/intel/ipu6/src/v4l2/MediaControl.cpp
 create mode 100644 camera/hal/intel/ipu6/src/v4l2/MediaControl.h
 create mode 100644 camera/hal/intel/ipu6/src/v4l2/NodeInfo.cpp
 create mode 100644 camera/hal/intel/ipu6/src/v4l2/NodeInfo.h
 create mode 100644 camera/hal/intel/ipu6/src/v4l2/SysCall.cpp
 create mode 100644 camera/hal/intel/ipu6/src/v4l2/SysCall.h
 create mode 100644 camera/hal/intel/ipu6/src/v4l2/V4l2DeviceFactory.cpp
 create mode 100644 camera/hal/intel/ipu6/src/v4l2/V4l2DeviceFactory.h

diff --git a/camera/hal/intel/ipu6/BUILD.gn b/camera/hal/intel/ipu6/BUILD.gn
new file mode 100644
index 000000000000..69ad531a0fda
--- /dev/null
+++ b/camera/hal/intel/ipu6/BUILD.gn
@@ -0,0 +1,482 @@
+#
+#  Copyright (C) 2019-2020 Intel Corporation
+#
+#  Licensed under the Apache License, Version 2.0 (the "License");
+#  you may not use this file except in compliance with the License.
+#  You may obtain a copy of the License at
+#
+#       http://www.apache.org/licenses/LICENSE-2.0
+#
+#  Unless required by applicable law or agreed to in writing, software
+#  distributed under the License is distributed on an "AS IS" BASIS,
+#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+#  See the License for the specific language governing permissions and
+#  limitations under the License.
+#
+
+group("all") {
+  deps = [
+    ":libcam_algo",
+    ":libcamhal",
+    ":libcam_gpu_algo",
+  ]
+}
+
+shared_library("libcamhal") {
+  sources = [
+    "aal/Camera3AMetadata.cpp",
+    "aal/Camera3HAL.cpp",
+    "aal/Camera3HALModule.cpp",
+    "aal/Camera3Stream.cpp",
+    "aal/HALv3Utils.cpp",
+    "aal/IntelAEStateMachine.cpp",
+    "aal/IntelAFStateMachine.cpp",
+    "aal/IntelAWBStateMachine.cpp",
+    "aal/MetadataConvert.cpp",
+    "aal/PostProcessor.cpp",
+    "aal/RequestManager.cpp",
+    "aal/ResultProcessor.cpp",
+    "modules/ia_cipr/src/Buffer.cpp",
+    "modules/ia_cipr/src/Command.cpp",
+    "modules/ia_cipr/src/Context.cpp",
+    "modules/ia_cipr/src/Event.cpp",
+    "modules/ia_cipr/src/Utils.cpp",
+    "modules/sandboxing/client/IntelAlgoClient.cpp",
+    "modules/sandboxing/client/IntelAlgoCommon.cpp",
+    "modules/sandboxing/client/IntelLard.cpp",
+    "modules/sandboxing/client/IntelFaceDetection.cpp",
+    "modules/sandboxing/client/GraphConfigImpl.cpp",
+    "modules/sandboxing/client/IntelCmc.cpp",
+    "modules/sandboxing/client/IntelMkn.cpp",
+    "modules/sandboxing/client/IntelLtm.cpp",
+    "modules/sandboxing/client/IntelAiq.cpp",
+    "modules/sandboxing/client/IntelDvs.cpp",
+    "modules/sandboxing/client/IntelIspParamAdaptor.cpp",
+    "modules/sandboxing/client/IntelPGParam.cpp",
+    "modules/sandboxing/IPCCommon.cpp",
+    "modules/sandboxing/IPCIntelLard.cpp",
+    "modules/sandboxing/IPCIntelFD.cpp",
+    "modules/sandboxing/IPCGraphConfig.cpp",
+    "modules/sandboxing/IPCIntelCmc.cpp",
+    "modules/sandboxing/IPCIntelMkn.cpp",
+    "modules/sandboxing/IPCIntelLtm.cpp",
+    "modules/sandboxing/IPCIntelAiq.cpp",
+    "modules/sandboxing/IPCIntelDvs.cpp",
+    "modules/sandboxing/IPCIspParamAdaptor.cpp",
+    "modules/sandboxing/IPCIntelPGParam.cpp",
+    "modules/memory/Camera3BufferPool.cpp",
+    "modules/memory/chrome/Camera3Buffer.cpp",
+    "src/3a/AiqCore.cpp",
+    "src/3a/AiqEngine.cpp",
+    "src/3a/AiqResult.cpp",
+    "src/3a/AiqResultStorage.cpp",
+    "src/3a/AiqSetting.cpp",
+    "src/3a/AiqStatistics.cpp",
+    "src/3a/AiqUnit.cpp",
+    "src/3a/AiqUtils.cpp",
+    "src/3a/I3AControlFactory.cpp",
+    "src/3a/DvsResult.cpp",
+    "src/3a/Dvs.cpp",
+    "src/3a/MakerNote.cpp",
+    "src/3a/LensManager.cpp",
+    "src/3a/Ltm.cpp",
+    "src/3a/SensorManager.cpp",
+    "src/3a/intel3a/Intel3AParameter.cpp",
+    "src/core/BufferQueue.cpp",
+    "src/core/CameraBuffer.cpp",
+    "src/core/CameraDevice.cpp",
+    "src/core/CameraEvent.cpp",
+    "src/core/CameraStream.cpp",
+    "src/core/CaptureUnit.cpp",
+    "src/core/DeviceBase.cpp",
+    "src/core/IspParamAdaptor.cpp",
+    "src/core/LensHw.cpp",
+    "src/core/PSysProcessor.cpp",
+    "src/core/ProcessorManager.cpp",
+    "src/core/RequestThread.cpp",
+    "src/core/SensorHwCtrl.cpp",
+    "src/core/SofSource.cpp",
+    "src/core/SwImageProcessor.cpp",
+    "src/core/SyncManager.cpp",
+    "src/core/psysprocessor/PGCommon.cpp",
+    "src/core/psysprocessor/PGUtils.cpp",
+    "src/core/psysprocessor/PSysDAG.cpp",
+    "src/core/psysprocessor/PipeLiteExecutor.cpp",
+    "src/core/psysprocessor/PolicyManager.cpp",
+    "src/fd/FaceDetection.cpp",
+    "src/hal/CameraHal.cpp",
+    "src/hal/ICamera.cpp",
+    "src/image_process/ImageConverter.cpp",
+    "src/image_process/ImageScalerCore.cpp",
+    "src/image_process/PostProcessorBase.cpp",
+    "src/image_process/PostProcessorCore.cpp",
+    "src/image_process/chrome/ImageProcessorCore.cpp",
+    "src/iutils/CameraDump.cpp",
+    "src/iutils/CameraLog.cpp",
+    "src/iutils/ScopedAtrace.cpp",
+    "src/iutils/SwImageConverter.cpp",
+    "src/iutils/Thread.cpp",
+    "src/iutils/Trace.cpp",
+    "src/iutils/Utils.cpp",
+    "src/jpeg/EXIFMaker.cpp",
+    "src/jpeg/EXIFMetaData.cpp",
+    "src/jpeg/ExifCreater.cpp",
+    "src/jpeg/JpegMaker.cpp",
+    "src/jpeg/sw/SWJpegEncoder.cpp",  # TODO: Migrate to hardware JPEG encoding later
+    "src/metadata/CameraMetadata.cpp",
+    "src/metadata/ParameterGenerator.cpp",
+    "src/metadata/ParameterHelper.cpp",
+    "src/metadata/Parameters.cpp",
+    "src/metadata/icamera_metadata_base.cpp",
+    "src/platformdata/AiqInitData.cpp",
+    "src/platformdata/CameraParser.cpp",
+    "src/platformdata/ParserBase.cpp",
+    "src/platformdata/PlatformData.cpp",
+    "src/platformdata/PolicyParser.cpp",
+    "src/platformdata/gc/FormatUtils.cpp",
+    "src/platformdata/gc/GraphConfig.cpp",
+    "src/platformdata/gc/GraphConfigManager.cpp",
+    "src/platformdata/gc/GraphUtils.cpp",
+    "src/v4l2/MediaControl.cpp",
+    "src/v4l2/SysCall.cpp",
+    "src/v4l2/V4l2DeviceFactory.cpp",
+    "src/v4l2/NodeInfo.cpp",
+  ]
+  cflags_cc = [
+    "-fvisibility=default",
+  ]
+  include_dirs = [
+    ".",
+    "include",
+    "include/api",
+    "include/utils",
+    "include/linux",
+    "src",
+    "src/fd",
+    "src/platformdata",
+    "src/platformdata/gc",
+    "src/v4l2",
+    "src/iutils",
+    "src/core",
+    "src/core/psysprocessor",
+    "src/metadata",
+    "src/3a/intel3a",
+    "src/3a/external",
+    "src/3a",
+    "src/image_process",
+    "src/image_process/chrome",
+    "src/jpeg/",
+    "src/jpeg/sw",
+    "aal",
+    "aal/chrome",
+    "modules/ia_cipr/include",
+    "modules/ia_cipr/src/common",
+    "modules/ia_cipr/src/linux/linux",
+    "modules/memory",
+    "modules/memory/chrome",
+    "include/ia_imaging",
+    "include",
+    "include/gcss",
+  ]
+  defines = [
+    "CAL_BUILD",
+    "IPU_SYSVER_IPU6",
+    "USE_PG_LITE_PIPE",
+    "HAVE_LINUX_OS",
+    "__STDC_FORMAT_MACROS",
+    "HAVE_PTHREADS",
+    "HAVE_IA_TYPES",
+    "HAVE_PRCTL",
+    "SW_JPEG_ENCODE",
+    "CAMERA_HAL_DEBUG",
+    "DUMP_IMAGE",
+    "HAL_PIXEL_FORMAT_NV12_LINEAR_CAMERA_INTEL=0x10F",
+    "ENABLE_SANDBOXING",
+    "FACE_DETECTION",
+  ]
+  libs = [
+    "camera_client",
+    "camera_metadata",
+    "camera_common",
+    "cbm",
+    "jpeg",
+    "dl",
+    "cros_config",
+  ]
+  pkg_deps = [
+    "cros-camera-android-headers",
+    "expat",
+    "libcab",
+    "libcamera_client",
+    "libcamera_common",
+    "libcamera_exif",
+    "libcamera_metadata",
+    "libcamera_v4l2_device",
+    "libcbm",
+    "libchrome-${libbase_ver}",
+    "libmojo-${libbase_ver}",
+    "libsync",
+    "libyuv",
+  ]
+  if (use.ipu6se) {
+    sources += [
+      "modules/ia_css/ipu6se/src/ia_css_program_group_param.c",
+      "modules/ia_css/ipu6se/src/ia_css_psys_process_group.c",
+      "modules/ia_css/ipu6se/src/ia_css_psys_program_group_manifest.c",
+      "modules/ia_css/ipu6se/src/ia_css_psys_terminal_manifest.c",
+      "modules/ia_css/ipu6se/src/ia_css_kernel_bitmap.c",
+      "modules/ia_css/ipu6se/src/ia_css_terminal_manifest.c",
+      "modules/ia_css/ipu6se/src/ia_css_psys_program_manifest.c",
+      "modules/ia_css/ipu6se/src/ia_css_psys_process.c",
+      "modules/ia_css/ipu6se/src/ia_css_psys_terminal.c",
+      "modules/ia_css/ipu6se/src/ia_css_psys_process_group_cmd_impl.c",
+      "modules/ia_css/ipu6se/src/vied_nci_psys_system.c",
+      "modules/ia_css/ipu6se/src/ia_css_terminal.c",
+      "modules/ia_css/ipu6se/src/ia_css_program_group_data.c",
+      "modules/ia_css/ipu6se/src/ia_css_rbm.c",
+      "src/core/psysprocessor/GPUExecutor.cpp",
+      "modules/sandboxing/client/IntelTNR7US.cpp",
+    ]
+    include_dirs += [
+      "modules/ia_css/ipu6se/include",
+      "include/igfxcmrt",
+    ]
+    libs += [
+      "tnr7_cm",
+      "broxton_ia_pal",
+    ]
+    defines += [
+      "IPU_SYSVER_ipu6v3",
+      "TNR7_CM",
+    ]
+  } else {
+    sources += [
+      "modules/ia_css/ipu6/src/ia_css_program_group_param.c",
+      "modules/ia_css/ipu6/src/ia_css_psys_process_group.c",
+      "modules/ia_css/ipu6/src/ia_css_psys_program_group_manifest.c",
+      "modules/ia_css/ipu6/src/ia_css_psys_terminal_manifest.c",
+      "modules/ia_css/ipu6/src/ia_css_kernel_bitmap.c",
+      "modules/ia_css/ipu6/src/ia_css_terminal_manifest.c",
+      "modules/ia_css/ipu6/src/ia_css_psys_program_manifest.c",
+      "modules/ia_css/ipu6/src/ia_css_psys_process.c",
+      "modules/ia_css/ipu6/src/ia_css_psys_terminal.c",
+      "modules/ia_css/ipu6/src/ia_css_psys_process_group_cmd_impl.c",
+      "modules/ia_css/ipu6/src/vied_nci_psys_system.c",
+      "modules/ia_css/ipu6/src/ia_css_terminal.c",
+      "modules/ia_css/ipu6/src/ia_css_program_group_data.c",
+      "modules/ia_css/ipu6/src/ia_css_rbm.c",
+    ]
+    include_dirs += [
+      "modules/ia_css/ipu6/include",
+    ]
+  }
+  deps = [ "//camera/common/libcamera_ipc:libcamera_ipc_mojom" ]
+}
+
+shared_library("libcam_algo") {
+  sources = [
+    "src/3a/DvsResult.cpp",
+    "src/iutils/Utils.cpp",
+    "src/iutils/Trace.cpp",
+    "src/iutils/ScopedAtrace.cpp",
+    "src/iutils/Thread.cpp",
+    "src/iutils/CameraLog.cpp",
+    "src/platformdata/gc/FormatUtils.cpp",
+    "src/platformdata/gc/GraphUtils.cpp",
+    "src/core/psysprocessor/PGUtils.cpp",
+    "modules/ia_cipr/src/Utils.cpp",
+    "modules/sandboxing/IPCCommon.cpp",
+    "modules/sandboxing/IPCIntelLard.cpp",
+    "modules/sandboxing/IPCIntelFD.cpp",
+    "modules/sandboxing/IPCGraphConfig.cpp",
+    "modules/sandboxing/IPCIntelCmc.cpp",
+    "modules/sandboxing/IPCIntelMkn.cpp",
+    "modules/sandboxing/IPCIntelLtm.cpp",
+    "modules/sandboxing/IPCIntelAiq.cpp",
+    "modules/sandboxing/IPCIntelDvs.cpp",
+    "modules/sandboxing/IPCIspParamAdaptor.cpp",
+    "modules/sandboxing/IPCIntelPGParam.cpp",
+    "modules/sandboxing/server/IntelCPUAlgoServer.cpp",
+    "modules/sandboxing/server/IntelAlgoServer.cpp",
+    "modules/sandboxing/server/IntelLardServer.cpp",
+    "modules/sandboxing/server/IntelFDServer.cpp",
+    "modules/sandboxing/server/GraphConfigServer.cpp",
+    "modules/sandboxing/server/IntelCmcServer.cpp",
+    "modules/sandboxing/server/IntelMknServer.cpp",
+    "modules/sandboxing/server/IntelLtmServer.cpp",
+    "modules/sandboxing/server/IntelAiqServer.cpp",
+    "modules/sandboxing/server/IntelDvsServer.cpp",
+    "modules/sandboxing/server/IspParamAdaptorServer.cpp",
+    "modules/sandboxing/server/IntelPGParamServer.cpp",
+    "modules/algowrapper/IntelLard.cpp",
+    "modules/algowrapper/IntelFaceDetection.cpp",
+    "modules/algowrapper/graph/GraphConfigImpl.cpp",
+    "modules/algowrapper/graph/GraphConfigPipe.cpp",
+    "modules/algowrapper/IntelCmc.cpp",
+    "modules/algowrapper/IntelMkn.cpp",
+    "modules/algowrapper/IntelLtm.cpp",
+    "modules/algowrapper/IntelAiq.cpp",
+    "modules/algowrapper/IntelDvs.cpp",
+    "modules/algowrapper/IntelIspParamAdaptor.cpp",
+    "modules/algowrapper/IntelPGParam.cpp",
+    "src/3a/AiqUtils.cpp",
+  ]
+  cflags_cc = [
+    "-fvisibility=default",
+  ]
+  include_dirs = [
+    ".",
+    "include",
+    "include/api",
+    "include/utils",
+    "include/linux",
+    "src",
+    "src/fd",
+    "src/platformdata",
+    "src/platformdata/gc",
+    "src/v4l2",
+    "src/iutils",
+    "src/core",
+    "src/core/psysprocessor",
+    "src/metadata",
+    "src/3a/intel3a",
+    "src/3a/external",
+    "src/3a",
+    "src/image_process",
+    "src/image_process/chrome",
+    "src/jpeg/",
+    "src/jpeg/sw",
+    "aal",
+    "aal/chrome",
+    "modules/memory",
+    "modules/memory/chrome",
+    "include/ia_imaging",
+    "include",
+    "include/gcss",
+ ]
+  defines = [
+    "CAL_BUILD",
+    "IPU_SYSVER_IPU6",
+    "USE_PG_LITE_PIPE",
+    "HAVE_LINUX_OS",
+    "__STDC_FORMAT_MACROS",
+    "HAVE_PTHREADS",
+    "HAVE_IA_TYPES",
+    "HAVE_PRCTL",
+    "SW_JPEG_ENCODE",
+    "CAMERA_HAL_DEBUG",
+    "DUMP_IMAGE",
+    "HAL_PIXEL_FORMAT_NV12_LINEAR_CAMERA_INTEL=0x10F",
+    "ENABLE_SANDBOXING",
+    "FACE_DETECTION",
+  ]
+  libs = [
+    "ia_aiq",
+    "ia_cmc_parser",
+    "ia_exc",
+    "ia_log",
+    "ia_mkn",
+    "ia_ltm",
+    "ia_isp_bxt",
+    "ia_dvs",
+    "ia_lard",
+    "pvl_eye_detection",
+    "pvl_face_detection",
+    "pvl_mouth_detection",
+    "gcss",
+    "ia_aiqb_parser",
+  ]
+  pkg_deps = [
+    "cros-camera-android-headers",
+    "libcab",
+    "libmojo-${libbase_ver}",
+  ]
+  if (use.ipu6se) {
+    libs += [
+      "ipu6sepla",
+      "ia_p2p_ipu6sepla",
+    ]
+    defines += [
+      "IPU_SYSVER_ipu6v3",
+    ]
+    include_dirs += [
+      "modules/ia_css/ipu6se/include",
+    ]
+  } else {
+    libs += [
+      "ipu6",
+      "ia_p2p_ipu6",
+    ]
+    include_dirs += [
+      "modules/ia_css/ipu6/include",
+    ]
+  }
+  deps = [ "//camera/common/libcamera_ipc:libcamera_ipc_mojom" ]
+}
+
+shared_library("libcam_gpu_algo") {
+  sources = [
+    "src/iutils/Utils.cpp",
+    "src/iutils/Trace.cpp",
+    "src/iutils/ScopedAtrace.cpp",
+    "src/iutils/Thread.cpp",
+    "src/iutils/CameraLog.cpp",
+    "modules/sandboxing/IPCCommon.cpp",
+    "modules/sandboxing/server/IntelAlgoServer.cpp",
+    "modules/sandboxing/server/IntelGPUAlgoServer.cpp",
+  ]
+  include_dirs = [
+    ".",
+    "include",
+    "include/api",
+    "include/utils",
+    "include/linux",
+    "src",
+    "src/core",
+    "src/platformdata",
+    "src/3a",
+    "src/platformdata/gc",
+    "src/iutils",
+    "src/metadata",
+    "src/v4l2",
+    "include",
+    "include/ia_imaging",
+    "include/gcss",
+  ]
+  defines = [
+    "CAL_BUILD",
+    "IPU_SYSVER_IPU6",
+    "HAVE_LINUX_OS",
+    "__STDC_FORMAT_MACROS",
+    "HAVE_PTHREADS",
+    "CAMERA_HAL_DEBUG",
+    "ENABLE_SANDBOXING",
+    "IPU_SYSVER_ipu6v3",
+    "GPU_ALGO_SERVER",
+  ]
+  libs = [
+    "ia_log",
+  ]
+  pkg_deps = [
+    "cros-camera-android-headers",
+    "libcab",
+    "libmojo-${libbase_ver}",
+  ]
+
+  if (use.ipu6se) {
+    sources += [
+      "modules/sandboxing/server/IntelTNRServer.cpp",
+      "modules/algowrapper/IntelTNR7US.cpp",
+    ]
+    defines += [
+      "TNR7_CM",
+    ]
+    libs += [
+      "tnr7_cm",
+    ]
+    include_dirs += [
+      "include/igfxcmrt",
+    ]
+  }
+  deps = [ "//camera/common/libcamera_ipc:libcamera_ipc_mojom" ]
+}
diff --git a/camera/hal/intel/ipu6/aal/Camera3AMetadata.cpp b/camera/hal/intel/ipu6/aal/Camera3AMetadata.cpp
new file mode 100644
index 000000000000..ba7dcc86ea5d
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/Camera3AMetadata.cpp
@@ -0,0 +1,132 @@
+/*
+ * Copyright (C) 2018-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Camera3AMetadata"
+
+#include "Camera3AMetadata.h"
+
+#include "HALv3Header.h"
+#include "HALv3Utils.h"
+#include "Utils.h"
+
+namespace camera3 {
+
+Camera3AMetadata::Camera3AMetadata(int cameraId) : mCameraId(cameraId) {
+    LOG1("@%s", __func__);
+
+    mIntelAFStateMachine = new IntelAFStateMachine(mCameraId);
+    mIntelAEStateMachine = new IntelAEStateMachine(mCameraId);
+    mIntelAWBStateMachine = new IntelAWBStateMachine(mCameraId);
+}
+
+Camera3AMetadata::~Camera3AMetadata() {
+    LOG1("@%s", __func__);
+
+    delete mIntelAFStateMachine;
+    delete mIntelAEStateMachine;
+    delete mIntelAWBStateMachine;
+}
+
+void Camera3AMetadata::process3Astate(const icamera::Parameters& parameter,
+                                      android::CameraMetadata* metadata) {
+    LOG2("@%s", __func__);
+    CheckError(!metadata, VOID_VALUE, "%s, metadata is nullptr", __func__);
+
+    // get 3a info from metadata
+    uint8_t afTrigger = ANDROID_CONTROL_AF_TRIGGER_IDLE;
+    camera_metadata_entry entry = metadata->find(ANDROID_CONTROL_AF_TRIGGER);
+    if (entry.count == 1) {
+        afTrigger = entry.data.u8[0];
+    }
+
+    uint8_t afMode = ANDROID_CONTROL_AF_MODE_AUTO;
+    entry = metadata->find(ANDROID_CONTROL_AF_MODE);
+    if (entry.count == 1) {
+        afMode = entry.data.u8[0];
+    }
+
+    mIntelAFStateMachine->processTriggers(afTrigger, afMode);
+
+    // get AF state
+    icamera::camera_af_state_t afState = icamera::AF_STATE_IDLE;
+    parameter.getAfState(afState);
+
+    bool lensMoving = false;
+    parameter.getLensState(lensMoving);
+    mIntelAFStateMachine->processResult(afState, lensMoving, metadata);
+
+    AeControls aeControls = {ANDROID_CONTROL_AE_MODE_ON, ANDROID_CONTROL_AE_LOCK_OFF,
+                             ANDROID_CONTROL_AE_PRECAPTURE_TRIGGER_IDLE,
+                             ANDROID_CONTROL_SCENE_MODE_DISABLED, 0};
+    uint8_t controlMode = ANDROID_CONTROL_MODE_AUTO;
+    entry = metadata->find(ANDROID_CONTROL_MODE);
+    if (entry.count == 1) {
+        controlMode = entry.data.u8[0];
+    }
+
+    uint8_t sceneMode = ANDROID_CONTROL_SCENE_MODE_DISABLED;
+    entry = metadata->find(ANDROID_CONTROL_SCENE_MODE);
+    if (entry.count == 1) {
+        sceneMode = entry.data.u8[0];
+    }
+
+    entry = metadata->find(ANDROID_CONTROL_AE_MODE);
+    if (entry.count == 1) {
+        aeControls.aeMode = entry.data.u8[0];
+    }
+
+    entry = metadata->find(ANDROID_CONTROL_AE_LOCK);
+    if (entry.count == 1) {
+        aeControls.aeLock = entry.data.u8[0];
+    }
+
+    entry = metadata->find(ANDROID_CONTROL_AE_PRECAPTURE_TRIGGER);
+    if (entry.count == 1) {
+        aeControls.aePreCaptureTrigger = entry.data.u8[0];
+    }
+
+    entry = metadata->find(ANDROID_CONTROL_AE_EXPOSURE_COMPENSATION);
+    if (entry.count == 1) {
+        aeControls.evCompensation = entry.data.i32[0];
+    }
+
+    mIntelAEStateMachine->processState(controlMode, sceneMode, aeControls);
+
+    // get AE state
+    icamera::camera_ae_state_t aeState = icamera::AE_STATE_NOT_CONVERGED;
+    parameter.getAeState(aeState);
+    mIntelAEStateMachine->processResult(aeState == icamera::AE_STATE_CONVERGED, metadata);
+
+    AwbControls awbControls = {ANDROID_CONTROL_AWB_MODE_AUTO, ANDROID_CONTROL_AWB_LOCK_OFF, 0, 0};
+    entry = metadata->find(ANDROID_CONTROL_AWB_MODE);
+    if (entry.count == 1) {
+        awbControls.awbMode = entry.data.u8[0];
+    }
+
+    entry = metadata->find(ANDROID_CONTROL_AWB_LOCK);
+    if (entry.count == 1) {
+        awbControls.awbLock = entry.data.u8[0];
+    }
+
+    mIntelAWBStateMachine->processState(controlMode, sceneMode, awbControls);
+
+    // get AWB state
+    icamera::camera_awb_state_t awbState = icamera::AWB_STATE_NOT_CONVERGED;
+    parameter.getAwbState(awbState);
+    mIntelAWBStateMachine->processResult(awbState == icamera::AWB_STATE_CONVERGED, metadata);
+}
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/Camera3AMetadata.h b/camera/hal/intel/ipu6/aal/Camera3AMetadata.h
new file mode 100644
index 000000000000..c85e5a768e16
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/Camera3AMetadata.h
@@ -0,0 +1,51 @@
+/*
+ * Copyright (C) 2018-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "IntelAEStateMachine.h"
+#include "IntelAFStateMachine.h"
+#include "IntelAWBStateMachine.h"
+#include "Utils.h"
+#include "Parameters.h"
+
+namespace camera3 {
+
+/**
+ * \class Camera3AMetadata
+ *
+ * This class is used to handle 3A related metadata. It also returns
+ * 3A state.
+ */
+class Camera3AMetadata {
+ public:
+    Camera3AMetadata(int cameraId);
+    ~Camera3AMetadata();
+
+    void process3Astate(const icamera::Parameters& parameter, android::CameraMetadata* metadata);
+
+ private:
+    DISALLOW_COPY_AND_ASSIGN(Camera3AMetadata);
+
+ private:
+    int mCameraId;
+
+    IntelAFStateMachine* mIntelAFStateMachine;
+    IntelAEStateMachine* mIntelAEStateMachine;
+    IntelAWBStateMachine* mIntelAWBStateMachine;
+};
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/Camera3HAL.cpp b/camera/hal/intel/ipu6/aal/Camera3HAL.cpp
new file mode 100644
index 000000000000..1ec9d2056bd2
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/Camera3HAL.cpp
@@ -0,0 +1,249 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Camera3HAL"
+
+#include "Camera3HAL.h"
+
+#include <memory>
+
+#include "Errors.h"
+#include "HALv3Utils.h"
+#include "ICamera.h"
+#include "Utils.h"
+
+namespace camera3 {
+
+/******************************************************************************
+ *  C DEVICE INTERFACE IMPLEMENTATION WRAPPER
+ *****************************************************************************/
+
+// Common check before the function call
+#define FUNCTION_PREPARED_RETURN \
+    if (!dev) return -EINVAL;    \
+    Camera3HAL* camera_priv = static_cast<Camera3HAL*>(dev->priv);
+
+static int hal_dev_initialize(const struct camera3_device* dev,
+                              const camera3_callback_ops_t* callback_ops) {
+    LOG1("@%s", __func__);
+
+    FUNCTION_PREPARED_RETURN
+
+    return camera_priv->initialize(callback_ops);
+}
+
+static int hal_dev_configure_streams(const struct camera3_device* dev,
+                                     camera3_stream_configuration_t* stream_list) {
+    LOG1("@%s", __func__);
+
+    FUNCTION_PREPARED_RETURN
+
+    return camera_priv->configure_streams(stream_list);
+}
+
+static const camera_metadata_t* hal_dev_construct_default_request_settings(
+    const struct camera3_device* dev, int type) {
+    LOG1("@%s", __func__);
+
+    if (!dev) return nullptr;
+    Camera3HAL* camera_priv = (Camera3HAL*)(dev->priv);
+
+    return camera_priv->construct_default_request_settings(type);
+}
+
+static int hal_dev_process_capture_request(const struct camera3_device* dev,
+                                           camera3_capture_request_t* request) {
+    LOG1("@%s", __func__);
+
+    FUNCTION_PREPARED_RETURN
+
+    return camera_priv->process_capture_request(request);
+}
+
+static void hal_dev_dump(const struct camera3_device* dev, int fd) {
+    LOG1("@%s", __func__);
+
+    if (!dev) return;
+
+    Camera3HAL* camera_priv = (Camera3HAL*)(dev->priv);
+
+    camera_priv->dump(fd);
+}
+
+static int hal_dev_flush(const struct camera3_device* dev) {
+    LOG1("@%s", __func__);
+
+    if (!dev) return -EINVAL;
+
+    Camera3HAL* camera_priv = (Camera3HAL*)(dev->priv);
+    return camera_priv->flush();
+}
+
+static camera3_device_ops hal_dev_ops = {
+    .initialize = hal_dev_initialize,
+    .configure_streams = hal_dev_configure_streams,
+    .register_stream_buffers = nullptr,
+    .construct_default_request_settings = hal_dev_construct_default_request_settings,
+    .process_capture_request = hal_dev_process_capture_request,
+    .get_metadata_vendor_tag_ops = nullptr,
+    .dump = hal_dev_dump,
+    .flush = hal_dev_flush,
+    .reserved = {0},
+};
+
+/******************************************************************************
+ *  C++ CLASS IMPLEMENTATION
+ *****************************************************************************/
+Camera3HAL::Camera3HAL(int cameraId, const hw_module_t* module)
+        : mCameraId(cameraId),
+          mInitialized(false) {
+    LOG1("@%s", __func__);
+
+    mDevice = {};
+    mDevice.common.tag = HARDWARE_DEVICE_TAG;
+    mDevice.common.version = CAMERA_DEVICE_API_VERSION_3_3;
+    mDevice.common.module = const_cast<hw_module_t*>(module);
+    // hal_dev_close is kept in the module for symmetry with dev_open
+    // it will be set there
+    mDevice.common.close = nullptr;
+    mDevice.ops = &hal_dev_ops;
+    mDevice.priv = this;
+
+    int ret = icamera::camera_device_open(cameraId);
+    if (ret != icamera::OK) {
+        LOGE("@%s, camera_device_open fails, ret:%d", __func__, ret);
+        icamera::camera_device_close(cameraId);
+
+        return;
+    }
+
+    mRequestManager = std::unique_ptr<RequestManager>(new RequestManager(cameraId));
+
+    mInitialized = true;
+}
+
+Camera3HAL::~Camera3HAL() {
+    LOG1("@%s", __func__);
+
+    if (mRequestManager) {
+        mRequestManager->flush();
+        mRequestManager->deinit();
+
+        mRequestManager.reset();  // mRequestManager must be released before device deinit
+    }
+
+    icamera::camera_device_close(mCameraId);
+}
+
+/* *********************************************************************
+ * Camera3 device  APIs
+ * ********************************************************************/
+int Camera3HAL::initialize(const camera3_callback_ops_t* callback_ops) {
+    LOG1("@%s", __func__);
+    CheckError(!mInitialized, -ENODEV, "@%s, mInitialized is false", __func__);
+    int status = icamera::OK;
+
+    if (callback_ops == nullptr) return -ENODEV;
+
+    status = mRequestManager->init(callback_ops);
+    if (status != icamera::OK) {
+        LOGE("Error register callback status = %d", status);
+        return -ENODEV;
+    }
+    return status;
+}
+
+int Camera3HAL::configure_streams(camera3_stream_configuration_t* stream_list) {
+    LOG1("@%s", __func__);
+    CheckError(!mInitialized, -EINVAL, "@%s, mInitialized is false", __func__);
+    CheckError(!stream_list, -EINVAL, "@%s, stream_list is nullptr", __func__);
+
+    if (!stream_list->streams || !stream_list->num_streams) {
+        LOGE("%s: Bad input! streams list ptr: %p, num %d", __func__, stream_list->streams,
+             stream_list->num_streams);
+        return -EINVAL;
+    }
+    int num = stream_list->num_streams;
+    LOG2("@%s, stream num:%d", __func__, num);
+    while (num--) {
+        if (!stream_list->streams[num]) {
+            LOGE("%s: Bad input! streams (%d) 's ptr: %p", __func__, num,
+                 stream_list->streams[num]);
+            return -EINVAL;
+        }
+    }
+
+    int status = mRequestManager->configureStreams(stream_list);
+    return (status == icamera::OK) ? 0 : -EINVAL;
+}
+
+const camera_metadata_t* Camera3HAL::construct_default_request_settings(int type) {
+    LOG1("@%s, type:%d", __func__, type);
+    CheckError(!mInitialized, nullptr, "@%s, mInitialized is false", __func__);
+
+    if (type < CAMERA3_TEMPLATE_PREVIEW || type >= CAMERA3_TEMPLATE_COUNT) return nullptr;
+
+    const camera_metadata_t* meta = nullptr;
+    int status = mRequestManager->constructDefaultRequestSettings(type, &meta);
+    CheckError(status != icamera::OK, nullptr, "construct default request setting error");
+
+    return meta;
+}
+
+int Camera3HAL::process_capture_request(camera3_capture_request_t* request) {
+    LOG2("@%s", __func__);
+    CheckError(!mInitialized, -EINVAL, "@%s, mInitialized is false", __func__);
+
+    if (request == nullptr) {
+        LOGE("%s: request is null!", __func__);
+        return -EINVAL;
+    } else if (!request->num_output_buffers || request->output_buffers == nullptr) {
+        LOGE("%s: num_output_buffers %d, output_buffers %p", __func__, request->num_output_buffers,
+             request->output_buffers);
+        return -EINVAL;
+    } else if (request->output_buffers->stream == nullptr) {
+        LOGE("%s: output_buffers->stream is null!", __func__);
+        return -EINVAL;
+    } else if (request->output_buffers->stream->priv == nullptr) {
+        LOGE("%s: output_buffers->stream->priv is null!", __func__);
+        return -EINVAL;
+    } else if (request->output_buffers->buffer == nullptr ||
+               *(request->output_buffers->buffer) == nullptr) {
+        LOGE("%s: output buffer is invalid", __func__);
+        return -EINVAL;
+    }
+
+    int status = mRequestManager->processCaptureRequest(request);
+    if (status == icamera::OK) return icamera::OK;
+
+    return (status == icamera::BAD_VALUE) ? -EINVAL : -ENODEV;
+}
+
+void Camera3HAL::dump(int fd) {
+    LOG1("@%s", __func__);
+    CheckError(!mInitialized, VOID_VALUE, "@%s, mInitialized is false", __func__);
+
+    mRequestManager->dump(fd);
+}
+
+int Camera3HAL::flush() {
+    LOG1("@%s", __func__);
+    CheckError(!mInitialized, icamera::UNKNOWN_ERROR, "@%s, mInitialized is false", __func__);
+
+    return mRequestManager->flush();
+}
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/Camera3HAL.h b/camera/hal/intel/ipu6/aal/Camera3HAL.h
new file mode 100644
index 000000000000..45c12f0af056
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/Camera3HAL.h
@@ -0,0 +1,68 @@
+/*
+ * Copyright (C) 2018-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <hardware/camera3.h>
+
+#include <memory>
+
+#include "RequestManager.h"
+
+namespace camera3 {
+
+/**
+ * \class Camera3HAL
+ *
+ * This class represents a single HAL device instance. It has the following
+ * roles:
+ * - It implements the camera3_device_ops_t  API  defined by Android.
+ * - It instantiates RequestManager.
+ */
+class Camera3HAL : public camera3_device_ops_t {
+ public:
+    Camera3HAL(int cameraId, const hw_module_t* module);
+    virtual ~Camera3HAL();
+    bool isInitialized() { return mInitialized; }
+    camera3_device_t* getDeviceStruct() { return &mDevice; }
+    int getCameraId() { return mCameraId; };
+
+    /**********************************************************************
+     * camera3_device_ops_t implementation
+     */
+    int initialize(const camera3_callback_ops_t* callback_ops);
+
+    int configure_streams(camera3_stream_configuration_t* stream_list);
+
+    const camera_metadata_t* construct_default_request_settings(int type);
+
+    int process_capture_request(camera3_capture_request_t* request);
+
+    void get_metadata_vendor_tag_ops(vendor_tag_query_ops_t* ops);
+
+    void dump(int fd);
+
+    int flush();
+
+ private:
+    int mCameraId;
+    std::unique_ptr<RequestManager> mRequestManager;
+    camera3_device_t mDevice;
+
+    bool mInitialized;
+};
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/Camera3HALModule.cpp b/camera/hal/intel/ipu6/aal/Camera3HALModule.cpp
new file mode 100644
index 000000000000..53612add6539
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/Camera3HALModule.cpp
@@ -0,0 +1,299 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#define LOG_TAG "Camera3HALModule"
+
+#include <cros-camera/cros_camera_hal.h>
+
+#include <hardware/camera3.h>
+#include <hardware/hardware.h>
+
+#include <mutex>
+
+#include "Camera3HAL.h"
+#include "HALv3Utils.h"
+#include "ICamera.h"
+#include "MetadataConvert.h"
+#include "PlatformData.h"
+#include "Utils.h"
+#include "iutils/CameraDump.h"
+
+namespace camera3 {
+
+#define MAX_CAMERAS 2
+
+/**
+ * \macro VISIBILITY_PUBLIC
+ *
+ * Controls the visibility of symbols in the shared library.
+ * In production builds all symbols in the shared library are hidden
+ * except the ones using this linker attribute.
+ */
+#define VISIBILITY_PUBLIC __attribute__((visibility("default")))
+
+static int hal_dev_close(hw_device_t* device);
+
+/**********************************************************************
+ * Camera Module API (C API)
+ **********************************************************************/
+
+static bool sInstances[MAX_CAMERAS] = {false, false};
+static int sInstanceCount = 0;
+// sCameraMetadata buffer won't be free in CAL
+static android::CameraMetadata* sCameraMetadata[MAX_CAMERAS] = {nullptr};
+
+/**
+ * Global mutex used to protect sInstanceCount and sInstances
+ */
+static std::mutex sCameraHalMutex;
+
+int openCameraHardware(int id, const hw_module_t* module, hw_device_t** device) {
+    LOG1("@%s", __func__);
+
+    if (sInstances[id]) return 0;
+
+    Camera3HAL* halDev = new Camera3HAL(id, module);
+    if (!halDev->isInitialized()) {
+        LOGE("HAL initialization fail!");
+        delete halDev;
+        return -EINVAL;
+    }
+    camera3_device_t* cam3Device = halDev->getDeviceStruct();
+
+    cam3Device->common.close = hal_dev_close;
+    *device = &cam3Device->common;
+
+    sInstanceCount++;
+    sInstances[id] = true;
+
+    LOG1("@%s end", __func__);
+    return 0;
+}
+
+static int hal_get_number_of_cameras(void) {
+    LOG1("@%s", __func__);
+
+    return icamera::get_number_of_cameras();
+}
+
+static int hal_get_camera_info(int cameraId, struct camera_info* cameraInfo) {
+    LOG1("@%s", __func__);
+
+    if (cameraId < 0 || !cameraInfo || cameraId >= hal_get_number_of_cameras()) return -EINVAL;
+
+    icamera::camera_info_t info;
+    icamera::get_camera_info(cameraId, info);
+
+    if (sCameraMetadata[cameraId] == nullptr) {
+        sCameraMetadata[cameraId] = new android::CameraMetadata;
+        MetadataConvert::HALCapabilityToStaticMetadata(*(info.capability),
+                                                       sCameraMetadata[cameraId]);
+    }
+    int32_t tag = ANDROID_LENS_FACING;
+    camera_metadata_entry entry = sCameraMetadata[cameraId]->find(tag);
+    if (entry.count == 1) {
+        info.facing = entry.data.u8[0];
+    }
+    tag = ANDROID_SENSOR_ORIENTATION;
+    entry = sCameraMetadata[cameraId]->find(tag);
+    if (entry.count == 1) {
+        info.orientation = entry.data.u8[0];
+    }
+    memset(cameraInfo, 0, sizeof(camera_info));
+    cameraInfo->facing = info.facing ? CAMERA_FACING_BACK : CAMERA_FACING_FRONT;
+    cameraInfo->device_version = CAMERA_DEVICE_API_VERSION_3_3;
+    cameraInfo->orientation = info.orientation;
+    const camera_metadata_t* settings = sCameraMetadata[cameraId]->getAndLock();
+    cameraInfo->static_camera_characteristics = settings;
+    sCameraMetadata[cameraId]->unlock(settings);
+
+    return 0;
+}
+
+static int hal_set_callbacks(const camera_module_callbacks_t* callbacks) {
+    LOG1("@%s", __func__);
+
+    UNUSED(callbacks);
+    return 0;
+}
+
+static int hal_dev_open(const hw_module_t* module, const char* name, hw_device_t** device) {
+    icamera::Log::setDebugLevel();
+    icamera::CameraDump::setDumpLevel();
+
+    LOG1("@%s", __func__);
+
+    int status = -EINVAL;
+    int camera_id;
+
+    if (!name || !module || !device) {
+        LOGE("Camera name is nullptr");
+        return status;
+    }
+
+    LOG1("%s, camera id: %s", __func__, name);
+    camera_id = atoi(name);
+    if (camera_id < 0 || camera_id >= hal_get_number_of_cameras()) {
+        LOGE("%s: Camera id %d is out of bounds, num. of cameras (%d)", __func__, camera_id,
+             hal_get_number_of_cameras());
+        return -ENODEV;
+    }
+
+    std::lock_guard<std::mutex> l(sCameraHalMutex);
+
+    if (sInstanceCount > 0 && sInstances[camera_id]) {
+        LOGW("Camera already has been opened!");
+        return -EUSERS;
+    }
+
+    return openCameraHardware(camera_id, module, device);
+}
+
+static int hal_dev_close(hw_device_t* device) {
+    LOG1("@%s", __func__);
+
+    if (!device || sInstanceCount == 0) {
+        LOGW("hal close, instance count %d", sInstanceCount);
+        return -EINVAL;
+    }
+
+    camera3_device_t* camera3_dev = (struct camera3_device*)device;
+    Camera3HAL* camera_priv = static_cast<Camera3HAL*>(camera3_dev->priv);
+
+    if (camera_priv != nullptr) {
+        std::lock_guard<std::mutex> l(sCameraHalMutex);
+        int id = camera_priv->getCameraId();
+        delete camera_priv;
+        sInstanceCount--;
+        sInstances[id] = false;
+    }
+
+    LOG1("%s, instance count %d", __func__, sInstanceCount);
+
+    return 0;
+}
+
+static int hal_init(void) {
+    LOG1("@%s", __func__);
+
+    /*
+     * Check the connection status with algo service and the detected
+     * camera number. Then the service decides whether to restart or
+     * not based on the return value
+     */
+#ifdef ENABLE_SANDBOXING
+    CheckError(icamera::IntelAlgoClient::getInstance()->initialize() != icamera::OK, -EINVAL,
+               "%s, Connect to algo service fails", __func__);
+#endif
+
+    // Initialize PlatformData
+    int ret = icamera::camera_hal_init();
+    CheckError(ret != icamera::OK, -EINVAL, "@%s, camera_hal_init fails, ret:%d", __func__, ret);
+
+    int currentCameraNum = icamera::PlatformData::numberOfCameras();
+    CheckError(currentCameraNum == 0, -EINVAL, "@%s, camera is not ready", __func__);
+
+    int hwCameraNum = camera3::HalV3Utils::getHwCameraNumber();
+    int xmlCameraNum = icamera::PlatformData::getXmlCameraNumber();
+    if (xmlCameraNum != 0) {
+        CheckError(xmlCameraNum != currentCameraNum, -EINVAL,
+                   "@%s, expected %d cameras defined in XML, found %d", __func__, xmlCameraNum,
+                   currentCameraNum);
+    } else if (hwCameraNum != 0) {
+        CheckError(hwCameraNum != currentCameraNum, -EINVAL,
+                   "@%s, expected %d cameras defined in device config, found %d", __func__,
+                   hwCameraNum, currentCameraNum);
+    }
+
+    return 0;
+}
+
+static int hal_set_torch_mode(const char* camera_id, bool enabled) {
+    LOG1("@%s", __func__);
+
+    UNUSED(camera_id);
+    UNUSED(enabled);
+    return -ENOSYS;
+}
+
+/*
+ * The setup sequence for camera hal module
+ *  1. dlopen()
+ *  2. set_up() : for chrome camera service only
+ *  3. init()
+ *  4. get_number_of_cameras()
+ *  ......
+ */
+static void hal_set_up(cros::CameraMojoChannelManager* mojoManager) {
+    LOG1("@%s", __func__);
+
+    icamera::Log::setDebugLevel();
+    icamera::CameraDump::setDumpLevel();
+
+#ifdef ENABLE_SANDBOXING
+    // Create IntelAlgoClient and set the mojo manager
+    icamera::IntelAlgoClient::getInstance()->setMojoManager(mojoManager);
+#endif
+}
+
+/*
+ * The close sequence for camera hal module
+ *  ......
+ *  1. tear_down() : for chrome camera service only
+ *  2. dlclose()
+ */
+static void hal_tear_down() {
+    LOG1("@%s", __func__);
+
+    int ret = icamera::camera_hal_deinit();
+    CheckError(ret != icamera::OK, VOID_VALUE, "@%s, camera_hal_deinit fails, ret:%d", __func__,
+               ret);
+#ifdef ENABLE_SANDBOXING
+    icamera::IntelAlgoClient::releaseInstance();
+#endif
+}
+
+static struct hw_module_methods_t hal_module_methods = {.open = hal_dev_open};
+
+static hw_module_t camera_common = {.tag = HARDWARE_MODULE_TAG,
+                                    .module_api_version = CAMERA_MODULE_API_VERSION_2_3,
+                                    .hal_api_version = HARDWARE_HAL_API_VERSION,
+                                    .id = CAMERA_HARDWARE_MODULE_ID,
+                                    .name = "Intel Camera3HAL Module",
+                                    .author = "Intel",
+                                    .methods = &hal_module_methods,
+                                    .dso = nullptr,
+                                    .reserved = {0}};
+
+extern "C" {
+camera_module_t VISIBILITY_PUBLIC HAL_MODULE_INFO_SYM = {
+    .common = camera_common,
+    .get_number_of_cameras = hal_get_number_of_cameras,
+    .get_camera_info = hal_get_camera_info,
+    .set_callbacks = hal_set_callbacks,
+    .get_vendor_tag_ops = nullptr,
+    .open_legacy = nullptr,
+    .set_torch_mode = hal_set_torch_mode,
+    .init = hal_init,
+    .reserved = {0}};
+
+// For Chrome OS
+cros::cros_camera_hal_t VISIBILITY_PUBLIC CROS_CAMERA_HAL_INFO_SYM = {
+    .set_up = hal_set_up,
+    .tear_down = hal_tear_down,
+};
+}
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/Camera3Stream.cpp b/camera/hal/intel/ipu6/aal/Camera3Stream.cpp
new file mode 100644
index 000000000000..a43ec5c2c635
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/Camera3Stream.cpp
@@ -0,0 +1,653 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Camera3Stream"
+
+#include "Camera3Stream.h"
+
+#include <utility>
+#include <vector>
+
+#include "CameraDump.h"
+#include "Errors.h"
+#include "HALv3Utils.h"
+#include "ICamera.h"
+#include "MetadataConvert.h"
+#include "PlatformData.h"
+#include "Utils.h"
+#include "stdlib.h"
+
+namespace camera3 {
+Camera3Stream::Camera3Stream(int cameraId, CallbackEventInterface* callback,
+                             uint32_t maxNumReqInProc, const icamera::stream_t& halStream,
+                             const camera3_stream_t& stream, const camera3_stream_t* inputStream,
+                             bool isHALStream)
+        : mCameraId(cameraId),
+          mEventCallback(callback),
+          mPostProcessType(icamera::POST_PROCESS_NONE),
+          mStreamState(false),
+          mHALStream(halStream),
+          mMaxNumReqInProc(maxNumReqInProc),
+          mBufferPool(nullptr),
+          mStream(stream),
+          mFaceDetection(nullptr),
+          mFDRunDefaultInterval(icamera::PlatformData::faceEngineRunningInterval(cameraId)),
+          mFDRunIntervalNoFace(icamera::PlatformData::faceEngineRunningIntervalNoFace(cameraId)),
+          mFDRunInterval(icamera::PlatformData::faceEngineRunningInterval(cameraId)),
+          mFrameCnt(0),
+          mInputPostProcessType(icamera::POST_PROCESS_NONE),
+          mIsHALStream(isHALStream) {
+    LOG1("[%p]@%s, buf num:%d, inputStream:%p, stream:%dx%d, format:%d, type:%d", this,
+         __func__, mMaxNumReqInProc, inputStream, mHALStream.width, mHALStream.height,
+         mHALStream.format, mStream.stream_type);
+
+    mPostProcessor = std::unique_ptr<PostProcessor>(new PostProcessor(mCameraId, stream));
+    mCaptureRequest.clear();
+    mQueuedBuffer.clear();
+    if (mIsHALStream) {
+        mBufferPool = std::unique_ptr<Camera3BufferPool>(new Camera3BufferPool());
+    }
+
+    if (inputStream) {
+        LOG2("@%s, inputStream: width:%d, height:%d, format:%d", __func__, inputStream->width,
+             inputStream->height, inputStream->format);
+
+        mInputPostProcessor = std::unique_ptr<PostProcessor>(new PostProcessor(mCameraId, stream));
+        mInputStream = std::unique_ptr<camera3_stream_t>(new camera3_stream_t);
+        *mInputStream = *inputStream;
+    }
+
+    LOG2("@%s, mFaceDetection:%p, Interval:%d, IntervalNoFace:%d", __func__,
+         mFaceDetection, mFDRunDefaultInterval, mFDRunIntervalNoFace);
+}
+
+Camera3Stream::~Camera3Stream() {
+    LOG1("[%p]@%s", this, __func__);
+
+    setActive(false);
+
+    for (auto& buf : mBuffers) {
+        buf.second->unlock();
+    }
+
+    mBuffers.clear();
+    std::lock_guard<std::mutex> l(mLock);
+    mCaptureResultMap.clear();
+}
+
+void Camera3Stream::sendEvent(const icamera::camera_msg_data_t& data) {
+    LOG2("@%s receive sof event: %ld", __func__, data.data.buffer_ready.timestamp);
+
+    std::lock_guard<std::mutex> sofLock(mSofLock);
+    mSofCondition.notify_one();
+}
+
+void Camera3Stream::handleSofAlignment() {
+    if (!icamera::PlatformData::swProcessingAlignWithIsp(mCameraId)) return;
+
+    std::unique_lock<std::mutex> sofLock(mSofLock);
+    std::cv_status ret =
+        mSofCondition.wait_for(sofLock, std::chrono::nanoseconds(kMaxDuration * SLOWLY_MULTIPLIER));
+
+    if (ret == std::cv_status::timeout) {
+        LOGW("%s, [%p] wait sof timeout, skip alignment this time", __func__, this);
+    }
+    LOG2("%s, [%p] running post processing align with sof event", __func__, this);
+}
+
+bool Camera3Stream::threadLoop() {
+    LOG1("[%p] isHALStream: %d @%s", this, mIsHALStream, __func__);
+
+    if (!waitCaptureResultReady()) {
+        return true;
+    }
+
+    auto captureResult = mCaptureResultMap.begin();
+    std::shared_ptr<CaptureResult> result = captureResult->second;
+    uint32_t frameNumber = captureResult->first;
+
+    // dequeue buffer from HAL
+    icamera::camera_buffer_t* buffer = nullptr;
+    icamera::Parameters parameter;
+    std::shared_ptr<Camera3Buffer> inputCam3Buf = result->inputCam3Buf;
+    std::shared_ptr<StreamComInfo> halOutput = nullptr;
+    long sequence = -1;
+
+    if (!inputCam3Buf && mIsHALStream) {
+        LOG1("[%p]@ dqbuf for frameNumber %d", this, frameNumber);
+        int ret = icamera::camera_stream_dqbuf(mCameraId, mHALStream.id, &buffer, &parameter);
+        CheckError(ret != icamera::OK || !buffer, true, "[%p]failed to dequeue buffer, ret %d",
+                   this, ret);
+        LOG2("[%p]@ %s, buffer->timestamp:%lld addr %p", this, __func__, buffer->timestamp,
+             buffer->addr);
+
+        sequence = buffer->sequence;
+        int32_t userRequestId = 0;
+        if (parameter.getUserRequestId(userRequestId) == icamera::OK) {
+            {
+                std::unique_lock<std::mutex> lock(mLock);
+                if (mCaptureResultMap.find(static_cast<uint32_t>(userRequestId)) !=
+                    mCaptureResultMap.end()) {
+                    frameNumber = static_cast<uint32_t>(userRequestId);
+                    result = mCaptureResultMap[frameNumber];
+                }
+            }
+        }
+
+        // sync before notify listeners
+        bool needAlignment = false;
+
+        if (mHALStream.usage != icamera::CAMERA_STREAM_OPAQUE_RAW &&
+            mPostProcessType != icamera::POST_PROCESS_NONE) {
+            needAlignment = true;
+        } else {
+            for (auto& iter : mListeners) {
+                if (!iter->getCaptureRequest(frameNumber)) {
+                    continue;
+                }
+                needAlignment = true;
+            }
+        }
+        if (needAlignment) {
+            handleSofAlignment();
+        }
+
+        halOutput = std::make_shared<StreamComInfo>();
+        {
+            std::unique_lock<std::mutex> lock(mLock);
+
+            /* get the Camera3Buffer object of camera_buffer_t dqueued from HAL
+             * check the 3 sources: HAL stream, listener stream and buffer pool.
+             */
+            if (mQueuedBuffer.find(frameNumber) != mQueuedBuffer.end()) {
+                /* check buffer pool first, the HAL stream may use buffer from pool even
+                 * itself or it's listener has requested buffer.
+                 */
+                halOutput->cam3Buf = mQueuedBuffer[frameNumber];
+            } else if (mCaptureRequest.find(frameNumber) != mCaptureRequest.end()) {
+                if (buffer->addr == mCaptureRequest[frameNumber]->cam3Buf->data()) {
+                    halOutput->cam3Buf = mCaptureRequest[frameNumber]->cam3Buf;
+                }
+            } else {
+                std::shared_ptr<StreamComInfo> request = nullptr;
+                for (auto& iter : mListeners) {
+                    request = iter->getCaptureRequest(frameNumber);
+                    if (request && request->cam3Buf->data() == buffer->addr) {
+                        halOutput->cam3Buf = request->cam3Buf;
+                        break;
+                    }
+                }
+            }
+
+            CheckError(!halOutput->cam3Buf, true, "can't identify the buffer source");
+            halOutput->parameter = parameter;
+            halOutput->cam3Buf->setTimeStamp(buffer->timestamp);
+        }
+
+        for (auto& iter : mListeners) {
+            iter->notifyListenerBufferReady(frameNumber, halOutput);
+        }
+
+        if (!getCaptureRequest(frameNumber)) {
+            // HAL stream is triggered by listener, itself not requested, start next
+            // loop
+            std::unique_lock<std::mutex> lock(mLock);
+            mCaptureResultMap.erase(frameNumber);
+            return true;
+        }
+    } else if (!inputCam3Buf) {
+        // listener stream get the buffer from HAL stream
+        std::unique_lock<std::mutex> lock(mLock);
+        if (mHALStreamOutput.find(frameNumber) == mHALStreamOutput.end()) {
+            LOGE("[%p] can't find HAL stream output", this);
+            return true;
+        }
+        halOutput = mHALStreamOutput[frameNumber];
+        mHALStreamOutput.erase(frameNumber);
+    }
+
+    {
+        std::unique_lock<std::mutex> lock(mLock);
+        mCaptureResultMap.erase(frameNumber);
+    }
+
+    icamera::camera_buffer_t outCamBuf = {};
+    shared_ptr<Camera3Buffer> outCam3Buf = nullptr;
+
+    // start process buffers, HAL stream and listeners will do the same process
+    if (!inputCam3Buf) {
+        outCam3Buf = halOutput->cam3Buf;
+        if (outCam3Buf) {
+            outCamBuf = outCam3Buf->getHalBuffer();
+        }
+        parameter = halOutput->parameter;
+    }
+
+    buffer_handle_t handle = result->handle;
+    std::shared_ptr<Camera3Buffer> ccBuf = nullptr;
+    {
+        std::unique_lock<std::mutex> lock(mLock);
+        CheckError(mBuffers.find(handle) == mBuffers.end(), false, "can't find handle %p", handle);
+
+        ccBuf = mBuffers[handle];
+        mBuffers.erase(handle);
+        CheckError(ccBuf == nullptr, false, "ccBuf is nullptr");
+    }
+
+    if (inputCam3Buf || mHALStream.usage == icamera::CAMERA_STREAM_OPAQUE_RAW) {
+        // notify shutter done
+        ShutterEvent shutterEvent = {frameNumber, inputCam3Buf ? 0 : outCamBuf.timestamp};
+        mEventCallback->shutterDone(shutterEvent);
+
+        // notify metadata done
+        MetadataEvent event = {frameNumber, &parameter};
+        mEventCallback->metadataDone(event);
+    }
+
+    int dumpOutputFmt = V4L2_PIX_FMT_NV12;
+    if (!inputCam3Buf && mHALStream.usage != icamera::CAMERA_STREAM_OPAQUE_RAW) {
+        LOG2("%s, hal buffer: %p, ccBuf address: %p", __func__, outCamBuf.addr, ccBuf->data());
+        if (mPostProcessType & icamera::POST_PROCESS_JPEG_ENCODING) {
+            dumpOutputFmt = V4L2_PIX_FMT_JPEG;
+            icamera::PlatformData::acquireMakernoteData(mCameraId, outCamBuf.timestamp, &parameter);
+        }
+        // handle normal postprocess
+        if (mPostProcessType != icamera::POST_PROCESS_NONE) {
+            LOG2("%s, do software postProcessing for sequence: %ld", __func__, outCamBuf.sequence);
+            icamera::status_t status =
+                mPostProcessor->doPostProcessing(outCam3Buf, parameter, ccBuf);
+            CheckError(status != icamera::OK, true,
+                       "@%s, doPostProcessing fails, mPostProcessType:%d", __func__,
+                       mPostProcessType);
+        } else if (outCam3Buf && outCam3Buf->data() != ccBuf->data()) {
+            MEMCPY_S(ccBuf->data(), ccBuf->size(), outCam3Buf->data(), outCam3Buf->size());
+        }
+    } else if (inputCam3Buf) {
+        parameter = result->param;
+        LOG1("[%p] @%s process input frameNumber: %d", this, __func__, frameNumber);
+        if (mInputPostProcessType & icamera::POST_PROCESS_JPEG_ENCODING) {
+            dumpOutputFmt = V4L2_PIX_FMT_JPEG;
+            icamera::PlatformData::acquireMakernoteData(mCameraId, inputCam3Buf->getTimeStamp(),
+                                                        &parameter);
+        }
+
+        inputCam3Buf->dumpImage(frameNumber, icamera::DUMP_AAL_INPUT, V4L2_PIX_FMT_NV12);
+        if (mInputPostProcessType != icamera::POST_PROCESS_NONE) {
+            icamera::status_t status =
+                mInputPostProcessor->doPostProcessing(inputCam3Buf, parameter, ccBuf);
+            CheckError(status != icamera::OK, true,
+                       "@%s, doPostProcessing fails, mInputPostProcessType:%d", __func__,
+                       mInputPostProcessType);
+        } else {
+            MEMCPY_S(ccBuf->data(), ccBuf->size(), inputCam3Buf->data(), inputCam3Buf->size());
+        }
+    }
+
+    faceRunningByCondition(ccBuf->getHalBuffer());
+
+    ccBuf->dumpImage(frameNumber, icamera::DUMP_AAL_OUTPUT, dumpOutputFmt);
+    ccBuf->unlock();
+    ccBuf->deinit();
+    ccBuf->getFence(&result->outputBuffer);
+
+    // notify frame done
+    BufferEvent bufferEvent = {frameNumber, &result->outputBuffer, 0, -1};
+    if (mHALStream.usage == icamera::CAMERA_STREAM_OPAQUE_RAW) {
+        bufferEvent.sequence = sequence;
+        bufferEvent.timestamp = outCamBuf.timestamp;
+    }
+    mEventCallback->bufferDone(bufferEvent);
+
+    if (!inputCam3Buf) {
+        std::unique_lock<std::mutex> lock(mLock);
+        mCaptureRequest.erase(frameNumber);
+    }
+
+    return true;
+}
+
+void Camera3Stream::faceRunningByCondition(const icamera::camera_buffer_t& buffer) {
+    if (!mFaceDetection) return;
+
+    LOG2("[%p]@%s", this, __func__);
+
+    /*
+       FD runs 1 frame every mFDRunInterval frames.
+       And the default value of mFDRunInterval is mFDRunDefaultInterval
+    */
+    if (mFrameCnt % mFDRunInterval == 0) {
+        mFaceDetection->runFaceDetection(buffer);
+    }
+
+    /*
+       When face doesn't be detected during mFDRunIntervalNoFace's frame,
+       we may change FD running's interval frames.
+    */
+    if (mFDRunIntervalNoFace > mFDRunDefaultInterval) {
+        static unsigned int noFaceCnt = 0;
+        int faceNum = mFaceDetection->getFaceNum();
+
+        /*
+           The purpose of changing the value of the variable is to run FD
+           immediately when face is detected.
+        */
+        if (faceNum == 0) {
+            if (mFDRunInterval != mFDRunIntervalNoFace) {
+                noFaceCnt = ++noFaceCnt % mFDRunIntervalNoFace;
+                if (noFaceCnt == 0) {
+                    mFDRunInterval = mFDRunIntervalNoFace;
+                }
+            }
+        } else {
+            if (mFDRunInterval != mFDRunDefaultInterval) {
+                mFDRunInterval = mFDRunDefaultInterval;
+                mFrameCnt = mFDRunInterval - 1;
+                noFaceCnt = 0;
+            }
+        }
+        LOG2("%s, Currently running one time face detection every %d frames", __func__,
+             mFDRunInterval);
+    }
+
+    mFrameCnt = ++mFrameCnt % mFDRunInterval;
+}
+
+void Camera3Stream::requestExit() {
+    LOG1("[%p]@%s", this, __func__);
+
+    icamera::Thread::requestExit();
+    std::lock_guard<std::mutex> l(mLock);
+
+    mBufferDoneCondition.notify_one();
+
+    if (mFaceDetection) {
+        icamera::FaceDetection::destoryInstance(mCameraId);
+        mFaceDetection = nullptr;
+    }
+}
+
+int Camera3Stream::processRequest(const std::shared_ptr<Camera3Buffer>& inputCam3Buf,
+                                  const camera3_stream_buffer_t& outputBuffer,
+                                  uint32_t frame_number) {
+    LOG1("[%p] isHALStream: %d @%s", this, mIsHALStream, __func__);
+
+    std::shared_ptr<Camera3Buffer> ccBuf = std::make_shared<Camera3Buffer>();
+    buffer_handle_t handle = *outputBuffer.buffer;
+
+    {
+        std::unique_lock<std::mutex> lock(mLock);
+        CheckError(mBuffers.find(handle) != mBuffers.end(), icamera::BAD_VALUE,
+                   "handle %p is duplicated!", handle);
+
+        mBuffers[handle] = ccBuf;
+    }
+
+    icamera::status_t status = ccBuf->init(&outputBuffer, mCameraId);
+    CheckError(status != icamera::OK, icamera::BAD_VALUE, "Failed to init CameraBuffer");
+    status = ccBuf->waitOnAcquireFence();
+    CheckError(status != icamera::OK, icamera::BAD_VALUE, "Failed to sync CameraBuffer");
+    status = ccBuf->lock();
+    CheckError(status != icamera::OK, icamera::BAD_VALUE, "Failed to lock buffer");
+
+    std::shared_ptr<StreamComInfo> streamInfo = std::make_shared<StreamComInfo>();
+    streamInfo->cam3Buf = nullptr;
+    {
+        std::lock_guard<std::mutex> l(mLock);
+
+        if (inputCam3Buf) {
+            LOG1("[%p] frameNumber %d input buffer requested ", this, frame_number);
+            return icamera::OK;
+        }
+
+        if (mPostProcessType == icamera::POST_PROCESS_NONE) {
+            streamInfo->cam3Buf = ccBuf;
+        }
+        mCaptureRequest[frame_number] = streamInfo;
+    }
+
+    return icamera::OK;
+}
+
+void Camera3Stream::queueBufferDone(uint32_t frameNumber,
+                                    const std::shared_ptr<Camera3Buffer>& inputCam3Buf,
+                                    const camera3_stream_buffer_t& outputBuffer,
+                                    const icamera::Parameters& param) {
+    LOG1("[%p]@%s, frameNumber:%d", this, __func__, frameNumber);
+    std::lock_guard<std::mutex> l(mLock);
+
+    std::shared_ptr<CaptureResult> result = std::make_shared<CaptureResult>();
+
+    result->frameNumber = frameNumber;
+    result->outputBuffer = outputBuffer;
+    result->handle = *outputBuffer.buffer;
+    result->outputBuffer.buffer = &result->handle;
+    result->inputCam3Buf = inputCam3Buf;
+    result->param = param;
+
+    mCaptureResultMap[frameNumber] = result;
+    mBufferDoneCondition.notify_one();
+}
+
+int Camera3Stream::setActive(bool state) {
+    LOG1("[%p]@%s isHALStream: %d state %d", this, __func__, mIsHALStream, state);
+
+    if (!mStreamState && state) {
+        std::string threadName = "Cam3Stream-";
+        threadName += std::to_string(mHALStream.id);
+
+        // Run Camera3Stream thread
+        run(threadName);
+
+        if (mHALStream.usage != icamera::CAMERA_STREAM_OPAQUE_RAW) {
+            // configure the post processing.
+            // Note: the mHALStream may be changed after calling this function
+            mPostProcessor->configure(mStream, mHALStream);
+            mPostProcessType = mPostProcessor->getPostProcessType();
+            LOG2("@%s, mPostProcessType:%d", __func__, mPostProcessType);
+        }
+
+        if (mIsHALStream) {
+            mBufferPool->createBufferPool(mCameraId, mMaxNumReqInProc, mHALStream);
+            LOG2("@%s, HAL stream create BufferPool", __func__);
+        }
+
+        if (mInputPostProcessor) {
+            mInputPostProcessor->configure(mStream, *mInputStream.get());
+            mInputPostProcessType = mInputPostProcessor->getPostProcessType();
+        }
+    } else if (mStreamState && !state) {
+        mPostProcessType = icamera::POST_PROCESS_NONE;
+
+        if (mInputPostProcessor) {
+            mInputPostProcessType = icamera::POST_PROCESS_NONE;
+        }
+
+        if (mBufferPool) {
+            mBufferPool->destroyBufferPool();
+        }
+
+        // Exit Camera3Stream thread
+        requestExit();
+    }
+
+    mStreamState = state;
+
+    return icamera::OK;
+}
+
+void Camera3Stream::activateFaceDetection(unsigned int maxFaceNum) {
+    LOG1("[%p]@%s maxFaceNum %d, mCameraId %d", this, __func__, maxFaceNum, mCameraId);
+
+    mFaceDetection = icamera::FaceDetection::createInstance(mCameraId, maxFaceNum, mHALStream.id,
+                                                            mHALStream.width, mHALStream.height);
+}
+
+void Camera3Stream::addListener(Camera3Stream* listener) {
+    mListeners.push_back(listener);
+}
+
+/* fetch the buffers will be queued to Hal, the buffer has 3 sources:
+ * 1st using HAL stream's request, then the buffer->addr should equal
+ *     request->cam3Buf->addr()
+ * 2nd if using listener buffer directly
+ * 3rd using bufferpool, the buffer from pool is stored in mQueuedBuffer
+ */
+bool Camera3Stream::fetchRequestBuffers(icamera::camera_buffer_t* buffer, uint32_t frameNumber) {
+    if (!mIsHALStream) return false;
+    LOG1("[%p]@%s isHALStream: %d frameNumber %d", this, __func__, mIsHALStream, frameNumber);
+
+    int requestStreamCount = 0;
+    std::shared_ptr<StreamComInfo> request = nullptr;
+    std::shared_ptr<Camera3Buffer> buf = nullptr;
+
+    // check if any one provided a buffer in listeners
+    for (auto& iter : mListeners) {
+        request = iter->getCaptureRequest(frameNumber);
+        if (request) {
+            requestStreamCount++;
+            buf = request->cam3Buf ? request->cam3Buf : buf;
+        }
+    }
+
+    // if HAL stream has a buffer, use HAL stream's buffer to qbuf/dqbuf
+    request = getCaptureRequest(frameNumber);
+    if (request) {
+        requestStreamCount++;
+        buf = request->cam3Buf ? request->cam3Buf : buf;
+    }
+
+    if (!requestStreamCount) {
+        // no stream requested
+        return false;
+    }
+
+    /* Fix me... if has 2 or more streams, use the same buffer.
+     ** if >= 2 streams request in same frame, we use buffer pool temporary.
+     ** to do: prefer to use user buffer to avoid memcpy
+     */
+    if (!buf || requestStreamCount >= 2) {
+        LOG1("[%p]@%s get buffer from pool", this, __func__);
+        if (mBufferPool) {
+            buf = mBufferPool->acquireBuffer();
+            CheckError(buf == nullptr, false, "no available internal buffer");
+            // using buffer pool, store the buffer, then can return it when frame done
+            mQueuedBuffer[frameNumber] = buf;
+        } else {
+            return false;
+        }
+    }
+
+    *buffer = buf->getHalBuffer();
+    // Fill the specific setting
+    buffer->s.usage = mHALStream.usage;
+    buffer->s.id = mHALStream.id;
+
+    return true;
+}
+
+// to check if a HW stream is triggered by it's listener
+void Camera3Stream::checkListenerRequest(uint32_t frameNumber) {
+    if (!mIsHALStream) return;
+
+    LOG1("[%p]@%s, frameNumber:%d", this, __func__, frameNumber);
+
+    bool listenerRequested = false;
+    for (auto& iter : mListeners) {
+        listenerRequested |= iter->getCaptureRequest(frameNumber) != nullptr;
+    }
+
+    if (!getCaptureRequest(frameNumber) && listenerRequested) {
+        // HW stream is enabled by listener's request
+        std::shared_ptr<CaptureResult> result = std::make_shared<CaptureResult>();
+        LOG1("[%p]@%s, frameNumber:%d, only listener requested", this, __func__,
+             frameNumber);
+        if (result) {
+            std::lock_guard<std::mutex> l(mLock);
+            result->frameNumber = frameNumber;
+            result->inputCam3Buf = nullptr;
+            mCaptureResultMap[frameNumber] = result;
+        }
+        mBufferDoneCondition.notify_one();
+    }
+}
+
+void Camera3Stream::notifyListenerBufferReady(uint32_t frameNumber,
+                                              const std::shared_ptr<StreamComInfo>& halOutput) {
+    LOG1("[%p] @%s", this, __func__);
+    std::lock_guard<std::mutex> l(mLock);
+    if (mCaptureRequest.find(frameNumber) != mCaptureRequest.end()) {
+        mHALStreamOutput[frameNumber] = halOutput;
+        mBufferDoneCondition.notify_one();
+    }
+}
+
+std::shared_ptr<StreamComInfo> Camera3Stream::getCaptureRequest(uint32_t frameNumber) {
+    std::lock_guard<std::mutex> l(mLock);
+    std::shared_ptr<StreamComInfo> request = nullptr;
+
+    if (mCaptureRequest.find(frameNumber) != mCaptureRequest.end()) {
+        request = mCaptureRequest[frameNumber];
+    }
+    return request;
+}
+
+bool Camera3Stream::waitCaptureResultReady() {
+    std::unique_lock<std::mutex> lock(mLock);
+    /* 1st loop, the HAL and listener stream wait on the CaptureResult
+     * BufferDoneCondition if the CaptureResultVector not empty.
+     * 2nd loop, the CaptureResult is not empty, and the HAL stream will start to
+     * dqbuf the listener stream should wait HAL Stream send out buffer ready event
+     * if it doesn't have inputCam3Buf.
+     * 3rd loop, (listener stream only) both vecotr are not empty, return true.
+     */
+    // HAL stream and listener stream should wait RequestManager notification
+    bool needWaitBufferReady = mCaptureResultMap.empty();
+    // listeners stream should wait HAL output buffer if not has input buffer
+    if (!mIsHALStream && !mCaptureResultMap.empty()) {
+        auto captureResult = mCaptureResultMap.begin();
+        std::shared_ptr<CaptureResult> result = captureResult->second;
+        needWaitBufferReady = !result->inputCam3Buf && mHALStreamOutput.empty();
+    }
+    if (needWaitBufferReady) {
+        std::cv_status ret = mBufferDoneCondition.wait_for(
+            lock, std::chrono::nanoseconds(kMaxDuration * SLOWLY_MULTIPLIER));
+        if (ret == std::cv_status::timeout) {
+            LOGW("[%p]%s, wait buffer ready time out", this, __func__);
+        }
+        // return false to make the threadLoop run again
+        return false;
+    }
+
+    return true;
+}
+
+void Camera3Stream::requestStreamDone(uint32_t frameNumber) {
+    if (!mIsHALStream) return;
+
+    LOG1("[%p] @%s frameNumber: %d", this, __func__, frameNumber);
+
+    /* release buffers. if the buffer used to qbuf/dqbuf is from listener or HAL
+     * stream, it will be released in its stream, because now we use buffer pool
+     * to sync buffer between frames.
+     */
+    std::unique_lock<std::mutex> lock(mLock);
+    if (mQueuedBuffer.find(frameNumber) != mQueuedBuffer.end()) {
+        // if HAL stream using buffer from pool to qbuf/dqbuf, return it
+        mBufferPool->returnBuffer(mQueuedBuffer[frameNumber]);
+        mQueuedBuffer.erase(frameNumber);
+    }
+}
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/Camera3Stream.h b/camera/hal/intel/ipu6/aal/Camera3Stream.h
new file mode 100644
index 000000000000..66cd05de56fb
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/Camera3Stream.h
@@ -0,0 +1,177 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <hardware/camera3.h>
+
+#include <map>
+#include <memory>
+#include <mutex>
+#include <unordered_map>
+#include <utility>
+#include <vector>
+
+#include "Camera3BufferPool.h"
+#include "FaceDetection.h"
+#include "PostProcessor.h"
+#include "ResultProcessor.h"
+#include "Thread.h"
+
+namespace camera3 {
+
+struct CaptureResult {
+    uint32_t frameNumber;
+    camera3_stream_buffer_t outputBuffer;
+    buffer_handle_t handle;
+    std::shared_ptr<Camera3Buffer> inputCam3Buf;
+    icamera::Parameters param;
+};
+
+struct StreamComInfo {
+    std::shared_ptr<Camera3Buffer> cam3Buf;
+    icamera::Parameters parameter;
+};
+
+/**
+ * \class InternalBufferPool
+ *
+ * This class is used to manage a local memory pool for still and post
+ * processing stream It needs to follow the calling sequence: allocBuffers ->
+ * acquireBuffer -> findBuffer -> returnBuffer
+ */
+class InternalBufferPool {
+ public:
+    InternalBufferPool();
+    ~InternalBufferPool();
+
+    icamera::status_t allocBuffers(const icamera::stream_t& stream, uint32_t numBuffers,
+                                   int cameraId);
+    void destroyBuffers();
+    std::shared_ptr<Camera3Buffer> acquireBuffer();
+    void returnBuffer(std::shared_ptr<Camera3Buffer> buffer);
+    std::shared_ptr<Camera3Buffer> findBuffer(void* memAddr);
+
+ private:
+    std::unordered_map<std::shared_ptr<Camera3Buffer>, bool>
+        mInterBuf;  // first: camera3Buffer, second: busy
+    std::mutex mLock;
+};
+
+/**
+ * \class Camera3Stream
+ *
+ * This class is used to handle requests. It has the following
+ * roles:
+ * - It instantiates PostProcessor.
+ */
+class Camera3Stream : public icamera::Thread {
+ public:
+    Camera3Stream(int cameraId, CallbackEventInterface* callback, uint32_t maxNumReqInProc,
+                  const icamera::stream_t& halStream, const camera3_stream_t& stream,
+                  const camera3_stream_t* inputStream = nullptr, bool isHWStream = false);
+    virtual ~Camera3Stream();
+
+    virtual bool threadLoop();
+    virtual void requestExit();
+
+    int processRequest(const std::shared_ptr<Camera3Buffer>& inputCam3Buf,
+                       const camera3_stream_buffer_t& outputBuffer, uint32_t frameNumber);
+
+    void queueBufferDone(uint32_t frameNumber, const std::shared_ptr<Camera3Buffer>& inputCam3Buf,
+                         const camera3_stream_buffer_t& outputBuffer,
+                         const icamera::Parameters& param);
+    int setActive(bool state);
+    bool isActive() { return mStreamState; }
+    void activateFaceDetection(unsigned int maxFaceNum);
+    int getPostProcessType() { return mPostProcessType; }
+    void sendEvent(const icamera::camera_msg_data_t& data);
+    void addListener(Camera3Stream* listener);
+    // fetch the buffers will be queued to Hal, HAL stream only
+    bool fetchRequestBuffers(icamera::camera_buffer_t* buffer, uint32_t frameNumber);
+    // check if the HW stream should be enabled by listener request
+    void checkListenerRequest(uint32_t frameNumber);
+    // called by RequestManager indicates the frame is done, release buffers
+    void requestStreamDone(uint32_t frameNumber);
+
+ private:
+    void handleSofAlignment();
+    /* get the request status anf Camera3Buf of this stream
+    ** return nullptr if stream not requested the frame
+    */
+    std::shared_ptr<StreamComInfo> getCaptureRequest(uint32_t frameNumber);
+    void notifyListenerBufferReady(uint32_t frameNumber,
+                                   const std::shared_ptr<StreamComInfo>& halOutput);
+
+    /* HAL stream or listener stream to wait capture buffer result ready,
+     ** called in ThreadLoop, return false if need to wait,
+     ** return true to continue the threadloop.
+     */
+    bool waitCaptureResultReady();
+
+ private:
+    const uint64_t kMaxDuration = 2000000000;  // 2000ms
+
+    int mCameraId;
+    std::condition_variable mBufferDoneCondition;
+    std::mutex mLock;
+
+    std::condition_variable mSofCondition;
+    std::mutex mSofLock;
+
+    CallbackEventInterface* mEventCallback;
+
+    int mPostProcessType;
+    std::unique_ptr<PostProcessor> mPostProcessor;
+
+    bool mStreamState;
+    icamera::stream_t mHALStream;
+    uint32_t mMaxNumReqInProc;
+    std::unique_ptr<Camera3BufferPool> mBufferPool;
+
+    camera3_stream_t mStream;
+
+    /* key is frame number, value is CaptureResult */
+    std::map<uint32_t, std::shared_ptr<CaptureResult>> mCaptureResultMap;
+    std::map<buffer_handle_t, std::shared_ptr<Camera3Buffer>> mBuffers;
+
+    icamera::FaceDetection* mFaceDetection;
+    unsigned int mFDRunDefaultInterval;  // FD running's interval frames.
+    unsigned int mFDRunIntervalNoFace;   // FD running's interval frames without face.
+    unsigned int mFDRunInterval;         // run 1 frame every mFDRunInterval frames.
+    unsigned int mFrameCnt;              // from 0 to (mFDRunInterval - 1).
+
+    int mInputPostProcessType;
+    std::unique_ptr<PostProcessor> mInputPostProcessor;
+    std::unique_ptr<camera3_stream_t> mInputStream;
+
+    bool mIsHALStream;
+
+    /* save output info, each stream can accept maxNumReqInProc
+     ** requests. used by HAL stream to get listener request status
+     */
+    std::unordered_map<uint32_t, std::shared_ptr<StreamComInfo>> mCaptureRequest;
+    std::vector<Camera3Stream*> mListeners;
+    // HAL streams output result, listener stream will wait on it before process
+    std::unordered_map<uint32_t, std::shared_ptr<StreamComInfo>> mHALStreamOutput;
+
+    // save buffer obj when HAL choose buffer from pool to do qbuf/dqbuf
+    std::unordered_map<uint32_t, std::shared_ptr<Camera3Buffer>> mQueuedBuffer;
+
+    void faceRunningByCondition(const icamera::camera_buffer_t& buffer);
+};
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/HALv3Interface.h b/camera/hal/intel/ipu6/aal/HALv3Interface.h
new file mode 100644
index 000000000000..28276dc0f2ed
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/HALv3Interface.h
@@ -0,0 +1,32 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+namespace camera3 {
+
+/**
+ * \brief An interface used to callback RequestManager.
+ */
+class RequestManagerCallback {
+ public:
+    RequestManagerCallback() {}
+    virtual ~RequestManagerCallback() {}
+
+    virtual void returnRequestDone(uint32_t frameNumber) = 0;
+};
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/HALv3Utils.cpp b/camera/hal/intel/ipu6/aal/HALv3Utils.cpp
new file mode 100644
index 000000000000..4dc094d6693d
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/HALv3Utils.cpp
@@ -0,0 +1,161 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "HalV3Utils"
+#include <chromeos-config/libcros_config/cros_config.h>
+#include <linux/videodev2.h>
+
+#include <memory>
+#include <string>
+
+#include "Errors.h"
+#include "HALv3Utils.h"
+#include "PlatformData.h"
+#include "Utils.h"
+
+namespace camera3 {
+namespace HalV3Utils {
+
+constexpr char kCrosConfigCameraPath[] = "/camera";
+constexpr char kCrosConfigCount[] = "count";
+
+static const char* Camera3StreamTypes[] = {"OUTPUT",         // CAMERA3_STREAM_OUTPUT
+                                           "INPUT",          // CAMERA3_STREAM_INPUT
+                                           "BIDIRECTIONAL",  // CAMERA3_STREAM_BIDIRECTIONAL
+                                           "INVALID"};
+
+const char* getCamera3StreamType(int type) {
+    int num = sizeof(Camera3StreamTypes) / sizeof(Camera3StreamTypes[0]);
+    return (type >= 0 && type < num) ? Camera3StreamTypes[type] : Camera3StreamTypes[num - 1];
+}
+
+int HALFormatToV4l2Format(int cameraId, int halFormat, int usage) {
+    LOG1("@%s", __func__);
+
+    int format = -1;
+    switch (halFormat) {
+        case HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED:
+            if (IS_ZSL_USAGE(usage)) {
+                format = icamera::PlatformData::getISysRawFormat(cameraId);
+            } else {
+                format = V4L2_PIX_FMT_NV12;
+            }
+            break;
+        case HAL_PIXEL_FORMAT_YCbCr_420_888:
+        case HAL_PIXEL_FORMAT_BLOB:
+            format = V4L2_PIX_FMT_NV12;
+            break;
+        case HAL_PIXEL_FORMAT_RAW_OPAQUE:
+            format = icamera::PlatformData::getISysRawFormat(cameraId);
+            break;
+        default:
+            LOGW("unsupport format %d", halFormat);
+            break;
+    }
+
+    return format;
+}
+
+int getRotationDegrees(const camera3_stream_t& stream) {
+    LOG1("@%s", __func__);
+    if (stream.stream_type != CAMERA3_STREAM_OUTPUT) {
+        LOG2("%s, no need rotation for stream type %d", __func__, stream.stream_type);
+        return 0;
+    }
+    switch (stream.crop_rotate_scale_degrees) {
+        case CAMERA3_STREAM_ROTATION_0:
+            return 0;
+        case CAMERA3_STREAM_ROTATION_90:
+            return 90;
+        case CAMERA3_STREAM_ROTATION_270:
+            return 270;
+        default:
+            LOGE("unsupport rotate degree: %d, the value must be (0,1,3)",
+                 stream.crop_rotate_scale_degrees);
+            return -1;
+    }
+}
+
+int fillHALStreams(int cameraId, const camera3_stream_t& camera3Stream, icamera::stream_t* stream) {
+    LOG1("@%s, cameraId:%d", __func__, cameraId);
+
+    stream->format = HALFormatToV4l2Format(cameraId, camera3Stream.format, camera3Stream.usage);
+    CheckError(stream->format == -1, icamera::BAD_VALUE, "unsupported format %x",
+               camera3Stream.format);
+
+    // For rotation cases, aal needs to get the psl output mapping to user requirement.
+    if (getRotationDegrees(camera3Stream) > 0) {
+        icamera::camera_resolution_t* psl = icamera::PlatformData::getPslOutputForRotation(
+            camera3Stream.width, camera3Stream.height, cameraId);
+
+        stream->width = psl ? psl->width : camera3Stream.height;
+        stream->height = psl ? psl->height : camera3Stream.width;
+        LOG1("%s, Use the psl output %dx%d to map user requirement: %dx%d", __func__, stream->width,
+             stream->height, camera3Stream.width, camera3Stream.height);
+    } else {
+        stream->width = camera3Stream.width;
+        stream->height = camera3Stream.height;
+    }
+
+    stream->field = 0;
+    stream->stride = icamera::CameraUtils::getStride(stream->format, stream->width);
+    stream->size =
+        icamera::CameraUtils::getFrameSize(stream->format, stream->width, stream->height);
+    stream->memType = V4L2_MEMORY_USERPTR;
+    stream->streamType = icamera::CAMERA_STREAM_OUTPUT;
+    if (camera3Stream.usage & GRALLOC_USAGE_HW_VIDEO_ENCODER) {
+        stream->usage = icamera::CAMERA_STREAM_VIDEO_CAPTURE;
+    } else if (IS_ZSL_USAGE(camera3Stream.usage)) {
+        stream->usage = icamera::CAMERA_STREAM_OPAQUE_RAW;
+    } else {
+        if (camera3Stream.format == HAL_PIXEL_FORMAT_BLOB) {
+            stream->usage = icamera::CAMERA_STREAM_STILL_CAPTURE;
+        } else if (camera3Stream.format == HAL_PIXEL_FORMAT_YCbCr_420_888) {
+            // Check if it is YUV capture
+            int size = stream->width * stream->height;
+            stream->usage = (size > RESOLUTION_1080P_WIDTH * RESOLUTION_1080P_HEIGHT)
+                                ? icamera::CAMERA_STREAM_STILL_CAPTURE
+                                : icamera::CAMERA_STREAM_PREVIEW;
+        } else if (camera3Stream.format == HAL_PIXEL_FORMAT_RAW_OPAQUE) {
+            stream->usage = icamera::CAMERA_STREAM_OPAQUE_RAW;
+        } else {
+            stream->usage = icamera::CAMERA_STREAM_PREVIEW;
+        }
+    }
+
+    LOG2("@%s, stream: width:%d, height:%d, usage %d", __func__, stream->width, stream->height,
+         stream->usage);
+    return icamera::OK;
+}
+
+// Only when /run/chromos-config/v1/camera/count exists on the board, it returns
+// the real camera number, otherwise it returns 0.
+int getHwCameraNumber() {
+    int cameraNumber = 0;
+    brillo::CrosConfig crosConfig;
+    bool status = crosConfig.Init();
+    CheckWarning(!status, 0, "@%s, Failed to initialize CrOS config", __func__);
+
+    std::string cameraCount;
+    status = crosConfig.GetString(kCrosConfigCameraPath, kCrosConfigCount, &cameraCount);
+    CheckWarning(!status, 0, "@%s, Failed to get camera number", __func__);
+    cameraNumber = atoi(cameraCount.c_str());
+
+    return cameraNumber;
+}
+
+}  // namespace HalV3Utils
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/HALv3Utils.h b/camera/hal/intel/ipu6/aal/HALv3Utils.h
new file mode 100644
index 000000000000..59b6b02311cf
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/HALv3Utils.h
@@ -0,0 +1,78 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <hardware/camera3.h>
+
+#include "Parameters.h"
+#include "iutils/CameraLog.h"
+
+namespace camera3 {
+
+#undef LOG1
+#undef LOG2
+#undef LOGI
+#undef LOGW
+#undef LOGE
+#undef SLOWLY_MULTIPLIER
+
+#define SLOWLY_MULTIPLIER (icamera::gSlowlyRunRatio ? icamera::gSlowlyRunRatio : 1)
+#define IS_ZSL_USAGE(usage) (((usage)&GRALLOC_USAGE_HW_CAMERA_ZSL) == GRALLOC_USAGE_HW_CAMERA_ZSL)
+
+#ifdef HAVE_LINUX_OS
+
+#define LOG1(format, args...)                                                              \
+    icamera::Log::print_log(icamera::gLogLevel& icamera::CAMERA_DEBUG_LOG_LEVEL1, LOG_TAG, \
+                            icamera::CAMERA_DEBUG_LOG_LEVEL1, format, ##args)
+#define LOG2(format, args...)                                                              \
+    icamera::Log::print_log(icamera::gLogLevel& icamera::CAMERA_DEBUG_LOG_LEVEL2, LOG_TAG, \
+                            icamera::CAMERA_DEBUG_LOG_LEVEL2, format, ##args)
+#define LOGFPS(format, args...)                                                         \
+    icamera::Log::print_log(icamera::gLogLevel& icamera::CAMERA_DEBUG_LOG_FPS, LOG_TAG, \
+                            icamera::CAMERA_DEBUG_LOG_FPS, format, ##args)
+#define LOGI(format, args...) \
+    icamera::Log::print_log(true, LOG_TAG, icamera::CAMERA_DEBUG_LOG_INFO, format, ##args)
+#define LOGW(format, args...) \
+    icamera::Log::print_log(true, LOG_TAG, icamera::CAMERA_DEBUG_LOG_WARNING, format, ##args)
+#define LOGE(format, args...) \
+    icamera::Log::print_log(true, LOG_TAG, icamera::CAMERA_DEBUG_LOG_ERR, format, ##args)
+
+#else
+
+#define LOG1(...)                                                                   \
+    icamera::__camera_hal_log(icamera::gLogLevel& icamera::CAMERA_DEBUG_LOG_LEVEL1, \
+                              ANDROID_LOG_DEBUG, LOG_TAG, __VA_ARGS__)
+#define LOG2(...)                                                                   \
+    icamera::__camera_hal_log(icamera::gLogLevel& icamera::CAMERA_DEBUG_LOG_LEVEL2, \
+                              ANDROID_LOG_DEBUG, LOG_TAG, __VA_ARGS__)
+#define LOGFPS(...)                                                              \
+    icamera::__camera_hal_log(icamera::gLogLevel& icamera::CAMERA_DEBUG_LOG_FPS, \
+                              ANDROID_LOG_DEBUG, LOG_TAG, __VA_ARGS__)
+#define LOGI(...) icamera::__camera_hal_log(true, ANDROID_LOG_INFO, LOG_TAG, __VA_ARGS__)
+#define LOGW(...) icamera::__camera_hal_log(true, ANDROID_LOG_WARN, LOG_TAG, __VA_ARGS__)
+#define LOGE(...) icamera::__camera_hal_log(true, ANDROID_LOG_ERROR, LOG_TAG, __VA_ARGS__)
+
+#endif
+namespace HalV3Utils {
+const char* getCamera3StreamType(int type);
+int HALFormatToV4l2Format(int cameraId, int halFormat, int usage);
+int getRotationDegrees(const camera3_stream_t& stream);
+int fillHALStreams(int cameraId, const camera3_stream_t& camera3Stream, icamera::stream_t* stream);
+int getHwCameraNumber();
+}  // namespace HalV3Utils
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/IntelAEStateMachine.cpp b/camera/hal/intel/ipu6/aal/IntelAEStateMachine.cpp
new file mode 100644
index 000000000000..1b79130ae2be
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/IntelAEStateMachine.cpp
@@ -0,0 +1,285 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelAEStateMachine"
+
+#include "IntelAEStateMachine.h"
+
+#include "Errors.h"
+#include "HALv3Utils.h"
+#include "Utils.h"
+
+namespace camera3 {
+
+IntelAEStateMachine::IntelAEStateMachine(int cameraId)
+        : mCameraId(cameraId),
+          mLastControlMode(0),
+          mLastSceneMode(0),
+          mCurrentAeMode(NULL) {
+    LOG1("%s mCameraId %d", __func__, mCameraId);
+    mCurrentAeMode = &mAutoMode;
+    CLEAR(mLastAeControls);
+    mLastAeControls.aeMode = ANDROID_CONTROL_AE_MODE_ON;
+}
+
+IntelAEStateMachine::~IntelAEStateMachine() {
+    LOG1("%s mCameraId %d", __func__, mCameraId);
+}
+
+/**
+ * Process states in input stage before the AE is run.
+ * It is initializing the current state if input
+ * parameters have an influence.
+ *
+ * \param[IN] controlMode: control.controlMode
+ * \param[IN] sceneMode: control.sceneMode
+ * \param[IN] aeControls: set of control.<ae>
+ */
+int IntelAEStateMachine::processState(uint8_t controlMode, uint8_t sceneMode,
+                                      const AeControls& aeControls) {
+    if (controlMode == ANDROID_CONTROL_MODE_OFF) {
+        LOG2("%s: Set AE offMode: controlMode = %d, aeMode = %d", __func__, controlMode,
+             aeControls.aeMode);
+        mCurrentAeMode = &mOffMode;
+    } else {
+        if (aeControls.aeMode == ANDROID_CONTROL_AE_MODE_OFF) {
+            mCurrentAeMode = &mOffMode;
+            LOG2("%s: Set AE offMode: controlMode = %d, aeMode = %d", __func__, controlMode,
+                 aeControls.aeMode);
+        } else {
+            LOG2("%s: Set AE AutoMode: controlMode = %d, aeMode = %d", __func__, controlMode,
+                 aeControls.aeMode);
+            mCurrentAeMode = &mAutoMode;
+        }
+    }
+
+    mLastAeControls = aeControls;
+    mLastSceneMode = sceneMode;
+    mLastControlMode = controlMode;
+
+    return mCurrentAeMode->processState(controlMode, sceneMode, aeControls);
+}
+
+/**
+ * Process results and define output state after the AE is run
+ *
+ * \param[IN] aeConverged: from the ae result
+ * \param[IN] results: cameraMetadata to write dynamic tags.
+ */
+int IntelAEStateMachine::processResult(bool aeConverged, android::CameraMetadata* result) {
+    CheckError(!mCurrentAeMode, icamera::UNKNOWN_ERROR, "Invalid AE mode");
+    CheckError(!result, icamera::UNKNOWN_ERROR, "%s, result is nullptr", __func__);
+
+    return mCurrentAeMode->processResult(aeConverged, result);
+}
+
+/******************************************************************************
+ * AE MODE   -  BASE
+ ******************************************************************************/
+IntelAEModeBase::IntelAEModeBase()
+        : mLastControlMode(0),
+          mLastSceneMode(0),
+          mEvChanged(false),
+          mLastAeConvergedFlag(false),
+          mAeRunCount(0),
+          mAeConvergedCount(0),
+          mCurrentAeState(ANDROID_CONTROL_AE_STATE_INACTIVE) {
+    LOG1("%s", __func__);
+    CLEAR(mLastAeControls);
+}
+
+void IntelAEModeBase::updateResult(android::CameraMetadata* results) {
+    CheckError(!results, VOID_VALUE, "%s, result is nullptr", __func__);
+    LOG2("%s: current AE state is: %d", __func__, mCurrentAeState);
+
+    //# METADATA_Dynamic control.aeMode done
+    results->update(ANDROID_CONTROL_AE_MODE, &mLastAeControls.aeMode, 1);
+    //# METADATA_Dynamic control.aeLock done
+    results->update(ANDROID_CONTROL_AE_LOCK, &mLastAeControls.aeLock, 1);
+    //# METADATA_Dynamic control.aePrecaptureTrigger done
+    results->update(ANDROID_CONTROL_AE_PRECAPTURE_TRIGGER, &mLastAeControls.aePreCaptureTrigger, 1);
+    //# METADATA_Dynamic control.aeState done
+    results->update(ANDROID_CONTROL_AE_STATE, &mCurrentAeState, 1);
+}
+
+void IntelAEModeBase::resetState() {
+    LOG2("%s", __func__);
+
+    mCurrentAeState = ANDROID_CONTROL_AE_STATE_INACTIVE;
+    mLastAeConvergedFlag = false;
+    mAeRunCount = 0;
+    mAeConvergedCount = 0;
+}
+
+/******************************************************************************
+ * AE MODE   -  OFF
+ ******************************************************************************/
+
+IntelAEModeOff::IntelAEModeOff() : IntelAEModeBase() {
+    LOG1("%s", __func__);
+}
+
+int IntelAEModeOff::processState(uint8_t controlMode, uint8_t sceneMode,
+                                 const AeControls& aeControls) {
+    LOG2("%s", __func__);
+
+    mLastAeControls = aeControls;
+    mLastSceneMode = sceneMode;
+    mLastControlMode = controlMode;
+
+    if (controlMode == ANDROID_CONTROL_MODE_OFF ||
+        aeControls.aeMode == ANDROID_CONTROL_AE_MODE_OFF) {
+        resetState();
+    } else {
+        LOGE("AE State machine should not be OFF! - Fix bug");
+        return icamera::UNKNOWN_ERROR;
+    }
+
+    return icamera::OK;
+}
+
+int IntelAEModeOff::processResult(bool aeConverged, android::CameraMetadata* result) {
+    UNUSED(aeConverged);
+    CheckError(!result, icamera::UNKNOWN_ERROR, "%s, result is nullptr", __func__);
+    LOG2("%s", __func__);
+
+    mCurrentAeState = ANDROID_CONTROL_AE_STATE_INACTIVE;
+    updateResult(result);
+
+    return icamera::OK;
+}
+
+/******************************************************************************
+ * AE MODE   -  AUTO
+ ******************************************************************************/
+
+IntelAEModeAuto::IntelAEModeAuto() : IntelAEModeBase() {
+    LOG1("%s", __func__);
+}
+
+int IntelAEModeAuto::processState(uint8_t controlMode, uint8_t sceneMode,
+                                  const AeControls& aeControls) {
+    if (controlMode != mLastControlMode) {
+        LOG1("%s: control mode has changed %d -> %d, reset AE State", __func__, controlMode,
+             mLastControlMode);
+        resetState();
+    }
+
+    if (aeControls.aeLock == ANDROID_CONTROL_AE_LOCK_ON) {
+        // If ev compensation changes, we have to let the AE run until
+        // convergence. Thus we need to figure out changes in compensation and
+        // only change the state immediately to locked,
+        // IF the EV did not change.
+        if (mLastAeControls.evCompensation != aeControls.evCompensation) mEvChanged = true;
+
+        if (!mEvChanged) mCurrentAeState = ANDROID_CONTROL_AE_STATE_LOCKED;
+    } else if (aeControls.aeMode != mLastAeControls.aeMode ||
+               (controlMode == ANDROID_CONTROL_MODE_USE_SCENE_MODE &&
+                sceneMode != mLastSceneMode)) {
+        resetState();
+    } else {
+        switch (mCurrentAeState) {
+            case ANDROID_CONTROL_AE_STATE_LOCKED:
+                mCurrentAeState = ANDROID_CONTROL_AE_STATE_INACTIVE;
+                break;
+            case ANDROID_CONTROL_AE_STATE_SEARCHING:
+            case ANDROID_CONTROL_AE_STATE_INACTIVE:
+            case ANDROID_CONTROL_AE_STATE_CONVERGED:
+            case ANDROID_CONTROL_AE_STATE_FLASH_REQUIRED:
+            case ANDROID_CONTROL_AE_STATE_PRECAPTURE:
+                if (aeControls.aePreCaptureTrigger == ANDROID_CONTROL_AE_PRECAPTURE_TRIGGER_START)
+                    mCurrentAeState = ANDROID_CONTROL_AE_STATE_PRECAPTURE;
+
+                if (aeControls.aePreCaptureTrigger == ANDROID_CONTROL_AE_PRECAPTURE_TRIGGER_CANCEL)
+                    mCurrentAeState = ANDROID_CONTROL_AE_STATE_INACTIVE;
+                break;
+            default:
+                LOGE("Invalid AE state!, State set to INACTIVE");
+                mCurrentAeState = ANDROID_CONTROL_AE_STATE_INACTIVE;
+
+                break;
+        }
+    }
+    mLastAeControls = aeControls;
+    mLastSceneMode = sceneMode;
+    mLastControlMode = controlMode;
+    return icamera::OK;
+}
+
+int IntelAEModeAuto::processResult(bool aeConverged, android::CameraMetadata* result) {
+    CheckError(!result, icamera::UNKNOWN_ERROR, "%s, result is nullptr", __func__);
+    switch (mCurrentAeState) {
+        case ANDROID_CONTROL_AE_STATE_LOCKED:
+            // do nothing
+            break;
+        case ANDROID_CONTROL_AE_STATE_INACTIVE:
+        case ANDROID_CONTROL_AE_STATE_SEARCHING:
+        case ANDROID_CONTROL_AE_STATE_CONVERGED:
+        case ANDROID_CONTROL_AE_STATE_FLASH_REQUIRED:
+            if (aeConverged) {
+                mEvChanged = false;  // converged -> reset
+                if (mLastAeControls.aeLock) {
+                    mCurrentAeState = ANDROID_CONTROL_AE_STATE_LOCKED;
+                } else {
+                    mCurrentAeState = ANDROID_CONTROL_AE_STATE_CONVERGED;
+                }
+            } else {
+                mCurrentAeState = ANDROID_CONTROL_AE_STATE_SEARCHING;
+            }
+            break;
+        case ANDROID_CONTROL_AE_STATE_PRECAPTURE:
+            if (aeConverged) {
+                mEvChanged = false;  // converged -> reset
+                if (mLastAeControls.aeLock) {
+                    mCurrentAeState = ANDROID_CONTROL_AE_STATE_LOCKED;
+                } else {
+                    mCurrentAeState = ANDROID_CONTROL_AE_STATE_CONVERGED;
+                }
+            }  // here the else is staying at the same state.
+            break;
+        default:
+            LOGE("Invalid AE state!, State set to INACTIVE");
+            mCurrentAeState = ANDROID_CONTROL_AE_STATE_INACTIVE;
+            break;
+    }
+
+    if (aeConverged) {
+        if (mLastAeConvergedFlag == true) {
+            mAeConvergedCount++;
+            LOG2("%s: AE converged for %d frames", __func__, mAeConvergedCount);
+        } else {
+            mAeConvergedCount = 1;
+            LOG1("%s: AE converging -> converged, after running AE for %d times", __func__,
+                 mAeRunCount);
+        }
+    } else {
+        if (mLastAeConvergedFlag == true) {
+            LOG1("%s: AE Converged -> converging", __func__);
+            mAeRunCount = 1;
+            mAeConvergedCount = 0;
+        } else {
+            mAeRunCount++;
+            LOG2("%s: AE converging for %d frames", __func__, mAeRunCount);
+        }
+    }
+    mLastAeConvergedFlag = aeConverged;
+
+    updateResult(result);
+
+    return icamera::OK;
+}
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/IntelAEStateMachine.h b/camera/hal/intel/ipu6/aal/IntelAEStateMachine.h
new file mode 100644
index 000000000000..83de125b8937
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/IntelAEStateMachine.h
@@ -0,0 +1,136 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "HALv3Header.h"
+#include "Utils.h"
+
+namespace camera3 {
+
+/**
+ * \struct AeControls
+ *
+ * Control Modes saved and passed back to control unit after reading
+ *
+ */
+struct AeControls {
+    uint8_t aeMode;              /**< AE_MODE */
+    uint8_t aeLock;              /**< AE_LOCK */
+    uint8_t aePreCaptureTrigger; /**< PRECAPTURE_TRIGGER */
+    uint8_t sceneMode;           /**< SCENE_MODE */
+    int32_t evCompensation;      /**< AE_EXPOSURE_COMPENSATION */
+};
+
+/**
+ * \class IntelAEModeBase
+ *
+ * Base class for all the Autoexposure modes as defined by the Android
+ * camera device V3.x API.
+ * Each mode will follow certain state transitions. See documentation for
+ * android.control.aeState
+ *
+ */
+class IntelAEModeBase {
+ public:
+    IntelAEModeBase();
+    virtual ~IntelAEModeBase(){};
+
+    virtual int processState(uint8_t controlMode, uint8_t sceneMode,
+                             const AeControls& aeControls) = 0;
+
+    virtual int processResult(bool aeConverged, android::CameraMetadata* results) = 0;
+
+    void resetState(void);
+    uint8_t getState() const { return mCurrentAeState; }
+
+ protected:
+    void updateResult(android::CameraMetadata* results);
+
+ protected:
+    AeControls mLastAeControls;
+    uint8_t mLastControlMode;
+    uint8_t mLastSceneMode;
+    bool mEvChanged; /**< set and kept to true when ev changes until
+                          converged */
+
+    bool mLastAeConvergedFlag;
+    uint8_t mAeRunCount;
+    uint8_t mAeConvergedCount;
+    uint8_t mCurrentAeState;
+};
+
+/**
+ * \class IntelAEModeAuto
+ * Derived class from IntelAEModeBase for Auto mode
+ *
+ */
+class IntelAEModeAuto : public IntelAEModeBase {
+ public:
+    IntelAEModeAuto();
+    virtual int processState(uint8_t controlMode, uint8_t sceneMode, const AeControls& aeControls);
+    virtual int processResult(bool aeConverged, android::CameraMetadata* result);
+};
+
+/**
+ * \class IntelAEModeOFF
+ * Derived class from IntelAEModeBase for OFF mode
+ *
+ */
+class IntelAEModeOff : public IntelAEModeBase {
+ public:
+    IntelAEModeOff();
+    virtual int processState(uint8_t controlMode, uint8_t sceneMode, const AeControls& aeControls);
+    virtual int processResult(bool aeConverged, android::CameraMetadata* result);
+};
+
+/**
+ * \class IntelAEStateMachine
+ *
+ * This class adapts the Android V3 AE triggers and state transitions to
+ * the ones implemented by the Intel AIQ algorithm
+ * This class is platform independent. Platform specific behaviors should be
+ * implemented in derived classes from this one or from the IntelAEModeBase
+ *
+ */
+class IntelAEStateMachine {
+ public:
+    IntelAEStateMachine(int cameraId);
+    virtual ~IntelAEStateMachine();
+
+    int processState(uint8_t controlMode, uint8_t sceneMode, const AeControls& aeControls);
+
+    int processResult(bool aeConverged, android::CameraMetadata* results);
+
+    uint8_t getState() const { return mCurrentAeMode->getState(); }
+
+ private:
+    // prevent copy constructor and assignment operator
+    DISALLOW_COPY_AND_ASSIGN(IntelAEStateMachine);
+
+ private: /* members*/
+    int mCameraId;
+    AeControls mLastAeControls;
+    uint8_t mLastControlMode;
+    uint8_t mLastSceneMode;
+
+    IntelAEModeBase* mCurrentAeMode;
+
+    IntelAEModeOff mOffMode;
+    IntelAEModeAuto mAutoMode;
+};
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/IntelAFStateMachine.cpp b/camera/hal/intel/ipu6/aal/IntelAFStateMachine.cpp
new file mode 100644
index 000000000000..c409afeb3378
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/IntelAFStateMachine.cpp
@@ -0,0 +1,415 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelAFStateMachine"
+
+#include "IntelAFStateMachine.h"
+
+#include "Errors.h"
+#include "HALv3Utils.h"
+#include "Utils.h"
+
+namespace camera3 {
+
+/**
+ * AF timeouts. Together these will make:
+ * timeout if: [MIN_AF_TIMEOUT - MAX_AF_FRAME_COUNT_TIMEOUT - MAX_AF_TIMEOUT]
+ * which translates to 2-4 seconds with the current values. Actual timeout value
+ * will depend on the FPS. E.g. >30FPS = 2s, 20FPS = 3s, <15FPS = 4s.
+ */
+
+/**
+ * MAX_AF_TIMEOUT
+ * Maximum time we allow the AF to iterate without a result.
+ * This timeout is the last resort, for very low FPS operation.
+ * Units are in microseconds.
+ * 4 seconds is a compromise between CTS & ITS. ITS allows for 10 seconds for
+ * 3A convergence. CTS1 allows only 5, but it doesn't require convergence, just
+ * a conclusion. We reserve one second for latencies to be safe. This makes the
+ * timeout 5 (cts1) - 1 (latency safety) = 4 seconds = 4000000us.
+ */
+static const long int MAX_AF_TIMEOUT = 4000000;  // 4 seconds
+
+/**
+ * MIN_AF_TIMEOUT
+ * For very high FPS use cases, we want to anyway allow some time for moving the
+ * lens.
+ */
+static const long int MIN_AF_TIMEOUT = 2000000;  // 2 seconds
+
+/**
+ * MAX_AF_FRAME_COUNT_TIMEOUT
+ * Maximum time we allow the AF to iterate without a result.
+ * Based on frames, as the AF algorithm itself needs frames for its operation,
+ * not just time, and the FPS varies.
+ * This is the timeout for normal operation, and translates to 2 seconds
+ * if FPS is 30.
+ */
+static const int MAX_AF_FRAME_COUNT_TIMEOUT = 60;  // 2 seconds if 30fps
+
+IntelAFStateMachine::IntelAFStateMachine(int cameraId) : mCameraId(cameraId) {
+    LOG1("%s mCameraId %d", __func__, mCameraId);
+    mCurrentAfMode = &mAutoMode;
+    mLastAfControls = {ANDROID_CONTROL_AF_MODE_AUTO, ANDROID_CONTROL_AF_TRIGGER_IDLE};
+}
+
+IntelAFStateMachine::~IntelAFStateMachine() {
+    LOG1("%s mCameraId %d", __func__, mCameraId);
+}
+
+int IntelAFStateMachine::processTriggers(uint8_t afTrigger, uint8_t afMode) {
+    if (afMode != mLastAfControls.afMode) {
+        LOG1("Change of AF mode from %d to %d", mLastAfControls.afMode, afMode);
+
+        switch (afMode) {
+            case ANDROID_CONTROL_AF_MODE_AUTO:
+            case ANDROID_CONTROL_AF_MODE_MACRO:
+                mCurrentAfMode = &mAutoMode;
+                break;
+            case ANDROID_CONTROL_AF_MODE_CONTINUOUS_VIDEO:
+            case ANDROID_CONTROL_AF_MODE_CONTINUOUS_PICTURE:
+                mCurrentAfMode = &mContinuousPictureMode;
+                break;
+            case ANDROID_CONTROL_AF_MODE_OFF:
+                mCurrentAfMode = &mOffMode;
+                break;
+            default:
+                LOGE("INVALID AF mode requested defaulting to AUTO");
+                mCurrentAfMode = &mAutoMode;
+                break;
+        }
+        mCurrentAfMode->resetState();
+    }
+    mLastAfControls.afTrigger = afTrigger;
+    mLastAfControls.afMode = afMode;
+
+    LOG2("%s: afMode %d", __func__, mLastAfControls.afMode);
+    return mCurrentAfMode->processTriggers(afTrigger, afMode);
+}
+
+int IntelAFStateMachine::processResult(int afState, bool lensMoving,
+                                       android::CameraMetadata* result) {
+    CheckError(!mCurrentAfMode, icamera::UNKNOWN_ERROR, "Invalid AF mode");
+    CheckError(!result, icamera::UNKNOWN_ERROR, "%s, result is nullptr", __func__);
+
+    return mCurrentAfMode->processResult(afState, lensMoving, result);
+}
+
+/**
+ * updateDefaults
+ *
+ * Used in case of error in the algorithm or fixed focus sensor
+ * In case of fixed focus sensor we always report locked
+ */
+int IntelAFStateMachine::updateDefaults(android::CameraMetadata* result, bool fixedFocus) const {
+    CheckError(!result, icamera::UNKNOWN_ERROR, "%s, result is nullptr", __func__);
+    mCurrentAfMode->updateResult(result);
+    uint8_t defaultState = ANDROID_CONTROL_AF_STATE_INACTIVE;
+    if (fixedFocus) defaultState = ANDROID_CONTROL_AF_STATE_FOCUSED_LOCKED;
+
+    result->update(ANDROID_CONTROL_AF_STATE, &defaultState, 1);
+
+    return icamera::OK;
+}
+
+/******************************************************************************
+ * AF MODE   -  BASE
+ ******************************************************************************/
+IntelAfModeBase::IntelAfModeBase()
+        : mCurrentAfState(ANDROID_CONTROL_AF_STATE_INACTIVE),
+          mLensState(ANDROID_LENS_STATE_STATIONARY),
+          mLastActiveTriggerTime(0),
+          mFramesSinceTrigger(0) {
+    LOG1("%s", __func__);
+    mLastAfControls = {ANDROID_CONTROL_AF_MODE_AUTO, ANDROID_CONTROL_AF_TRIGGER_IDLE};
+}
+
+/**
+ * processTriggers
+ *
+ * This method is called BEFORE auto focus algorithm has RUN
+ * Input parameters are pre-filled by the Intel3APlus::fillAfInputParams()
+ * by parsing the request settings.
+ * Other parameters from the capture request settings not filled in the input
+ * params structure is passed as argument
+ */
+int IntelAfModeBase::processTriggers(uint8_t afTrigger, uint8_t afMode) {
+    LOG2("%s", __func__);
+
+    if (afTrigger == ANDROID_CONTROL_AF_TRIGGER_START) {
+        resetTrigger(icamera::CameraUtils::systemTime() / 1000);
+        LOG1("AF TRIGGER START");
+    } else if (afTrigger == ANDROID_CONTROL_AF_TRIGGER_CANCEL) {
+        LOG1("AF TRIGGER CANCEL");
+        resetTrigger(0);
+    }
+    mLastAfControls.afTrigger = afTrigger;
+    mLastAfControls.afMode = afMode;
+    return icamera::OK;
+}
+
+void IntelAfModeBase::updateResult(android::CameraMetadata* results) {
+    CheckError(!results, VOID_VALUE, "%s, result is nullptr", __func__);
+    LOG2("%s", __func__);
+
+    LOG2("%s afMode = %d state = %d", __func__, mLastAfControls.afMode, mCurrentAfState);
+
+    results->update(ANDROID_CONTROL_AF_MODE, &mLastAfControls.afMode, 1);
+    //# METADATA_Dynamic control.afTrigger done
+    results->update(ANDROID_CONTROL_AF_TRIGGER, &mLastAfControls.afTrigger, 1);
+    //# METADATA_Dynamic control.afState done
+    results->update(ANDROID_CONTROL_AF_STATE, &mCurrentAfState, 1);
+    /**
+     * LENS STATE update
+     */
+    //# METADATA_Dynamic lens.state Done
+    results->update(ANDROID_LENS_STATE, &mLensState, 1);
+}
+
+void IntelAfModeBase::resetTrigger(usecs_t triggerTime) {
+    mLastActiveTriggerTime = triggerTime;
+    mFramesSinceTrigger = 0;
+}
+
+void IntelAfModeBase::resetState() {
+    mCurrentAfState = ANDROID_CONTROL_AF_STATE_INACTIVE;
+}
+
+void IntelAfModeBase::checkIfFocusTimeout() {
+    // give up if AF was iterating for too long
+    if (mLastActiveTriggerTime != 0) {
+        mFramesSinceTrigger++;
+        usecs_t now = icamera::CameraUtils::systemTime() / 1000;
+        usecs_t timeSinceTriggered = now - mLastActiveTriggerTime;
+        if (mCurrentAfState != ANDROID_CONTROL_AF_STATE_FOCUSED_LOCKED) {
+            /**
+             * Timeout IF either time has passed beyond MAX_AF_TIMEOUT
+             *                         OR
+             * Enough frames have been processed and time has passed beyond
+             * MIN_AF_TIMEOUT
+             */
+            if (timeSinceTriggered > MAX_AF_TIMEOUT ||
+                (mFramesSinceTrigger > MAX_AF_FRAME_COUNT_TIMEOUT &&
+                 timeSinceTriggered > MIN_AF_TIMEOUT)) {
+                resetTrigger(0);
+                mCurrentAfState = ANDROID_CONTROL_AF_STATE_NOT_FOCUSED_LOCKED;
+            }
+        }
+    }
+}
+
+/******************************************************************************
+ * AF MODE   -  OFF
+ ******************************************************************************/
+
+IntelAFModeOff::IntelAFModeOff() : IntelAfModeBase() {
+    LOG1("%s", __func__);
+}
+
+int IntelAFModeOff::processTriggers(uint8_t afTrigger, uint8_t afMode) {
+    LOG2("%s", __func__);
+
+    mLastAfControls.afTrigger = afTrigger;
+    mLastAfControls.afMode = afMode;
+    return icamera::OK;
+}
+
+int IntelAFModeOff::processResult(int afState, bool lensMoving, android::CameraMetadata* result) {
+    CheckError(!result, icamera::UNKNOWN_ERROR, "%s, result is nullptr", __func__);
+    /**
+     * IN MANUAL and EDOF AF state never changes
+     */
+    LOG2("%s", __func__);
+
+    mCurrentAfState = ANDROID_CONTROL_AF_STATE_INACTIVE;
+    mLensState = lensMoving ? ANDROID_LENS_STATE_MOVING : ANDROID_LENS_STATE_STATIONARY;
+    updateResult(result);
+
+    return icamera::OK;
+}
+
+/******************************************************************************
+ * AF MODE   -  AUTO
+ ******************************************************************************/
+
+IntelAFModeAuto::IntelAFModeAuto() : IntelAfModeBase() {
+    LOG1("%s", __func__);
+}
+
+int IntelAFModeAuto::processTriggers(uint8_t afTrigger, uint8_t afMode) {
+    LOG2("%s", __func__);
+
+    IntelAfModeBase::processTriggers(afTrigger, afMode);
+
+    // Override AF state if we just got an AF TRIGGER Start
+    // This is only valid for the AUTO/MACRO state machine
+    if (mLastAfControls.afTrigger == ANDROID_CONTROL_AF_TRIGGER_START) {
+        mCurrentAfState = ANDROID_CONTROL_AF_STATE_ACTIVE_SCAN;
+        LOG2("@%s AF state ACTIVE_SCAN (trigger start)", __PRETTY_FUNCTION__);
+    } else if (mLastAfControls.afTrigger == ANDROID_CONTROL_AF_TRIGGER_CANCEL) {
+        mCurrentAfState = ANDROID_CONTROL_AF_STATE_INACTIVE;
+        LOG2("@%s AF state INACTIVE (trigger cancel)", __PRETTY_FUNCTION__);
+    }
+
+    return icamera::OK;
+}
+
+int IntelAFModeAuto::processResult(int afState, bool lensMoving, android::CameraMetadata* result) {
+    CheckError(!result, icamera::UNKNOWN_ERROR, "%s, result is nullptr", __func__);
+    LOG2("%s", __func__);
+    mLensState = lensMoving ? ANDROID_LENS_STATE_MOVING : ANDROID_LENS_STATE_STATIONARY;
+
+    if (mLastActiveTriggerTime != 0) {
+        switch (afState) {
+            case icamera::AF_STATE_LOCAL_SEARCH:
+            case icamera::AF_STATE_EXTENDED_SEARCH:
+                LOG2("@%s AF state SCANNING", __PRETTY_FUNCTION__);
+                break;
+            case icamera::AF_STATE_SUCCESS:
+                mCurrentAfState = ANDROID_CONTROL_AF_STATE_FOCUSED_LOCKED;
+                resetTrigger(0);
+                LOG2("@%s AF state FOCUSED_LOCKED", __PRETTY_FUNCTION__);
+                break;
+            case icamera::AF_STATE_FAIL:
+                mCurrentAfState = ANDROID_CONTROL_AF_STATE_NOT_FOCUSED_LOCKED;
+                resetTrigger(0);
+                LOG2("@%s AF state NOT_FOCUSED_LOCKED", __PRETTY_FUNCTION__);
+                break;
+            default:
+            case icamera::AF_STATE_IDLE:
+                LOG2("@%s AF state INACTIVE", __PRETTY_FUNCTION__);
+                break;
+        }
+    }
+
+    checkIfFocusTimeout();
+
+    updateResult(result);
+
+    return icamera::OK;
+}
+
+/******************************************************************************
+ * AF MODE   -  CONTINUOUS PICTURE
+ ******************************************************************************/
+
+IntelAFModeContinuousPicture::IntelAFModeContinuousPicture() : IntelAfModeBase() {
+    LOG1("%s", __func__);
+}
+
+int IntelAFModeContinuousPicture::processTriggers(uint8_t afTrigger, uint8_t afMode) {
+    LOG2("%s", __func__);
+
+    IntelAfModeBase::processTriggers(afTrigger, afMode);
+
+    // Override AF state if we just got an AF TRIGGER CANCEL
+    if (mLastAfControls.afTrigger == ANDROID_CONTROL_AF_TRIGGER_CANCEL) {
+        /* Scan is supposed to be restarted, which we try by triggering a new
+         * scan. (see IntelAFStateMachine::processTriggers)
+         * This however, doesn't do anything at all, because AIQ does not
+         * want to play ball, at least yet.
+         *
+         * We can skip state transitions when allowed by the state
+         * machine documentation, so skip INACTIVE, also skip PASSIVE_SCAN if
+         * possible and go directly to either PASSIVE_FOCUSED or UNFOCUSED
+         *
+         * TODO: Remove this switch-statement, once triggering a scan starts to
+         * work. We could go directly to PASSIVE_SCAN always then, because a
+         * scan is really happening. Now it is not.
+         */
+        switch (mCurrentAfState) {
+            case ANDROID_CONTROL_AF_STATE_PASSIVE_SCAN:
+            case ANDROID_CONTROL_AF_STATE_NOT_FOCUSED_LOCKED:
+                mCurrentAfState = ANDROID_CONTROL_AF_STATE_PASSIVE_UNFOCUSED;
+                break;
+            case ANDROID_CONTROL_AF_STATE_FOCUSED_LOCKED:
+                mCurrentAfState = ANDROID_CONTROL_AF_STATE_PASSIVE_FOCUSED;
+                break;
+            default:
+                mCurrentAfState = ANDROID_CONTROL_AF_STATE_PASSIVE_SCAN;
+                break;
+        }
+    }
+    /* Override AF state if we just got an AF TRIGGER START, this will stop
+     * the scan as intended in the state machine documentation (see
+     * IntelAFStateMachine::processTriggers)
+     */
+    if (mLastAfControls.afTrigger == ANDROID_CONTROL_AF_TRIGGER_START) {
+        if (mCurrentAfState == ANDROID_CONTROL_AF_STATE_PASSIVE_FOCUSED)
+            mCurrentAfState = ANDROID_CONTROL_AF_STATE_FOCUSED_LOCKED;
+        else if (mCurrentAfState == ANDROID_CONTROL_AF_STATE_PASSIVE_UNFOCUSED ||
+                 mCurrentAfState == ANDROID_CONTROL_AF_STATE_PASSIVE_SCAN)
+            mCurrentAfState = ANDROID_CONTROL_AF_STATE_NOT_FOCUSED_LOCKED;
+    }
+
+    return icamera::OK;
+}
+
+int IntelAFModeContinuousPicture::processResult(int afState, bool lensMoving,
+                                                android::CameraMetadata* result) {
+    CheckError(!result, icamera::UNKNOWN_ERROR, "%s, result is nullptr", __func__);
+    LOG2("%s", __func__);
+    mLensState = lensMoving ? ANDROID_LENS_STATE_MOVING : ANDROID_LENS_STATE_STATIONARY;
+
+    // state transition from locked state are only allowed via triggers, which
+    // are handled in the currentAFMode processTriggers() and below in this
+    // function.
+    if (mCurrentAfState != ANDROID_CONTROL_AF_STATE_FOCUSED_LOCKED &&
+        mCurrentAfState != ANDROID_CONTROL_AF_STATE_NOT_FOCUSED_LOCKED) {
+        switch (afState) {
+            case icamera::AF_STATE_LOCAL_SEARCH:
+            case icamera::AF_STATE_EXTENDED_SEARCH:
+                LOG2("@%s AF state SCANNING", __PRETTY_FUNCTION__);
+                mCurrentAfState = ANDROID_CONTROL_AF_STATE_PASSIVE_SCAN;
+                break;
+            case icamera::AF_STATE_SUCCESS:
+                if (mLastActiveTriggerTime == 0) {
+                    mCurrentAfState = ANDROID_CONTROL_AF_STATE_PASSIVE_FOCUSED;
+                    LOG2("@%s AF state PASSIVE_FOCUSED", __PRETTY_FUNCTION__);
+                } else {
+                    resetTrigger(0);
+                    mCurrentAfState = ANDROID_CONTROL_AF_STATE_FOCUSED_LOCKED;
+                    LOG2("@%s AF state FOCUSED_LOCKED", __PRETTY_FUNCTION__);
+                }
+                break;
+            case icamera::AF_STATE_FAIL:
+                if (mLastActiveTriggerTime == 0) {
+                    mCurrentAfState = ANDROID_CONTROL_AF_STATE_PASSIVE_UNFOCUSED;
+                    LOG2("@%s AF state PASSIVE_UNFOCUSED", __PRETTY_FUNCTION__);
+                } else {
+                    resetTrigger(0);
+                    mCurrentAfState = ANDROID_CONTROL_AF_STATE_NOT_FOCUSED_LOCKED;
+                    LOG2("@%s AF state NOT_FOCUSED_LOCKED", __PRETTY_FUNCTION__);
+                }
+                break;
+            default:
+            case icamera::AF_STATE_IDLE:
+                if (mCurrentAfState == ANDROID_CONTROL_AF_STATE_INACTIVE) {
+                    mCurrentAfState = ANDROID_CONTROL_AF_STATE_PASSIVE_UNFOCUSED;
+                    LOG2("@%s AF state PASSIVE_UNFOCUSED (idle)", __PRETTY_FUNCTION__);
+                }
+                break;
+        }
+    }
+
+    checkIfFocusTimeout();
+
+    updateResult(result);
+
+    return icamera::OK;
+}
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/IntelAFStateMachine.h b/camera/hal/intel/ipu6/aal/IntelAFStateMachine.h
new file mode 100644
index 000000000000..a8eaac9a18d1
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/IntelAFStateMachine.h
@@ -0,0 +1,142 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "HALv3Header.h"
+#include "Utils.h"
+
+namespace camera3 {
+
+typedef int64_t usecs_t;
+
+/**
+ * \struct AfControls
+ *
+ * Control Modes saved and passed back to control unit after reading
+ *
+ */
+struct AfControls {
+    uint8_t afMode;    /**< AF_MODE */
+    uint8_t afTrigger; /**< AF_TRIGGER */
+};
+
+/**
+ * \class IntelAfModeBase
+ *
+ * Base class for all the AutoFocus modes as defined by the Android
+ * camera device V3.x API.
+ * Each mode will follow certain state transitions. See documentation for
+ * android.control.afState
+ *
+ */
+class IntelAfModeBase {
+ public:
+    IntelAfModeBase();
+    virtual ~IntelAfModeBase(){};
+
+    virtual int processTriggers(uint8_t afTrigger, uint8_t afMode) = 0;
+    virtual int processResult(int afState, bool lensMoving, android::CameraMetadata* result) = 0;
+
+    void resetState(void);
+    void resetTrigger(usecs_t triggerTime);
+    int getState() { return mCurrentAfState; }
+    void updateResult(android::CameraMetadata* results);
+
+ protected:
+    void checkIfFocusTimeout();
+
+ protected:
+    AfControls mLastAfControls;
+    uint8_t mCurrentAfState;
+    uint8_t mLensState;
+    usecs_t mLastActiveTriggerTime; /**< in useconds */
+    uint32_t mFramesSinceTrigger;
+};
+
+/**
+ * \class IntelAFModeAuto
+ * Derived class from IntelAFModeBase for Auto mode
+ *
+ */
+class IntelAFModeAuto : public IntelAfModeBase {
+ public:
+    IntelAFModeAuto();
+    virtual int processTriggers(uint8_t afTrigger, uint8_t afMode);
+    virtual int processResult(int afState, bool lensMoving, android::CameraMetadata* result);
+};
+
+/**
+ * \class IntelAFModeContinuousPicture
+ * Derived class from IntelAFModeBase for Continuous AF mode
+ *
+ */
+class IntelAFModeContinuousPicture : public IntelAfModeBase {
+ public:
+    IntelAFModeContinuousPicture();
+    virtual int processTriggers(uint8_t afTrigger, uint8_t afMode);
+    virtual int processResult(int afState, bool lensMoving, android::CameraMetadata* result);
+};
+
+/**
+ * \class IntelAFModeOff
+ * Derived class from IntelAFModeBase for OFF mode
+ *
+ */
+class IntelAFModeOff : public IntelAfModeBase {
+ public:
+    IntelAFModeOff();
+    virtual int processTriggers(uint8_t afTrigger, uint8_t afMode);
+    virtual int processResult(int afState, bool lensMoving, android::CameraMetadata* result);
+};
+
+/**
+ * \class IntelAFStateMachine
+ *
+ * This class adapts the Android V3 AF triggers and state transitions to
+ * the ones implemented by the Intel AIQ algorithm
+ * This class is platform independent. Platform specific behaviors should be
+ * implemented in derived classes from this one or from the IntelAFModeBase
+ *
+ */
+class IntelAFStateMachine {
+ public:
+    IntelAFStateMachine(int cameraId);
+    virtual ~IntelAFStateMachine();
+
+    int processTriggers(uint8_t afTrigger, uint8_t afMode);
+    int processResult(int afState, bool lensMoving, android::CameraMetadata* result);
+
+    int updateDefaults(android::CameraMetadata* result, bool fixedFocus = false) const;
+
+ private:
+    // prevent copy constructor and assignment operator
+    DISALLOW_COPY_AND_ASSIGN(IntelAFStateMachine);
+
+ private: /* members*/
+    int mCameraId;
+    AfControls mLastAfControls;
+    IntelAfModeBase* mCurrentAfMode;
+
+    std::vector<uint8_t> mAvailableModes;
+
+    IntelAFModeOff mOffMode;
+    IntelAFModeAuto mAutoMode;
+
+    IntelAFModeContinuousPicture mContinuousPictureMode;
+};
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/IntelAWBStateMachine.cpp b/camera/hal/intel/ipu6/aal/IntelAWBStateMachine.cpp
new file mode 100644
index 000000000000..2f87680f5336
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/IntelAWBStateMachine.cpp
@@ -0,0 +1,214 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelAWBStateMachine"
+
+#include "IntelAWBStateMachine.h"
+
+#include "Errors.h"
+#include "HALv3Utils.h"
+#include "Utils.h"
+
+namespace camera3 {
+
+IntelAWBStateMachine::IntelAWBStateMachine(int aCameraId)
+        : mCameraId(aCameraId),
+          mLastControlMode(0),
+          mLastSceneMode(0),
+          mCurrentAwbMode(NULL) {
+    LOG1("%s mCameraId %d", __func__, mCameraId);
+    mCurrentAwbMode = &mAutoMode;
+    CLEAR(mLastAwbControls);
+    mLastAwbControls.awbMode = ANDROID_CONTROL_AWB_MODE_AUTO;
+}
+
+IntelAWBStateMachine::~IntelAWBStateMachine() {
+    LOG1("%s mCameraId %d", __func__, mCameraId);
+}
+
+int IntelAWBStateMachine::processState(uint8_t controlMode, uint8_t sceneMode,
+                                       const AwbControls& awbControls) {
+    if (controlMode == ANDROID_CONTROL_MODE_OFF) {
+        mCurrentAwbMode = &mOffMode;
+
+        if (controlMode != mLastControlMode)
+            LOG1("%s: Set AWB offMode: controlMode = %d, awbMode = %d", __func__, controlMode,
+                 awbControls.awbMode);
+    } else {
+        if (awbControls.awbMode == ANDROID_CONTROL_AWB_MODE_OFF) {
+            mCurrentAwbMode = &mOffMode;
+            if (awbControls.awbMode != mLastAwbControls.awbMode)
+                LOG1("%s: Set AWB offMode: controlMode = %d, awbMode = %d", __func__, controlMode,
+                     awbControls.awbMode);
+        } else {
+            mCurrentAwbMode = &mAutoMode;
+            if (awbControls.awbMode != mLastAwbControls.awbMode)
+                LOG1("%s: Set AWB autoMode: controlMode = %d, awbMode = %d", __func__, controlMode,
+                     awbControls.awbMode);
+        }
+    }
+
+    mLastAwbControls = awbControls;
+    mLastSceneMode = sceneMode;
+    mLastControlMode = controlMode;
+    return mCurrentAwbMode->processState(controlMode, sceneMode, awbControls);
+}
+
+int IntelAWBStateMachine::processResult(bool converged, android::CameraMetadata* result) {
+    CheckError(!mCurrentAwbMode, icamera::UNKNOWN_ERROR, "Invalid AWB mode");
+    CheckError(!result, icamera::UNKNOWN_ERROR, "%s, result is nullptr", __func__);
+
+    return mCurrentAwbMode->processResult(converged, result);
+}
+
+/******************************************************************************
+ * AWB MODE   -  BASE
+ ******************************************************************************/
+IntelAWBModeBase::IntelAWBModeBase()
+        : mLastControlMode(0),
+          mLastSceneMode(0),
+          mCurrentAwbState(ANDROID_CONTROL_AWB_STATE_INACTIVE) {
+    LOG1("%s", __func__);
+
+    CLEAR(mLastAwbControls);
+}
+
+void IntelAWBModeBase::updateResult(android::CameraMetadata* results) {
+    CheckError(!results, VOID_VALUE, "%s, result is nullptr", __func__);
+    LOG2("%s: current AWB state is: %d", __func__, mCurrentAwbState);
+
+    //# METADATA_Dynamic control.awbMode done
+    results->update(ANDROID_CONTROL_AWB_MODE, &mLastAwbControls.awbMode, 1);
+    //# METADATA_Dynamic control.awbLock done
+    results->update(ANDROID_CONTROL_AWB_LOCK, &mLastAwbControls.awbLock, 1);
+    //# METADATA_Dynamic control.awbState done
+    results->update(ANDROID_CONTROL_AWB_STATE, &mCurrentAwbState, 1);
+}
+
+void IntelAWBModeBase::resetState() {
+    LOG2("%s", __func__);
+
+    mCurrentAwbState = ANDROID_CONTROL_AWB_STATE_INACTIVE;
+}
+
+/******************************************************************************
+ * AWB MODE   -  OFF
+ ******************************************************************************/
+
+IntelAWBModeOff::IntelAWBModeOff() : IntelAWBModeBase() {
+    LOG1("%s", __func__);
+}
+
+int IntelAWBModeOff::processState(uint8_t controlMode, uint8_t sceneMode,
+                                  const AwbControls& awbControls) {
+    LOG2("%s", __func__);
+
+    int ret = icamera::OK;
+
+    mLastAwbControls = awbControls;
+    mLastSceneMode = sceneMode;
+    mLastControlMode = controlMode;
+
+    if (controlMode == ANDROID_CONTROL_MODE_OFF ||
+        awbControls.awbMode == ANDROID_CONTROL_AWB_MODE_OFF) {
+        resetState();
+    } else {
+        LOGE("AWB State machine should not be OFF! - Fix bug");
+        ret = icamera::UNKNOWN_ERROR;
+    }
+
+    return ret;
+}
+
+int IntelAWBModeOff::processResult(bool converged, android::CameraMetadata* result) {
+    UNUSED(converged);
+    CheckError(!result, icamera::UNKNOWN_ERROR, "%s, result is nullptr", __func__);
+    LOG2("%s", __func__);
+
+    mCurrentAwbState = ANDROID_CONTROL_AWB_STATE_INACTIVE;
+    updateResult(result);
+
+    return icamera::OK;
+}
+
+/******************************************************************************
+ * AWB MODE   -  AUTO
+ ******************************************************************************/
+
+IntelAWBModeAuto::IntelAWBModeAuto() : IntelAWBModeBase() {
+    LOG1("%s", __func__);
+}
+
+int IntelAWBModeAuto::processState(uint8_t controlMode, uint8_t sceneMode,
+                                   const AwbControls& awbControls) {
+    if (controlMode != mLastControlMode) {
+        LOG1("%s: control mode has changed %d -> %d, reset AWB State", __func__, mLastControlMode,
+             controlMode);
+        resetState();
+    }
+
+    if (awbControls.awbLock == ANDROID_CONTROL_AWB_LOCK_ON) {
+        mCurrentAwbState = ANDROID_CONTROL_AWB_STATE_LOCKED;
+    } else if (awbControls.awbMode != mLastAwbControls.awbMode ||
+               (controlMode == ANDROID_CONTROL_MODE_USE_SCENE_MODE &&
+                sceneMode != mLastSceneMode)) {
+        resetState();
+    } else {
+        switch (mCurrentAwbState) {
+            case ANDROID_CONTROL_AWB_STATE_LOCKED:
+                mCurrentAwbState = ANDROID_CONTROL_AWB_STATE_INACTIVE;
+                break;
+            case ANDROID_CONTROL_AWB_STATE_INACTIVE:
+            case ANDROID_CONTROL_AWB_STATE_SEARCHING:
+            case ANDROID_CONTROL_AWB_STATE_CONVERGED:
+                // do nothing
+                break;
+            default:
+                LOGE("Invalid AWB state!, State set to INACTIVE");
+                mCurrentAwbState = ANDROID_CONTROL_AWB_STATE_INACTIVE;
+        }
+    }
+    mLastAwbControls = awbControls;
+    mLastSceneMode = sceneMode;
+    mLastControlMode = controlMode;
+    return icamera::OK;
+}
+
+int IntelAWBModeAuto::processResult(bool converged, android::CameraMetadata* result) {
+    CheckError(!result, icamera::UNKNOWN_ERROR, "%s, result is nullptr", __func__);
+    switch (mCurrentAwbState) {
+        case ANDROID_CONTROL_AWB_STATE_LOCKED:
+            // do nothing
+            break;
+        case ANDROID_CONTROL_AWB_STATE_INACTIVE:
+        case ANDROID_CONTROL_AWB_STATE_SEARCHING:
+        case ANDROID_CONTROL_AWB_STATE_CONVERGED:
+            if (converged)
+                mCurrentAwbState = ANDROID_CONTROL_AWB_STATE_CONVERGED;
+            else
+                mCurrentAwbState = ANDROID_CONTROL_AWB_STATE_SEARCHING;
+            break;
+        default:
+            LOGE("invalid AWB state!, State set to INACTIVE");
+            mCurrentAwbState = ANDROID_CONTROL_AWB_STATE_INACTIVE;
+    }
+
+    updateResult(result);
+
+    return icamera::OK;
+}
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/IntelAWBStateMachine.h b/camera/hal/intel/ipu6/aal/IntelAWBStateMachine.h
new file mode 100644
index 000000000000..2f7cf7c3b5fe
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/IntelAWBStateMachine.h
@@ -0,0 +1,134 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "HALv3Header.h"
+#include "Utils.h"
+
+namespace camera3 {
+
+/**
+ * \struct AwbControls
+ *
+ * Control Modes saved and passed back to control unit after reading
+ *
+ */
+struct AwbControls {
+    uint8_t awbMode;                       /**< AWB_MODE */
+    uint8_t awbLock;                       /**< AWB_LOCK */
+    uint8_t colorCorrectionMode;           /**< COLOR_CORRECTION_MODE */
+    uint8_t colorCorrectionAberrationMode; /**< COLOR_CORRECTION_ABERRATION_MODE */
+};
+
+/**
+ * \class IntelAWBModeBase
+ *
+ * Base class for all the Auto white balance modes as defined by the Android
+ * camera device V3.x API.
+ * Each mode will follow certain state transitions. See documentation for
+ * android.control.awbState
+ *
+ */
+class IntelAWBModeBase {
+ public:
+    IntelAWBModeBase();
+    virtual ~IntelAWBModeBase(){};
+
+    virtual int processState(uint8_t controlMode, uint8_t sceneMode,
+                             const AwbControls& awbControls) = 0;
+
+    virtual int processResult(bool converged, android::CameraMetadata* results) = 0;
+
+    void resetState(void);
+    uint8_t getState() const { return mCurrentAwbState; }
+
+ protected:
+    void updateResult(android::CameraMetadata* results);
+
+ protected:
+    AwbControls mLastAwbControls;
+    uint8_t mLastControlMode;
+    uint8_t mLastSceneMode;
+
+    uint8_t mCurrentAwbState;
+};
+
+/**
+ * \class IntelAWBModeAuto
+ * Derived class from IntelAWBModeBase for Auto mode
+ *
+ */
+class IntelAWBModeAuto : public IntelAWBModeBase {
+ public:
+    IntelAWBModeAuto();
+    virtual int processState(uint8_t controlMode, uint8_t sceneMode,
+                             const AwbControls& awbControls);
+
+    virtual int processResult(bool converged, android::CameraMetadata* results);
+};
+
+/**
+ * \class IntelAWBModeOFF
+ * Derived class from IntelAWBModeBase for OFF mode
+ *
+ */
+class IntelAWBModeOff : public IntelAWBModeBase {
+ public:
+    IntelAWBModeOff();
+    virtual int processState(uint8_t controlMode, uint8_t sceneMode,
+                             const AwbControls& awbControls);
+
+    virtual int processResult(bool converged, android::CameraMetadata* results);
+};
+
+/**
+ * \class IntelAWBStateMachine
+ *
+ * This class adapts the Android V3 AWB triggers and state transitions to
+ * the ones implemented by the Intel AIQ algorithm
+ * This class is platform independent. Platform specific behaviors should be
+ * implemented in derived classes from this one or from the IntelAWBModeBase
+ *
+ */
+class IntelAWBStateMachine {
+ public:
+    IntelAWBStateMachine(int CameraId);
+    virtual ~IntelAWBStateMachine();
+
+    int processState(uint8_t controlMode, uint8_t sceneMode, const AwbControls& awbControls);
+
+    int processResult(bool converged, android::CameraMetadata* results);
+
+    uint8_t getState() const { return mCurrentAwbMode->getState(); }
+
+ private:
+    // prevent copy constructor and assignment operator
+    DISALLOW_COPY_AND_ASSIGN(IntelAWBStateMachine);
+
+ private: /* members*/
+    int mCameraId;
+    AwbControls mLastAwbControls;
+    uint8_t mLastControlMode;
+    uint8_t mLastSceneMode;
+
+    IntelAWBModeBase* mCurrentAwbMode;
+
+    IntelAWBModeOff mOffMode;
+    IntelAWBModeAuto mAutoMode;
+};
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/MetadataConvert.cpp b/camera/hal/intel/ipu6/aal/MetadataConvert.cpp
new file mode 100644
index 000000000000..8315102ddd60
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/MetadataConvert.cpp
@@ -0,0 +1,2247 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "MetadataConvert"
+
+#include "MetadataConvert.h"
+
+#include <cmath>
+#include <sstream>
+#include <unordered_map>
+
+#include "Errors.h"
+#include "HALv3Utils.h"
+#include "ICamera.h"
+#include "Utils.h"
+
+namespace camera3 {
+
+#define NSEC_PER_SEC 1000000000LLU
+#define DEFAULT_FPS_RANGE_MIN 15
+#define DEFAULT_FPS_RANGE_MAX 30
+
+template <typename T>
+struct ValuePair {
+    int halValue;
+    T androidValue;
+};
+
+template <typename T>
+static int getAndroidValue(int halValue, const ValuePair<T>* table, int tableCount,
+                           T* androidValue) {
+    CheckError(!table, icamera::BAD_VALUE, "null table!");
+    CheckError(!androidValue, icamera::BAD_VALUE, "androidValue is nullptr!");
+
+    for (int i = 0; i < tableCount; i++) {
+        if (halValue == table[i].halValue) {
+            *androidValue = table[i].androidValue;
+            return icamera::OK;
+        }
+    }
+    return icamera::BAD_VALUE;
+}
+
+template <typename T>
+static int getHalValue(T androidValue, const ValuePair<T>* table, int tableCount, int* halValue) {
+    CheckError(!table, icamera::BAD_VALUE, "null table!");
+    CheckError(!halValue, icamera::BAD_VALUE, "halValue is nullptr!");
+
+    for (int i = 0; i < tableCount; i++) {
+        if (androidValue == table[i].androidValue) {
+            *halValue = table[i].halValue;
+            return icamera::OK;
+        }
+    }
+    return icamera::BAD_VALUE;
+}
+
+static const ValuePair<int32_t> testPatternTable[] = {
+    {icamera::TEST_PATTERN_OFF, ANDROID_SENSOR_TEST_PATTERN_MODE_OFF},
+    {icamera::SOLID_COLOR, ANDROID_SENSOR_TEST_PATTERN_MODE_SOLID_COLOR},
+    {icamera::COLOR_BARS, ANDROID_SENSOR_TEST_PATTERN_MODE_COLOR_BARS},
+    {icamera::COLOR_BARS_FADE_TO_GRAY, ANDROID_SENSOR_TEST_PATTERN_MODE_COLOR_BARS_FADE_TO_GRAY},
+    {icamera::PN9, ANDROID_SENSOR_TEST_PATTERN_MODE_PN9},
+    {icamera::TEST_PATTERN_CUSTOM1, ANDROID_SENSOR_TEST_PATTERN_MODE_CUSTOM1},
+};
+
+static const ValuePair<uint8_t> antibandingModesTable[] = {
+    {icamera::ANTIBANDING_MODE_AUTO, ANDROID_CONTROL_AE_ANTIBANDING_MODE_AUTO},
+    {icamera::ANTIBANDING_MODE_50HZ, ANDROID_CONTROL_AE_ANTIBANDING_MODE_50HZ},
+    {icamera::ANTIBANDING_MODE_60HZ, ANDROID_CONTROL_AE_ANTIBANDING_MODE_60HZ},
+    {icamera::ANTIBANDING_MODE_OFF, ANDROID_CONTROL_AE_ANTIBANDING_MODE_OFF},
+};
+
+static const ValuePair<uint8_t> aeModesTable[] = {
+    {icamera::AE_MODE_AUTO, ANDROID_CONTROL_AE_MODE_ON},
+    {icamera::AE_MODE_MANUAL, ANDROID_CONTROL_AE_MODE_OFF},
+};
+
+static const ValuePair<uint8_t> awbModesTable[] = {
+    {icamera::AWB_MODE_AUTO, ANDROID_CONTROL_AWB_MODE_AUTO},
+    {icamera::AWB_MODE_INCANDESCENT, ANDROID_CONTROL_AWB_MODE_INCANDESCENT},
+    {icamera::AWB_MODE_FLUORESCENT, ANDROID_CONTROL_AWB_MODE_FLUORESCENT},
+    {icamera::AWB_MODE_DAYLIGHT, ANDROID_CONTROL_AWB_MODE_DAYLIGHT},
+    {icamera::AWB_MODE_FULL_OVERCAST, ANDROID_CONTROL_AWB_MODE_TWILIGHT},
+    {icamera::AWB_MODE_PARTLY_OVERCAST, ANDROID_CONTROL_AWB_MODE_CLOUDY_DAYLIGHT},
+    {icamera::AWB_MODE_MANUAL_COLOR_TRANSFORM, ANDROID_CONTROL_AWB_MODE_OFF},
+};
+
+static const ValuePair<uint8_t> afModesTable[] = {
+    {icamera::AF_MODE_OFF, ANDROID_CONTROL_AF_MODE_OFF},
+    {icamera::AF_MODE_AUTO, ANDROID_CONTROL_AF_MODE_AUTO},
+    {icamera::AF_MODE_MACRO, ANDROID_CONTROL_AF_MODE_MACRO},
+    {icamera::AF_MODE_CONTINUOUS_VIDEO, ANDROID_CONTROL_AF_MODE_CONTINUOUS_VIDEO},
+    {icamera::AF_MODE_CONTINUOUS_PICTURE, ANDROID_CONTROL_AF_MODE_CONTINUOUS_PICTURE},
+};
+
+static const ValuePair<uint8_t> afTriggerTable[] = {
+    {icamera::AF_TRIGGER_START, ANDROID_CONTROL_AF_TRIGGER_START},
+    {icamera::AF_TRIGGER_CANCEL, ANDROID_CONTROL_AF_TRIGGER_CANCEL},
+    {icamera::AF_TRIGGER_IDLE, ANDROID_CONTROL_AF_TRIGGER_IDLE},
+};
+
+static const ValuePair<uint8_t> dvsModesTable[] = {
+    {icamera::VIDEO_STABILIZATION_MODE_OFF, ANDROID_CONTROL_VIDEO_STABILIZATION_MODE_OFF},
+    {icamera::VIDEO_STABILIZATION_MODE_ON, ANDROID_CONTROL_VIDEO_STABILIZATION_MODE_ON},
+};
+
+static const ValuePair<uint8_t> effectModesTable[] = {
+    {icamera::CAM_EFFECT_NONE, ANDROID_CONTROL_EFFECT_MODE_OFF},
+    {icamera::CAM_EFFECT_MONO, ANDROID_CONTROL_EFFECT_MODE_MONO},
+    {icamera::CAM_EFFECT_SEPIA, ANDROID_CONTROL_EFFECT_MODE_SEPIA},
+    {icamera::CAM_EFFECT_NEGATIVE, ANDROID_CONTROL_EFFECT_MODE_NEGATIVE},
+};
+
+static const ValuePair<uint8_t> shadingModeTable[] = {
+    {icamera::SHADING_MODE_OFF, ANDROID_SHADING_MODE_OFF},
+    {icamera::SHADING_MODE_FAST, ANDROID_SHADING_MODE_FAST},
+    {icamera::SHADING_MODE_HIGH_QUALITY, ANDROID_SHADING_MODE_HIGH_QUALITY},
+};
+
+static const ValuePair<uint8_t> lensShadingMapModeTable[] = {
+    {icamera::LENS_SHADING_MAP_MODE_OFF, ANDROID_STATISTICS_LENS_SHADING_MAP_MODE_OFF},
+    {icamera::LENS_SHADING_MAP_MODE_ON, ANDROID_STATISTICS_LENS_SHADING_MAP_MODE_ON},
+};
+
+static const ValuePair<uint8_t> tonemapModesTable[] = {
+    {icamera::TONEMAP_MODE_FAST, ANDROID_TONEMAP_MODE_FAST},
+    {icamera::TONEMAP_MODE_HIGH_QUALITY, ANDROID_TONEMAP_MODE_HIGH_QUALITY},
+    {icamera::TONEMAP_MODE_GAMMA_VALUE, ANDROID_TONEMAP_MODE_GAMMA_VALUE},
+    {icamera::TONEMAP_MODE_PRESET_CURVE, ANDROID_TONEMAP_MODE_PRESET_CURVE},
+};
+
+static const ValuePair<uint8_t> tonemapPresetCurvesTable[] = {
+    {icamera::TONEMAP_PRESET_CURVE_SRGB, ANDROID_TONEMAP_PRESET_CURVE_SRGB},
+    {icamera::TONEMAP_PRESET_CURVE_REC709, ANDROID_TONEMAP_PRESET_CURVE_REC709},
+};
+
+static bool isValueSupported(uint8_t mode, const icamera::CameraMetadata* caps, uint32_t tag) {
+    icamera_metadata_ro_entry entry = caps->find(tag);
+    if (entry.count > 0) {
+        for (size_t i = 0; i < entry.count; i++) {
+            if (mode == entry.data.u8[i]) return true;
+        }
+    }
+    return false;
+}
+
+MetadataConvert::MetadataConvert(int cameraId) : mCameraId(cameraId) {
+    LOG1("@%s, mCameraId %d", __func__, mCameraId);
+}
+
+MetadataConvert::~MetadataConvert() {
+    LOG1("@%s", __func__);
+}
+
+int MetadataConvert::constructDefaultMetadata(int cameraId, android::CameraMetadata* settings) {
+    LOG1("@%s", __func__);
+    const icamera::CameraMetadata* meta = StaticCapability::getInstance(cameraId)->getCapability();
+
+    // CAMERA_CONTROL_MAX_REGIONS: [AE, AWB, AF]
+    uint32_t tag = CAMERA_CONTROL_MAX_REGIONS;
+    icamera_metadata_ro_entry roEntry = meta->find(tag);
+    int32_t maxRegionAf = 0, maxRegionAe = 0;
+    if (roEntry.count == 3) {
+        maxRegionAe = roEntry.data.i32[0];
+        maxRegionAf = roEntry.data.i32[2];
+    }
+
+    // AE, AF region (AWB region is not supported)
+    int meteringRegion[5] = {0, 0, 0, 0, 0};
+    if (maxRegionAe) {
+        settings->update(ANDROID_CONTROL_AE_REGIONS, meteringRegion, 5);
+    }
+    if (maxRegionAf) {
+        settings->update(ANDROID_CONTROL_AF_REGIONS, meteringRegion, 5);
+    }
+
+    // Control AE, AF, AWB
+    uint8_t mode = ANDROID_CONTROL_AE_ANTIBANDING_MODE_AUTO;
+    settings->update(ANDROID_CONTROL_AE_ANTIBANDING_MODE, &mode, 1);
+    int32_t ev = 0;
+    settings->update(ANDROID_CONTROL_AE_EXPOSURE_COMPENSATION, &ev, 1);
+    uint8_t lock = ANDROID_CONTROL_AE_LOCK_OFF;
+    settings->update(ANDROID_CONTROL_AE_LOCK, &lock, 1);
+    mode = ANDROID_CONTROL_AE_MODE_ON;
+    settings->update(ANDROID_CONTROL_AE_MODE, &mode, 1);
+    mode = ANDROID_CONTROL_AE_PRECAPTURE_TRIGGER_IDLE;
+    settings->update(ANDROID_CONTROL_AE_PRECAPTURE_TRIGGER, &mode, 1);
+    mode = ANDROID_CONTROL_AE_STATE_INACTIVE;
+    settings->update(ANDROID_CONTROL_AE_STATE, &mode, 1);
+
+    mode = ANDROID_CONTROL_AF_MODE_OFF;
+    settings->update(ANDROID_CONTROL_AF_MODE, &mode, 1);
+    mode = ANDROID_CONTROL_AF_TRIGGER_IDLE;
+    settings->update(ANDROID_CONTROL_AF_TRIGGER, &mode, 1);
+    mode = ANDROID_CONTROL_AF_STATE_INACTIVE;
+    settings->update(ANDROID_CONTROL_AF_STATE, &mode, 1);
+
+    lock = ANDROID_CONTROL_AWB_LOCK_OFF;
+    settings->update(ANDROID_CONTROL_AWB_LOCK, &lock, 1);
+    mode = ANDROID_CONTROL_AWB_MODE_AUTO;
+    settings->update(ANDROID_CONTROL_AWB_MODE, &mode, 1);
+    mode = ANDROID_CONTROL_AWB_STATE_INACTIVE;
+    settings->update(ANDROID_CONTROL_AWB_STATE, &mode, 1);
+
+    // Control others
+    mode = ANDROID_CONTROL_CAPTURE_INTENT_PREVIEW;
+    settings->update(ANDROID_CONTROL_CAPTURE_INTENT, &mode, 1);
+    mode = ANDROID_CONTROL_EFFECT_MODE_OFF;
+    settings->update(ANDROID_CONTROL_EFFECT_MODE, &mode, 1);
+    mode = ANDROID_CONTROL_MODE_AUTO;
+    settings->update(ANDROID_CONTROL_MODE, &mode, 1);
+    mode = ANDROID_CONTROL_SCENE_MODE_DISABLED;
+    settings->update(ANDROID_CONTROL_SCENE_MODE, &mode, 1);
+    mode = ANDROID_CONTROL_VIDEO_STABILIZATION_MODE_OFF;
+    settings->update(ANDROID_CONTROL_VIDEO_STABILIZATION_MODE, &mode, 1);
+
+    // Flash
+    mode = ANDROID_FLASH_MODE_OFF;
+    settings->update(ANDROID_FLASH_MODE, &mode, 1);
+
+    mode = ANDROID_FLASH_STATE_UNAVAILABLE;
+    tag = CAMERA_FLASH_INFO_AVAILABLE;
+    roEntry = meta->find(tag);
+    if (roEntry.count == 1 && roEntry.data.u8[0] == CAMERA_FLASH_INFO_AVAILABLE_TRUE) {
+        mode = ANDROID_FLASH_STATE_READY;
+    }
+    settings->update(ANDROID_FLASH_STATE, &mode, 1);
+
+    // Black level
+    lock = ANDROID_BLACK_LEVEL_LOCK_OFF;
+    settings->update(ANDROID_BLACK_LEVEL_LOCK, &lock, 1);
+
+    // Lens
+    camera_metadata_entry entry = settings->find(ANDROID_LENS_INFO_AVAILABLE_APERTURES);
+    if (entry.count >= 1) {
+        settings->update(ANDROID_LENS_APERTURE, &entry.data.f[0], 1);
+    }
+    entry = settings->find(CAMERA_LENS_INFO_AVAILABLE_FOCAL_LENGTHS);
+    if (entry.count >= 1) {
+        settings->update(ANDROID_LENS_FOCAL_LENGTH, &entry.data.f[0], 1);
+    }
+    entry = settings->find(CAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE);
+    if (entry.count == 1) {
+        settings->update(ANDROID_LENS_FOCUS_DISTANCE, &entry.data.f[0], 1);
+    }
+
+    float filterDensity = 0.0f;
+    settings->update(ANDROID_LENS_FILTER_DENSITY, &filterDensity, 1);
+    mode = ANDROID_LENS_OPTICAL_STABILIZATION_MODE_OFF;
+    settings->update(ANDROID_LENS_OPTICAL_STABILIZATION_MODE, &mode, 1);
+
+    int64_t value_i64 = 0;
+    settings->update(ANDROID_SENSOR_ROLLING_SHUTTER_SKEW, &value_i64, 1);
+
+    // Sync
+    int64_t frameNumber = ANDROID_SYNC_FRAME_NUMBER_UNKNOWN;
+    settings->update(ANDROID_SYNC_FRAME_NUMBER, &frameNumber, 1);
+
+    // Request
+    mode = ANDROID_REQUEST_TYPE_CAPTURE;
+    settings->update(ANDROID_REQUEST_TYPE, &mode, 1);
+    mode = ANDROID_REQUEST_METADATA_MODE_NONE;
+    settings->update(ANDROID_REQUEST_METADATA_MODE, &mode, 1);
+
+    // Scale
+    int32_t region[] = {0, 0, 0, 0};
+    settings->update(ANDROID_SCALER_CROP_REGION, region, 4);
+
+    // Statistics
+    mode = ANDROID_STATISTICS_FACE_DETECT_MODE_OFF;
+    settings->update(ANDROID_STATISTICS_FACE_DETECT_MODE, &mode, 1);
+    mode = ANDROID_STATISTICS_HOT_PIXEL_MAP_MODE_OFF;
+    settings->update(ANDROID_STATISTICS_HOT_PIXEL_MAP_MODE, &mode, 1);
+    mode = ANDROID_STATISTICS_LENS_SHADING_MAP_MODE_OFF;
+    settings->update(ANDROID_STATISTICS_LENS_SHADING_MAP_MODE, &mode, 1);
+    mode = ANDROID_STATISTICS_SCENE_FLICKER_NONE;
+    settings->update(ANDROID_STATISTICS_SCENE_FLICKER, &mode, 1);
+
+    // Tonemap
+    mode = ANDROID_TONEMAP_MODE_FAST;
+    settings->update(ANDROID_TONEMAP_MODE, &mode, 1);
+
+    // Sensor
+    value_i64 = 0;
+    settings->update(ANDROID_SENSOR_EXPOSURE_TIME, &value_i64, 1);
+    int32_t sensitivity = 0;
+    settings->update(ANDROID_SENSOR_SENSITIVITY, &sensitivity, 1);
+    int64_t frameDuration = 33000000;
+    settings->update(ANDROID_SENSOR_FRAME_DURATION, &frameDuration, 1);
+    int32_t testPattern = ANDROID_SENSOR_TEST_PATTERN_MODE_OFF;
+    settings->update(ANDROID_SENSOR_TEST_PATTERN_MODE, &testPattern, 1);
+
+    // Jpeg
+    uint8_t quality = 95;
+    settings->update(ANDROID_JPEG_QUALITY, &quality, 1);
+    quality = 90;
+    settings->update(ANDROID_JPEG_THUMBNAIL_QUALITY, &quality, 1);
+
+    entry = settings->find(ANDROID_JPEG_AVAILABLE_THUMBNAIL_SIZES);
+    int32_t thumbSize[] = {0, 0};
+    if (entry.count >= 4) {
+        thumbSize[0] = entry.data.i32[2];
+        thumbSize[1] = entry.data.i32[3];
+    } else {
+        LOGE("Thumbnail size should have more than 2 resolutions");
+    }
+    settings->update(ANDROID_JPEG_THUMBNAIL_SIZE, thumbSize, 2);
+
+    entry = settings->find(ANDROID_TONEMAP_AVAILABLE_TONE_MAP_MODES);
+    if (entry.count > 0) {
+        mode = entry.data.u8[0];
+        for (uint32_t i = 0; i < entry.count; i++) {
+            if (entry.data.u8[i] == ANDROID_TONEMAP_MODE_HIGH_QUALITY) {
+                mode = ANDROID_TONEMAP_MODE_HIGH_QUALITY;
+                break;
+            }
+        }
+        settings->update(ANDROID_TONEMAP_MODE, &mode, 1);
+    }
+
+    // Color correction
+    mode = ANDROID_COLOR_CORRECTION_MODE_FAST;
+    settings->update(ANDROID_COLOR_CORRECTION_MODE, &mode, 1);
+
+    float colorTransform[9] = {1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0};
+    camera_metadata_rational_t transformMatrix[9];
+    for (int i = 0; i < 9; i++) {
+        transformMatrix[i].numerator = colorTransform[i];
+        transformMatrix[i].denominator = 1.0;
+    }
+    settings->update(ANDROID_COLOR_CORRECTION_TRANSFORM, transformMatrix, 9);
+
+    float colorGains[4] = {1.0, 1.0, 1.0, 1.0};
+    settings->update(ANDROID_COLOR_CORRECTION_GAINS, colorGains, 4);
+
+    mode = ANDROID_COLOR_CORRECTION_ABERRATION_MODE_OFF;
+    settings->update(ANDROID_COLOR_CORRECTION_ABERRATION_MODE, &mode, 1);
+
+    return icamera::OK;
+}
+
+int MetadataConvert::updateDefaultRequestSettings(int32_t cameraId, int type,
+                                                  android::CameraMetadata* settings) {
+    const icamera::CameraMetadata* caps = StaticCapability::getInstance(cameraId)->getCapability();
+
+    uint8_t intent =
+        (type == CAMERA3_TEMPLATE_PREVIEW)          ? ANDROID_CONTROL_CAPTURE_INTENT_PREVIEW
+        : (type == CAMERA3_TEMPLATE_STILL_CAPTURE)  ? ANDROID_CONTROL_CAPTURE_INTENT_STILL_CAPTURE
+        : (type == CAMERA3_TEMPLATE_VIDEO_RECORD)   ? ANDROID_CONTROL_CAPTURE_INTENT_VIDEO_RECORD
+        : (type == CAMERA3_TEMPLATE_VIDEO_SNAPSHOT) ? ANDROID_CONTROL_CAPTURE_INTENT_VIDEO_SNAPSHOT
+        : (type == CAMERA3_TEMPLATE_ZERO_SHUTTER_LAG)
+            ? ANDROID_CONTROL_CAPTURE_INTENT_ZERO_SHUTTER_LAG
+        : (type == CAMERA3_TEMPLATE_MANUAL) ? ANDROID_CONTROL_CAPTURE_INTENT_MANUAL
+                                            : ANDROID_CONTROL_CAPTURE_INTENT_CUSTOM;
+    settings->update(ANDROID_CONTROL_CAPTURE_INTENT, &intent, 1);
+
+    uint8_t ctrlMode = ANDROID_CONTROL_MODE_AUTO;
+    uint8_t aeMode = ANDROID_CONTROL_AE_MODE_ON;
+    uint8_t awbMode = ANDROID_CONTROL_AWB_MODE_AUTO;
+    uint8_t afMode = ANDROID_CONTROL_AF_MODE_OFF;
+    uint8_t edgeMode = ANDROID_EDGE_MODE_FAST;
+    uint8_t nrMode = ANDROID_NOISE_REDUCTION_MODE_FAST;
+    uint8_t sdMode = ANDROID_SHADING_MODE_FAST;
+    uint8_t hpMode = ANDROID_HOT_PIXEL_MODE_FAST;
+
+    switch (type) {
+        case CAMERA3_TEMPLATE_MANUAL:
+            ctrlMode = ANDROID_CONTROL_MODE_OFF;
+            aeMode = ANDROID_CONTROL_AE_MODE_OFF;
+            awbMode = ANDROID_CONTROL_AWB_MODE_OFF;
+            afMode = ANDROID_CONTROL_AF_MODE_OFF;
+            break;
+        case CAMERA3_TEMPLATE_ZERO_SHUTTER_LAG:
+            afMode = ANDROID_CONTROL_AF_MODE_CONTINUOUS_PICTURE;
+            edgeMode = ANDROID_EDGE_MODE_ZERO_SHUTTER_LAG;
+            nrMode = ANDROID_NOISE_REDUCTION_MODE_ZERO_SHUTTER_LAG;
+            sdMode = ANDROID_SHADING_MODE_HIGH_QUALITY;
+            hpMode = ANDROID_HOT_PIXEL_MODE_HIGH_QUALITY;
+            break;
+        case CAMERA3_TEMPLATE_STILL_CAPTURE:
+            afMode = ANDROID_CONTROL_AF_MODE_CONTINUOUS_PICTURE;
+            edgeMode = ANDROID_EDGE_MODE_HIGH_QUALITY;
+            nrMode = ANDROID_NOISE_REDUCTION_MODE_HIGH_QUALITY;
+            sdMode = ANDROID_SHADING_MODE_HIGH_QUALITY;
+            hpMode = ANDROID_HOT_PIXEL_MODE_HIGH_QUALITY;
+            break;
+        case CAMERA3_TEMPLATE_PREVIEW:
+            afMode = ANDROID_CONTROL_AF_MODE_CONTINUOUS_PICTURE;
+            break;
+        case CAMERA3_TEMPLATE_VIDEO_RECORD:
+        case CAMERA3_TEMPLATE_VIDEO_SNAPSHOT:
+            afMode = ANDROID_CONTROL_AF_MODE_CONTINUOUS_VIDEO;
+            break;
+        default:
+            break;
+    }
+
+    // Check if modes are supported or not.
+    if (!isValueSupported(afMode, caps, CAMERA_AF_AVAILABLE_MODES))
+        afMode = ANDROID_CONTROL_AF_MODE_OFF;
+    if (!isValueSupported(edgeMode, caps, CAMERA_EDGE_AVAILABLE_EDGE_MODES))
+        edgeMode = ANDROID_EDGE_MODE_FAST;
+    if (!isValueSupported(nrMode, caps, CAMERA_NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES))
+        nrMode = ANDROID_NOISE_REDUCTION_MODE_FAST;
+    if (!isValueSupported(sdMode, caps, CAMERA_SHADING_AVAILABLE_MODES))
+        sdMode = ANDROID_SHADING_MODE_FAST;
+    if (!isValueSupported(hpMode, caps, CAMERA_HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES))
+        hpMode = ANDROID_HOT_PIXEL_MODE_FAST;
+
+    LOG2("%s, type %d, ctrlMode %d, aeMode %d, awbMode %d, afMode %d", __func__, type, ctrlMode,
+         aeMode, awbMode, afMode);
+    settings->update(ANDROID_CONTROL_MODE, &ctrlMode, 1);
+    settings->update(ANDROID_CONTROL_AE_MODE, &aeMode, 1);
+    settings->update(ANDROID_CONTROL_AWB_MODE, &awbMode, 1);
+    settings->update(ANDROID_CONTROL_AF_MODE, &afMode, 1);
+    settings->update(ANDROID_EDGE_MODE, &edgeMode, 1);
+    settings->update(ANDROID_NOISE_REDUCTION_MODE, &nrMode, 1);
+    settings->update(ANDROID_SHADING_MODE, &sdMode, 1);
+    settings->update(ANDROID_HOT_PIXEL_MODE, &hpMode, 1);
+
+    uint32_t tag = ANDROID_CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES;
+    camera_metadata_entry fpsRangesEntry = settings->find(tag);
+    if ((fpsRangesEntry.count >= 2) && (fpsRangesEntry.count % 2 == 0)) {
+        int32_t delta = INT32_MAX;
+        int fpsRange[] = {DEFAULT_FPS_RANGE_MIN, DEFAULT_FPS_RANGE_MAX};
+
+        // choose closest (DEFAULT_FPS_RANGE_MIN, DEFAULT_FPS_RANGE_MAX) range
+        for (size_t i = 0; i < fpsRangesEntry.count; i += 2) {
+            int32_t diff = abs(fpsRangesEntry.data.i32[i] - DEFAULT_FPS_RANGE_MIN) +
+                           abs(fpsRangesEntry.data.i32[i + 1] - DEFAULT_FPS_RANGE_MAX);
+
+            if (delta > diff) {
+                fpsRange[0] = fpsRangesEntry.data.i32[i];
+                fpsRange[1] = fpsRangesEntry.data.i32[i + 1];
+                delta = diff;
+            }
+        }
+
+        if (type == CAMERA3_TEMPLATE_VIDEO_RECORD) {
+            // Stable range requried for video recording
+            fpsRange[0] = fpsRange[1];
+        }
+        settings->update(ANDROID_CONTROL_AE_TARGET_FPS_RANGE, &fpsRange[0], 2);
+    } else {
+        LOGW("The fpsRanges isn't correct, please check the profiles file");
+    }
+
+    return icamera::OK;
+}
+
+int MetadataConvert::requestMetadataToHALMetadata(const android::CameraMetadata& settings,
+                                                  icamera::Parameters* parameter,
+                                                  bool forceConvert) {
+    LOG1("@%s: settings entry count %d", __func__, settings.entryCount());
+    CheckError(parameter == nullptr, icamera::BAD_VALUE, "%s, parameter is nullptr", __func__);
+
+    // ANDROID_COLOR_CORRECTION
+    convertColorCorrectionMetadata(settings, parameter);
+
+    // ANDROID_CONTROL
+    convertControlMetadata(settings, parameter);
+
+    // ANDROID_DEMOSAIC
+    // ANDROID_EDGE
+    // ANDROID_HOT_PIXEL
+    // ANDROID_NOISE_REDUCTION
+    // ANDROID_SHADING
+    // ANDROID_TONEMAP
+    convertTonemapMetadata(settings, parameter);
+    // ANDROID_BLACK_LEVEL
+    convertAdvancedFeatureMetadata(settings, parameter);
+    // ANDROID_FLASH
+
+    // ANDROID_JPEG
+    convertJpegMetadata(settings, parameter);
+
+    // ANDROID_LENS
+    convertLensMetadata(settings, parameter);
+
+    // ANDROID_SCALER
+
+    // ANDROID_SENSOR
+    convertSensorMetadata(settings, parameter, forceConvert);
+
+    // ANDROID_STATISTICS
+
+    // ANDROID_LED
+
+    // ANDROID_REPROCESS
+
+    return icamera::OK;
+}
+
+int MetadataConvert::HALMetadataToRequestMetadata(const icamera::Parameters& parameter,
+                                                  android::CameraMetadata* settings, int cameraId) {
+    LOG1("@%s", __func__);
+
+    CheckError(settings == nullptr, icamera::BAD_VALUE, "%s, settings is nullptr", __func__);
+
+    // ANDROID_COLOR_CORRECTION
+    convertColorCorrectionParameter(parameter, settings);
+
+    // ANDROID_CONTROL
+    convertControlParameter(parameter, settings);
+
+    // ANDROID_FLASH
+    // ANDROID_FLASH_INFO
+    convertFlashParameter(parameter, settings);
+
+    // ANDROID_JPEG
+
+    // ANDROID_LENS
+    // ANDROID_LENS_INFO
+    convertLensParameter(parameter, settings);
+
+    // ANDROID_QUIRKS
+
+    // ANDROID_REQUEST
+    convertRequestParameter(parameter, settings, cameraId);
+
+    // ANDROID_SCALER
+
+    // ANDROID_SENSOR
+    // ANDROID_SENSOR_INFO
+    convertSensorParameter(parameter, settings);
+
+    // ANDROID_STATISTICS
+    // ANDROID_STATISTICS_INFO
+    convertStatisticsParameter(parameter, settings);
+
+    // ANDROID_TONEMAP
+    convertTonemapParameter(parameter, settings);
+
+    // ANDROID_DEMOSAIC, ANDROID_EDGE, ANDROID_HOT_PIXEL, ANDROID_NOISE_REDUCTION
+    // ANDROID_SHADING, ANDROID_INFO, ANDROID_BLACK_LEVEL, ANDROID_SYNC
+    convertAdvancedFeatureParameter(parameter, settings);
+
+    // ANDROID_LED
+
+    // ANDROID_REPROCESS
+
+    // ANDROID_DEPTH
+
+    LOG1("@%s: convert entry count %d", __func__, settings->entryCount());
+    return icamera::OK;
+}
+
+int MetadataConvert::HALCapabilityToStaticMetadata(const icamera::Parameters& parameter,
+                                                   android::CameraMetadata* settings) {
+    LOG1("@%s", __func__);
+
+    CheckError(settings == nullptr, icamera::BAD_VALUE, "%s, settings is nullptr", __func__);
+
+    // ANDROID_COLOR_CORRECTION
+    uint8_t aberrationAvailable = ANDROID_COLOR_CORRECTION_ABERRATION_MODE_OFF;
+    settings->update(ANDROID_COLOR_CORRECTION_AVAILABLE_ABERRATION_MODES, &aberrationAvailable, 1);
+
+    // ANDROID_CONTROL
+    fillControlStaticMetadata(parameter, settings);
+
+    // ANDROID_FLASH
+    // ANDROID_FLASH_INFO
+    uint8_t flashInfoAvailable = ANDROID_FLASH_INFO_AVAILABLE_FALSE;
+    settings->update(ANDROID_FLASH_INFO_AVAILABLE, &flashInfoAvailable, 1);
+
+    // ANDROID_JPEG
+    fillJpegStaticMetadata(parameter, settings);
+
+    // ANDROID_LENS
+    // ANDROID_LENS_INFO
+    fillLensStaticMetadata(parameter, settings);
+
+    // ANDROID_QUIRKS
+
+    // ANDROID_REQUEST
+    fillRequestStaticMetadata(parameter, settings);
+
+    // ANDROID_SCALER
+    fillScalerStaticMetadata(parameter, settings);
+
+    // ANDROID_SENSOR
+    // ANDROID_SENSOR_INFO
+    fillSensorStaticMetadata(parameter, settings);
+
+    // ANDROID_STATISTICS
+    // ANDROID_STATISTICS_INFO
+    fillStatisticsStaticMetadata(parameter, settings);
+
+    // ANDROID_TONEMAP
+    fillTonemapStaticMetadata(parameter, settings);
+
+    // ANDROID_LED
+    uint8_t availLeds = ANDROID_LED_AVAILABLE_LEDS_TRANSMIT;
+    settings->update(ANDROID_LED_AVAILABLE_LEDS, &availLeds, 1);
+
+    // ANDROID_REPROCESS
+
+    // ANDROID_DEPTH
+
+    fillAdvancedFeatureStaticMetadata(parameter, settings);
+
+    return icamera::OK;
+}
+
+void MetadataConvert::convertFaceDetectionMetadata(
+    const icamera::CVFaceDetectionAbstractResult& fdResult, android::CameraMetadata* settings) {
+    CheckError(settings == nullptr, VOID_VALUE, "@%s, settings is nullptr", __func__);
+
+    camera_metadata_entry entry = settings->find(ANDROID_STATISTICS_FACE_DETECT_MODE);
+    CheckError(entry.count == 0, VOID_VALUE, "@%s: No face detection mode setting", __func__);
+
+    const uint8_t mode = entry.data.u8[0];
+    if (mode == ANDROID_STATISTICS_FACE_DETECT_MODE_OFF) {
+        LOG2("%s: Face mode is off", __func__);
+        int faceIds[1] = {0};
+        settings->update(ANDROID_STATISTICS_FACE_IDS, faceIds, 1);
+        return;
+    } else if (mode == ANDROID_STATISTICS_FACE_DETECT_MODE_SIMPLE) {
+        LOG2("%s: Face mode is simple", __func__);
+        // Face id is expected to be -1 for SIMPLE mode
+        if (fdResult.faceNum > 0) {
+            int faceIds[MAX_FACES_DETECTABLE];
+            for (int i = 0; i < fdResult.faceNum; i++) {
+                faceIds[i] = -1;
+            }
+            settings->update(ANDROID_STATISTICS_FACE_IDS, faceIds, fdResult.faceNum);
+        } else {
+            int faceIds[1] = {-1};
+            settings->update(ANDROID_STATISTICS_FACE_IDS, faceIds, 1);
+        }
+    } else if (mode == ANDROID_STATISTICS_FACE_DETECT_MODE_FULL) {
+        LOG2("%s: Face mode is full", __func__);
+        /*
+         * from the spec:
+         * SIMPLE mode must fill in android.statistics.faceRectangles and
+         * android.statistics.faceScores. FULL mode must also fill in
+         * android.statistics.faceIds, and android.statistics.faceLandmarks.
+         */
+        settings->update(ANDROID_STATISTICS_FACE_IDS, fdResult.faceIds, fdResult.faceNum);
+        settings->update(ANDROID_STATISTICS_FACE_LANDMARKS, fdResult.faceLandmarks,
+                         LM_SIZE * fdResult.faceNum);
+    }
+
+    settings->update(ANDROID_STATISTICS_FACE_RECTANGLES, fdResult.faceRect,
+                     RECT_SIZE * fdResult.faceNum);
+    settings->update(ANDROID_STATISTICS_FACE_SCORES, fdResult.faceScores, fdResult.faceNum);
+}
+
+int MetadataConvert::convertColorCorrectionMetadata(const android::CameraMetadata& settings,
+                                                    icamera::Parameters* parameter) {
+    uint32_t tag = ANDROID_COLOR_CORRECTION_TRANSFORM;
+    camera_metadata_ro_entry entry = settings.find(tag);
+    if (entry.count == 9) {
+        icamera::camera_color_transform_t transform;
+        for (size_t i = 0; i < entry.count; i++) {
+            transform.color_transform[i / 3][i % 3] =
+                static_cast<float>(entry.data.r[i].numerator) / entry.data.r[i].denominator;
+        }
+        parameter->setColorTransform(transform);
+    }
+
+    tag = ANDROID_COLOR_CORRECTION_GAINS;
+    entry = settings.find(tag);
+    if (entry.count == 4) {
+        icamera::camera_color_gains_t gains;
+        for (size_t i = 0; i < entry.count; i++) {
+            gains.color_gains_rggb[i] = entry.data.f[i];
+        }
+        parameter->setColorGains(gains);
+    }
+
+    return icamera::OK;
+}
+
+int MetadataConvert::convertColorCorrectionParameter(const icamera::Parameters& parameter,
+                                                     android::CameraMetadata* settings) {
+    icamera::camera_color_transform_t transform;
+    if (parameter.getColorTransform(transform) == 0) {
+        camera_metadata_rational_t matrix[9];
+        for (int i = 0; i < 9; i++) {
+            matrix[i].numerator = round(transform.color_transform[i / 3][i % 3] * 1000);
+            matrix[i].denominator = 1000;
+        }
+        settings->update(ANDROID_COLOR_CORRECTION_TRANSFORM, &matrix[0], 9);
+    }
+
+    icamera::camera_color_gains_t colorGains;
+    if (parameter.getColorGains(colorGains) == 0) {
+        settings->update(ANDROID_COLOR_CORRECTION_GAINS, &colorGains.color_gains_rggb[0], 4);
+    }
+
+    uint8_t aberrationMode = ANDROID_COLOR_CORRECTION_ABERRATION_MODE_OFF;
+    settings->update(ANDROID_COLOR_CORRECTION_ABERRATION_MODE, &aberrationMode, 1);
+
+    return icamera::OK;
+}
+
+int MetadataConvert::convertControlMetadata(const android::CameraMetadata& settings,
+                                            icamera::Parameters* parameter) {
+    int ret = icamera::OK;
+    int mode = 0;
+    uint32_t tag = ANDROID_CONTROL_AE_MODE;
+    camera_metadata_ro_entry entry = settings.find(tag);
+    if (entry.count == 1) {
+        ret = getHalValue(entry.data.u8[0], aeModesTable, ARRAY_SIZE(aeModesTable), &mode);
+        if (ret == icamera::OK) {
+            parameter->setAeMode((icamera::camera_ae_mode_t)mode);
+        }
+    }
+
+    tag = ANDROID_STATISTICS_FACE_DETECT_MODE;
+    entry = settings.find(tag);
+    uint8_t fdValue = ANDROID_STATISTICS_FACE_DETECT_MODE_OFF;
+    if ((entry.count == 1) && (entry.data.u8[0] == ANDROID_STATISTICS_FACE_DETECT_MODE_OFF)) {
+        int faceIds[1] = {0};
+        parameter->setFaceIds(faceIds, 1);
+    } else {
+        fdValue = entry.data.u8[0];
+    }
+    parameter->setFaceDetectMode(fdValue);
+
+    tag = ANDROID_CONTROL_AE_LOCK;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        bool aeLock = (entry.data.u8[0] == ANDROID_CONTROL_AE_LOCK_ON);
+        parameter->setAeLock(aeLock);
+    }
+
+    tag = ANDROID_CONTROL_AE_REGIONS;
+    entry = settings.find(tag);
+    icamera::camera_window_list_t windows;
+    if (entry.count > 0) {
+        if (convertToHalWindow(entry.data.i32, entry.count, &windows) == 0) {
+            parameter->setAeRegions(windows);
+        }
+    }
+
+    tag = ANDROID_CONTROL_AE_TARGET_FPS_RANGE;
+    entry = settings.find(tag);
+    if (entry.count == 2) {
+        icamera::camera_range_t range;
+        range.min = entry.data.i32[0];
+        range.max = entry.data.i32[1];
+        parameter->setFpsRange(range);
+    }
+
+    tag = ANDROID_CONTROL_AE_EXPOSURE_COMPENSATION;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        parameter->setAeCompensation(entry.data.i32[0]);
+    }
+
+    tag = ANDROID_CONTROL_AE_ANTIBANDING_MODE;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        ret = getHalValue(entry.data.u8[0], antibandingModesTable,
+                          ARRAY_SIZE(antibandingModesTable), &mode);
+        if (ret == icamera::OK) {
+            parameter->setAntiBandingMode((icamera::camera_antibanding_mode_t)mode);
+        }
+    }
+
+    tag = ANDROID_CONTROL_AF_MODE;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        ret = getHalValue(entry.data.u8[0], afModesTable, ARRAY_SIZE(afModesTable), &mode);
+        if (ret == icamera::OK) {
+            parameter->setAfMode((icamera::camera_af_mode_t)mode);
+        }
+    }
+
+    tag = ANDROID_CONTROL_AF_TRIGGER;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        ret = getHalValue(entry.data.u8[0], afTriggerTable, ARRAY_SIZE(afTriggerTable), &mode);
+        if (ret == icamera::OK) {
+            parameter->setAfTrigger((icamera::camera_af_trigger_t)mode);
+        }
+    }
+
+    tag = ANDROID_CONTROL_AF_REGIONS;
+    entry = settings.find(tag);
+    windows.clear();
+    if (entry.count > 0) {
+        if (convertToHalWindow(entry.data.i32, entry.count, &windows) == 0) {
+            parameter->setAfRegions(windows);
+        }
+    }
+
+    tag = ANDROID_CONTROL_AWB_MODE;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        ret = getHalValue(entry.data.u8[0], awbModesTable, ARRAY_SIZE(awbModesTable), &mode);
+        if (ret == icamera::OK) {
+            parameter->setAwbMode((icamera::camera_awb_mode_t)mode);
+        }
+    }
+
+    tag = ANDROID_CONTROL_AWB_LOCK;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        bool awbLock = (entry.data.u8[0] == ANDROID_CONTROL_AWB_LOCK_ON);
+        parameter->setAwbLock(awbLock);
+    }
+
+    tag = ANDROID_CONTROL_AWB_REGIONS;
+    entry = settings.find(tag);
+    windows.clear();
+    if (entry.count > 0) {
+        if (convertToHalWindow(entry.data.i32, entry.count, &windows) == 0) {
+            parameter->setAwbRegions(windows);
+        }
+    }
+
+    tag = ANDROID_CONTROL_VIDEO_STABILIZATION_MODE;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        ret = getHalValue(entry.data.u8[0], dvsModesTable, ARRAY_SIZE(dvsModesTable), &mode);
+        if (ret == icamera::OK) {
+            parameter->setVideoStabilizationMode((icamera::camera_video_stabilization_mode_t)mode);
+        }
+    }
+
+    tag = ANDROID_CONTROL_EFFECT_MODE;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        ret = getHalValue(entry.data.u8[0], effectModesTable, ARRAY_SIZE(effectModesTable), &mode);
+        if (ret == icamera::OK) {
+            parameter->setImageEffect((icamera::camera_effect_mode_t)mode);
+        }
+    }
+
+    tag = ANDROID_CONTROL_CAPTURE_INTENT;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        parameter->setCaptureIntent(entry.data.u8[0]);
+    }
+
+    return icamera::OK;
+}
+
+int MetadataConvert::convertControlParameter(const icamera::Parameters& parameter,
+                                             android::CameraMetadata* settings) {
+    int ret = icamera::OK;
+    uint8_t mode = 0;
+    icamera::camera_ae_mode_t aeMode;
+    if (parameter.getAeMode(aeMode) == 0) {
+        ret = getAndroidValue(aeMode, aeModesTable, ARRAY_SIZE(aeModesTable), &mode);
+        if (ret == icamera::OK) {
+            settings->update(ANDROID_CONTROL_AE_MODE, &mode, 1);
+        }
+    }
+
+    bool aeLock;
+    if (parameter.getAeLock(aeLock) == 0) {
+        uint8_t mode = aeLock ? ANDROID_CONTROL_AE_LOCK_ON : ANDROID_CONTROL_AE_LOCK_OFF;
+        settings->update(ANDROID_CONTROL_AE_LOCK, &mode, 1);
+    }
+
+    icamera::camera_window_list_t windows;
+    parameter.getAeRegions(windows);
+    int count = windows.size() * 5;
+    if (count > 0) {
+        int regions[count];
+        count = convertToMetadataRegion(windows, windows.size() * 5, regions);
+        if (count > 0) {
+            settings->update(ANDROID_CONTROL_AE_REGIONS, &regions[0], count);
+        }
+    }
+
+    icamera::camera_range_t range;
+    if (parameter.getFpsRange(range) == 0) {
+        int fps[2] = {(int)range.min, (int)range.max};
+        settings->update(ANDROID_CONTROL_AE_TARGET_FPS_RANGE, &fps[0], 2);
+    }
+
+    int ev;
+    if (parameter.getAeCompensation(ev) == 0) {
+        settings->update(ANDROID_CONTROL_AE_EXPOSURE_COMPENSATION, &ev, 1);
+    }
+
+    icamera::camera_antibanding_mode_t antiMode;
+    if (parameter.getAntiBandingMode(antiMode) == 0) {
+        ret = getAndroidValue(antiMode, antibandingModesTable, ARRAY_SIZE(antibandingModesTable),
+                              &mode);
+        if (ret == icamera::OK) {
+            settings->update(ANDROID_CONTROL_AE_ANTIBANDING_MODE, &mode, 1);
+        }
+    }
+
+    icamera::camera_af_mode_t afMode;
+    if (parameter.getAfMode(afMode) == 0) {
+        ret = getAndroidValue(afMode, afModesTable, ARRAY_SIZE(afModesTable), &mode);
+        if (ret == icamera::OK) {
+            settings->update(ANDROID_CONTROL_AF_MODE, &mode, 1);
+        }
+    }
+
+    windows.clear();
+    parameter.getAfRegions(windows);
+    count = windows.size() * 5;
+    if (count > 0) {
+        int regions[count];
+        count = convertToMetadataRegion(windows, windows.size() * 5, regions);
+        if (count > 0) {
+            settings->update(ANDROID_CONTROL_AF_REGIONS, &regions[0], count);
+        }
+    }
+
+    icamera::camera_awb_mode_t awbMode;
+    if (parameter.getAwbMode(awbMode) == 0) {
+        ret = getAndroidValue(awbMode, awbModesTable, ARRAY_SIZE(awbModesTable), &mode);
+        if (ret == icamera::OK) {
+            settings->update(ANDROID_CONTROL_AWB_MODE, &mode, 1);
+        }
+    }
+
+    bool awbLock;
+    if (parameter.getAwbLock(awbLock) == 0) {
+        uint8_t mode = awbLock ? ANDROID_CONTROL_AWB_LOCK_ON : ANDROID_CONTROL_AWB_LOCK_OFF;
+        settings->update(ANDROID_CONTROL_AWB_LOCK, &mode, 1);
+    }
+
+    windows.clear();
+    parameter.getAwbRegions(windows);
+    count = windows.size() * 5;
+    if (count > 0) {
+        int regions[count];
+        count = convertToMetadataRegion(windows, windows.size() * 5, regions);
+        if (count > 0) {
+            settings->update(ANDROID_CONTROL_AWB_REGIONS, &regions[0], count);
+        }
+    }
+
+    icamera::camera_video_stabilization_mode_t dvsMode;
+    if (parameter.getVideoStabilizationMode(dvsMode) == 0) {
+        ret = getAndroidValue(dvsMode, dvsModesTable, ARRAY_SIZE(dvsModesTable), &mode);
+        if (ret == icamera::OK) {
+            settings->update(ANDROID_CONTROL_VIDEO_STABILIZATION_MODE, &mode, 1);
+        }
+    }
+
+    icamera::camera_effect_mode_t effectMode;
+    if (parameter.getImageEffect(effectMode) == 0) {
+        ret = getAndroidValue(effectMode, effectModesTable, ARRAY_SIZE(effectModesTable), &mode);
+        if (ret == icamera::OK) {
+            settings->update(ANDROID_CONTROL_EFFECT_MODE, &mode, 1);
+        }
+    }
+
+    return icamera::OK;
+}
+
+int MetadataConvert::convertJpegMetadata(const android::CameraMetadata& settings,
+                                         icamera::Parameters* parameter) {
+    uint32_t tag = ANDROID_JPEG_GPS_COORDINATES;
+    camera_metadata_ro_entry entry = settings.find(tag);
+    if (entry.count == 3) {
+        parameter->setJpegGpsCoordinates(entry.data.d);
+    }
+
+    tag = ANDROID_JPEG_GPS_PROCESSING_METHOD;
+    entry = settings.find(tag);
+    if (entry.count >= 1) {
+        char data[entry.count + 1];
+        MEMCPY_S(data, sizeof(data), entry.data.u8, entry.count);
+        data[entry.count] = 0;
+        parameter->setJpegGpsProcessingMethod(data);
+    }
+
+    tag = ANDROID_JPEG_GPS_TIMESTAMP;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        parameter->setJpegGpsTimeStamp(entry.data.i64[0]);
+    }
+
+    tag = ANDROID_JPEG_ORIENTATION;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        parameter->setJpegRotation(entry.data.i32[0]);
+    }
+
+    tag = ANDROID_JPEG_QUALITY;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        int quality = entry.data.u8[0];
+        parameter->setJpegQuality(quality);
+    }
+
+    tag = ANDROID_JPEG_THUMBNAIL_QUALITY;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        int quality = entry.data.u8[0];
+        parameter->setJpegThumbnailQuality(quality);
+    }
+
+    tag = ANDROID_JPEG_THUMBNAIL_SIZE;
+    entry = settings.find(tag);
+    if (entry.count == 2) {
+        icamera::camera_resolution_t size;
+        size.width = entry.data.i32[0];
+        size.height = entry.data.i32[1];
+        parameter->setJpegThumbnailSize(size);
+    }
+
+    return icamera::OK;
+}
+
+int MetadataConvert::convertTonemapMetadata(const android::CameraMetadata& settings,
+                                            icamera::Parameters* parameter) {
+    int ret = icamera::OK;
+
+    camera_metadata_ro_entry entry = settings.find(ANDROID_TONEMAP_MODE);
+    if (entry.count == 1) {
+        int32_t mode = 0;
+        ret =
+            getHalValue(entry.data.u8[0], tonemapModesTable, ARRAY_SIZE(tonemapModesTable), &mode);
+        if (ret == icamera::OK) {
+            parameter->setTonemapMode((icamera::camera_tonemap_mode_t)mode);
+        }
+    }
+
+    entry = settings.find(ANDROID_TONEMAP_PRESET_CURVE);
+    if (entry.count == 1) {
+        int32_t curve = 0;
+        ret = getHalValue(entry.data.u8[0], tonemapPresetCurvesTable,
+                          ARRAY_SIZE(tonemapPresetCurvesTable), &curve);
+        if (ret == icamera::OK) {
+            parameter->setTonemapPresetCurve((icamera::camera_tonemap_preset_curve_t)curve);
+        }
+    }
+
+    entry = settings.find(ANDROID_TONEMAP_GAMMA);
+    if (entry.count == 1) {
+        parameter->setTonemapGamma(entry.data.f[0]);
+    }
+
+    return icamera::OK;
+}
+
+int MetadataConvert::convertTonemapParameter(const icamera::Parameters& parameter,
+                                             android::CameraMetadata* settings) {
+    icamera::camera_tonemap_curves_t curves;
+    if (parameter.getTonemapCurves(curves) == 0) {
+        settings->update(ANDROID_TONEMAP_CURVE_RED, curves.rCurve, curves.rSize);
+        settings->update(ANDROID_TONEMAP_CURVE_BLUE, curves.bCurve, curves.bSize);
+        settings->update(ANDROID_TONEMAP_CURVE_GREEN, curves.gCurve, curves.gSize);
+    }
+
+    return icamera::OK;
+}
+
+int MetadataConvert::convertSensorMetadata(const android::CameraMetadata& settings,
+                                           icamera::Parameters* parameter, bool forceConvert) {
+    // get control ae mode
+    uint8_t manualAeMode = ANDROID_CONTROL_AE_MODE_ON;
+    uint32_t tag = ANDROID_CONTROL_AE_MODE;
+    camera_metadata_ro_entry entry = settings.find(tag);
+    if (entry.count == 1) {
+        manualAeMode = entry.data.u8[0];
+    }
+
+    // get control mode
+    uint8_t manualMode = ANDROID_CONTROL_MODE_AUTO;
+    tag = ANDROID_CONTROL_MODE;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        manualMode = entry.data.u8[0];
+    }
+
+    if (manualAeMode == ANDROID_CONTROL_AE_MODE_OFF || manualMode == ANDROID_CONTROL_MODE_OFF ||
+        forceConvert) {
+        // manual exposure control
+        tag = ANDROID_SENSOR_EXPOSURE_TIME;
+        entry = settings.find(tag);
+        if (entry.count == 1) {
+            parameter->setExposureTime(entry.data.i64[0] / 1000);  // ns -> us
+        }
+
+        // manual sensitivity control
+        tag = ANDROID_SENSOR_SENSITIVITY;
+        entry = settings.find(tag);
+        if (entry.count == 1) {
+            parameter->setSensitivityIso(entry.data.i32[0]);
+        }
+
+        // manual frame duration control
+        int64_t maxFrameDuration = 0;
+        entry = settings.find(ANDROID_SENSOR_INFO_MAX_FRAME_DURATION);
+        if (entry.count == 1) {
+            maxFrameDuration = entry.data.i64[0];
+            LOG2("@%s, maxFrameDuration:%ld ns", __func__, maxFrameDuration);
+        }
+
+        tag = ANDROID_SENSOR_FRAME_DURATION;
+        entry = settings.find(tag);
+        if (entry.count == 1) {
+            int64_t frameDuration = entry.data.i64[0];
+            LOG2("@%s, frameDuration:%ld ns", __func__, frameDuration);
+            if (maxFrameDuration > 0 && frameDuration > maxFrameDuration) {
+                frameDuration = maxFrameDuration;
+            }
+
+            if (frameDuration != 0) {
+                float fps = NSEC_PER_SEC / frameDuration;
+                parameter->setFrameRate(fps);
+            }
+        }
+    } else {
+        // Clear manual settings then AE algorithm works
+        int64_t exposureTime = 0;
+        parameter->setExposureTime(exposureTime);
+        int32_t iso = 0;
+        parameter->setSensitivityIso(iso);
+        float fps = 0.0;
+        parameter->setFrameRate(fps);
+    }
+
+    // Test Pattern Mode
+    tag = ANDROID_SENSOR_TEST_PATTERN_MODE;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        int halTestPatternMode = icamera::TEST_PATTERN_OFF;
+        int ret = getHalValue(entry.data.i32[0], testPatternTable, ARRAY_SIZE(testPatternTable),
+                              &halTestPatternMode);
+        if (ret == icamera::OK) {
+            parameter->setTestPatternMode(
+                static_cast<icamera::camera_test_pattern_mode_t>(halTestPatternMode));
+        }
+    }
+
+    return icamera::OK;
+}
+
+int MetadataConvert::convertRequestParameter(const icamera::Parameters& parameter,
+                                             android::CameraMetadata* settings, int cameraId) {
+    const icamera::CameraMetadata* meta = StaticCapability::getInstance(cameraId)->getCapability();
+
+    uint32_t tag = CAMERA_REQUEST_PIPELINE_MAX_DEPTH;
+    icamera_metadata_ro_entry entry = meta->find(tag);
+    uint8_t depth = (entry.count == 1) ? *entry.data.u8 : 6;
+
+    settings->update(ANDROID_REQUEST_PIPELINE_DEPTH, &depth, 1);
+
+    return icamera::OK;
+}
+
+int MetadataConvert::convertSensorParameter(const icamera::Parameters& parameter,
+                                            android::CameraMetadata* settings) {
+    int64_t exposure;
+    if (parameter.getExposureTime(exposure) == 0) {
+        int64_t time = exposure * 1000;  // us -> ns
+        settings->update(ANDROID_SENSOR_EXPOSURE_TIME, &time, 1);
+    }
+
+    int32_t iso;
+    if (parameter.getSensitivityIso(iso) == 0) {
+        settings->update(ANDROID_SENSOR_SENSITIVITY, &iso, 1);
+    }
+
+    float fps;
+    if (parameter.getFrameRate(fps) == icamera::OK) {
+        int64_t duration = NSEC_PER_SEC / fps;
+        settings->update(ANDROID_SENSOR_FRAME_DURATION, &duration, 1);
+    }
+
+    icamera::camera_test_pattern_mode_t halTestPatternMode = icamera::TEST_PATTERN_OFF;
+    if (parameter.getTestPatternMode(halTestPatternMode) == icamera::OK) {
+        int32_t androidPatternMode = ANDROID_SENSOR_TEST_PATTERN_MODE_OFF;
+        int ret = getAndroidValue(halTestPatternMode, testPatternTable,
+                                  ARRAY_SIZE(testPatternTable), &androidPatternMode);
+        if (ret == icamera::OK) {
+            settings->update(ANDROID_SENSOR_TEST_PATTERN_MODE, &androidPatternMode, 1);
+        }
+    }
+
+    icamera::CameraMetadata meta;
+    icamera::ParameterHelper::copyMetadata(parameter, &meta);
+
+    uint32_t tag = CAMERA_SENSOR_ROLLING_SHUTTER_SKEW;
+    icamera_metadata_entry entry = meta.find(tag);
+    if (entry.count == 1) {
+        int64_t rollingShutter = entry.data.i64[0] * 1000;  // us -> ns
+        settings->update(ANDROID_SENSOR_ROLLING_SHUTTER_SKEW, &rollingShutter, entry.count);
+    }
+
+    return icamera::OK;
+}
+
+int MetadataConvert::convertLensMetadata(const android::CameraMetadata& settings,
+                                         icamera::Parameters* parameter) {
+    uint32_t tag = ANDROID_LENS_FOCAL_LENGTH;
+    camera_metadata_ro_entry entry = settings.find(tag);
+    if (entry.count == 1) {
+        parameter->setFocalLength(entry.data.f[0]);
+    }
+
+    tag = ANDROID_LENS_APERTURE;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        parameter->setAperture(entry.data.f[0]);
+    }
+
+    tag = ANDROID_LENS_FOCUS_DISTANCE;
+    entry = settings.find(tag);
+    if (entry.count == 1) {
+        parameter->setFocusDistance(entry.data.f[0]);
+    }
+
+    return icamera::OK;
+}
+
+int MetadataConvert::convertLensParameter(const icamera::Parameters& parameter,
+                                          android::CameraMetadata* settings) {
+    icamera::CameraMetadata meta;
+    icamera::ParameterHelper::copyMetadata(parameter, &meta);
+
+    uint32_t tag = CAMERA_LENS_INFO_AVAILABLE_APERTURES;
+    icamera_metadata_entry entry = meta.find(tag);
+    if (entry.count >= 1) {
+        settings->update(ANDROID_LENS_APERTURE, entry.data.f, 1);
+    }
+
+    float focal = 0.0f;
+    parameter.getFocalLength(focal);
+    if (focal < EPSILON) {
+        icamera_metadata_entry entry = meta.find(CAMERA_LENS_INFO_AVAILABLE_FOCAL_LENGTHS);
+        if (entry.count >= 1) {
+            focal = entry.data.f[0];
+        }
+    }
+    settings->update(ANDROID_LENS_FOCAL_LENGTH, &focal, 1);
+
+    float focusDistanceDiopters = 0.0;
+    if (parameter.getFocusDistance(focusDistanceDiopters) == 0) {
+        settings->update(ANDROID_LENS_FOCUS_DISTANCE, &focusDistanceDiopters, 1);
+    }
+
+    icamera::camera_range_t focusRange = {};
+    if (parameter.getFocusRange(focusRange) == 0) {
+        float range[] = {focusRange.min, focusRange.max};
+        settings->update(ANDROID_LENS_FOCUS_RANGE, range, 2);
+    }
+
+    uint8_t mode = ANDROID_LENS_OPTICAL_STABILIZATION_MODE_OFF;
+    settings->update(ANDROID_LENS_OPTICAL_STABILIZATION_MODE, &mode, 1);
+    float filterDensity = 0.0;
+    settings->update(ANDROID_LENS_FILTER_DENSITY, &filterDensity, 1);
+
+    return icamera::OK;
+}
+
+int MetadataConvert::convertStatisticsParameter(const icamera::Parameters& /*parameter*/,
+                                                android::CameraMetadata* settings) {
+    camera_metadata_entry entry = settings->find(ANDROID_STATISTICS_FACE_DETECT_MODE);
+    if (entry.count == 1 && entry.data.u8[0] == ANDROID_STATISTICS_FACE_DETECT_MODE_OFF) {
+        LOG2("%s: Face mode is off", __func__);
+        int faceIds[1] = {0};
+        settings->update(ANDROID_STATISTICS_FACE_IDS, faceIds, 1);
+    }
+    return icamera::OK;
+}
+
+int MetadataConvert::convertFlashParameter(const icamera::Parameters& /*parameter*/,
+                                           android::CameraMetadata* settings) {
+    uint8_t flashMode = ANDROID_FLASH_MODE_OFF;
+    settings->update(ANDROID_FLASH_MODE, &flashMode, 1);
+
+    return icamera::OK;
+}
+
+int MetadataConvert::convertAdvancedFeatureMetadata(const android::CameraMetadata& settings,
+                                                    icamera::Parameters* parameter) {
+    int ret = icamera::OK;
+    // ANDROID_DEMOSAIC
+    // ANDROID_EDGE
+    // ANDROID_HOT_PIXEL
+    // ANDROID_NOISE_REDUCTION
+
+    // ANDROID_SHADING
+    int mode;
+    camera_metadata_ro_entry entry = settings.find(ANDROID_SHADING_MODE);
+    if (entry.count == 1) {
+        ret = getHalValue(entry.data.u8[0], shadingModeTable, ARRAY_SIZE(shadingModeTable), &mode);
+        if (ret == icamera::OK) {
+            parameter->setShadingMode((icamera::camera_shading_mode_t)mode);
+        }
+    }
+
+    entry = settings.find(ANDROID_STATISTICS_LENS_SHADING_MAP_MODE);
+    if (entry.count == 1) {
+        ret = getHalValue(entry.data.u8[0], lensShadingMapModeTable,
+                          ARRAY_SIZE(lensShadingMapModeTable), &mode);
+        if (ret == icamera::OK) {
+            parameter->setLensShadingMapMode((icamera::camera_lens_shading_map_mode_type_t)mode);
+        }
+    }
+
+    // ANDROID_TONEMAP
+    // ANDROID_INFO
+    // ANDROID_BLACK_LEVEL
+
+    return icamera::OK;
+}
+
+int MetadataConvert::convertAdvancedFeatureParameter(const icamera::Parameters& parameter,
+                                                     android::CameraMetadata* settings) {
+    int ret = icamera::OK;
+    icamera::CameraMetadata meta;
+    icamera::ParameterHelper::copyMetadata(parameter, &meta);
+
+    // ANDROID_DEMOSAIC
+
+    // ANDROID_EDGE
+
+    // ANDROID_HOT_PIXEL
+
+    // ANDROID_NOISE_REDUCTION
+
+    // ANDROID_SHADING
+    icamera::camera_shading_mode_t shadingMode;
+    uint8_t saMode = ANDROID_SHADING_MODE_OFF;
+    if (parameter.getShadingMode(shadingMode) == icamera::OK) {
+        ret = getAndroidValue(shadingMode, shadingModeTable, ARRAY_SIZE(shadingModeTable), &saMode);
+        if (ret == icamera::OK) {
+            settings->update(ANDROID_SHADING_MODE, &saMode, 1);
+        }
+    }
+
+    icamera::camera_lens_shading_map_mode_type_t lensShadingMapMode;
+    ret = parameter.getLensShadingMapMode(lensShadingMapMode);
+    if (ret == icamera::OK) {
+        uint8_t lensSMMode;
+        ret = getAndroidValue(lensShadingMapMode, shadingModeTable, ARRAY_SIZE(shadingModeTable),
+                              &lensSMMode);
+        if (ret == icamera::OK) {
+            settings->update(ANDROID_STATISTICS_LENS_SHADING_MAP_MODE, &lensSMMode, 1);
+        }
+    }
+
+    if (lensShadingMapMode == icamera::LENS_SHADING_MAP_MODE_ON) {
+        size_t lensShadingMapSize;
+        float* lensShadingMap = nullptr;
+        ret = parameter.getLensShadingMap(&lensShadingMap, lensShadingMapSize);
+        if (ret == icamera::OK) {
+            settings->update(ANDROID_STATISTICS_LENS_SHADING_MAP, lensShadingMap,
+                             lensShadingMapSize);
+            if (saMode == ANDROID_SHADING_MODE_OFF) {
+                saMode = ANDROID_SHADING_MODE_FAST;
+                settings->update(ANDROID_SHADING_MODE, &saMode, 1);
+            }
+        }
+    }
+
+    // ANDROID_TONEMAP
+    // ANDROID_INFO
+    // ANDROID_BLACK_LEVEL
+    // ANDROID_SYNC
+
+    return icamera::OK;
+}
+
+void MetadataConvert::fillControlStaticMetadata(const icamera::Parameters& parameter,
+                                                android::CameraMetadata* settings) {
+    int ret = icamera::OK;
+    icamera::CameraMetadata meta;
+    icamera::ParameterHelper::copyMetadata(parameter, &meta);
+
+    icamera_metadata_entry entry = meta.find(CAMERA_CONTROL_AVAILABLE_MODES);
+    if (entry.count != 0) {
+        settings->update(ANDROID_CONTROL_AVAILABLE_MODES, entry.data.u8, entry.count);
+    }
+
+    std::vector<icamera::camera_antibanding_mode_t> antibandingModes;
+    parameter.getSupportedAntibandingMode(antibandingModes);
+    if (antibandingModes.size() > 0) {
+        int size = antibandingModes.size();
+        uint8_t data[size];
+        int count = 0;
+        for (int i = 0; i < size; i++) {
+            ret = getAndroidValue(antibandingModes[i], antibandingModesTable,
+                                  ARRAY_SIZE(antibandingModesTable), &data[count]);
+            if (ret == icamera::OK) {
+                count++;
+            }
+        }
+        if (count > 0) {
+            settings->update(ANDROID_CONTROL_AE_AVAILABLE_ANTIBANDING_MODES, data, count);
+        }
+    } else {
+        LOGW("No antibanding modes provided!");
+    }
+
+    std::vector<icamera::camera_ae_mode_t> availAeModes;
+    parameter.getSupportedAeMode(availAeModes);
+    if (availAeModes.size() > 0) {
+        int size = availAeModes.size();
+        uint8_t data[size];
+        int count = 0;
+        for (int i = 0; i < size; i++) {
+            ret = getAndroidValue(availAeModes[i], aeModesTable, ARRAY_SIZE(aeModesTable),
+                                  &data[count]);
+            if (ret == icamera::OK) {
+                count++;
+            }
+        }
+        if (count > 0) {
+            settings->update(ANDROID_CONTROL_AE_AVAILABLE_MODES, data, count);
+        }
+    } else {
+        LOGW("No ae modes provided!");
+    }
+
+    uint8_t aeLockAvailable = parameter.getAeLockAvailable()
+                                  ? ANDROID_CONTROL_AE_LOCK_AVAILABLE_TRUE
+                                  : ANDROID_CONTROL_AE_LOCK_AVAILABLE_FALSE;
+    settings->update(ANDROID_CONTROL_AE_LOCK_AVAILABLE, &aeLockAvailable, 1);
+
+    icamera::camera_range_array_t fpsRanges;
+    if (parameter.getSupportedFpsRange(fpsRanges) == 0) {
+        int count = fpsRanges.size() * 2;
+        int32_t data[count];
+        for (size_t i = 0; i < fpsRanges.size(); i++) {
+            data[i * 2] = (int32_t)fpsRanges[i].min;
+            data[i * 2 + 1] = (int32_t)fpsRanges[i].max;
+        }
+        settings->update(ANDROID_CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES, data, count);
+    } else {
+        LOGW("No fps ranges provided!");
+    }
+
+    icamera::camera_range_t aeCompensationRange;
+    if (parameter.getAeCompensationRange(aeCompensationRange) == 0) {
+        int32_t data[2];
+        data[0] = (int32_t)aeCompensationRange.min;
+        data[1] = (int32_t)aeCompensationRange.max;
+        settings->update(ANDROID_CONTROL_AE_COMPENSATION_RANGE, data, 2);
+    } else {
+        LOGW("No ae compensation range provided!");
+    }
+
+    icamera::camera_rational_t aeCompensationStep;
+    if (parameter.getAeCompensationStep(aeCompensationStep) == 0) {
+        camera_metadata_rational rational;
+        rational.numerator = aeCompensationStep.numerator;
+        rational.denominator = aeCompensationStep.denominator;
+        settings->update(ANDROID_CONTROL_AE_COMPENSATION_STEP, &rational, 1);
+    } else {
+        LOGW("No ae compensation step provided!");
+    }
+
+    std::vector<icamera::camera_af_mode_t> availAfModes;
+    parameter.getSupportedAfMode(availAfModes);
+    if (availAfModes.size() > 0) {
+        int size = availAfModes.size();
+        uint8_t data[size];
+        int count = 0;
+        for (int i = 0; i < size; i++) {
+            ret = getAndroidValue(availAfModes[i], afModesTable, ARRAY_SIZE(afModesTable),
+                                  &data[count]);
+            if (ret == icamera::OK) {
+                count++;
+            }
+        }
+        if (count > 0) {
+            settings->update(ANDROID_CONTROL_AF_AVAILABLE_MODES, data, count);
+        }
+    } else {
+        LOGW("No af modes provided!");
+    }
+
+    uint8_t effectMode = ANDROID_CONTROL_EFFECT_MODE_OFF;
+    settings->update(ANDROID_CONTROL_AVAILABLE_EFFECTS, &effectMode, 1);
+
+    entry = meta.find(CAMERA_CONTROL_AVAILABLE_SCENE_MODES);
+    if (entry.count != 0) {
+        settings->update(ANDROID_CONTROL_AVAILABLE_SCENE_MODES, entry.data.u8, entry.count);
+    } else {
+        LOGW("No available scene modes");
+    }
+
+    icamera::camera_video_stabilization_list_t availDvsModes;
+    parameter.getSupportedVideoStabilizationMode(availDvsModes);
+    if (availDvsModes.size() > 0) {
+        int size = availDvsModes.size();
+        uint8_t data[size];
+        int count = 0;
+        for (int i = 0; i < size; i++) {
+            ret = getAndroidValue(availDvsModes[i], dvsModesTable, ARRAY_SIZE(dvsModesTable),
+                                  &data[count]);
+            if (ret == icamera::OK) {
+                count++;
+            }
+        }
+        if (count > 0) {
+            settings->update(ANDROID_CONTROL_AVAILABLE_VIDEO_STABILIZATION_MODES, data, count);
+        }
+    } else {
+        LOGW("No video stablization modes provided!");
+    }
+
+    std::vector<icamera::camera_awb_mode_t> availAwbModes;
+    parameter.getSupportedAwbMode(availAwbModes);
+    if (availAwbModes.size() > 0) {
+        int size = availAwbModes.size();
+        uint8_t data[size];
+        int count = 0;
+        for (int i = 0; i < size; i++) {
+            ret = getAndroidValue(availAwbModes[i], awbModesTable, ARRAY_SIZE(awbModesTable),
+                                  &data[count]);
+            if (ret == icamera::OK) {
+                count++;
+            }
+        }
+        if (count > 0) {
+            settings->update(ANDROID_CONTROL_AWB_AVAILABLE_MODES, data, count);
+        }
+    } else {
+        LOGW("No awb modes provided!");
+    }
+
+    uint8_t awbLockAvailable = parameter.getAwbLockAvailable()
+                                   ? ANDROID_CONTROL_AWB_LOCK_AVAILABLE_TRUE
+                                   : ANDROID_CONTROL_AWB_LOCK_AVAILABLE_FALSE;
+    settings->update(ANDROID_CONTROL_AWB_LOCK_AVAILABLE, &awbLockAvailable, 1);
+
+    int32_t rawSensitivity = 100;
+    settings->update(ANDROID_CONTROL_POST_RAW_SENSITIVITY_BOOST, &rawSensitivity, 1);
+
+    int32_t rawSensitivityRange[2] = {100, 100};
+    settings->update(ANDROID_CONTROL_POST_RAW_SENSITIVITY_BOOST_RANGE, rawSensitivityRange, 2);
+
+    entry = meta.find(CAMERA_CONTROL_MAX_REGIONS);
+    if (entry.count >= 1) {
+        settings->update(ANDROID_CONTROL_MAX_REGIONS, entry.data.i32, entry.count);
+    }
+}
+
+void MetadataConvert::fillScalerStaticMetadata(const icamera::Parameters& parameter,
+                                               android::CameraMetadata* settings) {
+// stream configuration: fmt, w, h, type
+#define STREAM_CFG_SIZE 4
+// duration: fmt, w, h, ns
+#define DURATION_SIZE 4
+
+    float maxDigitalZoom = 1.0;
+    settings->update(ANDROID_SCALER_AVAILABLE_MAX_DIGITAL_ZOOM, &maxDigitalZoom, 1);
+
+    uint8_t type = ANDROID_SCALER_CROPPING_TYPE_CENTER_ONLY;
+    settings->update(ANDROID_SCALER_CROPPING_TYPE, &type, 1);
+
+    icamera::CameraMetadata meta;
+    icamera::ParameterHelper::copyMetadata(parameter, &meta);
+
+    uint32_t tag = CAMERA_SCALER_AVAILABLE_INPUT_OUTPUT_FORMATS_MAP;
+    icamera_metadata_entry entry = meta.find(tag);
+    if (entry.count > 0) {
+        settings->update(ANDROID_SCALER_AVAILABLE_INPUT_OUTPUT_FORMATS_MAP, entry.data.i32,
+                         entry.count);
+    }
+
+    tag = CAMERA_REPROCESS_MAX_CAPTURE_STALL;
+    entry = meta.find(tag);
+    if (entry.count > 0) {
+        settings->update(ANDROID_REPROCESS_MAX_CAPTURE_STALL, entry.data.i32, entry.count);
+    }
+
+    tag = CAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS;
+    entry = meta.find(tag);
+    if (entry.count > 0) {
+        settings->update(ANDROID_SCALER_AVAILABLE_STREAM_CONFIGURATIONS, entry.data.i32,
+                         entry.count);
+    }
+
+    tag = CAMERA_SCALER_AVAILABLE_MIN_FRAME_DURATIONS;
+    entry = meta.find(tag);
+    if (entry.count > 0) {
+        settings->update(ANDROID_SCALER_AVAILABLE_MIN_FRAME_DURATIONS, entry.data.i64, entry.count);
+    }
+
+    tag = CAMERA_SCALER_AVAILABLE_STALL_DURATIONS;
+    entry = meta.find(tag);
+    if (entry.count > 0) {
+        settings->update(ANDROID_SCALER_AVAILABLE_STALL_DURATIONS, entry.data.i64, entry.count);
+    }
+}
+
+void MetadataConvert::fillTonemapStaticMetadata(const icamera::Parameters& parameter,
+                                                android::CameraMetadata* settings) {
+    int ret = icamera::OK;
+
+    int32_t maxPoint = 0;
+    if (parameter.getTonemapMaxCurvePoints(maxPoint) == 0) {
+        settings->update(ANDROID_TONEMAP_MAX_CURVE_POINTS, &maxPoint, 1);
+    }
+
+    std::vector<icamera::camera_tonemap_mode_t> tonemapModes;
+    parameter.getSupportedTonemapMode(tonemapModes);
+    if (tonemapModes.size() > 0) {
+        int size = tonemapModes.size();
+        uint8_t data[size];
+        int count = 0;
+        for (int i = 0; i < size; i++) {
+            ret = getAndroidValue(tonemapModes[i], tonemapModesTable, ARRAY_SIZE(tonemapModesTable),
+                                  &data[count]);
+            if (ret == icamera::OK) {
+                count++;
+            }
+        }
+        if (count > 0) {
+            settings->update(ANDROID_TONEMAP_AVAILABLE_TONE_MAP_MODES, data, count);
+        }
+    }
+}
+
+void MetadataConvert::fillSensorStaticMetadata(const icamera::Parameters& parameter,
+                                               android::CameraMetadata* settings) {
+    icamera::camera_range_t timeRange;
+    // Fill it if it is supported
+    if (parameter.getSupportedSensorExposureTimeRange(timeRange) == 0) {
+        int64_t range[2];
+        range[0] = timeRange.min * 1000LLU;  // us -> ns
+        range[1] = timeRange.max * 1000LLU;  // us -> ns
+        settings->update(ANDROID_SENSOR_INFO_EXPOSURE_TIME_RANGE, range, 2);
+        settings->update(ANDROID_SENSOR_INFO_MAX_FRAME_DURATION, &(range[1]), 1);
+    } else {
+        LOGW("No SensorExposureTimeRange provided!");
+    }
+
+    icamera::camera_range_t sensitivityRange;
+    if (parameter.getSupportedSensorSensitivityRange(sensitivityRange) == 0) {
+        int32_t range[2];
+        range[0] = (int32_t)sensitivityRange.min;
+        range[1] = (int32_t)sensitivityRange.max;
+        settings->update(ANDROID_SENSOR_INFO_SENSITIVITY_RANGE, range, 2);
+        settings->update(ANDROID_SENSOR_MAX_ANALOG_SENSITIVITY, &range[1], 1);
+    } else {
+        LOGW("No SensorSensitivityRange provided!");
+    }
+
+    icamera::CameraMetadata meta;
+    icamera::ParameterHelper::copyMetadata(parameter, &meta);
+
+    uint32_t tag = CAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE;
+    icamera_metadata_entry entry = meta.find(tag);
+    // Check if the count is correct
+    if (entry.count == 4) {
+        settings->update(ANDROID_SENSOR_INFO_ACTIVE_ARRAY_SIZE, entry.data.i32, entry.count);
+    }
+
+    tag = CAMERA_SENSOR_OPAQUE_RAW_SIZE;
+    entry = meta.find(tag);
+    if (entry.count > 0) {
+        settings->update(ANDROID_SENSOR_OPAQUE_RAW_SIZE, entry.data.i32, entry.count);
+    }
+
+    tag = CAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE;
+    entry = meta.find(tag);
+    if (entry.count == 2) {
+        settings->update(ANDROID_SENSOR_INFO_PIXEL_ARRAY_SIZE, entry.data.i32, entry.count);
+    }
+
+    tag = CAMERA_SENSOR_INFO_PHYSICAL_SIZE;
+    entry = meta.find(tag);
+    if (entry.count == 2) {
+        settings->update(ANDROID_SENSOR_INFO_PHYSICAL_SIZE, entry.data.f, entry.count);
+    }
+
+    tag = CAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT;
+    entry = meta.find(tag);
+    if (entry.count == 1) {
+        settings->update(ANDROID_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT, entry.data.u8, entry.count);
+    }
+
+    tag = CAMERA_SENSOR_AVAILABLE_TEST_PATTERN_MODES;
+    entry = meta.find(tag);
+    if (entry.count != 0) {
+        settings->update(ANDROID_SENSOR_AVAILABLE_TEST_PATTERN_MODES, entry.data.i32, entry.count);
+    }
+
+    int32_t whiteLevel = 0;
+    settings->update(ANDROID_SENSOR_INFO_WHITE_LEVEL, &whiteLevel, 1);
+
+    int32_t blackLevelPattern[4] = {0, 0, 0, 0};
+    settings->update(ANDROID_SENSOR_BLACK_LEVEL_PATTERN, blackLevelPattern, 4);
+
+    uint8_t timestampSource = ANDROID_SENSOR_INFO_TIMESTAMP_SOURCE_UNKNOWN;
+    settings->update(ANDROID_SENSOR_INFO_TIMESTAMP_SOURCE, &timestampSource, 1);
+
+    camera_metadata_rational_t baseGainFactor = {0, 1};
+    settings->update(ANDROID_SENSOR_BASE_GAIN_FACTOR, &baseGainFactor, 1);
+
+    int32_t orientation = 0;
+    tag = CAMERA_SENSOR_ORIENTATION;
+    entry = meta.find(tag);
+    if (entry.count == 1) {
+        orientation = entry.data.u8[0];
+    }
+    settings->update(ANDROID_SENSOR_ORIENTATION, &orientation, 1);
+
+    int32_t profileHueSatMapDimensions[3] = {0, 0, 0};
+    settings->update(ANDROID_SENSOR_PROFILE_HUE_SAT_MAP_DIMENSIONS, profileHueSatMapDimensions, 3);
+}
+
+void MetadataConvert::fillLensStaticMetadata(const icamera::Parameters& parameter,
+                                             android::CameraMetadata* settings) {
+    icamera::CameraMetadata meta;
+    icamera::ParameterHelper::copyMetadata(parameter, &meta);
+
+    float aperture = 0.0;
+    if (icamera::OK == parameter.getLensAperture(aperture)) {
+        settings->update(ANDROID_LENS_INFO_AVAILABLE_APERTURES, &aperture, 1);
+    }
+
+    float filterDensity = 0.0;
+    if (icamera::OK == parameter.getLensFilterDensity(filterDensity)) {
+        settings->update(ANDROID_LENS_INFO_AVAILABLE_FILTER_DENSITIES, &filterDensity, 1);
+    }
+
+    uint32_t tag = CAMERA_LENS_INFO_AVAILABLE_FOCAL_LENGTHS;
+    icamera_metadata_entry entry = meta.find(tag);
+    if (entry.count >= 1) {
+        settings->update(ANDROID_LENS_INFO_AVAILABLE_FOCAL_LENGTHS, entry.data.f, entry.count);
+    }
+
+    float hyperfocalDistance = 0.0;
+    if (icamera::OK == parameter.getLensHyperfocalDistance(hyperfocalDistance)) {
+        settings->update(ANDROID_LENS_INFO_HYPERFOCAL_DISTANCE, &hyperfocalDistance, 1);
+    }
+
+    float minFocusDistance = 0.0;
+    if (icamera::OK == parameter.getLensMinFocusDistance(minFocusDistance)) {
+        settings->update(ANDROID_LENS_INFO_MINIMUM_FOCUS_DISTANCE, &minFocusDistance, 1);
+    }
+
+    tag = CAMERA_LENS_INFO_SHADING_MAP_SIZE;
+    entry = meta.find(tag);
+    if (entry.count >= 1) {
+        settings->update(ANDROID_LENS_INFO_SHADING_MAP_SIZE, entry.data.i32, entry.count);
+    }
+
+    tag = CAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION;
+    entry = meta.find(tag);
+    if (entry.count == 1) {
+        settings->update(ANDROID_LENS_INFO_FOCUS_DISTANCE_CALIBRATION, entry.data.u8, entry.count);
+    }
+
+    tag = CAMERA_LENS_FACING;
+    entry = meta.find(tag);
+    uint8_t lensFacing = ANDROID_LENS_FACING_BACK;
+    if (entry.count == 1) {
+        lensFacing = entry.data.u8[0];
+    }
+    settings->update(ANDROID_LENS_FACING, &lensFacing, 1);
+
+    uint8_t availableOpticalStabilization = ANDROID_LENS_OPTICAL_STABILIZATION_MODE_OFF;
+    settings->update(ANDROID_LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION,
+                     &availableOpticalStabilization, 1);
+}
+
+void MetadataConvert::fillRequestStaticMetadata(const icamera::Parameters& parameter,
+                                                android::CameraMetadata* settings) {
+    icamera::CameraMetadata meta;
+    icamera::ParameterHelper::copyMetadata(parameter, &meta);
+
+    uint32_t tag = CAMERA_REQUEST_MAX_NUM_OUTPUT_STREAMS;
+    icamera_metadata_entry entry = meta.find(tag);
+    if (entry.count >= 1) {
+        settings->update(ANDROID_REQUEST_MAX_NUM_OUTPUT_STREAMS, entry.data.i32, entry.count);
+    }
+
+    tag = CAMERA_REQUEST_PIPELINE_MAX_DEPTH;
+    entry = meta.find(tag);
+    if (entry.count == 1) {
+        settings->update(ANDROID_REQUEST_PIPELINE_MAX_DEPTH, entry.data.u8, entry.count);
+    }
+
+    tag = CAMERA_REQUEST_AVAILABLE_CAPABILITIES;
+    entry = meta.find(tag);
+    if (entry.count >= 1) {
+        settings->update(ANDROID_REQUEST_AVAILABLE_CAPABILITIES, entry.data.u8, entry.count);
+    }
+
+    tag = CAMERA_REQUEST_MAX_NUM_INPUT_STREAMS;
+    entry = meta.find(tag);
+    if (entry.count >= 1) {
+        settings->update(ANDROID_REQUEST_MAX_NUM_INPUT_STREAMS, entry.data.i32, entry.count);
+    }
+
+    int32_t partialResultCount = 1;
+    settings->update(ANDROID_REQUEST_PARTIAL_RESULT_COUNT, &partialResultCount, 1);
+
+    int32_t requestKeysBasic[] = {ANDROID_BLACK_LEVEL_LOCK,
+                                  ANDROID_COLOR_CORRECTION_ABERRATION_MODE,
+                                  ANDROID_COLOR_CORRECTION_GAINS,
+                                  ANDROID_COLOR_CORRECTION_TRANSFORM,
+                                  ANDROID_CONTROL_AE_ANTIBANDING_MODE,
+                                  ANDROID_CONTROL_AE_EXPOSURE_COMPENSATION,
+                                  ANDROID_CONTROL_AE_LOCK,
+                                  ANDROID_CONTROL_AE_MODE,
+                                  ANDROID_CONTROL_AE_TARGET_FPS_RANGE,
+                                  ANDROID_CONTROL_AF_MODE,
+                                  ANDROID_CONTROL_AE_REGIONS,
+                                  ANDROID_CONTROL_AF_REGIONS,
+                                  ANDROID_CONTROL_AF_TRIGGER,
+                                  ANDROID_CONTROL_AWB_LOCK,
+                                  ANDROID_CONTROL_AWB_MODE,
+                                  ANDROID_CONTROL_CAPTURE_INTENT,
+                                  ANDROID_CONTROL_EFFECT_MODE,
+                                  ANDROID_CONTROL_MODE,
+                                  ANDROID_CONTROL_SCENE_MODE,
+                                  ANDROID_CONTROL_VIDEO_STABILIZATION_MODE,
+                                  ANDROID_EDGE_MODE,
+                                  ANDROID_FLASH_MODE,
+                                  ANDROID_JPEG_ORIENTATION,
+                                  ANDROID_JPEG_QUALITY,
+                                  ANDROID_JPEG_THUMBNAIL_QUALITY,
+                                  ANDROID_JPEG_THUMBNAIL_SIZE,
+                                  ANDROID_SCALER_CROP_REGION,
+                                  ANDROID_STATISTICS_FACE_DETECT_MODE,
+                                  ANDROID_SENSOR_FRAME_DURATION,
+                                  ANDROID_SENSOR_EXPOSURE_TIME,
+                                  ANDROID_SENSOR_SENSITIVITY,
+                                  ANDROID_HOT_PIXEL_MODE,
+                                  ANDROID_LENS_APERTURE,
+                                  ANDROID_LENS_FOCAL_LENGTH,
+                                  ANDROID_LENS_FOCUS_DISTANCE,
+                                  ANDROID_LENS_FILTER_DENSITY,
+                                  ANDROID_LENS_OPTICAL_STABILIZATION_MODE,
+                                  ANDROID_NOISE_REDUCTION_MODE,
+                                  ANDROID_REQUEST_ID,
+                                  ANDROID_REQUEST_TYPE,
+                                  ANDROID_TONEMAP_MODE,
+                                  ANDROID_TONEMAP_PRESET_CURVE,
+                                  ANDROID_TONEMAP_GAMMA,
+                                  ANDROID_SHADING_MODE,
+                                  ANDROID_STATISTICS_LENS_SHADING_MAP_MODE};
+    size_t requestKeysCnt = sizeof(requestKeysBasic) / sizeof(requestKeysBasic[0]);
+    settings->update(ANDROID_REQUEST_AVAILABLE_REQUEST_KEYS, requestKeysBasic, requestKeysCnt);
+
+    int32_t resultKeysBasic[] = {ANDROID_REQUEST_ID,
+                                 ANDROID_REQUEST_TYPE,
+                                 ANDROID_COLOR_CORRECTION_MODE,
+                                 ANDROID_COLOR_CORRECTION_GAINS,
+                                 ANDROID_COLOR_CORRECTION_TRANSFORM,
+                                 ANDROID_COLOR_CORRECTION_ABERRATION_MODE,
+                                 ANDROID_CONTROL_AE_ANTIBANDING_MODE,
+                                 ANDROID_CONTROL_AE_EXPOSURE_COMPENSATION,
+                                 ANDROID_CONTROL_AE_LOCK,
+                                 ANDROID_CONTROL_AE_MODE,
+                                 ANDROID_CONTROL_AE_TARGET_FPS_RANGE,
+                                 ANDROID_CONTROL_AF_MODE,
+                                 ANDROID_CONTROL_AE_REGIONS,
+                                 ANDROID_CONTROL_AF_REGIONS,
+                                 ANDROID_CONTROL_AF_TRIGGER,
+                                 ANDROID_CONTROL_AWB_LOCK,
+                                 ANDROID_CONTROL_AWB_MODE,
+                                 ANDROID_CONTROL_CAPTURE_INTENT,
+                                 ANDROID_CONTROL_EFFECT_MODE,
+                                 ANDROID_CONTROL_MODE,
+                                 ANDROID_CONTROL_SCENE_MODE,
+                                 ANDROID_CONTROL_VIDEO_STABILIZATION_MODE,
+                                 ANDROID_CONTROL_AE_STATE,
+                                 ANDROID_CONTROL_AF_STATE,
+                                 ANDROID_CONTROL_AWB_STATE,
+                                 ANDROID_SYNC_FRAME_NUMBER,
+                                 ANDROID_EDGE_MODE,
+                                 ANDROID_FLASH_MODE,
+                                 ANDROID_JPEG_ORIENTATION,
+                                 ANDROID_JPEG_QUALITY,
+                                 ANDROID_JPEG_THUMBNAIL_QUALITY,
+                                 ANDROID_JPEG_THUMBNAIL_SIZE,
+                                 ANDROID_LENS_APERTURE,
+                                 ANDROID_LENS_FOCAL_LENGTH,
+                                 ANDROID_LENS_FOCUS_DISTANCE,
+                                 ANDROID_LENS_FILTER_DENSITY,
+                                 ANDROID_LENS_FOCUS_RANGE,
+                                 ANDROID_LENS_STATE,
+                                 ANDROID_LENS_OPTICAL_STABILIZATION_MODE,
+                                 ANDROID_SCALER_CROP_REGION,
+                                 ANDROID_SENSOR_FRAME_DURATION,
+                                 ANDROID_SENSOR_EXPOSURE_TIME,
+                                 ANDROID_SENSOR_SENSITIVITY,
+                                 ANDROID_HOT_PIXEL_MODE,
+                                 ANDROID_REQUEST_PIPELINE_DEPTH,
+                                 ANDROID_SHADING_MODE,
+                                 ANDROID_STATISTICS_FACE_DETECT_MODE,
+                                 ANDROID_STATISTICS_HOT_PIXEL_MAP_MODE,
+                                 ANDROID_STATISTICS_LENS_SHADING_MAP_MODE,
+                                 ANDROID_STATISTICS_SCENE_FLICKER,
+                                 ANDROID_NOISE_REDUCTION_MODE,
+                                 ANDROID_TONEMAP_CURVE_RED,
+                                 ANDROID_TONEMAP_CURVE_BLUE,
+                                 ANDROID_TONEMAP_CURVE_GREEN};
+    size_t resultKeysCnt = sizeof(resultKeysBasic) / sizeof(resultKeysBasic[0]);
+    settings->update(ANDROID_REQUEST_AVAILABLE_RESULT_KEYS, resultKeysBasic, resultKeysCnt);
+
+    int32_t characteristicsKeysBasic[] = {ANDROID_COLOR_CORRECTION_AVAILABLE_ABERRATION_MODES,
+                                          ANDROID_CONTROL_AVAILABLE_MODES,
+                                          ANDROID_CONTROL_AE_AVAILABLE_ANTIBANDING_MODES,
+                                          ANDROID_CONTROL_AE_AVAILABLE_MODES,
+                                          ANDROID_CONTROL_AE_AVAILABLE_TARGET_FPS_RANGES,
+                                          ANDROID_CONTROL_AE_COMPENSATION_RANGE,
+                                          ANDROID_CONTROL_AE_COMPENSATION_STEP,
+                                          ANDROID_CONTROL_AE_LOCK_AVAILABLE,
+                                          ANDROID_CONTROL_AF_AVAILABLE_MODES,
+                                          ANDROID_CONTROL_AVAILABLE_EFFECTS,
+                                          ANDROID_CONTROL_AVAILABLE_SCENE_MODES,
+                                          ANDROID_CONTROL_AVAILABLE_VIDEO_STABILIZATION_MODES,
+                                          ANDROID_CONTROL_AWB_AVAILABLE_MODES,
+                                          ANDROID_CONTROL_AWB_LOCK_AVAILABLE,
+                                          ANDROID_EDGE_AVAILABLE_EDGE_MODES,
+                                          ANDROID_FLASH_INFO_AVAILABLE,
+                                          ANDROID_HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES,
+                                          ANDROID_INFO_SUPPORTED_HARDWARE_LEVEL,
+                                          ANDROID_JPEG_AVAILABLE_THUMBNAIL_SIZES,
+                                          ANDROID_LENS_FACING,
+                                          ANDROID_LENS_INFO_AVAILABLE_APERTURES,
+                                          ANDROID_LENS_INFO_AVAILABLE_FILTER_DENSITIES,
+                                          ANDROID_LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION,
+                                          ANDROID_LENS_INFO_FOCUS_DISTANCE_CALIBRATION,
+                                          ANDROID_LENS_INFO_HYPERFOCAL_DISTANCE,
+                                          ANDROID_LENS_INFO_MINIMUM_FOCUS_DISTANCE,
+                                          ANDROID_LENS_INFO_AVAILABLE_FOCAL_LENGTHS,
+                                          ANDROID_NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES,
+                                          ANDROID_REQUEST_AVAILABLE_CAPABILITIES,
+                                          ANDROID_REQUEST_MAX_NUM_INPUT_STREAMS,
+                                          ANDROID_REQUEST_MAX_NUM_OUTPUT_STREAMS,
+                                          ANDROID_REQUEST_PARTIAL_RESULT_COUNT,
+                                          ANDROID_REQUEST_PIPELINE_MAX_DEPTH,
+                                          ANDROID_REQUEST_AVAILABLE_REQUEST_KEYS,
+                                          ANDROID_REQUEST_AVAILABLE_RESULT_KEYS,
+                                          ANDROID_REQUEST_AVAILABLE_CHARACTERISTICS_KEYS,
+                                          ANDROID_SCALER_AVAILABLE_MAX_DIGITAL_ZOOM,
+                                          ANDROID_SCALER_AVAILABLE_STREAM_CONFIGURATIONS,
+                                          ANDROID_SCALER_CROPPING_TYPE,
+                                          ANDROID_SENSOR_BLACK_LEVEL_PATTERN,
+                                          ANDROID_SENSOR_ORIENTATION,
+                                          ANDROID_SENSOR_INFO_ACTIVE_ARRAY_SIZE,
+                                          ANDROID_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT,
+                                          ANDROID_SENSOR_INFO_EXPOSURE_TIME_RANGE,
+                                          ANDROID_SENSOR_INFO_MAX_FRAME_DURATION,
+                                          ANDROID_SENSOR_INFO_PIXEL_ARRAY_SIZE,
+                                          ANDROID_SENSOR_INFO_SENSITIVITY_RANGE,
+                                          ANDROID_SENSOR_INFO_PHYSICAL_SIZE,
+                                          ANDROID_SENSOR_INFO_WHITE_LEVEL,
+                                          ANDROID_SENSOR_INFO_TIMESTAMP_SOURCE,
+                                          ANDROID_SENSOR_AVAILABLE_TEST_PATTERN_MODES,
+                                          ANDROID_SENSOR_MAX_ANALOG_SENSITIVITY,
+                                          ANDROID_SHADING_AVAILABLE_MODES,
+                                          ANDROID_STATISTICS_INFO_AVAILABLE_FACE_DETECT_MODES,
+                                          ANDROID_STATISTICS_INFO_MAX_FACE_COUNT,
+                                          ANDROID_SYNC_MAX_LATENCY,
+                                          ANDROID_TONEMAP_AVAILABLE_TONE_MAP_MODES,
+                                          ANDROID_TONEMAP_MAX_CURVE_POINTS};
+    settings->update(ANDROID_REQUEST_AVAILABLE_CHARACTERISTICS_KEYS, characteristicsKeysBasic,
+                     sizeof(characteristicsKeysBasic) / sizeof(int32_t));
+}
+
+void MetadataConvert::fillStatisticsStaticMetadata(const icamera::Parameters& parameter,
+                                                   android::CameraMetadata* settings) {
+    icamera::CameraMetadata meta;
+    icamera::ParameterHelper::copyMetadata(parameter, &meta);
+
+    icamera_metadata_entry entry;
+    entry = meta.find(CAMERA_STATISTICS_INFO_AVAILABLE_FACE_DETECT_MODES);
+    if (entry.count != 0) {
+        settings->update(ANDROID_STATISTICS_INFO_AVAILABLE_FACE_DETECT_MODES, entry.data.u8,
+                         entry.count);
+    } else {
+        uint8_t availFaceDetectMode = ANDROID_STATISTICS_FACE_DETECT_MODE_OFF;
+        settings->update(ANDROID_STATISTICS_INFO_AVAILABLE_FACE_DETECT_MODES, &availFaceDetectMode,
+                         1);
+    }
+
+    entry = meta.find(CAMERA_STATISTICS_INFO_MAX_FACE_COUNT);
+    if (entry.count != 0) {
+        settings->update(ANDROID_STATISTICS_INFO_MAX_FACE_COUNT, entry.data.i32, entry.count);
+    } else {
+        int32_t maxFaceCount = 0;
+        settings->update(ANDROID_STATISTICS_INFO_MAX_FACE_COUNT, &maxFaceCount, 1);
+    }
+
+    int32_t histogramBucketCount = 0;
+    settings->update(ANDROID_STATISTICS_INFO_HISTOGRAM_BUCKET_COUNT, &histogramBucketCount, 1);
+
+    int32_t maxHistogramCount = 0;
+    settings->update(ANDROID_STATISTICS_INFO_MAX_HISTOGRAM_COUNT, &maxHistogramCount, 1);
+
+    int32_t maxSharpnessMapValue = 0;
+    settings->update(ANDROID_STATISTICS_INFO_MAX_SHARPNESS_MAP_VALUE, &maxSharpnessMapValue, 1);
+
+    int32_t sharpnessMapSize[2] = {0, 0};
+    settings->update(ANDROID_STATISTICS_INFO_SHARPNESS_MAP_SIZE, sharpnessMapSize, 2);
+
+    uint8_t availableHotPixelMapModes = ANDROID_STATISTICS_HOT_PIXEL_MAP_MODE_OFF;
+    settings->update(ANDROID_STATISTICS_INFO_AVAILABLE_HOT_PIXEL_MAP_MODES,
+                     &availableHotPixelMapModes, 1);
+
+    uint8_t availableLensShadingMapModes = ANDROID_STATISTICS_LENS_SHADING_MAP_MODE_OFF;
+    settings->update(ANDROID_STATISTICS_INFO_AVAILABLE_LENS_SHADING_MAP_MODES,
+                     &availableLensShadingMapModes, 1);
+}
+
+void MetadataConvert::fillJpegStaticMetadata(const icamera::Parameters& parameter,
+                                             android::CameraMetadata* settings) {
+    icamera::CameraMetadata meta;
+    icamera::ParameterHelper::copyMetadata(parameter, &meta);
+
+    uint32_t tag = CAMERA_JPEG_MAX_SIZE;
+    icamera_metadata_entry entry = meta.find(tag);
+    if (entry.count == 1) {
+        settings->update(ANDROID_JPEG_MAX_SIZE, entry.data.i32, entry.count);
+    }
+
+    tag = CAMERA_JPEG_AVAILABLE_THUMBNAIL_SIZES;
+    entry = meta.find(tag);
+    if (entry.count >= 2) {
+        settings->update(ANDROID_JPEG_AVAILABLE_THUMBNAIL_SIZES, entry.data.i32, entry.count);
+    }
+}
+
+void MetadataConvert::fillAdvancedFeatureStaticMetadata(const icamera::Parameters& parameter,
+                                                        android::CameraMetadata* settings) {
+    icamera::CameraMetadata meta;
+    icamera::ParameterHelper::copyMetadata(parameter, &meta);
+
+    // ANDROID_DEMOSAIC
+
+    // ANDROID_EDGE
+    uint32_t tag = CAMERA_EDGE_AVAILABLE_EDGE_MODES;
+    icamera_metadata_entry entry = meta.find(tag);
+    if (entry.count >= 1) {
+        settings->update(ANDROID_EDGE_AVAILABLE_EDGE_MODES, entry.data.u8, entry.count);
+    }
+
+    // ANDROID_HOT_PIXEL
+    tag = CAMERA_HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES;
+    entry = meta.find(tag);
+    if (entry.count >= 1) {
+        settings->update(ANDROID_HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES, entry.data.u8, entry.count);
+    }
+
+    // ANDROID_NOISE_REDUCTION
+    tag = CAMERA_NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES;
+    entry = meta.find(tag);
+    if (entry.count >= 1) {
+        settings->update(ANDROID_NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES, entry.data.u8,
+                         entry.count);
+    }
+
+    // ANDROID_SHADING
+    tag = CAMERA_SHADING_AVAILABLE_MODES;
+    entry = meta.find(tag);
+    if (entry.count != 0) {
+        settings->update(ANDROID_SHADING_AVAILABLE_MODES, entry.data.u8, entry.count);
+    }
+
+    // ANDROID_TONEMAP
+    tag = CAMERA_TONEMAP_MAX_CURVE_POINTS;
+    entry = meta.find(tag);
+    if (entry.count == 1) {
+        settings->update(ANDROID_TONEMAP_MAX_CURVE_POINTS, entry.data.i32, entry.count);
+    }
+
+    tag = CAMERA_TONEMAP_AVAILABLE_TONE_MAP_MODES;
+    entry = meta.find(tag);
+    if (entry.count >= 1) {
+        settings->update(ANDROID_TONEMAP_AVAILABLE_TONE_MAP_MODES, entry.data.u8, entry.count);
+    }
+
+    // ANDROID_INFO
+    tag = CAMERA_INFO_SUPPORTED_HARDWARE_LEVEL;
+    entry = meta.find(tag);
+    if (entry.count == 1) {
+        settings->update(ANDROID_INFO_SUPPORTED_HARDWARE_LEVEL, entry.data.u8, entry.count);
+    }
+
+    // ANDROID_BLACK_LEVEL
+
+    // ANDROID_SYNC
+    tag = CAMERA_SYNC_MAX_LATENCY;
+    entry = meta.find(tag);
+    if (entry.count == 1) {
+        settings->update(ANDROID_SYNC_MAX_LATENCY, entry.data.i32, entry.count);
+    }
+}
+
+int MetadataConvert::convertToHalWindow(const int32_t* data, int dataCount,
+                                        icamera::camera_window_list_t* windows) {
+    windows->clear();
+    CheckError((!data), icamera::BAD_VALUE, "null data to convert hal window!");
+    CheckError((dataCount % 5 != 0), icamera::BAD_VALUE, "wrong data count %d!", dataCount);
+
+    icamera::camera_window_t window;
+    for (int i = 0; i < dataCount / 5; i += 5) {
+        window.left = data[i];
+        window.top = data[i + 1];
+        window.right = data[i + 2];
+        window.bottom = data[i + 3];
+        window.weight = data[i + 4];
+        windows->push_back(window);
+    }
+    return icamera::OK;
+}
+
+int MetadataConvert::convertToMetadataRegion(const icamera::camera_window_list_t& windows,
+                                             int dataCount, int32_t* data) {
+    size_t num = windows.size();
+    CheckError((!data), 0, "null data to convert Metadata region!");
+    CheckError(((unsigned int)dataCount < num * 5), 0, "small dataCount!");
+
+    for (size_t i = 0; i < windows.size(); i++) {
+        data[i * 5] = windows[i].left;
+        data[i * 5 + 1] = windows[i].top;
+        data[i * 5 + 2] = windows[i].right;
+        data[i * 5 + 3] = windows[i].bottom;
+        data[i * 5 + 4] = windows[i].weight;
+    }
+
+    return num * 5;
+}
+
+void MetadataConvert::dumpMetadata(const camera_metadata_t* meta) {
+    if (!meta || !icamera::Log::isDebugLevelEnable(icamera::CAMERA_DEBUG_LOG_LEVEL2)) {
+        return;
+    }
+
+    LOG2("%s", __func__);
+    int entryCount = get_camera_metadata_entry_count(meta);
+
+    for (int i = 0; i < entryCount; i++) {
+        camera_metadata_entry_t entry;
+        if (get_camera_metadata_entry(const_cast<camera_metadata_t*>(meta), i, &entry)) {
+            continue;
+        }
+
+        // Print tag & type
+        const char *tagName, *tagSection;
+        tagSection = get_camera_metadata_section_name(entry.tag);
+        if (tagSection == nullptr) {
+            tagSection = "unknownSection";
+        }
+        tagName = get_camera_metadata_tag_name(entry.tag);
+        if (tagName == nullptr) {
+            tagName = "unknownTag";
+        }
+        const char* typeName;
+        if (entry.type >= NUM_TYPES) {
+            typeName = "unknown";
+        } else {
+            typeName = camera_metadata_type_names[entry.type];
+        }
+        LOG2("(%d)%s.%s (%05x): %s[%zu], type: %d", i, tagSection, tagName, entry.tag, typeName,
+             entry.count, entry.type);
+
+        // Print data
+        size_t j;
+        const uint8_t* u8;
+        const int32_t* i32;
+        const float* f;
+        const int64_t* i64;
+        const double* d;
+        const camera_metadata_rational_t* r;
+        std::ostringstream stringStream;
+        stringStream << "[";
+
+        switch (entry.type) {
+            case TYPE_BYTE:
+                u8 = entry.data.u8;
+                for (j = 0; j < entry.count; j++) stringStream << (int32_t)u8[j] << " ";
+                break;
+            case TYPE_INT32:
+                i32 = entry.data.i32;
+                for (j = 0; j < entry.count; j++) stringStream << " " << i32[j] << " ";
+                break;
+            case TYPE_FLOAT:
+                f = entry.data.f;
+                for (j = 0; j < entry.count; j++) stringStream << " " << f[j] << " ";
+                break;
+            case TYPE_INT64:
+                i64 = entry.data.i64;
+                for (j = 0; j < entry.count; j++) stringStream << " " << i64[j] << " ";
+                break;
+            case TYPE_DOUBLE:
+                d = entry.data.d;
+                for (j = 0; j < entry.count; j++) stringStream << " " << d[j] << " ";
+                break;
+            case TYPE_RATIONAL:
+                r = entry.data.r;
+                for (j = 0; j < entry.count; j++)
+                    stringStream << " (" << r[j].numerator << ", " << r[j].denominator << ") ";
+                break;
+        }
+        stringStream << "]";
+        std::string str = stringStream.str();
+        LOG2("%s", str.c_str());
+    }
+}
+
+StaticCapability::StaticCapability(int cameraId) : mCameraId(cameraId) {
+    LOG2("@%s, mCameraId %d", __func__, mCameraId);
+
+    icamera::camera_info_t cameraInfo = {};
+    icamera::get_camera_info(mCameraId, cameraInfo);
+    icamera::ParameterHelper::copyMetadata(*cameraInfo.capability, &mMetadata);
+}
+
+StaticCapability::~StaticCapability() {
+    LOG2("@%s, mCameraId: %d", __func__, mCameraId);
+}
+
+std::mutex StaticCapability::sLock;
+std::unordered_map<int, StaticCapability*> StaticCapability::sInstances;
+
+StaticCapability* StaticCapability::getInstance(int cameraId) {
+    std::lock_guard<std::mutex> lock(sLock);
+    if (sInstances.find(cameraId) == sInstances.end()) {
+        sInstances[cameraId] = new StaticCapability(cameraId);
+    }
+
+    return sInstances[cameraId];
+}
+
+void StaticCapability::releaseInstance(int cameraId) {
+    std::lock_guard<std::mutex> lock(sLock);
+    if (sInstances.find(cameraId) != sInstances.end()) {
+        StaticCapability* capability = sInstances[cameraId];
+        sInstances.erase(cameraId);
+        delete capability;
+    }
+}
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/MetadataConvert.h b/camera/hal/intel/ipu6/aal/MetadataConvert.h
new file mode 100644
index 000000000000..ba4021af2af2
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/MetadataConvert.h
@@ -0,0 +1,142 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <hardware/camera3.h>
+
+#include <unordered_map>
+
+#include "FaceDetection.h"
+#include "HALv3Header.h"
+#include "ParameterHelper.h"
+#include "Parameters.h"
+
+namespace camera3 {
+
+/**
+ * \class MetadataConvert
+ *
+ * This class is used to convert application metadata to HAL metadata.
+ *
+ */
+class MetadataConvert {
+ public:
+    MetadataConvert(int cameraId);
+    virtual ~MetadataConvert();
+
+    static int constructDefaultMetadata(int cameraId, android::CameraMetadata* settings);
+    static int updateDefaultRequestSettings(int cameraId, int type,
+                                            android::CameraMetadata* settings);
+
+    static int requestMetadataToHALMetadata(const android::CameraMetadata& settings,
+                                            icamera::Parameters* parameter, bool forceConvert);
+    static int HALMetadataToRequestMetadata(const icamera::Parameters& parameter,
+                                            android::CameraMetadata* settings, int cameraId);
+    static int HALCapabilityToStaticMetadata(const icamera::Parameters& parameter,
+                                             android::CameraMetadata* settings);
+    static void dumpMetadata(const camera_metadata_t* meta);
+    static void convertFaceDetectionMetadata(const icamera::CVFaceDetectionAbstractResult& fdResult,
+                                             android::CameraMetadata* settings);
+
+ private:
+    DISALLOW_COPY_AND_ASSIGN(MetadataConvert);
+
+    // Parameters -> Android dynamic metadata
+    static int convertColorCorrectionParameter(const icamera::Parameters& parameter,
+                                               android::CameraMetadata* settings);
+    static int convertControlParameter(const icamera::Parameters& parameter,
+                                       android::CameraMetadata* settings);
+    static int convertRequestParameter(const icamera::Parameters& parameter,
+                                       android::CameraMetadata* settings, int cameraId);
+    static int convertSensorParameter(const icamera::Parameters& parameter,
+                                      android::CameraMetadata* settings);
+    static int convertLensParameter(const icamera::Parameters& parameter,
+                                    android::CameraMetadata* settings);
+    static int convertStatisticsParameter(const icamera::Parameters& /*parameter*/,
+                                          android::CameraMetadata* settings);
+    static int convertTonemapParameter(const icamera::Parameters& parameter,
+                                       android::CameraMetadata* settings);
+    static int convertFlashParameter(const icamera::Parameters& /*parameter*/,
+                                     android::CameraMetadata* settings);
+    static int convertAdvancedFeatureMetadata(const android::CameraMetadata& settings,
+                                              icamera::Parameters* parameter);
+    static int convertAdvancedFeatureParameter(const icamera::Parameters& parameter,
+                                               android::CameraMetadata* settings);
+
+    // Android control metadata -> parameters
+    static int convertColorCorrectionMetadata(const android::CameraMetadata& settings,
+                                              icamera::Parameters* parameter);
+    static int convertControlMetadata(const android::CameraMetadata& settings,
+                                      icamera::Parameters* parameter);
+    static int convertTonemapMetadata(const android::CameraMetadata& settings,
+                                      icamera::Parameters* parameter);
+    static int convertJpegMetadata(const android::CameraMetadata& settings,
+                                   icamera::Parameters* parameter);
+    static int convertSensorMetadata(const android::CameraMetadata& settings,
+                                     icamera::Parameters* parameter, bool forceConvert);
+    static int convertLensMetadata(const android::CameraMetadata& settings,
+                                   icamera::Parameters* parameter);
+
+    // Capabilities -> Android static metadata
+    static void fillControlStaticMetadata(const icamera::Parameters& parameter,
+                                          android::CameraMetadata* settings);
+    static void fillScalerStaticMetadata(const icamera::Parameters& parameter,
+                                         android::CameraMetadata* settings);
+    static void fillSensorStaticMetadata(const icamera::Parameters& parameter,
+                                         android::CameraMetadata* settings);
+    static void fillLensStaticMetadata(const icamera::Parameters& parameter,
+                                       android::CameraMetadata* settings);
+    static void fillRequestStaticMetadata(const icamera::Parameters& parameter,
+                                          android::CameraMetadata* settings);
+    static void fillStatisticsStaticMetadata(const icamera::Parameters& parameter,
+                                             android::CameraMetadata* settings);
+    static void fillTonemapStaticMetadata(const icamera::Parameters& parameter,
+                                          android::CameraMetadata* settings);
+    static void fillJpegStaticMetadata(const icamera::Parameters& parameter,
+                                       android::CameraMetadata* settings);
+    static void fillAdvancedFeatureStaticMetadata(const icamera::Parameters& parameter,
+                                                  android::CameraMetadata* settings);
+
+    static int convertToHalWindow(const int32_t* data, int dataCount,
+                                  icamera::camera_window_list_t* windows);
+    static int convertToMetadataRegion(const icamera::camera_window_list_t& windows, int dataCount,
+                                       int32_t* data);
+
+ private:
+    int mCameraId;
+};
+
+class StaticCapability {
+ public:
+    const icamera::CameraMetadata* getCapability() { return &mMetadata; }
+    static StaticCapability* getInstance(int cameraId);
+    static void releaseInstance(int cameraId);
+
+ private:
+    StaticCapability(int cameraId);
+    virtual ~StaticCapability();
+
+ private:
+    // Guard for singleton instance creation.
+    static std::mutex sLock;
+    static std::unordered_map<int, StaticCapability*> sInstances;
+
+    icamera::CameraMetadata mMetadata;
+    int mCameraId;
+};
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/PostProcessor.cpp b/camera/hal/intel/ipu6/aal/PostProcessor.cpp
new file mode 100644
index 000000000000..962486a278dc
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/PostProcessor.cpp
@@ -0,0 +1,193 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "PostProcessor"
+
+#include "PostProcessor.h"
+
+#include "HALv3Utils.h"
+
+namespace camera3 {
+
+PostProcessor::PostProcessor(int cameraId, const camera3_stream_t& stream)
+        : mCameraId(cameraId),
+          mPostProcessType(icamera::POST_PROCESS_NONE),
+          mPostProcessorCore(std::unique_ptr<icamera::PostProcessorCore>(
+              new icamera::PostProcessorCore(cameraId))) {
+    LOG1("@%s", __func__);
+}
+
+PostProcessor::~PostProcessor() {
+    LOG1("@%s", __func__);
+}
+
+icamera::status_t PostProcessor::configure(const camera3_stream_t& stream,
+                                           const camera3_stream_t& srcStream) {
+    LOG1("@%s, stream: w:%d, h:%d, f:%d", __func__, stream.width, stream.height, stream.format);
+    LOG1("@%s, srcStream: w:%d, h:%d, f:%d", __func__, srcStream.width, srcStream.height,
+         srcStream.format);
+
+    icamera::stream_t halStream;
+    int ret = camera3::HalV3Utils::fillHALStreams(mCameraId, srcStream, &halStream);
+    LOG1("@%s, halStream: w:%d, h:%d, f:%d, size:%d, stride:%d, ret:%d", __func__, halStream.width,
+         halStream.height, halStream.format, halStream.size, halStream.stride, ret);
+    CheckError(ret != icamera::OK, ret, "fillHALStreams fails, ret %d", ret);
+
+    return configure(stream, halStream);
+}
+
+/* configure
+ *
+ * Decide post-processing is needed based on user stream and hal stream.
+ * The default processing order is rotate -> crop -> scale -> convert -> encode.
+ */
+icamera::status_t PostProcessor::configure(const camera3_stream_t& stream,
+                                           const icamera::stream_t& halStream) {
+    LOG1("@%s, stream: w:%d, h:%d, f:%d", __func__, stream.width, stream.height, stream.format);
+    LOG1("@%s, halStream: w:%d, h:%d, f:%d, size:%d, stride:%d", __func__, halStream.width,
+         halStream.height, halStream.format, halStream.size, halStream.stride);
+
+    icamera::PostProcessInfo info;
+    mPostProcessType = icamera::POST_PROCESS_NONE;
+    std::vector<icamera::PostProcessInfo> processingOrder;
+    int angle = HalV3Utils::getRotationDegrees(stream);
+
+    /* Fill the input/output information for the post processing unit.
+     * The input info of processing unit is the output info of last unit.
+     */
+    icamera::stream_t inputStreamInfo = halStream;
+    if (angle > 0 && mPostProcessorCore->isPostProcessTypeSupported(icamera::POST_PROCESS_ROTATE)) {
+        mPostProcessType |= icamera::POST_PROCESS_ROTATE;
+        info.angle = angle;
+        info.type = icamera::POST_PROCESS_ROTATE;
+        info.inputInfo = inputStreamInfo;
+        info.outputInfo = inputStreamInfo;
+        info.outputInfo.width = inputStreamInfo.height;
+        info.outputInfo.height = inputStreamInfo.width;
+        info.outputInfo.stride = inputStreamInfo.height;
+        info.outputInfo.format = inputStreamInfo.format;
+        info.outputInfo.size = icamera::CameraUtils::getFrameSize(
+            info.outputInfo.format, info.outputInfo.width, info.outputInfo.height);
+        LOG2("%s, Rotate: input %dx%d, output: %dx%d, angle: %d", __func__, inputStreamInfo.width,
+             inputStreamInfo.height, info.outputInfo.width, info.outputInfo.height, angle);
+
+        inputStreamInfo = info.outputInfo;
+        processingOrder.push_back(info);
+    }
+
+    // Crop
+    if (inputStreamInfo.width * stream.height != inputStreamInfo.height * stream.width &&
+        mPostProcessorCore->isPostProcessTypeSupported(icamera::POST_PROCESS_CROP)) {
+        mPostProcessType |= icamera::POST_PROCESS_CROP;
+        info.type = icamera::POST_PROCESS_CROP;
+        info.inputInfo = inputStreamInfo;
+
+        // Caclulate the best crop size with same aspect ratio
+        if (inputStreamInfo.width * stream.height < inputStreamInfo.height * stream.width) {
+            info.outputInfo.width = info.inputInfo.width;
+            info.outputInfo.height = ALIGN(info.inputInfo.width * stream.height / stream.width, 2);
+        } else {
+            info.outputInfo.width = ALIGN(info.inputInfo.height * stream.width / stream.height, 2);
+            info.outputInfo.height = info.inputInfo.height;
+        }
+        info.outputInfo.format = inputStreamInfo.format;
+        info.outputInfo.stride = info.outputInfo.width;
+        info.outputInfo.size = icamera::CameraUtils::getFrameSize(
+            info.outputInfo.format, info.outputInfo.width, info.outputInfo.height);
+        LOG2("%s, Crop: input %dx%d, output: %dx%d", __func__, inputStreamInfo.width,
+             inputStreamInfo.height, info.outputInfo.width, info.outputInfo.height);
+
+        inputStreamInfo = info.outputInfo;
+        processingOrder.push_back(info);
+    }
+
+    // Scale
+    if ((uint32_t)inputStreamInfo.width * inputStreamInfo.height != stream.width * stream.height &&
+        mPostProcessorCore->isPostProcessTypeSupported(icamera::POST_PROCESS_SCALING)) {
+        mPostProcessType |= icamera::POST_PROCESS_SCALING;
+        info.type = icamera::POST_PROCESS_SCALING;
+        info.inputInfo = inputStreamInfo;
+        info.outputInfo.width = stream.width;
+        info.outputInfo.height = stream.height;
+        info.outputInfo.stride = stream.width;
+        info.outputInfo.format = inputStreamInfo.format;
+        info.outputInfo.size = icamera::CameraUtils::getFrameSize(
+            info.outputInfo.format, info.outputInfo.width, info.outputInfo.height);
+        LOG2("%s, Scale: input %dx%d, output: %dx%d", __func__, inputStreamInfo.width,
+             inputStreamInfo.height, info.outputInfo.width, info.outputInfo.height);
+
+        inputStreamInfo = info.outputInfo;
+        processingOrder.push_back(info);
+    }
+
+    // Convert
+    if (inputStreamInfo.format !=
+            HalV3Utils::HALFormatToV4l2Format(mCameraId, stream.format, stream.usage) &&
+        mPostProcessorCore->isPostProcessTypeSupported(icamera::POST_PROCESS_CONVERT)) {
+        mPostProcessType |= icamera::POST_PROCESS_CONVERT;
+        info.type = icamera::POST_PROCESS_CONVERT;
+        info.inputInfo = inputStreamInfo;
+        info.outputInfo.width = stream.width;
+        info.outputInfo.height = stream.height;
+        info.outputInfo.stride = stream.width;
+        info.outputInfo.format =
+            HalV3Utils::HALFormatToV4l2Format(mCameraId, stream.format, stream.usage);
+        info.outputInfo.size = icamera::CameraUtils::getFrameSize(
+            info.outputInfo.format, info.outputInfo.width, info.outputInfo.height);
+        LOG2("%s, Convert: input %dx%d, output: %dx%d", __func__, inputStreamInfo.width,
+             inputStreamInfo.height, info.outputInfo.width, info.outputInfo.height);
+
+        inputStreamInfo = info.outputInfo;
+        processingOrder.push_back(info);
+    }
+
+    // Encode
+    if (stream.format == HAL_PIXEL_FORMAT_BLOB &&
+        mPostProcessorCore->isPostProcessTypeSupported(icamera::POST_PROCESS_JPEG_ENCODING)) {
+        mPostProcessType |= icamera::POST_PROCESS_JPEG_ENCODING;
+        info.type = icamera::POST_PROCESS_JPEG_ENCODING;
+        info.inputInfo = inputStreamInfo;
+        info.outputInfo.width = stream.width;
+        info.outputInfo.height = stream.height;
+        info.outputInfo.stride = stream.width;
+        info.outputInfo.format =
+            HalV3Utils::HALFormatToV4l2Format(mCameraId, stream.format, stream.usage);
+        info.outputInfo.size = icamera::CameraUtils::getFrameSize(
+            info.outputInfo.format, info.outputInfo.width, info.outputInfo.height);
+        inputStreamInfo = info.outputInfo;
+        processingOrder.push_back(info);
+    }
+
+    if ((uint32_t)inputStreamInfo.width != stream.width ||
+        (uint32_t)inputStreamInfo.height != stream.height ||
+        inputStreamInfo.format !=
+            HalV3Utils::HALFormatToV4l2Format(mCameraId, stream.format, stream.usage)) {
+        LOGE("%s, stream info doesn't match between input and output stream.", __func__);
+        return icamera::UNKNOWN_ERROR;
+    }
+    LOG1("@%s, camera id %d, mPostProcessType %d, processing unit number: %zu", __func__, mCameraId,
+         mPostProcessType, processingOrder.size());
+    mPostProcessorCore->configure(processingOrder);
+
+    return icamera::OK;
+}
+
+icamera::status_t PostProcessor::doPostProcessing(const std::shared_ptr<Camera3Buffer>& inBuf,
+                                                  const icamera::Parameters& parameter,
+                                                  std::shared_ptr<Camera3Buffer> outBuf) {
+    return mPostProcessorCore->doPostProcessing(inBuf, parameter, outBuf);
+}
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/PostProcessor.h b/camera/hal/intel/ipu6/aal/PostProcessor.h
new file mode 100644
index 000000000000..736e7104a970
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/PostProcessor.h
@@ -0,0 +1,60 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <hardware/camera3.h>
+
+#include <memory>
+
+#include "Camera3Buffer.h"
+#include "Errors.h"
+#include "ICamera.h"
+#include "PostProcessorCore.h"
+#include "Utils.h"
+
+namespace camera3 {
+/**
+ * \class PostProcessor
+ *
+ * A wrapper based on PostProcessorCore for handling post-processing sequence,
+ * there are two main purposes of this class.
+ * 1. Provide the wrapper to implement post-processing feature.
+ * 2. Parsing the processing type and formulate the processing sequence
+ */
+class PostProcessor {
+ public:
+    PostProcessor(int cameraId, const camera3_stream_t& stream);
+    virtual ~PostProcessor();
+
+    // srcStream will convert to stream_t and call the other configure
+    icamera::status_t configure(const camera3_stream_t& stream, const camera3_stream_t& srcStream);
+    icamera::status_t configure(const camera3_stream_t& stream, const icamera::stream_t& halStream);
+    int getPostProcessType() { return mPostProcessType; }
+    icamera::status_t doPostProcessing(const std::shared_ptr<Camera3Buffer>& inBuf,
+                                       const icamera::Parameters& parameter,
+                                       std::shared_ptr<Camera3Buffer> outBuf);
+
+ private:
+    DISALLOW_COPY_AND_ASSIGN(PostProcessor);
+
+ private:
+    int mCameraId;
+    int mPostProcessType;
+    std::unique_ptr<icamera::PostProcessorCore> mPostProcessorCore;
+};
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/RequestManager.cpp b/camera/hal/intel/ipu6/aal/RequestManager.cpp
new file mode 100644
index 000000000000..26785775d77e
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/RequestManager.cpp
@@ -0,0 +1,788 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "RequestManager"
+
+#include "RequestManager.h"
+
+#include <hardware/gralloc.h>
+#include <linux/videodev2.h>
+#include <math.h>
+
+#include <algorithm>
+#include <cstdlib>
+#include <list>
+#include <map>
+#include <unordered_map>
+#include <utility>
+#include <vector>
+
+#include "Errors.h"
+#include "HALv3Utils.h"
+#include "ICamera.h"
+#include "MetadataConvert.h"
+#include "Parameters.h"
+#include "PlatformData.h"
+#include "Utils.h"
+
+namespace camera3 {
+RequestManager::RequestManager(int cameraId)
+        : mCameraId(cameraId),
+          mCallbackOps(nullptr),
+          mCameraDeviceStarted(false),
+          mResultProcessor(nullptr),
+          mInputStreamConfigured(false),
+          mRequestInProgress(0) {
+    LOG1("@%s", __func__);
+
+    CLEAR(mCameraBufferInfo);
+}
+
+RequestManager::~RequestManager() {
+    LOG1("@%s", __func__);
+
+    deleteStreams(false);
+
+    delete mResultProcessor;
+}
+
+int RequestManager::init(const camera3_callback_ops_t* callback_ops) {
+    LOG1("@%s", __func__);
+
+    // Update the default settings from camera HAL
+    icamera::Parameters parameter;
+    int ret = icamera::camera_get_parameters(mCameraId, parameter);
+    CheckError(ret != icamera::OK, ret, "failed to get parameters, ret %d", ret);
+    StaticCapability::getInstance(mCameraId);
+
+    android::CameraMetadata defaultRequestSettings;
+    // Get static metadata
+    MetadataConvert::HALCapabilityToStaticMetadata(parameter, &defaultRequestSettings);
+
+    // Get defalut settings
+    MetadataConvert::constructDefaultMetadata(mCameraId, &defaultRequestSettings);
+    MetadataConvert::HALMetadataToRequestMetadata(parameter, &defaultRequestSettings, mCameraId);
+
+    mDefaultRequestSettings[CAMERA3_TEMPLATE_PREVIEW] = defaultRequestSettings;
+    MetadataConvert::updateDefaultRequestSettings(
+        mCameraId, CAMERA3_TEMPLATE_PREVIEW, &mDefaultRequestSettings[CAMERA3_TEMPLATE_PREVIEW]);
+
+    mResultProcessor = new ResultProcessor(mCameraId, callback_ops, this);
+    mCallbackOps = callback_ops;
+
+    // Register callback to icamera HAL
+    icamera::camera_callback_ops_t::notify = RequestManager::callbackNotify;
+    icamera::camera_callback_register(mCameraId,
+                                      static_cast<icamera::camera_callback_ops_t*>(this));
+
+    return icamera::OK;
+}
+
+int RequestManager::deinit() {
+    LOG1("@%s", __func__);
+
+    if (mCameraDeviceStarted) {
+        int ret = icamera::camera_device_stop(mCameraId);
+        CheckError(ret != icamera::OK, ret, "failed to stop camera device, ret %d", ret);
+        mCameraDeviceStarted = false;
+    }
+
+    mRequestInProgress = 0;
+    StaticCapability::releaseInstance(mCameraId);
+    return icamera::OK;
+}
+
+void RequestManager::callbackNotify(const icamera::camera_callback_ops* cb,
+                                    const icamera::camera_msg_data_t& data) {
+    LOG2("@%s, type %d", __func__, data.type);
+    RequestManager* callback = const_cast<RequestManager*>(static_cast<const RequestManager*>(cb));
+
+    callback->mResultProcessor->callbackNotify(data);
+    callback->handleCallbackEvent(data);
+}
+
+void RequestManager::handleCallbackEvent(const icamera::camera_msg_data_t& data) {
+    LOG2("@%s, cameraId: %d", __func__, mCameraId);
+
+    if (!icamera::PlatformData::swProcessingAlignWithIsp(mCameraId)) return;
+
+    for (auto& stream : mCamera3StreamVector) {
+        if (stream->getPostProcessType() != icamera::POST_PROCESS_NONE) {
+            stream->sendEvent(data);
+        }
+    }
+}
+
+int RequestManager::configureStreams(camera3_stream_configuration_t* stream_list) {
+    LOG1("@%s", __func__);
+
+    int ret = checkStreamRotation(stream_list);
+
+    CheckError(ret != icamera::OK, icamera::BAD_VALUE, "Unsupport rotation degree!");
+
+    if (mCameraDeviceStarted) {
+        ret = icamera::camera_device_stop(mCameraId);
+        CheckError(ret != icamera::OK, ret, "failed to stop camera device, ret %d", ret);
+        mCameraDeviceStarted = false;
+    }
+
+    icamera::stream_t requestStreams[kMaxStreamNum];  // not include CAMERA3_STREAM_INPUT stream
+    uint32_t streamsNum = stream_list->num_streams;
+    uint32_t operationMode = stream_list->operation_mode;
+    LOG1("@%s, streamsNum:%d, operationMode:%d", __func__, streamsNum, operationMode);
+    CheckError((operationMode != CAMERA3_STREAM_CONFIGURATION_NORMAL_MODE &&
+                operationMode != CAMERA3_STREAM_CONFIGURATION_CONSTRAINED_HIGH_SPEED_MODE),
+               icamera::BAD_VALUE, "Unknown operation mode %d!", operationMode);
+
+    int inputStreamNum = 0;
+    int outStreamNum = 0;
+    camera3_stream_t* stream = nullptr;
+    for (uint32_t i = 0; i < streamsNum; i++) {
+        stream = stream_list->streams[i];
+        LOG1("@%s, Config stream (%s):%dx%d, f:%d, u:%d, buf num:%d, priv:%p", __func__,
+             HalV3Utils::getCamera3StreamType(stream->stream_type), stream->width,
+             stream->height, stream->format, stream->usage, stream->max_buffers, stream->priv);
+        if (stream->stream_type == CAMERA3_STREAM_OUTPUT) {
+            outStreamNum++;
+        } else if (stream->stream_type == CAMERA3_STREAM_INPUT) {
+            inputStreamNum++;
+            mInputStreamConfigured = true;
+        } else if (stream->stream_type == CAMERA3_STREAM_BIDIRECTIONAL) {
+            inputStreamNum++;
+            outStreamNum++;
+            mInputStreamConfigured = true;
+        } else {
+            LOGE("@%s, Unknown stream type %d!", __func__, stream->stream_type);
+            return icamera::BAD_VALUE;
+        }
+        // In ZSL case, RAW input and YUV input will be configured together.
+        CheckError(inputStreamNum > 2, icamera::BAD_VALUE, "Too many input streams : %d !",
+                   inputStreamNum);
+    }
+    CheckError(outStreamNum == 0, icamera::BAD_VALUE, "No output streams!");
+
+    /*
+     * Configure stream
+     */
+    mResultProcessor->clearRawBufferInfoMap();
+    int requestStreamNum = 0;
+    camera3_stream_t* inputStream = nullptr;
+    // Enable video pipe if yuv stream exists (for 3A stats data)
+    bool needAssignPreviewStream = true;
+    icamera::stream_t* yuvStream = nullptr;
+    for (uint32_t i = 0; i < streamsNum; i++) {
+        /*
+         * 1, for CAMERA3_STREAM_INPUT stream, format YCbCr_420_888 is for YUV
+         * reprocessing, other formats (like IMPLEMENTATION_DEFINED, RAW_OPAQUE) are
+         * for RAW reprocessing.
+         * 2, for CAMERA3_STREAM_BIDIRECTIONAL stream, it is for RAW reprocessing.
+         * 3, for CAMERA3_STREAM_OUTPUT stream, if format is IMPLEMENTATION_DEFINED
+         * and usage doesn't include COMPOSE or TEXTURE, it is for RAW reprocessing.
+         * if format is RAW_OPAQUE, it is for RAW reprocessing.
+         */
+        if (stream_list->streams[i]->stream_type == CAMERA3_STREAM_INPUT) {
+            if (stream_list->streams[i]->format == HAL_PIXEL_FORMAT_YCbCr_420_888) {
+                inputStream = stream_list->streams[i];
+                LOG1("@%s, input stream: w:%d, h:%d, f:%d", __func__, inputStream->width,
+                     inputStream->height, inputStream->format);
+            } else {
+                stream_list->streams[i]->usage |= GRALLOC_USAGE_HW_CAMERA_ZSL;
+            }
+            stream_list->streams[i]->max_buffers = 2;
+            continue;
+        } else if (stream_list->streams[i]->stream_type == CAMERA3_STREAM_BIDIRECTIONAL) {
+            stream_list->streams[i]->usage |= GRALLOC_USAGE_HW_CAMERA_ZSL;
+        } else {
+            if (stream_list->streams[i]->format == HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED &&
+                inputStreamNum > 0) {
+                if (!(stream_list->streams[i]->usage &
+                      (GRALLOC_USAGE_HW_COMPOSER | GRALLOC_USAGE_HW_TEXTURE))) {
+                    stream_list->streams[i]->usage |= GRALLOC_USAGE_HW_CAMERA_ZSL;
+                }
+            } else if (stream_list->streams[i]->format == HAL_PIXEL_FORMAT_RAW_OPAQUE) {
+                stream_list->streams[i]->usage |= GRALLOC_USAGE_HW_CAMERA_ZSL;
+            }
+        }
+
+        ret = HalV3Utils::fillHALStreams(mCameraId, *stream_list->streams[i],
+                                         &requestStreams[requestStreamNum]);
+        CheckError(ret != icamera::OK, ret, "failed to fill requestStreams[%d], ret:%d", ret,
+                   requestStreamNum);
+
+        if (!yuvStream && stream_list->streams[i]->format != HAL_PIXEL_FORMAT_BLOB &&
+            !IS_ZSL_USAGE(stream_list->streams[i]->usage)) {
+            yuvStream = &requestStreams[requestStreamNum];
+        }
+        if (requestStreams[requestStreamNum].usage == icamera::CAMERA_STREAM_PREVIEW ||
+            requestStreams[requestStreamNum].usage == icamera::CAMERA_STREAM_VIDEO_CAPTURE)
+            needAssignPreviewStream = false;
+
+        requestStreamNum++;
+    }
+    if (needAssignPreviewStream && yuvStream) {
+        yuvStream->usage = icamera::CAMERA_STREAM_PREVIEW;
+    }
+
+    CLEAR(mHALStream);
+
+    int halStreamFlag[kMaxStreamNum];
+    int halStreamNum = chooseHALStreams(requestStreamNum, halStreamFlag, requestStreams);
+    // halStreamNum should not exceed videoNum + 2 (1 opaque raw and 1 still)
+    int maxSupportStreamNum = icamera::PlatformData::getVideoStreamNum(mCameraId) + 2;
+    CheckError(halStreamNum > maxSupportStreamNum || halStreamNum <= 0,
+        icamera::BAD_VALUE, "failed to find HAL stream");
+
+    // index of stream in mHALStream
+    int halStreamIndex = 0;
+    // first:stream index in requestStreams[], second:HAL stream index in mHALStream[]
+    std::map<int, int> streamToHALIndex;
+    for (int i = 0; i < requestStreamNum; i++) {
+        // fill HAL stream with right requestStreams object
+        if (halStreamFlag[i] == i) {
+            mHALStream[halStreamIndex] = requestStreams[i];
+            streamToHALIndex[i] = halStreamIndex;
+            halStreamIndex++;
+        }
+    }
+
+    icamera::stream_config_t streamCfg = {
+        halStreamNum, mHALStream,
+        icamera::camera_stream_configuration_mode_t::CAMERA_STREAM_CONFIGURATION_MODE_AUTO};
+
+    for (int i = 0; i < requestStreamNum; i++) {
+        const icamera::stream_t& s = requestStreams[i];
+        LOG1("@%s, requestStreams[%d]: w:%d, h:%d, f:%d, u:%d", __func__, i, s.width, s.height,
+             s.format, s.usage);
+    }
+
+    for (int i = 0; i < halStreamNum; i++) {
+        const icamera::stream_t& s = mHALStream[i];
+        LOG1("@%s, configured mHALStream[%d]: w:%d, h:%d, f:%d, u:%d", __func__, i, s.width,
+             s.height, s.format, s.usage);
+    }
+
+    // Mark all streams as NOT active
+    for (auto& stream : mCamera3StreamVector) {
+        stream->setActive(false);
+    }
+
+    int enableFDStreamNum = -1;
+    Camera3Stream* faceDetectionOwner = nullptr;
+    // Mark one camera3Stream run face detection
+    if (icamera::PlatformData::isFaceAeEnabled(mCameraId)) {
+        LOG1("Face detection is enable");
+        chooseStreamForFaceDetection(streamsNum, stream_list->streams, &enableFDStreamNum);
+    }
+
+    ret = icamera::camera_device_config_streams(mCameraId, &streamCfg);
+    CheckError(ret != icamera::OK, ret, "failed to configure stream, ret %d", ret);
+
+    // Create Stream for new streams
+    requestStreamNum = 0;
+    for (uint32_t i = 0; i < streamsNum; i++) {
+        camera3_stream_t* stream = stream_list->streams[i];
+        if (stream->stream_type == CAMERA3_STREAM_INPUT) {
+            continue;
+        }
+
+        /* use halStreamFlag[] to find it's HAL stream index in requestStreams
+        ** streamToHALIndex to find it's HAL Stream index in mHALStream[]*/
+        int halStreamIndex = streamToHALIndex[halStreamFlag[requestStreamNum]];
+        bool isHALStream = halStreamFlag[requestStreamNum] == requestStreamNum;
+        CheckError(halStreamIndex < 0 || halStreamIndex >= halStreamNum, icamera::BAD_VALUE,
+                   "failed to find hal stream %d", halStreamIndex);
+        Camera3Stream* s =
+            new Camera3Stream(mCameraId, mResultProcessor, mHALStream[halStreamIndex].max_buffers,
+                              mHALStream[halStreamIndex], *stream, inputStream, isHALStream);
+        s->setActive(true);
+        stream->priv = s;
+        stream->max_buffers = mHALStream[halStreamIndex].max_buffers;
+        stream->usage |= GRALLOC_USAGE_HW_CAMERA_WRITE | GRALLOC_USAGE_SW_READ_OFTEN |
+                         GRALLOC_USAGE_SW_WRITE_NEVER;
+        mCamera3StreamVector.push_back(s);
+
+        requestStreamNum++;
+        LOGI("OUTPUT max buffer %d, usage %x, format %x", stream->max_buffers, stream->usage,
+             stream->format);
+
+        if (enableFDStreamNum == i) {
+            faceDetectionOwner = static_cast<Camera3Stream*>(stream->priv);
+        }
+    }
+
+    // Remove useless Camera3Stream
+    deleteStreams(true);
+
+    // bind streams to HAL streams
+    for (int i = 0; i < mCamera3StreamVector.size(); i++) {
+        if (halStreamFlag[i] != i)
+            mCamera3StreamVector[halStreamFlag[i]]->addListener(mCamera3StreamVector[i]);
+    }
+
+    if (faceDetectionOwner != nullptr) {
+        faceDetectionOwner->activateFaceDetection(MAX_FACES_DETECTABLE);
+    }
+
+    return icamera::OK;
+}
+
+void RequestManager::chooseStreamForFaceDetection(uint32_t streamsNum, camera3_stream_t** streams,
+                                                  int* enableFDStreamNum) {
+    LOG1("@%s", __func__);
+    camera3_stream_t* preStream = nullptr;
+    camera3_stream_t* yuvStream = nullptr;
+    int maxWidth = MAX_FACE_FRAME_WIDTH;
+    int maxHeight = MAX_FACE_FRAME_HEIGHT;
+    int preStreamNum = -1;
+    int yuvStreamNum = -1;
+
+    for (uint32_t i = 0; i < streamsNum; i++) {
+        camera3_stream_t* s = streams[i];
+        if (!s || s->stream_type != CAMERA3_STREAM_OUTPUT || s->width > maxWidth ||
+            s->height > maxHeight) {
+            continue;
+        }
+
+        LOG1("stream information:format=%d, width=%d, height=%d", s->format, s->width, s->height);
+        // We assume HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED stream is the
+        // preview stream and it's requested in every capture request.
+        // If there are two HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED streams,
+        // We pick the smaller stream due to performance concern.
+        if (s->format == HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED && !IS_ZSL_USAGE(s->usage)) {
+            if (preStream && preStream->width * preStream->height <= s->width * s->height) {
+                continue;
+            }
+            preStream = s;
+            preStreamNum = i;
+        }
+
+        if (s->format == HAL_PIXEL_FORMAT_YCbCr_420_888) {
+            if (yuvStream && yuvStream->width * yuvStream->height <= s->width * s->height) {
+                continue;
+            }
+            yuvStream = s;
+            yuvStreamNum = i;
+        }
+    }
+
+    *enableFDStreamNum = -1;
+    if (preStreamNum >= 0) {
+        *enableFDStreamNum = preStreamNum;
+    } else if (yuvStreamNum >= 0) {
+        *enableFDStreamNum = yuvStreamNum;
+    }
+    LOG1("enableFDStreamNum %d", *enableFDStreamNum);
+}
+
+int RequestManager::constructDefaultRequestSettings(int type, const camera_metadata_t** meta) {
+    LOG1("@%s, type %d", __func__, type);
+
+    if (mDefaultRequestSettings.count(type) == 0) {
+        mDefaultRequestSettings[type] = mDefaultRequestSettings[CAMERA3_TEMPLATE_PREVIEW];
+        MetadataConvert::updateDefaultRequestSettings(mCameraId, type,
+                                                      &mDefaultRequestSettings[type]);
+    }
+    const camera_metadata_t* setting = mDefaultRequestSettings[type].getAndLock();
+    *meta = setting;
+    mDefaultRequestSettings[type].unlock(setting);
+
+    return icamera::OK;
+}
+
+int RequestManager::processCaptureRequest(camera3_capture_request_t* request) {
+    CheckError(!request, icamera::UNKNOWN_ERROR, "@%s, request is nullptr", __func__);
+    LOG1("@%s, frame_number:%d, input_buffer:%d, num_output_buffers:%d", __func__,
+         request->frame_number, request->input_buffer ? 1 : 0, request->num_output_buffers);
+
+    TRACE_LOG_POINT("RequestManager", __func__, MAKE_COLOR(request->frame_number),
+                    request->frame_number);
+
+    // Valid buffer and request
+    CheckError(request->num_output_buffers > kMaxStreamNum, icamera::BAD_VALUE,
+               "@%s, num_output_buffers:%d", __func__, request->num_output_buffers);
+
+    int ret = icamera::OK;
+
+    waitProcessRequest();
+
+    int index = -1;
+    for (int i = 0; i < kMaxProcessRequestNum; i++) {
+        if (!mCameraBufferInfo[i].frameInProcessing) {
+            index = i;
+        }
+    }
+    CheckError(index < 0, icamera::UNKNOWN_ERROR, "no empty CameraBufferInfo!");
+    CLEAR(mCameraBufferInfo[index]);
+
+    if (request->settings) {
+        MetadataConvert::dumpMetadata(request->settings);
+        mLastSettings = request->settings;
+    } else if (mLastSettings.isEmpty()) {
+        LOGE("nullptr settings for the first reqeust!");
+        return icamera::BAD_VALUE;
+    }
+
+    std::shared_ptr<Camera3Buffer> inputCam3Buf = nullptr;
+    icamera::sensor_raw_info_t opaqueRawInfo = {-1, 0};
+    if (request->input_buffer) {
+        inputCam3Buf = std::make_shared<Camera3Buffer>();
+        icamera::status_t status = inputCam3Buf->init(request->input_buffer, mCameraId);
+        CheckError(status != icamera::OK, icamera::BAD_VALUE, "Failed to init CameraBuffer");
+        status = inputCam3Buf->waitOnAcquireFence();
+        CheckError(status != icamera::OK, icamera::BAD_VALUE, "Failed to sync CameraBuffer");
+        status = inputCam3Buf->lock();
+        CheckError(status != icamera::OK, icamera::BAD_VALUE, "Failed to lock buffer");
+
+        camera_metadata_entry entry = mLastSettings.find(ANDROID_SENSOR_TIMESTAMP);
+        if (entry.count == 1) {
+            inputCam3Buf->setTimeStamp(entry.data.i64[0]);
+        }
+
+        if (IS_ZSL_USAGE(request->input_buffer->stream->usage)) {
+            MEMCPY_S(&opaqueRawInfo, sizeof(opaqueRawInfo), inputCam3Buf->data(),
+                     inputCam3Buf->size());
+            mResultProcessor->checkAndChangeRawbufferInfo(&opaqueRawInfo.sequence,
+                                                          &opaqueRawInfo.timestamp);
+            LOG2("%s, sequence id %ld, timestamp %ld", __func__, opaqueRawInfo.sequence,
+                 opaqueRawInfo.timestamp);
+        }
+    }
+
+    icamera::Parameters param;
+    param.setMakernoteMode(icamera::MAKERNOTE_MODE_OFF);
+    param.setUserRequestId(static_cast<int32_t>(request->frame_number));
+
+    for (uint32_t i = 0; i < request->num_output_buffers; i++) {
+        camera3_stream_t* aStream = request->output_buffers[i].stream;        // app stream
+        Camera3Stream* lStream = static_cast<Camera3Stream*>(aStream->priv);  // local stream
+        if (mInputStreamConfigured || aStream->format == HAL_PIXEL_FORMAT_BLOB) {
+            param.setMakernoteMode(icamera::MAKERNOTE_MODE_JPEG);
+        }
+
+        ret = lStream->processRequest(opaqueRawInfo.sequence >= 0 ? nullptr : inputCam3Buf,
+                                      request->output_buffers[i], request->frame_number);
+        CheckError(ret != icamera::OK, ret, "Failed to process request, ret:%d", ret);
+    }
+
+    // Convert metadata to Parameters
+    bool forceConvert = inputCam3Buf ? true : false;
+    MetadataConvert::requestMetadataToHALMetadata(mLastSettings, &param, forceConvert);
+
+    mResultProcessor->registerRequest(request, inputCam3Buf);
+
+    if (!inputCam3Buf || opaqueRawInfo.sequence >= 0) {
+        icamera::camera_buffer_t* buffer[kMaxStreamNum] = {nullptr};
+        int numBuffers = 0;
+        for (auto& stream : mCamera3StreamVector) {
+            if (stream->fetchRequestBuffers(&mCameraBufferInfo[index].halBuffer[numBuffers],
+                                            request->frame_number)) {
+                mCameraBufferInfo[index].halBuffer[numBuffers].sequence = opaqueRawInfo.sequence;
+                mCameraBufferInfo[index].halBuffer[numBuffers].timestamp = opaqueRawInfo.timestamp;
+                buffer[numBuffers] = &mCameraBufferInfo[index].halBuffer[numBuffers];
+                numBuffers++;
+            }
+        }
+        ret = icamera::camera_stream_qbuf(mCameraId, buffer, numBuffers, &param);
+        CheckError(ret != icamera::OK, ret, "@%s, camera_stream_qbuf fails,ret:%d", __func__, ret);
+    }
+
+    increaseRequestCount();
+
+    if (!mCameraDeviceStarted) {
+        ret = icamera::camera_device_start(mCameraId);
+        CheckError(ret != icamera::OK, ret, "failed to start device, ret %d", ret);
+
+        mCameraDeviceStarted = true;
+    }
+
+    for (uint32_t i = 0; i < request->num_output_buffers; i++) {
+        Camera3Stream* s = static_cast<Camera3Stream*>(request->output_buffers[i].stream->priv);
+        s->queueBufferDone(request->frame_number,
+                           opaqueRawInfo.sequence >= 0 ? nullptr : inputCam3Buf,
+                           request->output_buffers[i], param);
+    }
+
+    for (auto& stream : mCamera3StreamVector) {
+        /* incase the HAL stream is not requested by user request, scan all the HAL
+        ** streams check if any one is triggered by listener
+        */
+        stream->checkListenerRequest(request->frame_number);
+    }
+
+    mCameraBufferInfo[index].frameInProcessing = true;
+    mCameraBufferInfo[index].frameNumber = request->frame_number;
+
+    return ret;
+}
+
+void RequestManager::dump(int fd) {
+    LOG1("@%s", __func__);
+}
+
+int RequestManager::flush() {
+    LOG1("@%s", __func__);
+
+    icamera::nsecs_t startTime = icamera::CameraUtils::systemTime();
+    icamera::nsecs_t interval = 0;
+    const icamera::nsecs_t ONE_SECOND = 1000000000;
+
+    // wait 1000ms at most while there are requests in the HAL
+    while (mRequestInProgress > 0 && interval <= ONE_SECOND) {
+        usleep(10000);  // wait 10ms
+        interval = icamera::CameraUtils::systemTime() - startTime;
+    }
+
+    LOG2("@%s, line:%d, mRequestInProgress:%d, time spend:%ld us", __func__, __LINE__,
+         mRequestInProgress, interval / 1000);
+
+    // based on API, -ENODEV (NO_INIT) error should be returned.
+    CheckError(interval > ONE_SECOND, icamera::NO_INIT, "flush() > 1s, timeout:%ld us",
+               interval / 1000);
+
+    return icamera::OK;
+}
+
+void RequestManager::deleteStreams(bool inactiveOnly) {
+    LOG1("@%s", __func__);
+
+    unsigned int i = 0;
+    while (i < mCamera3StreamVector.size()) {
+        Camera3Stream* s = mCamera3StreamVector.at(i);
+
+        if (!inactiveOnly || !s->isActive()) {
+            mCamera3StreamVector.erase(mCamera3StreamVector.begin() + i);
+            delete s;
+        } else {
+            ++i;
+        }
+    }
+}
+
+int RequestManager::waitProcessRequest() {
+    LOG1("@%s", __func__);
+    std::unique_lock<std::mutex> lock(mRequestLock);
+    // check if it is ready to process next request
+    while (mRequestInProgress >= mHALStream[0].max_buffers) {
+        std::cv_status ret = mRequestCondition.wait_for(
+            lock, std::chrono::nanoseconds(kMaxDuration * SLOWLY_MULTIPLIER));
+        if (ret == std::cv_status::timeout) {
+            LOGW("%s, wait to process request time out", __func__);
+        }
+    }
+
+    return icamera::OK;
+}
+
+void RequestManager::increaseRequestCount() {
+    LOG1("@%s", __func__);
+
+    std::lock_guard<std::mutex> l(mRequestLock);
+    ++mRequestInProgress;
+}
+
+void RequestManager::returnRequestDone(uint32_t frameNumber) {
+    LOG1("@%s  frame %d", __func__, frameNumber);
+
+    std::lock_guard<std::mutex> l(mRequestLock);
+
+    // Update mCameraBufferInfo based on frameNumber
+    for (int i = 0; i < kMaxProcessRequestNum; i++) {
+        if (mCameraBufferInfo[i].frameNumber == frameNumber) {
+            mCameraBufferInfo[i].frameInProcessing = false;
+        }
+    }
+    mRequestInProgress--;
+    mRequestCondition.notify_one();
+
+    for (auto& stream : mCamera3StreamVector) {
+        stream->requestStreamDone(frameNumber);
+    }
+}
+
+int RequestManager::checkStreamRotation(camera3_stream_configuration_t* stream_list) {
+    int rotationDegree0 = -1, countOutputStream = 0;
+
+    for (size_t i = 0; i < stream_list->num_streams; i++) {
+        if (stream_list->streams[i]->stream_type != CAMERA3_STREAM_OUTPUT) {
+            continue;
+        }
+        countOutputStream++;
+
+        int rotationDegree = HalV3Utils::getRotationDegrees(*(stream_list->streams[i]));
+        CheckError(rotationDegree < 0, icamera::BAD_VALUE, "Unsupport rotation degree!");
+
+        if (countOutputStream == 1) {
+            rotationDegree0 = rotationDegree;
+        } else {
+            CheckError(rotationDegree0 != rotationDegree, icamera::BAD_VALUE,
+                       "rotationDegree0:%d, stream[%lu] rotationDegree:%d, not the same",
+                       rotationDegree0, i, rotationDegree);
+        }
+    }
+
+    return icamera::OK;
+}
+
+int RequestManager::chooseHALStreams(const uint32_t requestStreamNum, int* halStreamFlag,
+                                     icamera::stream_t* halStreamList) {
+    int activeHALNum = 0;
+    int avaVideoSlot = icamera::PlatformData::getVideoStreamNum(mCameraId);
+    vector<icamera::stream_t*> videoHALStream;
+    int videoMaxResStreamIndex = -1;
+    int stillHALStreamIndex = -1;
+
+    // HAL stream with configure index
+    std::unordered_map<icamera::stream_t*, int> videoHALStreamIndex;
+    /* save streams with their configure index, the index in this deque is
+    ** their priority to be HWStream, from low to high
+    */
+    std::list<std::pair<icamera::stream_t*, int>> videoHALStreamOrder;
+    // anchor where to insert next potential HAL stream in videoHALStreamOrder
+    auto anchor = videoHALStreamOrder.end();
+    // save sorted hal streams with their configure index
+    std::vector<std::pair<icamera::stream_t*, int>> requestStreams;
+
+    int opaqueCount = 0;
+    int stillCount = 0;
+    int videoCount = 0;
+    for (uint32_t i = 0; i < requestStreamNum; i++) {
+        // set flags to it's index, every stream is a HAL stream by default
+        halStreamFlag[i] = i;
+        if (halStreamList[i].usage == icamera::CAMERA_STREAM_OPAQUE_RAW) {
+            opaqueCount++;
+        } else if (halStreamList[i].usage == icamera::CAMERA_STREAM_STILL_CAPTURE) {
+            stillCount++;
+        } else {
+            videoCount++;
+        }
+    }
+    // if HAL stream slots are enough, make all streams as HAL stream
+    if (opaqueCount <= 1 && stillCount <= 1 && videoCount <= avaVideoSlot) {
+        return requestStreamNum;
+    }
+
+    for (uint32_t i = 0; i < requestStreamNum; i++) {
+        // clear the flags to invalid value
+        halStreamFlag[i] = -1;
+        requestStreams.push_back(std::make_pair(&halStreamList[i], i));
+    }
+
+    // sort stream by resolution, easy to find largest resolution stream
+    std::sort(requestStreams.begin(), requestStreams.end(),
+              [](std::pair<icamera::stream_t*, int>& s1, std::pair<icamera::stream_t*, int>& s2) {
+                  if (s1.first->width * s1.first->height > s2.first->width * s2.first->height)
+                      return true;
+                  else
+                      return false;
+              });
+
+    for (uint32_t i = 0; i < requestStreamNum; i++) {
+        icamera::stream_t* s = requestStreams[i].first;
+        int index = requestStreams[i].second;
+        if (s->usage == icamera::CAMERA_STREAM_OPAQUE_RAW) {
+            // choose hal stream for opaque stream, only 1 opaque stream
+            halStreamFlag[index] = index;
+            activeHALNum++;
+        } else if (s->usage == icamera::CAMERA_STREAM_STILL_CAPTURE) {
+            // choose hal stream for still streams, choose the MAX resolution
+            if (stillHALStreamIndex == -1) {
+                stillHALStreamIndex = index;
+                activeHALNum++;
+            }
+        } else {
+            // videoHALStream stores different resolution streams
+            if (videoMaxResStreamIndex == -1) {
+                // choose max resolution video stream
+                anchor = videoHALStreamOrder.insert(anchor, std::make_pair(s, index));
+                videoHALStream.push_back(s);
+            } else {
+                bool found = false;
+                // choose different resolution stream
+                for (auto& vs : videoHALStream) {
+                    if (s->width == vs->width && s->height == vs->height) {
+                        found = true;
+                        break;
+                    }
+                }
+                if (found) {
+                    /* same resolution stream put in the front of list, has low priority
+                    ** to be HALStream
+                    */
+                    videoHALStreamOrder.push_front(std::make_pair(s, index));
+                } else {
+                    /* different resolution stream has higher priority than same ones,
+                     * insert it in front of max resolution stream.
+                     */
+                    anchor = videoHALStreamOrder.insert(anchor, std::make_pair(s, index));
+                    videoHALStream.push_back(s);
+                }
+            }
+        }
+    }
+
+    if (stillHALStreamIndex >= 0) {
+        for (uint32_t i = 0; i < requestStreamNum; i++) {
+            // has only 1 Still HAL stream, other still stream should be listeners
+            if (halStreamList[i].usage == icamera::CAMERA_STREAM_STILL_CAPTURE) {
+                halStreamFlag[i] = stillHALStreamIndex;
+            }
+        }
+    }
+
+    // videoHALStream store selected HALStreams
+    videoHALStream.clear();
+    /* scan videoHALStreamOrder from back to front by the priority,
+    ** and put them into Video HAL stream slot
+    */
+    while (avaVideoSlot > 0 && !videoHALStreamOrder.empty()) {
+        auto stream = videoHALStreamOrder.back();
+        videoHALStream.push_back(stream.first);
+        videoHALStreamIndex[stream.first] = stream.second;
+        halStreamFlag[stream.second] = stream.second;
+        activeHALNum++;
+        avaVideoSlot--;
+        videoHALStreamOrder.pop_back();
+    }
+
+    // scan other streams not been put into HAL slot, bind to the HAL stream
+    while (!videoHALStreamOrder.empty()) {
+        auto stream = videoHALStreamOrder.back();
+        videoHALStreamOrder.pop_back();
+        int index = stream.second;
+        icamera::stream_t* s = stream.first;
+        // search same resolution in video HAL streams
+        int slot;
+        for (slot = 0; slot < videoHALStream.size(); slot++) {
+            if (s->width == videoHALStream.at(slot)->width &&
+                s->height == videoHALStream.at(slot)->height)
+                break;
+        }
+        if (slot != videoHALStream.size()) {
+            // bind to same resolution slot
+            halStreamFlag[index] = videoHALStreamIndex[videoHALStream.at(slot)];
+        } else {
+            // bind to max resolution slot
+            halStreamFlag[index] = videoHALStreamIndex[videoHALStream.at(0)];
+        }
+    }
+    LOG1("has %d HAL Streams", activeHALNum);
+    for (int i = 0; i < requestStreamNum; i++)
+        LOG1("user Stream %d bind to HAL Stream %d", i, halStreamFlag[i]);
+
+    return activeHALNum;
+}
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/RequestManager.h b/camera/hal/intel/ipu6/aal/RequestManager.h
new file mode 100644
index 000000000000..c4a169b0cfe1
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/RequestManager.h
@@ -0,0 +1,115 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <hardware/camera3.h>
+
+#include <mutex>
+#include <vector>
+
+#include "Camera3Stream.h"
+#include "HALv3Header.h"
+#include "HALv3Interface.h"
+#include "PlatformData.h"
+#include "ResultProcessor.h"
+
+namespace camera3 {
+
+struct Camera3Request {
+    uint32_t frameNumber;
+    android::CameraMetadata settings;
+};
+
+/**
+ * \class RequestManager
+ *
+ * This class is used to handle requests. It has the following
+ * roles:
+ * - It instantiates ResultProcessor.
+ */
+class RequestManager : public RequestManagerCallback, public icamera::camera_callback_ops_t {
+ public:
+    RequestManager(int cameraId);
+    virtual ~RequestManager();
+
+    int init(const camera3_callback_ops_t* callback_ops);
+
+    int deinit();
+
+    int configureStreams(camera3_stream_configuration_t* stream_list);
+
+    int constructDefaultRequestSettings(int type, const camera_metadata_t** meta);
+
+    int processCaptureRequest(camera3_capture_request_t* request);
+
+    void dump(int fd);
+
+    int flush();
+
+    void returnRequestDone(uint32_t frameNumber);
+
+ private:
+    void deleteStreams(bool inactiveOnly);
+    void increaseRequestCount();
+    int waitProcessRequest();
+    void chooseStreamForFaceDetection(uint32_t streamsNum, camera3_stream_t** streams,
+                                      int* enableFDStreamNum);
+    int checkStreamRotation(camera3_stream_configuration_t* stream_list);
+
+    static void callbackNotify(const icamera::camera_callback_ops* cb,
+                               const icamera::camera_msg_data_t& data);
+    void handleCallbackEvent(const icamera::camera_msg_data_t& data);
+
+ private:
+    static const int kMaxStreamNum = 5;        // OPAQUE RAW, PREVIEW, VIDEO, STILL and POSTVIEW
+    const uint64_t kMaxDuration = 2000000000;  // 2000ms
+
+    static const int kMaxProcessRequestNum = 10;
+    struct CameraBufferInfo {
+        icamera::camera_buffer_t halBuffer[kMaxStreamNum];
+        uint32_t frameNumber;
+        bool frameInProcessing;
+    };
+    struct CameraBufferInfo mCameraBufferInfo[kMaxProcessRequestNum];
+
+    int mCameraId;
+    const camera3_callback_ops_t* mCallbackOps;
+    bool mCameraDeviceStarted;
+    ResultProcessor* mResultProcessor;
+
+    std::map<int, android::CameraMetadata> mDefaultRequestSettings;
+    std::vector<Camera3Stream*> mCamera3StreamVector;
+    bool mInputStreamConfigured;
+
+    std::condition_variable mRequestCondition;
+    // mRequestLock is used to protect mRequestInProgress
+    std::mutex mRequestLock;
+    uint32_t mRequestInProgress;
+    android::CameraMetadata mLastSettings;
+    icamera::stream_t mHALStream[kMaxStreamNum];
+
+    /* choose HAL stream to do qbuf/dqbuf from stream list.
+     * halStreamFlag: array keeps the result.
+     * halStreamFlag[i] = n means the halStreams[i] is the Listener of
+     * halStreams[n], if i==n, then the stream is HAL stream.
+     * return value is the total Number of HAL streams
+     */
+    int chooseHALStreams(const uint32_t requestStreamNum, int* halStreamFlag,
+                         icamera::stream_t* halStreams);
+};
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/ResultProcessor.cpp b/camera/hal/intel/ipu6/aal/ResultProcessor.cpp
new file mode 100644
index 000000000000..26d70812ef53
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/ResultProcessor.cpp
@@ -0,0 +1,512 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "ResultProcessor"
+
+#include "ResultProcessor.h"
+
+#include <mutex>
+
+#include "Errors.h"
+#include "HALv3Utils.h"
+#include "MetadataConvert.h"
+#include "PlatformData.h"
+#include "Utils.h"
+
+namespace camera3 {
+
+#define META_ENTRY_COUNT 256
+#define META_DATA_COUNT 80000
+#define FPS_FRAME_COUNT 60  // the frame interval to print fps
+
+MetadataMemory::MetadataMemory() : mMeta(META_ENTRY_COUNT, META_DATA_COUNT), mMemory(nullptr) {}
+
+MetadataMemory::~MetadataMemory() {
+    // Return memory to metadata
+    getMetadata();
+}
+
+android::CameraMetadata* MetadataMemory::getMetadata() {
+    if (mMemory) {
+        mMeta.acquire(mMemory);
+        mMemory = nullptr;
+    }
+    return &mMeta;
+}
+
+camera_metadata_t* MetadataMemory::getMemory() {
+    if (!mMemory) {
+        mMemory = mMeta.release();
+    }
+    return mMemory;
+}
+
+void MetadataMemory::copyMetadata(const camera_metadata_t* src) {
+    getMemory();
+    // Clear old metadata
+    mMemory = place_camera_metadata(mMemory, get_camera_metadata_size(mMemory),
+                                    get_camera_metadata_entry_capacity(mMemory),
+                                    get_camera_metadata_data_capacity(mMemory));
+    getMetadata();
+    mMeta.append(src);
+}
+
+ResultProcessor::ResultProcessor(int cameraId, const camera3_callback_ops_t* callback,
+                                 RequestManagerCallback* requestManagerCallback)
+        : mCameraId(cameraId),
+          mCallbackOps(callback),
+          mLastSettings(nullptr),
+          mRequestManagerCallback(requestManagerCallback) {
+    UNUSED(mCameraId);
+    LOG1("@%s, mCameraId %d", __func__, mCameraId);
+
+    mLastSettings = acquireMetadataMemory();
+
+    mResultThread = std::unique_ptr<ResultThread>(new ResultThread(cameraId, this));
+
+    mCamera3AMetadata = new Camera3AMetadata(mCameraId);
+    gettimeofday(&mRequestTime, nullptr);
+
+    mLastParams.sensorExposure = -1;
+    mLastParams.sensorIso = -1;
+}
+
+ResultProcessor::~ResultProcessor() {
+    LOG1("@%s", __func__);
+
+    for (auto& item : mRequestStateVector) {
+        releaseMetadataMemory(item.metaResult);
+    }
+    mRequestStateVector.clear();
+
+    releaseMetadataMemory(mLastSettings);
+    while (mMetadataVector.size() > 0) {
+        LOG1("%s: release meta %p", __func__, mMetadataVector.back());
+        delete mMetadataVector.back();
+        mMetadataVector.pop_back();
+    }
+
+    delete mCamera3AMetadata;
+
+    mInputCam3Bufs.clear();
+}
+
+void ResultProcessor::callbackNotify(const icamera::camera_msg_data_t& data) {
+    LOG2("@%s, type %d", __func__, data.type);
+
+    mResultThread->sendEvent(data);
+}
+
+int ResultProcessor::registerRequest(const camera3_capture_request_t* request,
+                                     std::shared_ptr<Camera3Buffer> inputCam3Buf) {
+    LOG1("@%s frame_number:%u, inputCam3Buf:%p", __func__, request->frame_number,
+         inputCam3Buf.get());
+
+    RequestState req;
+    req.frameNumber = request->frame_number;
+    req.buffersToReturn = request->num_output_buffers;
+    req.partialResultCount = 1;
+
+    std::lock_guard<std::mutex> l(mLock);
+    // Copy settings
+    if (request->settings) {
+        mLastSettings->copyMetadata(request->settings);
+    }
+
+    req.metaResult = acquireMetadataMemory();
+    req.metaResult->copyMetadata(mLastSettings->getMemory());
+
+    if (inputCam3Buf) {
+        mInputCam3Bufs[req.frameNumber] = inputCam3Buf;
+    }
+    mRequestStateVector.push_back(req);
+
+    return icamera::OK;
+}
+
+int ResultProcessor::shutterDone(const ShutterEvent& event) {
+    std::lock_guard<std::mutex> l(mLock);
+    bool inputBuffer = mInputCam3Bufs.find(event.frameNumber) != mInputCam3Bufs.end();
+    for (uint32_t i = 0; i < mRequestStateVector.size(); i++) {
+        if (mRequestStateVector.at(i).frameNumber != event.frameNumber ||
+            mRequestStateVector.at(i).isShutterDone) {
+            continue;
+        }
+
+        camera3_notify_msg_t notifyMsg;
+        notifyMsg.type = CAMERA3_MSG_SHUTTER;
+        notifyMsg.message.shutter.frame_number = mRequestStateVector.at(i).frameNumber;
+        notifyMsg.message.shutter.timestamp = event.timestamp;
+
+        MetadataMemory* metaResult = mRequestStateVector[i].metaResult;
+        android::CameraMetadata* meta = metaResult->getMetadata();
+        if (!inputBuffer) {
+            meta->update(ANDROID_SENSOR_TIMESTAMP,
+                         reinterpret_cast<const int64_t*>(&event.timestamp), 1);
+        } else {
+            // update shutter timestamp if there is input stream
+            camera_metadata_entry entry = meta->find(ANDROID_SENSOR_TIMESTAMP);
+            if (entry.count == 1) {
+                notifyMsg.message.shutter.timestamp = entry.data.i64[0];
+            }
+        }
+
+        mCallbackOps->notify(mCallbackOps, &notifyMsg);
+        mRequestStateVector.at(i).isShutterDone = true;
+        LOG2("@%s, frame_number:%u, shutter timestamp:%lld", __func__,
+             notifyMsg.message.shutter.frame_number, notifyMsg.message.shutter.timestamp);
+        if (checkRequestDone(mRequestStateVector.at(i))) {
+            returnRequestDone(notifyMsg.message.shutter.frame_number);
+            releaseMetadataMemory(mRequestStateVector.at(i).metaResult);
+            mRequestStateVector.erase(mRequestStateVector.begin() + i);
+        }
+        return icamera::OK;
+    }
+
+    LOGW("@%s frame_number:%u wasn't found!", __func__, event.frameNumber);
+    return icamera::OK;
+}
+
+int ResultProcessor::metadataDone(const MetadataEvent& event) {
+    MetadataMemory* metaMem = nullptr;
+    {
+        std::lock_guard<std::mutex> l(mLock);
+        for (auto& reqStat : mRequestStateVector) {
+            if (reqStat.frameNumber == event.frameNumber &&
+                reqStat.partialResultReturned < reqStat.partialResultCount) {
+                reqStat.partialResultReturned = 1;
+                metaMem = reqStat.metaResult;
+            }
+        }
+    }
+
+    if (metaMem) {
+        camera3_capture_result_t result;
+        CLEAR(result);
+        result.frame_number = event.frameNumber;
+        result.output_buffers = nullptr;
+        result.num_output_buffers = 0;
+
+        if (mInputCam3Bufs.find(result.frame_number) == mInputCam3Bufs.end()) {
+            android::CameraMetadata* metaResult = metaMem->getMetadata();
+            MetadataConvert::HALMetadataToRequestMetadata(*(event.parameter), metaResult,
+                                                          mCameraId);
+            updateMetadata(*(event.parameter), metaResult);
+            mCamera3AMetadata->process3Astate(*(event.parameter), metaResult);
+        }
+
+        result.result = metaMem->getMemory();
+        result.partial_result = 1;
+        mCallbackOps->process_capture_result(mCallbackOps, &result);
+
+        LOG2("@%s frame_number:%u, metadataDone", __func__, event.frameNumber);
+    }
+
+    bool found = false;
+    std::lock_guard<std::mutex> l(mLock);
+    for (uint32_t i = 0; i < mRequestStateVector.size(); i++) {
+        if (mRequestStateVector.at(i).frameNumber == event.frameNumber) {
+            if (checkRequestDone(mRequestStateVector.at(i))) {
+                returnInputBuffer(event.frameNumber);
+                returnRequestDone(event.frameNumber);
+                releaseMetadataMemory(mRequestStateVector.at(i).metaResult);
+                mRequestStateVector.erase(mRequestStateVector.begin() + i);
+            }
+            found = true;
+        }
+    }
+    if (!found) {
+        LOGW("%s, event.frameNumber %u wasn't found!", __func__, event.frameNumber);
+    } else {
+        LOG2("%s, event.frameNumber %u was returned", __func__, event.frameNumber);
+    }
+
+    return icamera::OK;
+}
+
+int ResultProcessor::bufferDone(const BufferEvent& event) {
+    camera3_capture_result_t result;
+    CLEAR(result);
+
+    result.frame_number = event.frameNumber;
+    result.output_buffers = event.outputBuffer;
+    result.num_output_buffers = 1;
+    result.result = nullptr;
+    result.partial_result = 0;
+
+    mCallbackOps->process_capture_result(mCallbackOps, &result);
+
+    bool found = false;
+    {
+        std::lock_guard<std::mutex> l(mLock);
+        for (uint32_t i = 0; i < mRequestStateVector.size(); i++) {
+            if (mRequestStateVector.at(i).frameNumber == event.frameNumber) {
+                mRequestStateVector.at(i).buffersReturned++;
+                if (checkRequestDone(mRequestStateVector.at(i))) {
+                    returnInputBuffer(event.frameNumber);
+                    returnRequestDone(event.frameNumber);
+                    releaseMetadataMemory(mRequestStateVector.at(i).metaResult);
+                    mRequestStateVector.erase(mRequestStateVector.begin() + i);
+                }
+                found = true;
+            }
+        }
+    }
+    if (!found) {
+        LOGW("%s, event.frameNumber %u wasn't found!", __func__, event.frameNumber);
+    } else {
+        LOG2("%s, event.frameNumber %u was returned", __func__, event.frameNumber);
+    }
+
+    if (event.timestamp != 0 && event.sequence != -1) {
+        std::lock_guard<std::mutex> lock(mmOpaqueRawInfoMapLock);
+        // Raw buffer cached in HAL
+        int savedRawBufNum = icamera::PlatformData::getMaxRawDataNum(mCameraId) -
+                             icamera::PlatformData::getMaxRequestsInflight(mCameraId);
+        // There are buffers processed in PSYS which may return to sensor, so the
+        // last max in fight buffers are not safe now.
+        int securityRawBufNum =
+            savedRawBufNum - icamera::PlatformData::getMaxRequestsInflight(mCameraId);
+        if (mOpaqueRawInfoMap.size() >= securityRawBufNum)
+            mOpaqueRawInfoMap.erase(mOpaqueRawInfoMap.begin());
+        // Only save Raw buffer info matching with saved Raw bufer Queue in PSYS
+        mOpaqueRawInfoMap[event.sequence] = event.timestamp;
+    }
+    return icamera::OK;
+}
+
+void ResultProcessor::clearRawBufferInfoMap() {
+    std::lock_guard<std::mutex> lock(mmOpaqueRawInfoMapLock);
+    mOpaqueRawInfoMap.clear();
+}
+
+void ResultProcessor::checkAndChangeRawbufferInfo(long* sequence, uint64_t* timestamp) {
+    CheckError(!sequence || !timestamp, VOID_VALUE, "invilid input parameter!");
+
+    std::lock_guard<std::mutex> lock(mmOpaqueRawInfoMapLock);
+    if (mOpaqueRawInfoMap.empty()) return;
+    if (mOpaqueRawInfoMap.find(*sequence) != mOpaqueRawInfoMap.end()) return;
+
+    // Raw buffer is too old and can't be handled, just use oldest buffer
+    auto it = mOpaqueRawInfoMap.cbegin();
+    *sequence = (*it).first;
+    *timestamp = (*it).second;
+    LOG2("%s, update raw info sequence %ld, timestamp %ld", __func__, *sequence, *timestamp);
+}
+
+void ResultProcessor::updateMetadata(const icamera::Parameters& parameter,
+                                     android::CameraMetadata* settings) {
+    /*
+     *  if we support face ae and the face detection mode is not off,
+     *  set face detect to request metadata, then the face area will be drawn.
+     */
+    if (icamera::PlatformData::isFaceAeEnabled(mCameraId)) {
+        uint8_t faceDetectMode;
+        int ret = parameter.getFaceDetectMode(faceDetectMode);
+        if (ret == icamera::OK && faceDetectMode != icamera::FD_MODE_OFF) {
+            icamera::CVFaceDetectionAbstractResult faceResult;
+            ret = icamera::FaceDetection::getResult(mCameraId, &faceResult);
+            if (ret == icamera::OK) {
+                MetadataConvert::convertFaceDetectionMetadata(faceResult, settings);
+                LOG2("@%s, set face detection metadata, face number:%d", __func__,
+                     faceResult.faceNum);
+            }
+        }
+    }
+
+    int64_t exposure = 0;
+    int32_t sensorIso = 0;
+    parameter.getExposureTime(exposure);
+    parameter.getSensitivityIso(sensorIso);
+
+    // If the state of black level lock is ON in the first request, the value must
+    // be set ON. Other request sets the black level lock value according sensor
+    // exposure time and iso.
+    uint8_t lockMode = ANDROID_BLACK_LEVEL_LOCK_OFF;
+    camera_metadata_entry entry = settings->find(ANDROID_BLACK_LEVEL_LOCK);
+    if (entry.count == 1 && (entry.data.u8[0] == ANDROID_BLACK_LEVEL_LOCK_ON)) {
+        lockMode =
+            ((exposure == mLastParams.sensorExposure && sensorIso == mLastParams.sensorIso) ||
+             (-1 == mLastParams.sensorExposure && -1 == mLastParams.sensorIso))
+                ? ANDROID_BLACK_LEVEL_LOCK_ON
+                : ANDROID_BLACK_LEVEL_LOCK_OFF;
+    }
+    LOG2("@%s, the black level lock metadata: %d", __func__, lockMode);
+    settings->update(ANDROID_BLACK_LEVEL_LOCK, &lockMode, 1);
+
+    mLastParams.sensorExposure = exposure;
+    mLastParams.sensorIso = sensorIso;
+}
+
+// the input buffer must be returned as the last one buffer
+void ResultProcessor::returnInputBuffer(uint32_t frameNumber) {
+    if (mInputCam3Bufs.find(frameNumber) == mInputCam3Bufs.end()) {
+        return;
+    }
+
+    std::shared_ptr<Camera3Buffer> inBuf = mInputCam3Bufs[frameNumber];
+    if (!inBuf) {
+        return;
+    }
+
+    camera3_stream_t* s = inBuf->getStream();
+    if (s) {
+        LOG2("@%s, frame_number:%u, w:%d, h:%d, f:%d", __func__, frameNumber, s->width, s->height,
+             s->format);
+    }
+
+    camera3_stream_buffer_t buf = {};
+    buf.stream = s;
+    buf.buffer = inBuf->getBufferHandle();
+    buf.status = inBuf->status();
+
+    inBuf->getFence(&buf);
+    inBuf->unlock();
+    inBuf->deinit();
+
+    camera3_capture_result_t result = {};
+    result.frame_number = frameNumber;
+    result.result = nullptr;
+    result.input_buffer = &buf;
+
+    LOG1("@%s, frame_number:%u, return the input buffer", __func__, frameNumber);
+    mCallbackOps->process_capture_result(mCallbackOps, &result);
+
+    mInputCam3Bufs.erase(frameNumber);
+}
+
+bool ResultProcessor::checkRequestDone(const RequestState& requestState) {
+    LOG1("@%s", __func__);
+
+    return (requestState.isShutterDone &&
+            requestState.partialResultCount == requestState.partialResultReturned &&
+            requestState.buffersToReturn == requestState.buffersReturned);
+}
+
+void ResultProcessor::returnRequestDone(uint32_t frameNumber) {
+    LOG2("@%s frame_number:%d", __func__, frameNumber);
+    TRACE_LOG_POINT("ResultProcessor", __func__, MAKE_COLOR(frameNumber), frameNumber);
+
+    if ((frameNumber % FPS_FRAME_COUNT == 0) &&
+        icamera::Log::isDebugLevelEnable(icamera::CAMERA_DEBUG_LOG_FPS)) {
+        struct timeval curTime;
+        gettimeofday(&curTime, nullptr);
+        int duration = static_cast<int>(curTime.tv_usec - mRequestTime.tv_usec +
+                                        ((curTime.tv_sec - mRequestTime.tv_sec) * 1000000));
+        if (frameNumber == 0) {
+            LOGFPS("%s, time of launch to preview: %dms", __func__, (duration / 1000));
+        } else {
+            float curFps =
+                static_cast<float>(1000000) / static_cast<float>(duration / FPS_FRAME_COUNT);
+            LOGFPS("@%s, fps: %02f", __func__, curFps);
+        }
+        gettimeofday(&mRequestTime, nullptr);
+    }
+
+    mRequestManagerCallback->returnRequestDone(frameNumber);
+}
+
+MetadataMemory* ResultProcessor::acquireMetadataMemory() {
+    MetadataMemory* metaMem = nullptr;
+    if (mMetadataVector.size() > 0) {
+        metaMem = mMetadataVector.back();
+        mMetadataVector.pop_back();
+    } else {
+        metaMem = new MetadataMemory();
+        LOG1("%s: allocate new one: %p", __func__, metaMem);
+    }
+
+    return metaMem;
+}
+
+void ResultProcessor::releaseMetadataMemory(MetadataMemory* metaMem) {
+    CheckError(metaMem == nullptr, VOID_VALUE, "%s: null metaMem!", __func__);
+    mMetadataVector.push_back(metaMem);
+}
+
+ResultProcessor::ResultThread::ResultThread(int cameraId, ResultProcessor* resultProcessor)
+        : mCameraId(cameraId),
+          mResultProcessor(resultProcessor) {
+    LOG1("@%s", __func__);
+
+    run("ResultThread");
+}
+
+ResultProcessor::ResultThread::~ResultThread() {
+    LOG1("@%s", __func__);
+
+    requestExit();
+    std::lock_guard<std::mutex> l(mEventQueueLock);
+    mEventCondition.notify_one();
+}
+
+void ResultProcessor::ResultThread::sendEvent(const icamera::camera_msg_data_t& data) {
+    LOG2("@%s", __func__);
+    std::lock_guard<std::mutex> l(mEventQueueLock);
+    mEventQueue.push(data);
+    mEventCondition.notify_one();
+}
+
+bool ResultProcessor::ResultThread::threadLoop() {
+    LOG2("@%s", __func__);
+
+    icamera::camera_msg_data_t data;
+    // mutex only protects mEventQueue
+    {
+        std::unique_lock<std::mutex> l(mEventQueueLock);
+        // check if there is event queued
+        if (mEventQueue.empty()) {
+            std::cv_status ret = mEventCondition.wait_for(
+                l, std::chrono::nanoseconds(kMaxDuration * SLOWLY_MULTIPLIER));
+
+            if (ret == std::cv_status::timeout) {
+                LOGW("%s, wait event timeout", __func__);
+            }
+
+            return true;
+        }
+
+        // parse event
+        data = std::move(mEventQueue.front());
+        mEventQueue.pop();
+    }
+
+    // handle message
+    switch (data.type) {
+        // Regards isp buffer ready as shutter event
+        case icamera::CAMERA_ISP_BUF_READY: {
+            ShutterEvent event = {data.data.buffer_ready.frameNumber,
+                                  data.data.buffer_ready.timestamp};
+            LOG2("@%s, frameNumber %d, timestamp %ld, mResultProcessor:%p", __func__,
+                 event.frameNumber, event.timestamp, mResultProcessor);
+            mResultProcessor->shutterDone(event);
+
+            icamera::Parameters parameter;
+            icamera::camera_get_parameters(mCameraId, parameter, data.data.buffer_ready.sequence);
+            MetadataEvent metadataEvent = {data.data.buffer_ready.frameNumber, &parameter};
+            mResultProcessor->metadataDone(metadataEvent);
+            break;
+        }
+        default: {
+            LOGW("unknown message type %d", data.type);
+            break;
+        }
+    }
+
+    return true;
+}
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/ResultProcessor.h b/camera/hal/intel/ipu6/aal/ResultProcessor.h
new file mode 100644
index 000000000000..9d81a132bcf1
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/ResultProcessor.h
@@ -0,0 +1,196 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <hardware/camera3.h>
+#include <sys/time.h>
+
+#include <map>
+#include <mutex>
+#include <queue>
+#include <unordered_map>
+#include <vector>
+
+#include "Camera3AMetadata.h"
+#include "Camera3Buffer.h"
+#include "HALv3Header.h"
+#include "HALv3Interface.h"
+#include "Parameters.h"
+#include "Thread.h"
+
+namespace camera3 {
+
+// Store metadata that are created by the AAL
+// To avoid continuous allocation/de-allocation of metadata buffers
+class MetadataMemory {
+ public:
+    MetadataMemory();
+    ~MetadataMemory();
+
+    // Don't access to metadata and memory in parellel
+    // because metadata may reallocate memory when new entries are added.
+    android::CameraMetadata* getMetadata();  // For entries update
+    camera_metadata_t* getMemory();          // For metadata copy
+
+    // Helper function to avoid memory reallocation
+    void copyMetadata(const camera_metadata_t* src);
+
+ private:
+    android::CameraMetadata mMeta;  // May reallocate buffer if entries are added
+    camera_metadata_t* mMemory;
+};
+
+struct RequestState {
+    uint32_t frameNumber;
+
+    bool isShutterDone;
+
+    unsigned int partialResultReturned;
+    unsigned int partialResultCount;
+
+    unsigned int buffersReturned;
+    unsigned int buffersToReturn;
+
+    MetadataMemory* metaResult;
+
+    RequestState() {
+        frameNumber = 0;
+        isShutterDone = false;
+        partialResultReturned = 0;
+        partialResultCount = 0;
+        buffersReturned = 0;
+        buffersToReturn = 0;
+        metaResult = nullptr;
+    }
+};
+
+struct MetadataEvent {
+    uint32_t frameNumber;
+    const icamera::Parameters* parameter;
+};
+
+struct ShutterEvent {
+    uint32_t frameNumber;
+    uint64_t timestamp;
+};
+
+struct BufferEvent {
+    uint32_t frameNumber;
+    const camera3_stream_buffer_t* outputBuffer;
+    uint64_t timestamp;
+    long sequence;
+};
+
+struct ReferenceParam {
+    int64_t sensorExposure;
+    int32_t sensorIso;
+};
+
+/**
+ * \brief An interface used to callback buffer event.
+ */
+class CallbackEventInterface {
+ public:
+    CallbackEventInterface() {}
+    virtual ~CallbackEventInterface() {}
+
+    virtual int metadataDone(const MetadataEvent& event) = 0;
+    virtual int bufferDone(const BufferEvent& event) = 0;
+    virtual int shutterDone(const ShutterEvent& event) = 0;
+};
+
+/**
+ * \class ResultProcessor
+ *
+ * This class is used to handle shutter done, buffer done and metadata done
+ * event.
+ *
+ */
+class ResultProcessor : public CallbackEventInterface {
+ public:
+    ResultProcessor(int cameraId, const camera3_callback_ops_t* callback,
+                    RequestManagerCallback* requestManagerCallback);
+    virtual ~ResultProcessor();
+
+    int registerRequest(const camera3_capture_request_t* request,
+                        std::shared_ptr<Camera3Buffer> inputCam3Buf);
+
+    virtual int metadataDone(const MetadataEvent& event);
+    virtual int shutterDone(const ShutterEvent& event);
+    virtual int bufferDone(const BufferEvent& event);
+
+    void callbackNotify(const icamera::camera_msg_data_t& data);
+
+    // Used to handle Opaque raw reprocessing
+    void clearRawBufferInfoMap(void);
+    void checkAndChangeRawbufferInfo(long* sequence, uint64_t* timestamp);
+
+ private:
+    bool checkRequestDone(const RequestState& requestState);
+    void returnRequestDone(uint32_t frameNumber);
+
+    MetadataMemory* acquireMetadataMemory();
+    void releaseMetadataMemory(MetadataMemory* metaMem);
+
+    void returnInputBuffer(uint32_t frameNumber);
+    void updateMetadata(const icamera::Parameters& parameter, android::CameraMetadata* settings);
+
+ private:
+    class ResultThread : public icamera::Thread {
+     public:
+        ResultThread(int cameraId, ResultProcessor* resultProcessor);
+        ~ResultThread();
+
+        void sendEvent(const icamera::camera_msg_data_t& data);
+
+     private:
+        virtual bool threadLoop();
+
+     private:
+        const uint64_t kMaxDuration = 2000000000;  // 2000ms
+        int mCameraId;
+        ResultProcessor* mResultProcessor;
+
+        std::condition_variable mEventCondition;
+        std::mutex mEventQueueLock;
+        std::queue<icamera::camera_msg_data_t> mEventQueue;
+    };
+    std::unique_ptr<ResultThread> mResultThread;
+
+    int mCameraId;
+    const camera3_callback_ops_t* mCallbackOps;
+
+    // mLock is used to protect mRequestStateVector
+    std::mutex mLock;
+    std::vector<RequestState> mRequestStateVector;
+    std::vector<MetadataMemory*> mMetadataVector;
+    MetadataMemory* mLastSettings;
+
+    RequestManagerCallback* mRequestManagerCallback;
+
+    Camera3AMetadata* mCamera3AMetadata;
+
+    std::unordered_map<int, std::shared_ptr<Camera3Buffer>> mInputCam3Bufs;
+    timeval mRequestTime;
+    ReferenceParam mLastParams;
+
+    // first key is sequence from HAL, second key is timestamp of RAW buffer
+    std::map<int64_t, uint64_t> mOpaqueRawInfoMap;
+    std::mutex mmOpaqueRawInfoMapLock;  // used to protect mOpaqueRawInfoMap
+};
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/aal/chrome/HALv3Header.h b/camera/hal/intel/ipu6/aal/chrome/HALv3Header.h
new file mode 100644
index 000000000000..3fc2deff906b
--- /dev/null
+++ b/camera/hal/intel/ipu6/aal/chrome/HALv3Header.h
@@ -0,0 +1,20 @@
+/*
+ * Copyright (C) 2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+// metadata related header file
+#include <camera/camera_metadata.h>
diff --git a/camera/hal/intel/ipu6/include/api/ICamera.h b/camera/hal/intel/ipu6/include/api/ICamera.h
new file mode 100644
index 000000000000..6e75e1b0311a
--- /dev/null
+++ b/camera/hal/intel/ipu6/include/api/ICamera.h
@@ -0,0 +1,578 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ *
+ * Filename: ICamera.h
+ *
+ * ------------------------------------------------------------------------------
+ * REVISION HISTORY
+ *******************************************************************************
+ *     Version        0.10       Initialize camera HAL API
+ *******************************************************************************
+ *     Version        0.20       Update for the device handle and stream handle
+ *                               Update for the coding style of the type
+ *******************************************************************************
+ *     Version        0.21       Remove API camera_device_set_param and camera_device_get_param
+ *                               Remove type define camera_stream_t
+ *******************************************************************************
+ *     Version        0.30       Remove API camera_device_del_stream
+ *                               Rename API camera_device_add_stream -> camera_device_config_streams
+ *                                   * All the streams are added at the same time
+ *                               Rename camera_device_init -> camera_hal_init
+ *                               Rename camera_device_deinit -> camera_hal_deinit
+ *                               Remove the typedef for struct camera_buffer
+ *******************************************************************************
+ *     Version        0.31       Remove duplicated camera_frame_info_t
+ *                               Cleanup the camera_buffer_t structure
+ *******************************************************************************
+ *     Version        0.40       Delete redundant query_frame_info
+ *                               Add assist API: get_frame_size
+ *******************************************************************************
+ *     Version        0.41       Add API camera_device_allocate_memory to enable mmap memory mode
+ *******************************************************************************
+ *     Version        0.42       Merge all the common types to Parameters.h
+ *******************************************************************************
+  *    Version        0.43       Add sensor description in camera_info_t
+ *******************************************************************************
+ *     Version        0.44       Change output parameter of get_frame_size
+ *******************************************************************************
+ *     Version        0.45       Add two parameters(number of buffers, parameters) for camera_stream_qbuf
+                                 Add one parameter(parameters) for camera_stream_dqbuf
+*******************************************************************************
+ *     Version        0.46       Add virtual channel camera number for camera_device_open
+                                 Add virtual channel information in camera_info_t
+ *******************************************************************************
+ *     Version        0.50       Support specifying input format (aka ISYS output format).
+ *******************************************************************************
+ *     Version        0.51       Support specifying inputConfig inlucding inputformat/resolution
+                                 Params int inputFmt changed to stream_t *inputConfig
+ *******************************************************************************
+ *     Version        0.60       Overload interface camera_stream_qbuf() with refined parameters
+                                 old camera_stream_qbuf() will be removed soon
+ *******************************************************************************
+ *     Version        0.61       Add API camera_callback_register() to notify event to AAL
+ *******************************************************************************
+ *     Version        0.62       Add sequence in camera_get_parameters to fetch settings
+ * ------------------------------------------------------------------------------
+ *
+ */
+
+#pragma once
+
+#include "Parameters.h"
+
+#include <stdlib.h> // For including definition of NULL
+
+namespace icamera {
+
+/**
+ * \struct camera_info_t: Define each camera basic information
+ */
+typedef struct {
+    int facing;
+    int orientation;
+    int device_version;
+    const char* name; /**< Sensor name */
+    const char* description; /**< Sensor description */
+    const Parameters *capability; /**< camera capability */
+} camera_info_t;
+
+/**
+ * \brief
+ *   Get numbers of camera
+ *
+ * \note
+ *   This allows user to get the numbers of camera without init or open camera device.
+ *
+ * \return
+ *   >0 the numbers of camera
+ * \return
+ *   <= 0 failed to get camera numbers
+ *
+ * \par Sample code:
+ *
+ * \code
+ *   int num = get_number_of_cameras();
+ * \endcode
+ **/
+int get_number_of_cameras();
+
+/**
+ * \brief
+ *   Get camera info including camera capability.
+ *
+ * \note
+ *   It can be called before hal init
+ *
+ * \param[in]
+ *   int camera_id: ID of the camera
+ * \param[out]
+ *   camera_info_t info: Camera info filled by libcamhal
+ *
+ * \return
+ *   0 succeed to get camera info
+ * \return
+ *   <0 error code, failed to get camera info
+ *
+ * \par Sample code
+ *
+ * \code
+ *   int camera_id = 0;
+ *   camera_info_t info;
+ *   get_camera_info(camera_id, info);
+ *
+ * \endcode
+ *
+ **/
+int get_camera_info(int camera_id, camera_info_t& info);
+
+/**
+ * \brief
+ *   Initialize camera HAL
+ *
+ * \return
+ *   0 succeed to init camera HAL
+ * \return
+ *   <0 error code, failed to init camera HAL
+ *
+ * \par Sample code:
+ *
+ * \code
+ *   int ret = camera_hal_init();
+ * \endcode
+ **/
+int camera_hal_init();
+
+/**
+ * \brief
+ *   De-initialize camera HAL
+ *
+ * \return
+ *   0 succeed to deinit camera HAL
+ * \return
+ *   <0 error code, failed to deinit camera device
+ *
+ * \par Sample code:
+ *
+ * \code
+ *   int ret = camera_hal_deinit();
+ * \endcode
+ **/
+int camera_hal_deinit();
+
+/**
+ * \brief
+ *   Register callback function
+ *
+ * \param[in]
+ *   int camera_id: ID of the camera
+ * \param[in]
+ *   camera_callback_ops_t *callback: callback handle
+ *
+ **/
+void camera_callback_register(int camera_id, const camera_callback_ops_t *callback);
+
+/**
+ * \brief
+ *   Open camera device by camera ID
+ *
+ * \param[in]
+ *   int camera_id: ID of the camera
+ *
+ * \return
+ *   0 succeed to open camera device
+ * \return
+ *   <0 error code, failed to open camera device
+ *
+ * \par Sample code:
+ *
+ * \code
+ *   int camera_id = 0;
+ *   int ret = camera_device_open(camera_id);
+ * \endcode
+ **/
+int camera_device_open(int camera_id);
+
+/**
+ * \brief
+ *   Close camera device by camera ID
+ *
+ * \param[in]
+ *   int camera_id: ID of the camera
+ *
+ * \par Sample code:
+ *
+ * \code
+ *   int camera_id = 0;
+ *   int ret = camera_device_open(camera_id);
+ *   camera_device_close(camera_id);
+ * \endcode
+ **/
+void camera_device_close(int camera_id);
+
+/**
+ * \brief
+ *   Configure sensor input of camera device, it is not allowed to call this when camera is started.
+ *   Optional call.
+ *
+ * \note
+ *   1. To re-configure sensor input, camera device must be stopped first.
+ *   2. The new sensor configuration will overwrite the previous config.
+ *   3. The new "inputConfig" will be used for all the future operation until the device is closed.
+ *
+ * \param[in]
+ *   int camera_id: ID of the camera
+ * \param[in]
+ *   int inputConfig: Specify which input format, resolution(the output of ISYS) should be used.
+ *
+ * \return
+ *   0 succeed to configure streams
+ * \return
+ *   <0 error code, failed to configure stream
+ *
+ * \par Sample code:
+ *
+ * \code
+ *   int camera_id = 0;
+ *   stream_t input_config;
+ *   CLEAR(input_config);
+ *   input_config.format = V4L2_PIX_FMT_SGRBG8V32;
+ *   ret = camera_device_config_sensor_input(camera_id, &input_config);
+ * \endcode
+ **/
+int camera_device_config_sensor_input(int camera_id, const stream_t *inputConfig);
+
+/**
+ * \brief
+ *   Configure streams to camera device, it is not allowed to call this when camera is started.
+ *
+ * \note
+ *   1. To re-configure streams, camera device must be stopped first.
+ *   2. The new streams configuration will overwrite the previous streams.
+ *
+ * \param[in]
+ *   int camera_id: ID of the camera
+ * \param[in]
+ *   stream_config_t stream_list: stream configuration list, if success, stream id is filled in streams[]
+ *
+ * \return
+ *   0 succeed to configure streams
+ * \return
+ *   <0 error code, failed to configure stream
+ *
+ * \par Sample code:
+ *
+ * \code
+ *   int camera_id = 0;
+ *   stream_config_t stream_list;
+ *   stream_t streams[1];
+ *   streams[0].format = V4L2_PIX_FMT_SGRBG8;
+ *   streams[0].width = 1920;
+ *   streams[0].height = 1080;
+ *   streams[0].memType = V4L2_MEMORY_USERPTR;
+ *   stream_list.num_streams = 1;
+ *   stream_list.streams = streams;
+ *   ret = camera_device_config_streams(camera_id, &stream_list);
+ * \endcode
+ **/
+int camera_device_config_streams(int camera_id, stream_config_t *stream_list);
+
+/**
+ * \brief
+ *   Start camera device
+ *
+ * \param[in]
+ *   int camera_id: ID of the camera
+ *
+ * \return
+ *   0 succeed to start device
+ * \return
+ *   <0 error code, failed to start device
+ *
+ * \par Sample code:
+ *
+ * \code
+ *   int camera_id=0;
+ *   stream_config_t stream_list;
+ *   ...
+ *   ret = camera_device_config_streams(camera_id, &stream_list);
+ *   ret = camera_device_start(camera_id);
+ *   ... ...
+ *   ret = camera_device_stop(camera_id);
+ * \endcode
+ *
+ **/
+int camera_device_start(int camera_id);
+
+/**
+ * \brief
+ *   Stop camera device.
+ *
+ * \param[in]
+ *   int camera_id: ID of the camera
+ *
+ * \return
+ *   0 succeed to stop device
+ * \return
+ *   <0 error code, failed to stop device
+ *
+ * \see camera_device_start()
+ **/
+int camera_device_stop(int camera_id);
+
+/**
+ * \brief
+ *   Allocate memory for mmap & dma export io-mode
+ *
+ * \param[in]
+ *   int camera_id: ID of the camera
+ * \param[out]
+ *   camera_buffer_t buffer: in Mmap mode, mmaped address is filled in addr;
+ *   in DMA export mode, the dma fd is flled in dmafd.
+ *
+ * \return
+ *   0 succeed to allocate memory
+ * \return
+ *   <0 error code, failed to allocate memory
+ *
+ * \par Sample code:
+ *
+ * \code
+ *   camera_buffer_t *buffers = (camera_buffer_t *)malloc(sizeof(camera_buffer_t)*buffer_count);
+ *   camera_buffer_t *buf = buffers;
+ *   buf.s = stream;
+ *   for (int i = 0; i < buffer_count; i++, buf++) {
+ *     camera_device_allocate_memory(camera_id, buf):
+ *   }
+ *
+ *   buf = buffers;
+ *   for (int i = 0; i < buffer_count; i++, buf++) {
+ *       camera_stream_qbuf(camera_id, stream_id, buf);
+ *   }
+ *
+ *   camera_device_start(camera_id);
+ * \endcode
+ *
+ */
+int camera_device_allocate_memory(int camera_id, camera_buffer_t *buffer);
+
+/**
+ * \brief
+ *   Queue a buffer to device (deprecated, will be removed soon.)
+ *
+ * \param[in]
+ *   int camera_id: ID of the camera
+ * \param[in]
+ *   int stream_id: ID of stream
+ * \param[in]
+ *   camera_buffer_t buffer: buffer queued to device
+ * \param[in]
+ *   int num_buffers: indicates how many buffers need to be queued at the same time,
+                      and these buffers MUST be for different streams.
+                      And stream id in buffer MUST be filled correctly.
+ * \param[in]
+ *   Parameters settings: Settings used for this group of buffers.
+ *
+ * \return
+ *   0 succeed to queue buffers
+ * \return
+ *   <0 error code, failed to queue buffers
+ *
+ * \see camera_stream_dqbuf();
+ **/
+int camera_stream_qbuf(int camera_id, int stream_id, camera_buffer_t *buffer,
+                       int num_buffers = 1, const Parameters* settings = NULL);
+
+/**
+ * \brief
+ *   Queue one or serveral buffers to the camera device
+ *
+ * \param[in]
+ *   int camera_id: ID of the camera
+ * \param[in]
+ *   camera_buffer_t buffer: array of pointers to camera_buffer_t
+ *   buffer[i]->s MUST be filled before calling this API.
+ * \param[in]
+ *   int num_buffers: indicates how many buffers are in the buffer pointer array,
+ *                    and these buffers MUST be for different streams. Stream id is
+ *                    filled and give back to app when camera_device_config_streams()
+ *                    is called, HAL will do the mapping when parsing queued buffers
+ *                    according to num_buffers.
+ * \param[in]
+ *   Parameters settings: Settings used for this group of buffers.
+ *                        This is used for per-frame setting, which means the settings should be
+ *                        applied for the group of buffers.
+ *
+ * \return
+ *   0 succeed to queue buffers
+ * \return
+ *   <0 error code, failed to queue buffers
+ *
+ * \see camera_stream_qbuf();
+ **/
+int camera_stream_qbuf(int camera_id, camera_buffer_t **buffer,
+                       int num_buffers = 1, const Parameters* settings = NULL);
+
+/**
+ * \brief
+ *   Dequeue a buffer from device per stream id.
+ *
+ * \note
+ *   It's a block function, that means the caller will be blocked until buffer is ready.
+ *
+ * \param[in]
+ *   int camera_id: ID of the camera
+ * \param[in]
+ *   int stream_id: ID of stream
+ * \param[out]
+ *   camera_buffer_t buffer: buffer dqueued from device
+ * \param[out]
+ *   Parameters settings: Settings used for this buffer.
+ *
+ * \return
+ *   0 succeed to dqueue buffer
+ * \return
+ *   <0 error code, failed to dqueue buffer
+ *
+ * \par Sample code
+ *
+ * \code
+ *   const int buffer_count = 8;
+ *   int bpp = 0;
+ *   int buffer_size = get_frame_size(camera_id, V4L2_PIX_FMT_SGRBG8, 1920, 1080, V4L2_FIELD_ANY, &bpp);
+ *   camera_buffer_t buffers[buffer_count];
+ *   camera_buffer_t *buf = nullptr;
+ *   for (int i = 0; i < buffer_count; i++) {
+ *     buf = &buffers[i];
+ *     posix_memalign(&buf->addr, getpagesize(), buffer_size);
+ *     buf->s = stream; // stream here comes from parameter and result of camera_device_config_streams.
+ *   }
+ *
+ *   for (int i = 0; i < buffer_count; i++) {
+ *       buf = &buffers[i];
+ *       camera_stream_qbuf(camera_id, stream_id, &buf);
+ *   }
+ *
+ *   camera_device_start(camera_id);
+ *
+ *   for (int i = 0; i < buffer_count; i++) {
+ *       camera_stream_dqbuf(camera_id, stream_id, &buf);
+ *       // processing data with buf
+ *   }
+ * \endcode
+ *
+ **/
+int camera_stream_dqbuf(int camera_id, int stream_id, camera_buffer_t **buffer,
+                        Parameters* settings = NULL);
+
+/**
+ * \brief
+ *   Set a set of parameters to the gaven camera device.
+ *
+ * \note
+ *   It MUST be called after device opened, otherwise error will be returned.
+ *   Which buffer the paramters takes effect in is not guaranteed.
+ *
+ * \param[in]
+ *   int camera_id: ID of the camera
+ * \param[in]
+ *   Parameters param: A set of parameters.
+ *
+ * \return
+ *   0 succeed to set camera parameters
+ * \return
+ *   <0 error code, failed to set camera parameters
+ *
+ * \par Sample code
+ *
+ * \code
+ *   Parameters param;
+ *    camera_ae_mode_t aeMode = AE_MODE_MANUAL;
+ *    int64_t expTime = 10 * 1000;
+ *    param.setAeMode(aeMode);
+ *    param.setExposureTime(expTime);
+ *    param.setXXX(); // Set other parameters...
+ *
+ *   int ret = camera_set_parameters(camera_id, param);
+ *
+ * \endcode
+ *
+ **/
+int camera_set_parameters(int camera_id, const Parameters& param);
+
+/**
+ * \brief
+ *   Get parameter from the gaven camera device.
+ *
+ * \note
+ *   It MUST be called after device opened, otherwise error will be returned.
+ *
+ * \param[in]
+ *   int camera_id: ID of the camera
+ * \param[out]
+ *   Parameters param:  parameters need to be filled in
+ * \param[in]
+ *   long sequence: sequence used to find target parameter and results, default is -1
+ *
+ * \return
+ *   0 succeed to get camera parameters
+ * \return
+ *   <0 error code, failed to get camera parameters
+ *
+ * \par Sample code
+ *
+ * \code
+ *   Parameters param;
+ *   int ret = camera_get_parameters(camera_id, param);
+ *   camera_ae_mode_t aeMode = AE_MODE_MANUAL;
+ *   ret = param.getAeMode(aeMode);
+ *
+ * \endcode
+ *
+ **/
+int camera_get_parameters(int camera_id, Parameters& param, long sequence = -1);
+
+/**************************************Optional API ******************************
+ * The API defined in this section is optional.
+ */
+
+/**
+ * \brief
+ *   Return the size information of a frame.
+ *
+ * \note
+ *   It is used to assist the test cases to double confirm the final buffer size
+ *
+ * \param[in]
+ *   int camera_id: The camera device index
+ * \param[in]
+ *   int format: The v4l2 format of the frame
+ * \param[in]
+ *   int width: The width of the frame
+ * \param[in]
+ *   int height: The height of the frame
+ * \param[in]
+ *   int field: The interlace field of the frame
+ * \param[out]
+ *   int bpp: The bpp of the format
+ *
+ * \return
+ *   frame size.
+ **/
+int get_frame_size(int camera_id, int format, int width, int height, int field, int *bpp);
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/include/api/Parameters.h b/camera/hal/intel/ipu6/include/api/Parameters.h
new file mode 100644
index 000000000000..1cfce85b4c27
--- /dev/null
+++ b/camera/hal/intel/ipu6/include/api/Parameters.h
@@ -0,0 +1,2630 @@
+/*
+ * Copyright (C) 2013 The Android Open Source Project
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*
+ *
+ * Filename: Parameters.h
+ *
+ * ------------------------------------------------------------------------------
+ * REVISION HISTORY
+ *     Version        0.1        Initialize camera parameters API
+ *     Version        0.2        Merge all the types to this file
+ *     Version        0.3        Add AE compensation related APIs.
+ *     Version        0.31       Add manual color matrix APIs.
+ *     Version        0.32       Add manual AE/AWB converge speed APIs.
+ *     Version        0.33       Add timestamp variable in camera_buffer_t
+ *     Version        0.34       Add AE window weight grid API
+ *     Version        0.40       Add Data Structure for HAL 3.3
+ *     Version        0.41       Add API getSupportedAeExposureTimeRange
+ *                               Add API getSupportedAeGainRange
+ *     Version        0.42       Add API updateDebugLevel
+ *     Version        0.43       Add API set and get deinterlace mode
+ *     Version        0.44       Add API set and get gps processing method
+ *                               Add API set and get focal length
+ *     Version        0.45       Add get supported static metadata APIs
+ *     Version        0.50       Support low level ISP feature control
+ *     Version        0.51       Support getting supported ISP control feature list
+ *     Version        0.52       Add API set and get awb result
+ *     Version        0.53       Add API to get/set enabled ISP control feature list
+ *     Version        0.54       Add API to get/set fisheye dewarping mode
+ *     Version        0.55       Add API to get/set LTM tuning data
+ *     Version        0.56       Add API to get/set LDC/RSC/digital zoom ratio
+ *     Version        0.58       Add API to get/set 3A state, and lens state.
+ *     Version        0.59       Add API to get/set AE/AWB lock
+ *     Version        0.61       Add API to support vertical and horizontal flip.
+ *     Version        0.62       Add API to support 3A cadence.
+ *     Version        0.63       Add API to enable/disable MONO Downscale feature.
+ *     Version        0.64       Add callback message definition.
+ *     Version        0.65       Add API to support OUTPUT/INPUT streams.
+ *     Version        0.66       modifies callback message definition.
+ *     Version        0.67       Add API to support lens.focusDistance and lens.focalLength
+ *     Version        0.68       Add API to support shading map.
+ *     Version        0.69       Add API to support statistics lens shading map control.
+ *     Version        0.70       Add API to support tonemap.
+ *     Version        0.71       Add API to support OPAQUE RAW usage for RAW reprocessing.
+ *     Version        0.72       Add streamType into supported_stream_config_t.
+ *     Version        0.73       Remove supported_stream_config_t structure.
+ *     Version        0.74       Add API to support sensor iso.
+ *     Version        0.75       Add API to support lens static info about apertures,
+ *                               filter densities, min focus densities and hyperfocal distance.
+ *     Version        0.76       Remove the marco for lsc grid size
+ *     Version        0.77       Add API to support capture intent
+ *
+ *
+ * ------------------------------------------------------------------------------
+ */
+
+#pragma once
+
+#include <vector>
+#include <set>
+#include <stdint.h>
+
+namespace icamera {
+
+/***************Start of Camera Basic Data Structure ****************************/
+/**
+ * Basic definition will be inherited by more complicated structure.
+ * MUST be all "int" in this structure.
+ */
+typedef struct {
+    int width;
+    int height;
+} camera_resolution_t;
+
+/**
+ * \struct stream_t: stream basic info
+ *
+ * \note
+ *   MUST use int if new member added.
+ */
+typedef struct {
+    int format;    /**< stream format refer to v4l2 definition https://linuxtv.org/downloads/v4l-dvb-apis/pixfmt.html */
+    int width;     /**< image width */
+    int height;    /**< image height */
+    int field;     /**< refer to v4l2 definition https://linuxtv.org/downloads/v4l-dvb-apis/field-order.html#v4l2-field */
+
+/*
+* The buffer geometry introduction.
+* The YUV image is formed with Y:Luma and UV:Chroma. And there are
+* two kinds of styles for YUV format: planar and packed.
+*
+*   YUV420:NV12
+*
+*            YUV420(720x480) sampling
+*
+*       |<----width+padding=alignedBpl----->|
+*     Y *-------*-------*-------*-------*....-----
+*       |                               |   :  ^
+*       |   # UV            #           |   :  |
+*       |                               |   :  |
+*       *-------*-------*-------*-------*....  |
+*       |                               |   :  |
+*       |   #               #           |   :  |
+*       |                               |   :  |
+*       *-------*-------*-------*-------*.... (height * 3 / 2)
+*       |                               |   :  |
+*       |   #               #           |   :  |
+*       |                               |   :  |
+*       *-------*-------*-------*-------*....  |
+*       |                               |   :  |
+*       |   #               #           |   :  |
+*       |                               |   :  v
+*       *-------*-------*-------*-------*....-----
+*
+*         The data stored in memory
+*          ____________w___________ .....
+*         |Y0|Y1                   |    :
+*         |                        |    :
+*         h                        h    :
+*         |                        |    :
+*         |                        |    :
+*         |________________________|....:
+*         |U|V|U|V                 |    :
+*        h/2                      h/2   :
+*         |____________w___________|....:
+*
+*       bpp = 12
+*       bpl = width;
+*       stride = align64(bpl):
+*
+*   YUV422:YUY2
+*
+*           YUV422(720x480) sampling
+*
+*       |<--(width*2)+padding=alignedBpl-->|
+*   YUV *#----*#-----*#-----*#-----*#....-----
+*       *#----*#-----*#-----*#-----*#....  |
+*       *#----*#-----*#-----*#-----*#....  |
+*       *#----*#-----*#-----*#-----*#....  |
+*       *#----*#-----*#-----*#-----*#.... (height)
+*       *#----*#-----*#-----*#-----*#....  |
+*       *#----*#-----*#-----*#-----*#....  |
+*       *#----*#-----*#-----*#-----*#....  |
+*       *#----*#-----*#-----*#-----*#....  |
+*       *#----*#-----*#-----*#-----*#....-----
+*
+*         The data stored in memory
+*          ____________w___________ .....
+*         |Y0|Cb|Y1|Cr             |    :
+*         |                        |    :
+*         |                        |    :
+*         |                        |    :
+*         h                        h    :
+*         |                        |    :
+*         |                        |    :
+*         |                        |    :
+*         |____________w___________|....:
+*
+*       bpp = 16
+*       bpl = width * bpp / 8 = width * 2;
+*       stride = align64(bpl):
+*
+*       Note: The stride defined in HAL is same as aligned bytes per line.
+*/
+    int stride;    /**< stride = aligned bytes per line */
+    int size;      /**< real buffer size */
+
+    int id;        /**< Id that is filled by HAL. */
+    int memType;   /**< buffer memory type filled by app, refer to https://linuxtv.org/downloads/v4l-dvb-apis/io.html */
+
+    /**
+     * The maximum number of buffers the HAL device may need to have dequeued at
+     * the same time. The HAL device may not have more buffers in-flight from
+     * this stream than this value.
+     */
+    uint32_t max_buffers;
+
+    int usage; /**<The usage of this stream defined in camera_stream_usage_t. */
+    int streamType; /**<The stream type of this stream defined in camera_stream_type_t. */
+} stream_t;
+
+typedef std::vector<stream_t> stream_array_t;
+
+/**
+ * \struct stream_config_t: stream configuration info
+ *
+ * Contains all streams info in this configuration.
+ */
+typedef struct {
+    int num_streams; /**< number of streams in this configuration */
+    stream_t    *streams; /**< streams list */
+    /**
+     * The operation mode of the streams in this configuration. It should be one of the value
+     * defined in camera_stream_configuration_mode_t.
+     * The HAL uses this mode as an indicator to set the stream property (e.g.,
+     * camera_stream->max_buffers) appropriately. For example, if the configuration is
+     * CAMERA_STREAM_CONFIGURATION_CONSTRAINED_HIGH_SPEED_MODE, the HAL may want to set aside more
+     * buffers for batch mode operation (see camera.control.availableHighSpeedVideoConfigurations
+     * for batch mode definition).
+     */
+    uint32_t operation_mode;
+} stream_config_t;
+
+/**
+ * \struct camera_buffer_flags_t: Specify a buffer's properties.
+ *
+ * The buffer's properties can be one of them or combined with some of them.
+ */
+typedef enum {
+    BUFFER_FLAG_DMA_EXPORT = 1<<0,
+    BUFFER_FLAG_INTERNAL = 1<<1,
+    BUFFER_FLAG_SW_READ = 1<<2,
+    BUFFER_FLAG_SW_WRITE = 1<<3,
+} camera_buffer_flags_t;
+
+/**
+ * \struct camera_buffer_t: camera buffer info
+ *
+ * camera buffer is used to carry device frames. Application allocate buffer structure,
+ * according to memory type to allocate memory and queue to device.
+ */
+typedef struct {
+    stream_t s;   /**< stream info */
+    void *addr;   /**< buffer addr for userptr and mmap memory mode */
+    int index;    /**< buffer index, filled by HAL. it is used for qbuf and dqbuf in order */
+    long sequence; /**< buffer sequence, filled by HAL, to record buffer dqueue sequence from device */
+    int dmafd;    /**< buffer dmafd for DMA import and export mode */
+    int flags;    /**< buffer flags, its type is camera_buffer_flags_t, used to specify buffer properties */
+    uint64_t timestamp; /**< buffer timestamp, it's a time reference measured in nanosecond */
+    int reserved; /**< reserved for future */
+} camera_buffer_t;
+
+/**
+ * camera_stream_type_t:
+ *
+ * The type of the camera stream, which defines whether the camera HAL device
+ * is the producer or the consumer for that stream, and how the buffers of that
+ * stream relate to the other streams.
+ */
+typedef enum {
+    /**
+     * This stream is an output stream; the camera HAL device will be responsible to
+     * fill the buffers of this stream with newly captured or reprocessed image data.
+     */
+    CAMERA_STREAM_OUTPUT = 0,
+
+    /**
+     * This stream is an input stream; the camera HAL device will be responsible
+     * to read buffers from this stream and to send them through the camera
+     * processing pipeline, as if the buffer was a newly captured image from
+     * the imager.
+     *
+     * The pixel format for an input stream can be any format reported by
+     * camera.scaler.availableInputOutputFormatsMap. The pixel format of the
+     * output stream used to produce the reprocessing data may be any format
+     * reported by camera.scaler.availableStreamConfigurations. The supported
+     * inputoutput stream combinations depends on the camera device capabilities.
+     * See camera.scaler.availableInputOutputFormatsMap for stream map details.
+     *
+     * This kind of stream is generally used to reprocess data into higher
+     * quality images (that otherwise would cause a frame rate performance loss),
+     * or to do off-line reprocessing.
+     * The typical use cases are OPAQUE (typically ZSL) and YUV reprocessing.
+     */
+    CAMERA_STREAM_INPUT = 1,
+
+    /**
+     * This stream can be used for input and output. Typically, the stream is
+     * used as an output stream, but occasionally one already-filled buffer may
+     * be sent back to the HAL device for reprocessing.
+     *
+     * This kind of stream is generally meant for Zero Shutter Lag (ZSL)
+     * features, where copying the captured image from the output buffer to the
+     * reprocessing input buffer would be expensive.
+     *
+     * Note that the HAL will always be reprocessing data it produced.
+     *
+     */
+    CAMERA_STREAM_BIDIRECTIONAL = 2,
+
+    /**
+     * Total number of framework-defined stream types
+     */
+    CAMERA_NUM_STREAM_TYPES
+
+} camera_stream_type_t;
+
+/**
+ * camera_stream_usage_t:
+ *
+ * The type of the camera stream, which defines whether the camera HAL device
+ * is the producer or the consumer for that stream, and how the buffers of that
+ * stream relate to the other streams.
+ */
+typedef enum {
+    /**
+     * This stream is an output stream for preview;
+     */
+    CAMERA_STREAM_PREVIEW = 0,
+
+    /**
+     * This stream is an output stream for VIDEO CAPTURE;
+     */
+    CAMERA_STREAM_VIDEO_CAPTURE,
+
+    /**
+     * This stream is an output stream for STILL IMAGE CAPTURE;
+     */
+    CAMERA_STREAM_STILL_CAPTURE,
+
+    /**
+     * This stream is an output stream for Application processing which is accessed by CPU;
+     */
+    CAMERA_STREAM_APP,
+
+    /**
+     * This stream is an output stream for Opaque RAW reprocess.
+     */
+    CAMERA_STREAM_OPAQUE_RAW,
+} camera_stream_usage_t;
+
+/**
+ * camera_stream_configuration_mode_t:
+ *
+ * This defines the general operation mode for the HAL (for a given stream configuration), where
+ * modes besides NORMAL have different semantics, and usually the generality of the APIs are
+ * limited in exchange for higher performance in some particular area.
+ */
+typedef enum {
+    /**
+     * Normal stream configuration operation mode.
+     * This is the default camera operation mode, where all semantics of HAL APIs and metadata
+     * controls apply.
+     */
+    CAMERA_STREAM_CONFIGURATION_MODE_NORMAL = 0,
+
+    /**
+     * CONSTRAINED_HIGH_SPEED is the special constrained high speed operation mode for devices
+     * that do not support high speed output in NORMAL mode.
+     * To support this configuration mode, camera.control.availableHighSpeedVideoConfigurations
+     * should be implemented and CONSTRAINED_HIGH_SPEED should be reported in
+     * camera.request.availableCapabilities.
+     * All streams in this configuration mode operate at high speed mode and have different
+     * characteristics and limitations to achieve high speed output. The NORMAL mode can also be
+     * used for high speed output, if the HAL supports high speed output while satisfying all the
+     * semantics of HAL APIs and metadata controls. It is recommended for the HAL to support high
+     * speed output in NORMAL mode (by advertising the high speed FPS ranges in
+     * camera.control.aeAvailableTargetFpsRanges) if possible.
+     *
+     * This mode has below limitations/requirements:
+     *
+     *   1. The HAL must support up to 2 streams with sizes reported by
+     *       camera.control.availableHighSpeedVideoConfigurations.
+     *   2. In this mode, the HAL is expected to output up to 120fps or higher. It must
+     *       support the targeted FPS range and resolution configurations reported by
+     *       camera.control.availableHighSpeedVideoConfigurations.
+     *   3. To achieve efficient high speed streaming, the HAL may have to aggregate multiple
+     *       frames together and send the batch to camera device for processing there the request
+     *       controls are same for all the frames in this batch (batch mode). The HAL must
+     *       support the max batch size. And the max batch size requirements are defined by
+     *       camera.control.availableHighSpeedVideoConfigurations.
+     *   4. The HAL will override {aeMode, awbMode, afMode} to {ON, ON, CONTINUOUS_VIDEO}.
+     *       All post-processing block mode controls must be overridden to be FAST. Therefore, no
+     *       manual control of capture and post-processing parameters is possible. All other
+     *       controls operate the same as when camera.control.mode == AUTO.
+     *       This means that all other camera.control.* fields must continue to work, such as
+     *           camera.control.aeTargetFpsRange
+     *           camera.control.aeExposureCompensation
+     *           camera.control.aeLock
+     *           camera.control.awbLock
+     *           camera.control.effectMode
+     *           camera.control.aeRegions
+     *           camera.control.afRegions
+     *           camera.control.awbRegions
+     *           camera.control.afTrigger
+     *           camera.control.aePrecaptureTrigger
+     *       Outside of camera.control.*, the following controls must work:
+     *           camera.flash.mode (TORCH mode only, automatic flash for still capture will not
+     *                                           work since aeMode is ON)
+     *           camera.lens.opticalStabilizationMode (if it is supported)
+     *           camera.scaler.cropRegion
+     *           camera.statistics.faceDetectMode (if it is supported)
+     *
+     * TODO: The high speed mode is not completely supported yet.
+     *       1) Now the HAL supports up to 60fps@1080p.
+     *       2) The static metadata camera.control.availableHighSpeedVideoConfigurations should be
+     *           implemented.
+     */
+    CAMERA_STREAM_CONFIGURATION_MODE_CONSTRAINED_HIGH_SPEED = 1,
+
+    /**
+     * CAMERA_STREAM_CONFIGURATION_MODE_AUTO is a configurable mode, but not a real
+     * mode in HAL. The user uses this mode to allow the HAL selects appropriate config mode
+     * internally, so it should NOT be regarded as specific ConfigMode, but operation mode only.
+     *
+     * TuningModes used in AUTO mode depends on ConfigMode the HAL selects.
+     */
+    CAMERA_STREAM_CONFIGURATION_MODE_AUTO,
+    /**
+     * CAMERA_STREAM_CONFIGURATION_MODE_HDR is used to select PSYS pipeline,
+     * TuningMode and MediaCtlConfig HDR pipe.
+     */
+    CAMERA_STREAM_CONFIGURATION_MODE_HDR,
+    /**
+     * CAMERA_STREAM_CONFIGURATION_MODE_ULL is used to select PSYS pipeline,
+     * TuningMode and MediaCtlConfig ULL pipe.
+     */
+    CAMERA_STREAM_CONFIGURATION_MODE_ULL,
+    /**
+     * CAMERA_STREAM_CONFIGURATION_MODE_HLC is used to select PSYS pipeline,
+     * TuningMode and MediaCtlConfig HLC pipe.
+     */
+    CAMERA_STREAM_CONFIGURATION_MODE_HLC,
+    /**
+     * CAMERA_STREAM_CONFIGURATION_MODE_CUSTOM_AIC is used to select PSYS pipeline,
+     * TuningMode and MediaCtlConfig CUSTOM_AIC pipe.
+     */
+    CAMERA_STREAM_CONFIGURATION_MODE_CUSTOM_AIC,
+
+    /**
+     * CAMERA_STREAM_CONFIGURATION_MODE_VIDEO_LL is used to select PSYS pipeline,
+     * TuningMode and MediaCtlConfig Video LL pipe.
+     */
+    CAMERA_STREAM_CONFIGURATION_MODE_VIDEO_LL,
+
+    /**
+     * CAMERA_STREAM_CONFIGURATION_MODE_STILL_CAPTURE is used to select PSYS pipeline,
+     * Create only still pipe
+     * TuningMode and MediaCtlConfig still pipe.
+     */
+    CAMERA_STREAM_CONFIGURATION_MODE_STILL_CAPTURE,
+
+    /**
+     * CAMERA_STREAM_CONFIGURATION_MODE_HDR2 is used to select PSYS pipeline,
+     * TuningMode and MediaCtlConfig HDR2 pipe.
+     */
+    CAMERA_STREAM_CONFIGURATION_MODE_HDR2,
+
+    CAMERA_STREAM_CONFIGURATION_MODE_END
+} camera_stream_configuration_mode_t;
+
+/***************End of Camera Basic Data Structure ****************************/
+
+/*******************Start of Camera Parameters Definition**********************/
+/**
+ * \enum camera_features: camera supportted features.
+ */
+typedef enum {
+    MANUAL_EXPOSURE,       /**< Allow user to controll exposure time and ISO manually */
+    MANUAL_WHITE_BALANCE,  /**< Allow user to controll AWB mode, cct range, and gain */
+    IMAGE_ENHANCEMENT,     /**< Sharpness, Brightness, Contrast, Hue, Saturation */
+    NOISE_REDUCTION,       /**< Allow user to control NR mode and NR level */
+    SCENE_MODE,            /**< Allow user to control scene mode */
+    WEIGHT_GRID_MODE,      /**< Allow user to control custom weight grid mode */
+    PER_FRAME_CONTROL,     /**< Allow user to control most of parameters for each frame */
+    ISP_CONTROL,           /**< Allow user to control low level ISP features */
+    INVALID_FEATURE
+} camera_features;
+typedef std::vector<camera_features> camera_features_list_t;
+
+/**
+ * \struct camera_range_t: Used to specify the range info for something like FPS.
+ */
+typedef struct {
+    float min;
+    float max;
+} camera_range_t;
+typedef std::vector<camera_range_t> camera_range_array_t;
+
+/**
+ * \enum camera_ae_mode_t: Used to control how AE works.
+ */
+typedef enum {
+    AE_MODE_AUTO,   /**< */
+    AE_MODE_MANUAL, /**< */
+    AE_MODE_MAX     /**< Invalid AE mode, any new mode should be added before this */
+} camera_ae_mode_t;
+
+typedef enum {
+    AE_STATE_NOT_CONVERGED,
+    AE_STATE_CONVERGED
+} camera_ae_state_t;
+
+/**
+ * \enum camera_antibanding_mode_t: Used to control antibanding mode.
+ */
+typedef enum {
+    ANTIBANDING_MODE_AUTO, /**< Auto detect the flicker frequency. */
+    ANTIBANDING_MODE_50HZ, /**< Specify the flicker frequency to 50Hz. */
+    ANTIBANDING_MODE_60HZ, /**< Specify the flicker frequency to 60Hz. */
+    ANTIBANDING_MODE_OFF,  /**< Do not try to remove the flicker. */
+} camera_antibanding_mode_t;
+
+/**
+ * \enum camera_scene_mode_t: Used to control scene mode.
+ *
+ * Different scene mode may have different WB effects or different exposure behavior.
+ */
+typedef enum {
+    SCENE_MODE_AUTO,
+    SCENE_MODE_HDR,
+    SCENE_MODE_ULL,
+    SCENE_MODE_HLC,
+    SCENE_MODE_NORMAL,
+    SCENE_MODE_CUSTOM_AIC,
+    SCENE_MODE_VIDEO_LL,
+    SCENE_MODE_STILL_CAPTURE,
+    SCENE_MODE_HDR2,
+    SCENE_MODE_MAX
+} camera_scene_mode_t;
+
+/**
+ * \struct camera_ae_exposure_time_range_t: Provide supported exposure time range info per scene mode.
+ */
+typedef struct {
+    camera_scene_mode_t scene_mode;
+    camera_range_t et_range; /**< The exposure time range whose unit is us. */
+} camera_ae_exposure_time_range_t;
+
+/**
+ * \struct camera_ae_gain_range_t: Provide supported gain range info per scene mode.
+ */
+typedef struct {
+    camera_scene_mode_t scene_mode;
+    camera_range_t gain_range; /**< The available sensor gain range whose unit is db. */
+} camera_ae_gain_range_t;
+
+/*
+ * \enum camera_weight_grid_mode_t: Use to select which customized weight grid should be used.
+ */
+typedef enum {
+    WEIGHT_GRID_AUTO,
+    CUSTOM_WEIGHT_GRID_1,
+    CUSTOM_WEIGHT_GRID_2,
+    CUSTOM_WEIGHT_GRID_3,
+    CUSTOM_WEIGHT_GRID_4,
+    CUSTOM_WEIGHT_GRID_5,
+    CUSTOM_WEIGHT_GRID_6,
+    CUSTOM_WEIGHT_GRID_7,
+    CUSTOM_WEIGHT_GRID_8,
+    CUSTOM_WEIGHT_GRID_9,
+    CUSTOM_WEIGHT_GRID_10,
+    CUSTOM_WEIGHT_GRID_MAX
+} camera_weight_grid_mode_t;
+
+/**
+ * \enum camera_yuv_color_range_mode_t: Specify which YUV color range will be used.
+ */
+typedef enum {
+    CAMERA_FULL_MODE_YUV_COLOR_RANGE,       /*!< Full range (0 - 255) YUV data. */
+    CAMERA_REDUCED_MODE_YUV_COLOR_RANGE     /*!< Reduced range aka. BT.601 (16-235) YUV data range. */
+} camera_yuv_color_range_mode_t;
+
+/**
+ * \enum camera_awb_mode_t: Used to control AWB working mode.
+ */
+typedef enum {
+    AWB_MODE_AUTO,
+    AWB_MODE_INCANDESCENT,
+    AWB_MODE_FLUORESCENT,
+    AWB_MODE_DAYLIGHT,
+    AWB_MODE_FULL_OVERCAST,
+    AWB_MODE_PARTLY_OVERCAST,
+    AWB_MODE_SUNSET,
+    AWB_MODE_VIDEO_CONFERENCE,
+    AWB_MODE_MANUAL_CCT_RANGE,
+    AWB_MODE_MANUAL_WHITE_POINT,
+    AWB_MODE_MANUAL_GAIN,
+    AWB_MODE_MANUAL_COLOR_TRANSFORM,
+    AWB_MODE_MAX
+} camera_awb_mode_t;
+
+typedef enum {
+    AWB_STATE_NOT_CONVERGED,
+    AWB_STATE_CONVERGED
+} camera_awb_state_t;
+
+/**
+ * \enum camera_af_mode_t: Used to control af working mode.
+ *
+ * OFF:
+ * Af algo is disabled, len position is controlled by application if supported.
+ *
+ * AUTO:
+ * In this mode, the lens does not move unless the af trigger is activated.
+ * The af algo will update af state every frame, and lock lens position when action is
+ * completed.
+ * The af trigger can be activated repeatedly.
+ * Cancelling af trigger resets the lens position to default.
+ *
+ * MACRO:
+ * Similar to AUTO and focus on objects very close to the camera.
+ *
+ * CONTINUOUS_VIDEO:
+ * In this mode, the af algo modifies the lens position continually to
+ * attempt to provide a constantly-in-focus image stream.
+ * When the af trigger is activated,  af algo locks the lens position
+ * until a cancel AF trigger is received.
+ *
+ * CONTINUOUS_PICTURE:
+ * Similar to CONTINUOUS_VIDEO, except:
+ * When the af trigger is activated, af algo can finish the current scan
+ * before locking the lens position.
+ *
+ * Please refer to camera_af_trigger_t about how to trigger auto focus.
+ * Please refer to camera_af_state_t about how to get autofocus result.
+ */
+typedef enum {
+    AF_MODE_OFF,
+    AF_MODE_AUTO,
+    AF_MODE_MACRO,
+    AF_MODE_CONTINUOUS_VIDEO,
+    AF_MODE_CONTINUOUS_PICTURE,
+    AF_MODE_MAX,
+} camera_af_mode_t;
+
+/**
+ * \enum camera_af_trigger_t: Used trigger/cancel autofocus
+ *
+ * When af algo is enabled and it is changed to START, the HAL will
+ * trigger autofocus.
+ * When it is changed to CANCEL, the HAL will cancel any active trigger.
+ *
+ * Generally, applications should set it to START or CANCEL for only a
+ * single frame capture, and then return it to IDLE, to get ready for
+ * the next action.
+ */
+typedef enum {
+    AF_TRIGGER_IDLE,
+    AF_TRIGGER_START,
+    AF_TRIGGER_CANCEL,
+} camera_af_trigger_t;
+
+/**
+ * \enum camera_af_state_t: Used to return af state.
+ */
+typedef enum {
+    AF_STATE_IDLE,               /*!< Focus is idle */
+    AF_STATE_LOCAL_SEARCH,       /*!< Focus is in local search state */
+    AF_STATE_EXTENDED_SEARCH,    /*!< Focus is in extended search state */
+    AF_STATE_SUCCESS,            /*!< Focus has succeeded */
+    AF_STATE_FAIL                /*!< Focus has failed */
+} camera_af_state_t;
+
+/**
+ * \enum camera_awb_mode_t: Used to control which preset effect will be used.
+ */
+typedef enum {
+    CAM_EFFECT_NONE = 0,
+    CAM_EFFECT_MONO,
+    CAM_EFFECT_SEPIA,
+    CAM_EFFECT_NEGATIVE,
+    CAM_EFFECT_SKY_BLUE,
+    CAM_EFFECT_GRASS_GREEN,
+    CAM_EFFECT_SKIN_WHITEN_LOW,
+    CAM_EFFECT_SKIN_WHITEN,
+    CAM_EFFECT_SKIN_WHITEN_HIGH,
+    CAM_EFFECT_VIVID,
+} camera_effect_mode_t;
+
+/**
+ * \enum camera_test_pattern_mode_t: Use to control test pattern mode.
+ */
+typedef enum {
+    TEST_PATTERN_OFF = 0,
+    SOLID_COLOR,
+    COLOR_BARS,
+    COLOR_BARS_FADE_TO_GRAY,
+    PN9,
+    TEST_PATTERN_CUSTOM1,
+} camera_test_pattern_mode_t;
+
+/**
+ * \enum camera_tonemap_mode_t: Use to control tonemap mode.
+ */
+typedef enum {
+    TONEMAP_MODE_CONTRAST_CURVE,
+    TONEMAP_MODE_FAST,
+    TONEMAP_MODE_HIGH_QUALITY,
+    TONEMAP_MODE_GAMMA_VALUE,
+    TONEMAP_MODE_PRESET_CURVE,
+} camera_tonemap_mode_t;
+
+/**
+ * \enum camera_tonemap_preset_curve_t: Use to control preset curve type.
+ */
+typedef enum {
+    TONEMAP_PRESET_CURVE_SRGB,
+    TONEMAP_PRESET_CURVE_REC709,
+} camera_tonemap_preset_curve_t;
+
+typedef struct {
+    int32_t rSize;
+    int32_t bSize;
+    int32_t gSize;
+    const float* rCurve;
+    const float* bCurve;
+    const float* gCurve;
+} camera_tonemap_curves_t;
+
+/**
+ * \enum camera_msg_type_t: Use to indicate the type of message sent.
+ */
+typedef enum {
+    CAMERA_EVENT_NONE = 0,
+    CAMERA_ISP_BUF_READY,
+    CAMERA_DEVICE_ERROR,
+    CAMERA_IPC_ERROR,
+} camera_msg_type_t;
+
+/**
+ * \struct Sensor RAW data info for ZSL.
+ */
+typedef struct {
+    long sequence;
+    uint64_t timestamp;
+} sensor_raw_info_t;
+
+/**
+ * \struct isp_buffer_ready_t: Use to send isp buffer ready event data.
+ */
+typedef struct {
+    uint32_t frameNumber;
+    uint64_t timestamp;
+    long sequence;
+} isp_buffer_ready_t;
+
+/**
+ * \struct camera_msg_data_t: Use to specify msg data.
+ */
+typedef struct {
+    camera_msg_type_t type;
+    union {
+        isp_buffer_ready_t buffer_ready;
+    } data;
+} camera_msg_data_t;
+
+/**
+ * \struct camera_callback_ops_t
+ */
+typedef struct camera_callback_ops {
+    void (*notify)(const camera_callback_ops* cb, const camera_msg_data_t &data);
+} camera_callback_ops_t;
+
+/**
+ * \struct camera_awb_gains_t: Used to specify AWB gain and AWB gain shift.
+ */
+typedef struct {
+    int r_gain;
+    int g_gain;
+    int b_gain;
+} camera_awb_gains_t;
+
+/*!< camera_crop_region_t: Set crop region related parameters*/
+typedef struct {
+    int flag;
+    int x;
+    int y;
+} camera_crop_region_t;
+
+/**
+ * \struct camera_color_transform_t: Specify the color transform matrix.
+ */
+typedef struct {
+    float color_transform[3][3];
+} camera_color_transform_t;
+
+/**
+ * \struct camera_color_gains_t: Specify the color correction gains.
+ */
+typedef struct {
+    float color_gains_rggb[4];
+} camera_color_gains_t;
+
+/**
+ * \enum camera_nr_mode_t: Specify the noise reduction mode.
+ */
+typedef enum {
+    NR_MODE_OFF,
+    NR_MODE_AUTO,
+    NR_MODE_MANUAL_NORMAL,
+    NR_MODE_MANUAL_EXPERT,
+} camera_nr_mode_t;
+
+/**
+ * \struct camera_nr_level_t: Specify the noise reduction level.
+ */
+typedef struct {
+    int overall;
+    int spatial;
+    int temporal;
+} camera_nr_level_t;
+
+/**
+ * \enum camera_iris_mode_t: Specify the IRIS mode.
+ */
+typedef enum {
+    IRIS_MODE_AUTO,
+    IRIS_MODE_MANUAL,
+    IRIS_MODE_CUSTOMIZED,
+} camera_iris_mode_t;
+
+/**
+ * \enum camera_wdr_mode_t: Specify the WDR/HDR mode. (deprecated)
+ */
+typedef enum {
+    WDR_MODE_AUTO,
+    WDR_MODE_ON,
+    WDR_MODE_OFF,
+} camera_wdr_mode_t;
+
+/**
+ * \enum camera_blc_area_mode_t: Switch black area mode.
+ */
+typedef enum {
+    BLC_AREA_MODE_OFF,
+    BLC_AREA_MODE_ON,
+} camera_blc_area_mode_t;
+
+/**
+ * \struct camera_window_t: Used to specify AE/AWB weighted regions.
+ */
+typedef struct {
+    int left;
+    int top;
+    int right;
+    int bottom;
+    int weight;
+} camera_window_t;
+typedef std::vector<camera_window_t> camera_window_list_t;
+
+/**
+ * \struct camera_image_enhancement_t: Used to specify the image enhancement effect.
+ */
+typedef struct {
+    int sharpness;
+    int brightness;
+    int contrast;
+    int hue;
+    int saturation;
+} camera_image_enhancement_t;
+
+/**
+ * \struct camera_coordinate_t: The coordinate of a point in a specified coordinate system.
+ */
+typedef struct {
+    int x;
+    int y;
+} camera_coordinate_t;
+
+/**
+ * \struct camera_coordinate_system_t: Used to specify the coordinate system.
+ */
+typedef struct {
+    int left;   /*!< Left coordinate value in the coordinate system. */
+    int top;    /*!< Top coordinate value in the coordinate system. */
+    int right;  /*!< Right coordinate value in the coordinate system. */
+    int bottom; /*!< Bottom coordinate value in the coordinate system. */
+} camera_coordinate_system_t;
+
+/**
+ * \struct camera_rational_t: Used to present a rational.
+ */
+typedef struct {
+    int numerator;
+    int denominator;
+} camera_rational_t;
+
+/**
+ * \struct camera_awb_result_t: Present AWB result.
+ */
+typedef struct {
+    float r_per_g; /*!< Accurate White Point (R) for the image: relative value*/
+    float b_per_g; /*!< Accurate White Point (B) for the image. relative value*/
+} camera_awb_result_t;
+
+/**
+ * \enum camera_converge_speed_t: Used to control AE/AWB converge speed.
+ */
+typedef enum {
+    CONVERGE_NORMAL,
+    CONVERGE_MID,
+    CONVERGE_LOW,
+    CONVERGE_MAX
+} camera_converge_speed_t;
+
+/**
+ * \enum camera_converge_speed_mode_t: Used to control AE/AWB converge speed mode.
+ */
+typedef enum {
+    CONVERGE_SPEED_MODE_AIQ, /*!< Use AIQ Aglo to control converge speed. */
+    CONVERGE_SPEED_MODE_HAL  /*!< Implement converge speed control in HAL. */
+} camera_converge_speed_mode_t;
+
+/**
+ * \enum camera_ae_distribution_priority_t: Used to control exposure priority mode.
+ */
+typedef enum {
+    DISTRIBUTION_AUTO,    /*!< The AIQ Aglo decides completely */
+    DISTRIBUTION_SHUTTER, /*!< Shutter speed priority mode */
+    DISTRIBUTION_ISO,     /*!< ISO priority mode */
+    DISTRIBUTION_APERTURE /*!< Aperture priority mode */
+} camera_ae_distribution_priority_t;
+
+/**
+ * \enum camera_deinterlace_mode_t: Used to control the deinterlace mode.
+ */
+typedef enum {
+    DEINTERLACE_OFF,    /*!< Do not do any deinterlace */
+    DEINTERLACE_WEAVING /*!< Weave the two frame buffers into one. */
+} camera_deinterlace_mode_t;
+
+/**
+ * \enum camera_fisheye_dewarping_mode_t: Used to control the dewarping mode.
+ */
+typedef enum {
+    FISHEYE_DEWARPING_OFF,
+    FISHEYE_DEWARPING_REARVIEW,
+    FISHEYE_DEWARPING_HITCHVIEW
+} camera_fisheye_dewarping_mode_t;
+
+/**
+ * \enum camera_makernote_mode_t: Used to control makernote mode.
+ */
+typedef enum {
+    MAKERNOTE_MODE_OFF,
+    MAKERNOTE_MODE_JPEG,
+    MAKERNOTE_MODE_RAW
+} camera_makernote_mode_t;
+
+/**
+ * \enum camera_ldc_mode_t: Used to toggle lens distortion correction.
+ */
+typedef enum {
+    LDC_MODE_OFF,
+    LDC_MODE_ON
+} camera_ldc_mode_t;
+
+/**
+ * \enum camera_rsc_mode_t: Used to toggle rolling shutter correction.
+ */
+typedef enum {
+    RSC_MODE_OFF,
+    RSC_MODE_ON
+} camera_rsc_mode_t;
+
+/**
+ * \enum camera_flip_mode_t: Used to set output slip.
+ */
+typedef enum {
+    FLIP_MODE_NONE = 0,
+    FLIP_MODE_VFLIP,
+    FLIP_MODE_HFLIP,
+    FLIP_MODE_VHFLIP
+} camera_flip_mode_t;
+
+/**
+ * \enum camera_mono_downscale_mode_t: Used to enable/disable MONO Downscale.
+ */
+typedef enum {
+    MONO_DS_MODE_OFF,
+    MONO_DS_MODE_ON
+} camera_mono_downscale_mode_t;
+
+/**
+ * \enum camera_video_stabilization_mode_t: Used to control the video stabiliztion mode.
+ */
+typedef enum {
+    VIDEO_STABILIZATION_MODE_OFF,
+    VIDEO_STABILIZATION_MODE_ON
+} camera_video_stabilization_mode_t;
+typedef std::vector<camera_video_stabilization_mode_t> camera_video_stabilization_list_t;
+
+/**
+ * \enum camera_mount_type_t: camera mount type
+ */
+typedef enum {
+    WALL_MOUNTED,
+    CEILING_MOUNTED,
+} camera_mount_type_t;
+
+/**
+* \enum camera_shading_mode_t: camera shading mode type
+*/
+typedef enum {
+    SHADING_MODE_OFF,
+    SHADING_MODE_FAST,
+    SHADING_MODE_HIGH_QUALITY
+} camera_shading_mode_t;
+
+/**
+* \enum camera_lens_shading_map_mode_type_t: camera lens shading map mode type
+*/
+typedef enum {
+    LENS_SHADING_MAP_MODE_OFF,
+    LENS_SHADING_MAP_MODE_ON
+} camera_lens_shading_map_mode_type_t;
+
+/**
+ * \class Parameters
+ *
+ * \brief
+ *   Manage parameter's data structure, and provide set and get parameters
+ *
+ * This class provides a thread safe management to internal parameter's data
+ * structure, and helps client to easily set parameters to and get parameters
+ * from camera device.
+ *
+ * \version 0.1
+ *
+ */
+class Parameters {
+public:
+    Parameters();
+    Parameters(const Parameters& other);
+    Parameters& operator=(const Parameters& other);
+    ~Parameters();
+    /**
+     * \brief Merge and update current parameter with other
+     *
+     * \param[in] Parameters other: parameter source
+     *
+     * \return void
+     */
+    void merge(const Parameters& other);
+
+    // Belows are camera capability related parameters operations
+    /**
+     * \brief Get supported fps range list
+     *
+     * \param[out] camera_range_array_t& ranges
+     *
+     * \return 0 if fps range supported, otherwise non-0 value is returned.
+     */
+    int getSupportedFpsRange(camera_range_array_t& ranges) const;
+
+    /**
+     * \brief Get supported Stream Config list
+     *
+     * \param[out] stream_array_t& config
+     *
+     * \return 0 if Stream Configs supported, otherwise non-0 value is returned.
+     */
+    int getSupportedStreamConfig(stream_array_t& config) const;
+
+    // Belows are camera capability related parameters operations
+    /**
+     * \brief Get supported sensor exposure time range (microsecond)
+     *
+     * \param[out] camera_range_t& range
+     *
+     * \return 0 if it is supported, otherwise non-0 value is returned.
+     */
+    int getSupportedSensorExposureTimeRange(camera_range_t& range) const;
+
+    // Belows are camera capability related parameters operations
+    /**
+     * \brief Get supported sensor sensitivity time range
+     *
+     * \param[out] camera_range_t& range
+     *
+     * \return 0 if it is supported, otherwise non-0 value is returned.
+     */
+    int getSupportedSensorSensitivityRange(camera_range_t& range) const;
+
+    /**
+     * \brief Get supported feature list.
+     *
+     * Camera application MUST check if the feature is supported before trying to enable it.
+     * Otherwise the behavior is undefined currently, HAL may just ignore the request.
+     *
+     * \param[out] camera_features_list_t& features: All supported feature will be filled in "features"
+     *
+     * \return: If no feature supported, features will be empty
+     */
+    int getSupportedFeatures(camera_features_list_t& features) const;
+
+    /**
+     * \brief Get ae compensation range supported by camera device
+     *
+     * \param[out] camera_range_t& evRange
+     *
+     * \return 0 if ae compensation supported, non-0 or evRange equals [0, 0] means ae compensation not supported.
+     */
+    int getAeCompensationRange(camera_range_t& evRange) const;
+
+    /**
+     * \brief Get ae compensation step supported by camera device
+     *
+     * Smallest step by which the exposure compensation can be changed.
+     * This is the unit for setAeCompensation. For example, if this key has
+     * a value of `1/2`, then a setting of `-2` for setAeCompensation means
+     * that the target EV offset for the auto-exposure routine is -1 EV.
+     *
+     * One unit of EV compensation changes the brightness of the captured image by a factor
+     * of two. +1 EV doubles the image brightness, while -1 EV halves the image brightness.
+     *
+     * \param[out] camera_rational_t& evStep
+     *
+     * \return 0 if ae compensation supported, non-0 means ae compensation not supported.
+     */
+    int getAeCompensationStep(camera_rational_t& evStep) const;
+
+    /**
+     * \brief Get supported manual exposure time range
+     *
+     * Different sensors or same sensor in different settings may have different supported exposure
+     * time range, so camera application needs to use this API to check if the user's settings is
+     * in the supported range, if application pass an out of exposure time, HAL will clip it
+     * according to this supported range.
+     *
+     * \param[out] vector<camera_ae_exposure_time_range_t>& etRanges
+     *
+     * \return 0 if exposure time range is filled by HAL.
+     */
+    int getSupportedAeExposureTimeRange(std::vector<camera_ae_exposure_time_range_t>& etRanges) const;
+
+    /**
+     * \brief Get supported manual sensor gain range
+     *
+     * Different sensors or same sensor in different settings may have different supported sensor
+     * gain range, so camera application needs to use this API to check if the user's settings is
+     * in the supported range, if application pass an out of range gain, HAL will clip it according
+     * to this supported range.
+     *
+     * \param[out] vector<camera_ae_gain_range_t>& gainRanges
+     *
+     * \return 0 if exposure time range is filled by HAL.
+     */
+    int getSupportedAeGainRange(std::vector<camera_ae_gain_range_t>& gainRanges) const;
+
+    // Belows are AE related parameters operations
+    /**
+     * \brief Set exposure mode(auto/manual).
+     *
+     * "auto" means 3a algorithm will control exposure time and gain automatically.
+     * "manual" means user can control exposure time or gain, or both of them.
+     * Under manual mode, if user only set one of exposure time or gain, then 3a algorithm
+     * will help to calculate the other one.
+     *
+     * \param[in] camera_ae_mode_t aeMode
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAeMode(camera_ae_mode_t aeMode);
+
+    /**
+     * \brief Get exposure mode
+     *
+     * \param[out] aeMode: Currently used ae mode will be set to aeMode if 0 is returned.
+     *
+     * \return 0 if exposure mode was set, otherwise non-0 value is returned.
+     */
+    int getAeMode(camera_ae_mode_t& aeMode) const;
+
+    /**
+     * \brief Set AE state.
+     *
+     * \param[in] camera_ae_state_t aeState
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAeState(camera_ae_state_t aeState);
+
+    /**
+     * \brief Get AE state
+     *
+     * \param[out] aeState: Currently AE state will be set to aeState if 0 is returned.
+     *
+     * \return 0 if AE state was set, otherwise non-0 value is returned.
+     */
+    int getAeState(camera_ae_state_t& aeState) const;
+
+    /**
+     * \brief Set ae lock
+     *
+     * \param[in] bool lock
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAeLock(bool lock);
+
+    /**
+     * \brief Get ae lock
+     *
+     * \param[out] bool lock
+     *
+     * \return 0 if lock was set, otherwise non-0 value is returned.
+     */
+    int getAeLock(bool& lock) const;
+
+    /**
+     * \brief Get supported video stabilization mode
+     *
+     * Camera application MUST check if the video stabilization mode is supported before trying
+     * to enable it. Otherwise one error occuring, HAL may just ignore the request.
+     *
+     * \param[out] supportedModes: All supported video stabilization mode will be filled in "supportedModes"
+     *
+     * \return: If no mode supported, supportedModes will be empty
+     */
+    int getSupportedVideoStabilizationMode(camera_video_stabilization_list_t &supportedModes) const;
+
+    /**
+     * \brief Get supported ae mode
+     *
+     * Camera application MUST check if the ae mode is supported before trying to enable it.
+     * Otherwise one error occuring, HAL may just ignore the request.
+     *
+     * \param[out] supportedAeModes: All supported ae mode will be filled in "supportedAeModes"
+     *
+     * \return: If no ae mode supported, supportedAeModes will be empty
+     */
+    int getSupportedAeMode(std::vector<camera_ae_mode_t> &supportedAeModes) const;
+
+    /**
+     * \brief Get supported awb mode
+     *
+     * Camera application MUST check if the awb mode is supported before trying to enable it.
+     * Otherwise one error occuring, HAL may just ignore the request.
+     *
+     * \param[out] supportedAwbModes: All supported awb mode will be filled in "supportedAwbModes"
+     *
+     * \return: If no awb mode supported, supportedAwbModes will be empty
+     */
+    int getSupportedAwbMode(std::vector<camera_awb_mode_t> &supportedAwbModes) const;
+
+    /**
+     * \brief Get supported af mode
+     *
+     * Camera application MUST check if the af mode is supported before trying to enable it.
+     * Otherwise one error occuring, HAL may just ignore the request.
+     *
+     * \param[out] supportedAfModes: All supported af mode will be filled in "supportedAfModes"
+     *
+     * \return: If no af mode supported, supportedAfModes will be empty
+     */
+    int getSupportedAfMode(std::vector<camera_af_mode_t> &supportedAfModes) const;
+
+    /**
+     * \brief Get supported scene mode
+     *
+     * Camera application MUST check if the scene mode is supported before trying to enable it.
+     * Otherwise one error occuring, HAL may just ignore the request.
+     *
+     * \param[out] supportedSceneModes: All supported scene mode will be filled in "supportedSceneModes"
+     *
+     * \return: If no scene mode supported, supportedSceneModes will be empty
+     */
+    int getSupportedSceneMode(std::vector<camera_scene_mode_t> &supportedSceneModes) const;
+
+    /**
+     * \brief Get supported antibanding mode
+     *
+     * Camera application MUST check if the antibanding mode is supported before trying to enable it.
+     * Otherwise one error occuring, HAL may just ignore the request.
+     *
+     * \param[out] supportedAntibindingModes: All supported scene mode will be filled in "supportedAntibindingModes"
+     *
+     * \return: If no antibanding mode supported, supportedAntibindingModes will be empty
+     */
+    int getSupportedAntibandingMode(std::vector<camera_antibanding_mode_t> &supportedAntibindingModes) const;
+
+    /**
+     * \brief Get if ae lock is available
+     *
+     * Camera application MUST check if ae lock is supported before trying to lock it.
+     * Otherwise one error occuring, HAL may just ignore the request.
+     *
+     * \return: true if lock is supported, false if not
+     */
+    bool getAeLockAvailable() const;
+
+    /**
+     * \brief Get if awb lock is available
+     *
+     * Camera application MUST check if awb lock is supported before trying to lock it.
+     * Otherwise one error occuring, HAL may just ignore the request.
+     *
+     * \return: true if lock is supported, false if not
+     */
+    bool getAwbLockAvailable() const;
+
+    /**
+     * \brief Set AE region
+     *
+     * Current only fisrt region can take effect when BLC mode is BLC_AREA_MODE_ON;
+     * if BLC_AREA_MODE_OFF, AE region function will be diabled.
+     *
+     * \param[in] camera_window_list_t aeRegions
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAeRegions(camera_window_list_t aeRegions);
+
+    /**
+     * \brief Get AE region
+     *
+     * \param[out] camera_window_list_t aeRegions
+     *
+     * \return 0 if aeRegions were set, otherwise non-0 value is returned.
+     */
+    int getAeRegions(camera_window_list_t& aeRegions) const;
+
+    /**
+     * \brief Set exposure time whose unit is microsecond(us).
+     *
+     * The exposure time only take effect when ae mode set to manual.
+     *
+     * \param[in] int64_t exposureTime
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setExposureTime(int64_t exposureTime);
+
+    /**
+     * \brief Get exposure time whose unit is microsecond(us).
+     *
+     * \param[out] int64_t& exposureTime: exposure time if be set in exposureTime if 0 is returned.
+     *
+     * \return 0 if exposure time was set, non-0 means no exposure time was set.
+     */
+    int getExposureTime(int64_t& exposureTime) const;
+
+    /**
+     * \brief Set sensor gain whose unit is db.
+     * The sensor gain only take effect when ae mode set to manual.
+     *
+     * \param[in] float gain
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setSensitivityGain(float gain);
+
+    /**
+     * \brief Get sensor gain whose unit is db.
+     *
+     * \param[out] float gain
+     *
+     * \return 0 if sensor gain was set, non-0 means no sensor gain was set.
+     */
+    int getSensitivityGain(float& gain) const;
+
+    /**
+     * \brief Set sensor ISO, will overwrite value of setSensitivityGain.
+     * The sensor ISO only take effect when ae mode set to manual.
+     *
+     * \param[in] int32 iso
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setSensitivityIso(int32_t iso);
+
+    /**
+     * \brief Get sensor iso.
+     *
+     * \param[out] int32 iso
+     *
+     * \return 0 if sensor iso was set, non-0 means no sensor iso was set.
+     */
+    int getSensitivityIso(int& iso) const;
+
+    /**
+     * \brief Set ae compensation whose unit is compensation step.
+     *
+     * The adjustment is measured as a count of steps, with the
+     * step size defined ae compensation step and the allowed range by ae compensation range.
+     *
+     * For example, if the exposure value (EV) step is 0.333, '6'
+     * will mean an exposure compensation of +2 EV; -3 will mean an
+     * exposure compensation of -1 EV. One EV represents a doubling of image brightness.
+     *
+     * In the event of exposure compensation value being changed, camera device
+     * may take several frames to reach the newly requested exposure target.
+     *
+     * \param[in] int ev
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAeCompensation(int ev);
+
+    /**
+     * \brief Get ae compensation whose unit is compensation step.
+     *
+     * \param[out] int ev
+     *
+     * \return 0 if ae compensation was set, non-0 means no ae compensation was set.
+     */
+    int getAeCompensation(int& ev) const;
+
+    /**
+     * \brief Set frame rate
+     *
+     * \param[in] float fps
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setFrameRate(float fps);
+
+    /**
+     * \brief Get frame rate
+     *
+     * \param[out] float& fps
+     *
+     * \return 0 if frame rate was set, otherwise non-0 value is returned.
+     */
+    int getFrameRate(float& fps) const;
+
+    /**
+     * \brief Set antibanding mode
+     *
+     * \param[in] camera_antibanding_mode_t bandingMode
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAntiBandingMode(camera_antibanding_mode_t bandingMode);
+
+    /**
+     * \brief Get antibanding mode
+     *
+     * \param[out] camera_antibanding_mode_t& bandingMode
+     *
+     * \return 0 if antibanding mode was set, otherwise non-0 value is returned.
+     */
+    int getAntiBandingMode(camera_antibanding_mode_t& bandingMode) const;
+
+    /**
+     * \brief Set AE distribution priority.
+     *
+     * \param[in] camera_ae_distribution_priority_t priority: the AE distribution priority to be set.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAeDistributionPriority(camera_ae_distribution_priority_t priority);
+
+    /**
+     * \brief Get AE distribution priority.
+     *
+     * \param[out] camera_ae_distribution_priority_t priority: the AE distribution priority.
+     *
+     * \return 0 if get successfully, otherwise non-0 value is returned.
+     */
+    int getAeDistributionPriority(camera_ae_distribution_priority_t& priority) const;
+
+    /**
+     * \brief set exposure time range
+     *
+     * \param[in] camera_range_t: the exposure time range to be set.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setExposureTimeRange(camera_range_t exposureTimeRange);
+
+    /**
+     * \brief get exposure time range
+     *
+     * \param[out] camera_range_t: the exposure time had been set.
+     *
+     * \return 0 if get successfully, otherwise non-0 value is returned.
+     */
+    int getExposureTimeRange(camera_range_t& exposureTimeRange) const;
+
+    /**
+     * \brief set sensitivity gain range
+     *
+     * \param[in] camera_range_t: the sensitivity gain range(the unit is db) to be set.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setSensitivityGainRange(camera_range_t sensitivityGainRange);
+
+    /**
+     * \brief get sensitivity gain range
+     *
+     * \param[out] camera_range_t: the sensitivity gain(the unit is db) had been set.
+     *
+     * \return 0 if get successfully, otherwise non-0 value is returned.
+     */
+    int getSensitivityGainRange(camera_range_t& sensitivityGainRange) const;
+
+    /**
+     * \brief Set weight grid mode
+     *
+     * \param[in] camera_weight_grid_mode_t weightGridMode
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setWeightGridMode(camera_weight_grid_mode_t weightGridMode);
+
+    /**
+     * \brief Get weight grid mode
+     *
+     * \param[out] camera_weight_grid_mode_t& weightGridMode
+     *
+     * \return 0 if weight grid mode was set, otherwise non-0 value is returned.
+     */
+    int getWeightGridMode(camera_weight_grid_mode_t& weightGridMode) const;
+
+    /**
+     * \brief Set BLC (backlight compensation) area mode
+     *
+     * \param[in] camera_blc_area_mode_t blcAreaMode
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setBlcAreaMode(camera_blc_area_mode_t blcAreaMode);
+
+    /**
+     * \brief Get BLC (backlight compensation) area mode
+     *
+     * \param[out] camera_blc_area_mode_t& blcAreaMode
+     *
+     * \return 0 if BLC area mode was set, otherwise non-0 value is returned.
+     */
+    int getBlcAreaMode(camera_blc_area_mode_t& blcAreaMode) const;
+
+    int setFpsRange(camera_range_t fpsRange);
+    int getFpsRange(camera_range_t& fpsRange) const;
+
+    // Belows are AWB related parameters operations
+    /**
+     * \brief Set white balance mode
+     *
+     * White balance mode could be one of totally auto, preset cct range, customized cct range, customized
+     * white area, customize gains.
+     *
+     * \param[in] camera_awb_mode_t awbMode
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAwbMode(camera_awb_mode_t awbMode);
+
+    /**
+     * \brief Get white balance mode currently used.
+     *
+     * \param[out] camera_awb_mode_t& awbMode
+     *
+     * \return 0 if awb mode was set, non-0 means no awb mode was set.
+     */
+    int getAwbMode(camera_awb_mode_t& awbMode) const;
+
+    /**
+     * \brief Set AWB state.
+     *
+     * \param[in] camera_awb_state_t awbState
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAwbState(camera_awb_state_t awbState);
+
+    /**
+     * \brief Get AWB state
+     *
+     * \param[out] awbState: Currently AWB state will be set to awbState if 0 is returned.
+     *
+     * \return 0 if AWB state was set, otherwise non-0 value is returned.
+     */
+    int getAwbState(camera_awb_state_t& awbState) const;
+
+    /**
+     * \brief Set awb lock
+     *
+     * \param[in] bool lock
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAwbLock(bool lock);
+
+    /**
+     * \brief Get awb lock
+     *
+     * \param[out] bool lock
+     *
+     * \return 0 if lock was set, otherwise non-0 value is returned.
+     */
+    int getAwbLock(bool& lock) const;
+
+    /**
+     * \brief Set customized cct range.
+     *
+     * Customized cct range only take effect when awb mode is set to AWB_MODE_MANUAL_CCT_RANGE
+     *
+     * \param[in] camera_range_t cct range, which specify min and max cct for 3a algorithm to use.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAwbCctRange(camera_range_t cct);
+
+    /**
+     * \brief Get customized cct range currently used.
+     *
+     * \param[out] camera_range_t& cct range
+     *
+     * \return 0 if cct range was set, non-0 means no cct range was set.
+     */
+    int getAwbCctRange(camera_range_t& cct) const;
+
+    /**
+     * \brief Set customized awb gains.
+     *
+     * Customized awb gains only take effect when awb mode is set to AWB_MODE_MANUAL_GAIN
+     *
+     * The range of each gain is (0, 255).
+     *
+     * \param[in] camera_awb_gains_t awb gains, which specify r,g,b gains for overriding awb result.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAwbGains(camera_awb_gains_t awbGains);
+
+    /**
+     * \brief Get customized awb gains currently used.
+     *
+     * \param[out] camera_awb_gains_t& awb gains
+     *
+     * \return 0 if awb gain was set, non-0 means no awb gain was set.
+     */
+    int getAwbGains(camera_awb_gains_t& awbGains) const;
+
+    /**
+     * \brief Set awb gain shift.
+     *
+     * Customized awb gain shift only take effect when awb mode is NOT set to AWB_MODE_MANUAL_GAIN
+     *
+     * The range of each gain shift is (0, 255).
+     *
+     * \param[in] camera_awb_gains_t awb gain shift, which specify r,g,b gains for updating awb result.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAwbGainShift(camera_awb_gains_t awbGainShift);
+
+    /**
+     * \brief Get customized awb gains shift currently used.
+     *
+     * \param[out] camera_awb_gains_t& awb gain shift
+     *
+     * \return 0 if awb gain shift was set, non-0 means no awb gain shift was set.
+     */
+    int getAwbGainShift(camera_awb_gains_t& awbGainShift) const;
+
+    /**
+     * \brief Set awb result.
+     *
+     * \param[in] data: The data used to override awb result.
+     *
+     * Note: data is allocated by the caller and NULL is for erasing the param.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAwbResult(void *data);
+
+    /**
+     * \brief Get awb result currently used.
+     *
+     * \param[out] data: the awb result pointer to user
+     *
+     * Note: data is allocated by the caller and it must never be NULL.
+     *
+     * \return 0 if get successfully, otherwise non-0 value is returned.
+     */
+    int getAwbResult(void *data) const;
+
+    /**
+     * \brief Set manual white point coordinate.
+     *
+     * Only take effect when awb mode is set to AWB_MODE_MANUAL_WHITE_POINT.
+     * The coordinate system is based on frame which is currently displayed.
+     *
+     * \param[in] camera_coordinate_t white point
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAwbWhitePoint(camera_coordinate_t whitePoint);
+
+    /**
+     * \brief Get manual white point coordinate.
+     *
+     * \param[out] camera_coordinate_t& white point
+     *
+     * \return 0 if white point was set, non-0 means no white point was set.
+     */
+    int getAwbWhitePoint(camera_coordinate_t& whitePoint) const;
+
+    /**
+     * \brief Set customized color transform which is a 3x3 matrix.
+     *
+     *  Manual color transform only takes effect when awb mode set to AWB_MODE_MANUAL_COLOR_TRANSFORM.
+     *
+     * \param[in] camera_color_transform_t colorTransform: a 3x3 matrix for color convertion.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setColorTransform(camera_color_transform_t colorTransform);
+
+    /**
+     * \brief Get color transform matrix currently used.
+     *
+     * \param[out] camera_color_transform_t& color transform matrix
+     *
+     * \return 0 if color transform matrix was set, non-0 means no color transform matrix was set.
+     */
+    int getColorTransform(camera_color_transform_t& colorTransform) const;
+
+    /**
+     * \brief Set customized color correction gains which is a 4 array.
+     *
+     *  Manual color correction gains only takes effect when awb mode set to AWB_MODE_MANUAL_COLOR_TRANSFORM.
+     *
+     * \param[in] camera_color_gains_t colorGains: a 4 array for color correction gains.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setColorGains(camera_color_gains_t colorGains);
+    /**
+     * \brief Get color correction gains currently used.
+     *
+     * \param[out] camera_color_gains_t& color correction gains
+     *
+     * \return 0 if color correction gains was set, non-0 means no color correction gains was set.
+     */
+    int getColorGains(camera_color_gains_t& colorGains) const;
+
+    int setAwbRegions(camera_window_list_t awbRegions);
+    int getAwbRegions(camera_window_list_t& awbRegions) const;
+
+    // Belows are convergence speed related parameters operations
+    /**
+     * \brief Set customized Ae converge speed.
+     *
+     * \param[in] camera_converge_speed_t speed: the converge speed to be set.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAeConvergeSpeed(camera_converge_speed_t speed);
+
+    /**
+     * \brief Get customized Ae converge speed.
+     *
+     * \param[out] camera_converge_speed_t& speed: the converge speed been set.
+     *
+     * \return 0 if get successfully, otherwise non-0 value is returned.
+     */
+    int getAeConvergeSpeed(camera_converge_speed_t& speed) const;
+
+    /**
+     * \brief Set customized Awb converge speed.
+     *
+     * \param[in] camera_converge_speed_t speed: the converge speed to be set.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAwbConvergeSpeed(camera_converge_speed_t speed);
+
+    /**
+     * \brief Get customized Awb converge speed.
+     *
+     * \param[out] camera_converge_speed_t& speed: the converge speed been set.
+     *
+     * \return 0 if get successfully, otherwise non-0 value is returned.
+     */
+    int getAwbConvergeSpeed(camera_converge_speed_t& speed) const;
+
+    /**
+     * \brief Set customized Ae converge speed mode.
+     *
+     * \param[in] camera_converge_speed_mode_t mode: the converge speed mode to be set.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAeConvergeSpeedMode(camera_converge_speed_mode_t mode);
+
+    /**
+     * \brief Get customized Ae converge speed mode.
+     *
+     * \param[out] camera_converge_speed_mode_t mode: the converge speed mode to be set.
+     *
+     * \return 0 if get successfully, otherwise non-0 value is returned.
+     */
+    int getAeConvergeSpeedMode(camera_converge_speed_mode_t& mode) const;
+
+    /**
+     * \brief Set customized Awb converge speed mode.
+     *
+     * \param[in] camera_converge_speed_mode_t mode: the converge speed mode to be set.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAwbConvergeSpeedMode(camera_converge_speed_mode_t mode);
+
+    /**
+     * \brief Get customized Awb converge speed mode.
+     *
+     * \param[out] camera_converge_speed_mode_t mode: the converge speed mode to be set.
+     *
+     * \return 0 if get successfully, otherwise non-0 value is returned.
+     */
+    int getAwbConvergeSpeedMode(camera_converge_speed_mode_t& mode) const;
+
+    // Belows are ISP related parameters operations
+    int setNrMode(camera_nr_mode_t nrMode);
+    int getNrMode(camera_nr_mode_t& nrMode) const;
+
+    int setNrLevel(camera_nr_level_t level);
+    int getNrLevel(camera_nr_level_t& level) const;
+
+    /**
+     * \brief Set YUV color range mode
+     *
+     * \param[in] camera_yuv_color_range_mode_t colorRange: the YUV color range mode to be set.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setYuvColorRangeMode(camera_yuv_color_range_mode_t colorRange);
+
+    /**
+     * \brief Get YUV color range mode
+     *
+     * \param[out] camera_yuv_color_range_mode_t colorRange: the YUV color range mode.
+     *
+     * \return 0 if get successfully, otherwise non-0 value is returned.
+     */
+    int getYuvColorRangeMode(camera_yuv_color_range_mode_t & colorRange) const;
+
+    /**
+     * \brief Set customized effects.
+     *
+     * One of sharpness, brightness, contrast, hue, saturation could be controlled by this API.
+     * Valid range should be [-128, 127]
+     *
+     * \param[in] camera_image_enhancement_t effects
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setImageEnhancement(camera_image_enhancement_t effects);
+
+    /**
+     * \brief Get customized effects.
+     *
+     * \param[out] effects
+     *
+     * \return 0 if effects was set, non-0 return value means no effects was set.
+     */
+    int getImageEnhancement(camera_image_enhancement_t& effects) const;
+
+    // Belows are other parameters operations
+    int setIrisMode(camera_iris_mode_t irisMode);
+    int getIrisMode(camera_iris_mode_t& irisMode);
+
+    int setIrisLevel(int level);
+    int getIrisLevel(int& level);
+
+    /**
+     * \brief Set WDR Level
+     *
+     * \param[in] uint8_t level
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setWdrLevel(uint8_t level);
+
+    /**
+     * \brief Get WDR level currently used.
+     *
+     * \param[out] uint8_t& level
+     *
+     * \return 0 if get WDR level, non-0 means error.
+     */
+    int getWdrLevel(uint8_t& level) const;
+
+    /**
+     * \brief Set effect scene mode
+     *
+     * \param[in] camera_scene_mode_t: scene mode
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setEffectSceneMode(camera_scene_mode_t sceneMode);
+
+    /**
+     * \brief Get effect scene mode based on runtime
+     *
+     * \param[out] camera_scene_mode_t&: scene mode
+     *
+     * \return 0 if get scene mode, non-0 means error.
+     */
+    int getEffectSceneMode(camera_scene_mode_t& sceneMode) const;
+
+    /**
+     * \brief Set scene mode
+     *
+     * \param[in] camera_scene_mode_t: scene mode
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setSceneMode(camera_scene_mode_t sceneMode);
+
+    /**
+     * \brief Get scene mode current set by user.
+     *
+     * \param[out] camera_scene_mode_t&: scene mode
+     *
+     * \return 0 if get scene mode, non-0 means error.
+     */
+    int getSceneMode(camera_scene_mode_t& sceneMode) const;
+
+    /**
+     * \brief Set deinterlace mode
+     *
+     * \param[in] camera_deinterlace_mode_t deinterlaceMode
+     *
+     * Setting deinterlace mode only takes effect before camera_device_config_streams called
+     * That's it cannot be changed after camera_device_config_streams.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setDeinterlaceMode(camera_deinterlace_mode_t deinterlaceMode);
+
+    /**
+     * \brief Get deinterlace mode
+     *
+     * \param[out] camera_deinterlace_mode_t& deinterlaceMode
+     *
+     * \return 0 if deinterlace mode was set, non-0 means no deinterlace mode was set.
+     */
+    int getDeinterlaceMode(camera_deinterlace_mode_t &deinterlaceMode) const;
+
+    /**
+     * \brief Set Makernote Data
+     *
+     * \param[in] const void* data: the pointer of data.
+     * \param[in] unsigned int size: the size of the data.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setMakernoteData(const void* data, unsigned int size);
+
+    /**
+     * \brief Get Makernote Data
+     *
+     * \param[out] void* data: the pointer of destination buffer.
+     * \param[in/out] in: the buffer size; out: the buffer used size.
+     *
+     * \return 0 if get successfully, otherwise non-0 value is returned.
+     */
+    int getMakernoteData(void* data, unsigned int* size) const;
+
+    /**
+     * \brief Set Custom Aic Param
+     *
+     * \param[in] const void* data: the pointer of data.
+     * \param[in] int length: the length of the data.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setCustomAicParam(const void* data, unsigned int length);
+
+    /**
+     * \brief Get Custom Aic Param
+     *
+     * \param[out] void* data: the pointer of destination buffer.
+     * \param[in/out] in: the buffer size; out: the buffer used size.
+     *
+     * \return 0 if get successfully, otherwise non-0 value is returned.
+     */
+    int getCustomAicParam(void* data, unsigned int* length) const;
+
+    /**
+     * \brief Set makernote mode
+     *
+     * \param[in] camera_makernote_mode_t mode
+     *
+     * \return 0 if get successfully, otherwise non-0 value is returned.
+     */
+    int setMakernoteMode(camera_makernote_mode_t mode);
+
+    /**
+     * \brief get makernote mode
+     *
+     * \param[out] camera_makernote_mode_t &mode
+     *
+     * \return 0 if makernote mode was set, otherwise return non-0 value.
+     */
+    int getMakernoteMode(camera_makernote_mode_t &mode) const;
+
+    /**
+     * \brief Set digital zoom ratio
+     *
+     * \param[in] float ratio
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setDigitalZoomRatio(float ratio);
+
+    /**
+     * \brief Get digital zoom ratio
+     *
+     * \param[out] float& ratio
+     *
+     * \return 0 if find the corresponding data, otherwise non-0 value is returned.
+     */
+    int getDigitalZoomRatio(float& ratio) const;
+
+    /**
+     * \brief Set lens distortion correction mode
+     *
+     * \param[in] camera_ldc_mode_t mode
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setLdcMode(camera_ldc_mode_t mode);
+
+    /**
+     * \brief Get lens distortion correction mode
+     *
+     * \param[out] camera_ldc_mode_t& mode
+     *
+     * \return 0 if find the corresponding data, otherwise non-0 value is returned.
+     */
+    int getLdcMode(camera_ldc_mode_t &mode) const;
+
+    /**
+     * \brief Set rolling shutter correction mode
+     *
+     * \param[in] camera_rsc_mode_t mode
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setRscMode(camera_rsc_mode_t mode);
+
+    /**
+     * \brief Get rolling shutter correction mode
+     *
+     * \param[out] camera_rsc_mode_t& mode
+     *
+     * \return 0 if find the corresponding data, otherwise non-0 value is returned.
+     */
+    int getRscMode(camera_rsc_mode_t &mode) const;
+
+    /**
+     * \brief flip mode
+     *
+     * \param[in] camera_flip_mode_t mode
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setFlipMode(camera_flip_mode_t mode);
+
+    /**
+     * \brief Get flip mode
+     *
+     * \param[out] camera_flip_mode_t& mode
+     *
+     * \return 0 if find the corresponding data, otherwise non-0 value is returned.
+     */
+    int getFlipMode(camera_flip_mode_t &mode) const;
+
+    /**
+     * \brief set frame interval to run 3A
+     *
+     * \param[in] int cadence which is frame interval
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setRun3ACadence(int cadence);
+
+    /**
+     * \brief Get frame interval to run 3A
+     *
+     * \param[out] int& cadence which is frame interval
+     *
+     * \return 0 if find the corresponding data, otherwise non-0 value is returned.
+     */
+    int getRun3ACadence(int &cadence) const;
+
+    /**
+     * \brief mono downscale mode
+     *
+     * \param[in] camera_mono_downscale_mode_t mode
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setMonoDsMode(camera_mono_downscale_mode_t mode);
+
+    /**
+     * \brief Get mono downscale mode
+     *
+     * \param[out] camera_mono_downscale_mode_t& mode
+     *
+     * \return 0 if find the corresponding data, otherwise non-0 value is returned.
+     */
+    int getMonoDsMode(camera_mono_downscale_mode_t &mode) const;
+
+    /**
+     * \brief Set Fisheye Dewarping Mode
+     *
+     * \param[in] camera_fisheye_dewarping_mode_t dewarpingMode
+     *
+     * Setting dewarping mode only takes effect before camera_device_config_streams called
+     * That's it cannot be changed after camera_device_config_streams.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setFisheyeDewarpingMode(camera_fisheye_dewarping_mode_t dewarpingMode);
+
+    /**
+     * \brief Get Fisheye Dewarping Mode
+     *
+     * \param[out] camera_fisheye_dewarping_mode_t &dewarpingMode
+     *
+     * \return 0 if dewarping mode was set, non-0 means no dewarping mode was set.
+     */
+    int getFisheyeDewarpingMode(camera_fisheye_dewarping_mode_t &dewarpingMode) const;
+
+    // Belows are Jpeg related parameters operations
+    int getJpegQuality(uint8_t *quality) const;
+    int setJpegQuality(uint8_t quality);
+
+    int getJpegThumbnailQuality(uint8_t *quality) const;
+    int setJpegThumbnailQuality(uint8_t quality);
+
+    int setJpegThumbnailSize(const camera_resolution_t& res);
+    int getJpegThumbnailSize(camera_resolution_t& res) const;
+
+    int getJpegRotation(int &rotation) const;
+    int setJpegRotation(int  rotation);
+
+    int setJpegGpsCoordinates(const double *coordinates);
+    int getJpegGpsLatitude(double &latitude) const;
+    int getJpegGpsLongitude(double &longitude) const;
+    int getJpegGpsAltitude(double &altiude) const;
+
+    int getJpegGpsTimeStamp(int64_t &timestamp) const;
+    int setJpegGpsTimeStamp(int64_t  timestamp);
+
+    int getJpegGpsProcessingMethod(int &processMethod) const;
+    int setJpegGpsProcessingMethod(int  processMethod);
+
+    int getJpegGpsProcessingMethod(int size, char* processMethod) const;
+    int setJpegGpsProcessingMethod(const char* processMethod);
+
+    int getImageEffect(camera_effect_mode_t &effect) const;
+    int setImageEffect(camera_effect_mode_t  effect);
+
+    int getVideoStabilizationMode(camera_video_stabilization_mode_t &mode) const;
+    int setVideoStabilizationMode(camera_video_stabilization_mode_t mode);
+
+    int getFocalLength(float &focal) const;
+    int setFocalLength(float focal);
+
+    /**
+     * \brief Get aperture value currently used
+     *
+     * \param[in] float& aperture
+     *
+     * \return 0 if aperture was set, non=0 means no aperture was set
+     */
+    int getAperture(float &aperture) const;
+    /**
+     * \brief Set aperture value
+     *
+     * \param[in] float aperture
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAperture(float aperture);
+
+    /**
+     * \brief Get focus distance value currently used
+     *
+     * \param[in] float& distance
+     *
+     * \return 0 if distance was set, non-0 means no focus distance was set
+     */
+    int getFocusDistance(float &distance) const;
+    /**
+     * \brief Set focus distance value
+     *
+     * \param[in] float distance
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setFocusDistance(float distance);
+
+    /**
+     * \brief Get focus range value currently used
+     *
+     * \param[in] camera_range_t& focusRange
+     *
+     * \return 0 if focus range was set, non-0 means no focus range was set
+     */
+    int getFocusRange(camera_range_t& focusRange) const;
+    /**
+     * \brief Set focus range value
+     *
+     * \param[in] camera_range_t focusRange
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setFocusRange(const camera_range_t& focusRange);
+
+    /**
+     * \brief Set af mode
+     *
+     * \param[in] camera_af_mode_t afMode
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAfMode(camera_af_mode_t afMode);
+
+    /**
+     * \brief Get af mode currently used.
+     *
+     * \param[out] camera_af_mode_t& afMode
+     *
+     * \return 0 if af mode was set, non-0 means no af mode was set.
+     */
+    int getAfMode(camera_af_mode_t& afMode) const;
+
+    /**
+     * \brief Trigger or cancel auto focus
+     *
+     * \param[in] camera_af_trigger_t afTrigger
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAfTrigger(camera_af_trigger_t afTrigger);
+
+    /**
+     * \brief Get auto focus trigger value
+     *
+     * \param[out] camera_af_trigger_t afTrigger
+     *
+     * \return 0 if af trigger was set, otherwise non-0 value is returned.
+     */
+    int getAfTrigger(camera_af_trigger_t& afTrigger) const;
+
+    /**
+     * \brief Set AF state.
+     *
+     * \param[in] camera_af_state_t afState
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAfState(camera_af_state_t afState);
+
+    /**
+     * \brief Get AF state
+     *
+     * \param[out] afState: Currently AF state will be set to afState if 0 is returned.
+     *
+     * \return 0 if AF state was set, otherwise non-0 value is returned.
+     */
+    int getAfState(camera_af_state_t& afState) const;
+
+    /**
+     * \brief Set lens state.
+     *
+     * \param[in] if lens is moving
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setLensState(bool lensMoving);
+
+    /**
+     * \brief Get lens state
+     *
+     * \param[out] isMoving: if lens is moving currently
+     *
+     * \return 0 if lens state was set, otherwise non-0 value is returned.
+     */
+    int getLensState(bool& lensMoving) const;
+
+    /**
+     * \brief Get lens aperture.
+     *
+     * \param[out] aperture
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int getLensAperture(float &aperture) const;
+
+    /**
+     * \brief Get lens filter density.
+     *
+     * \param[out] filter density
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int getLensFilterDensity(float &filterDensity) const;
+
+    /**
+     * \brief Get lens min focus distance.
+     *
+     * \param[out] min focus distance
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int getLensMinFocusDistance(float &minFocusDistance) const;
+
+    /**
+     * \brief Get lens hyperfocal distance.
+     *
+     * \param[out] hyperfocal distance
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int getLensHyperfocalDistance(float &hyperfocalDistance) const;
+
+    /**
+     * \brief Set af region
+     *
+     * \param[in] camera_window_list_t afRegions
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setAfRegions(camera_window_list_t afRegions);
+
+    /**
+     * \brief Get af region
+     *
+     * \param[out] camera_window_list_t afRegions
+     *
+     * \return 0 if afRegions were set, otherwise non-0 value is returned.
+     */
+    int getAfRegions(camera_window_list_t& afRegions) const;
+
+    /**
+     * \brief Get camera sensor mount type
+     *
+     * \param[out] sensorMountType sensor mount type: WALL_MOUNT or CEILING_MOUNT
+     *
+     * \return 0 if sensorMountType was set, otherwise non-0 value is returned.
+     */
+    int getSensorMountType(camera_mount_type_t& sensorMountType) const;
+
+    int updateDebugLevel();
+
+    /**
+     * \brief Set camera test pattern mode
+     *
+     * \param[in] mode: the camera device test pattern mode.
+     *
+     * \return 0 if set successfully, otherwise non-0 value is returned.
+     */
+    int setTestPatternMode(camera_test_pattern_mode_t mode);
+
+    /**
+     * \brief Get camera test pattern mode
+     *
+     * \param[out] mode: the camera device test pattern mode.
+     *
+     * \return 0 if test pattern mode was set, otherwise non-0 value is returned.
+     */
+    int getTestPatternMode(camera_test_pattern_mode_t& mode) const;
+
+    /**
+     * \brief Set crop region
+     *
+     * \param[in] cropRegion  the crop region related parameters
+     *
+     * \return 0 if successfully, otherwise non-0 value is returned.
+     */
+    int setCropRegion(camera_crop_region_t cropRegion);
+
+    /**
+     * \brief Get crop region
+     *
+     * \param[out] cropRegion  the crop related parameters
+     *
+     * \return 0 if successfully, otherwise non-0 value is returned.
+     */
+    int getCropRegion(camera_crop_region_t& cropRegion) const;
+
+    /**
+    * \brief Set control scene mode
+    *
+    * \param[in] sceneModeValue the control scene mode related parameters
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int setControlSceneMode(uint8_t sceneModeValue);
+
+    /**
+    * \brief Set face detect mode
+    *
+    * \param[in] faceDetectMode the face detect mode related parameters
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int setFaceDetectMode(uint8_t faceDetectMode);
+
+    /**
+    * \brief Get face detect mode
+    *
+    * \param[out] faceDetectMode the face detect mode related parameters, 0:OFF 1:SIMPLE 2:FULL
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int getFaceDetectMode(uint8_t& faceDetectMode) const;
+
+    /**
+    * \brief Set face id
+    *
+    * \param[in] int *faceIds, int faceNum
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int setFaceIds(int *faceIds, int faceNum);
+
+    /**
+     * Get sensor active array size
+     *
+     * \param[out] camera_coordinate_system_t& arraySize
+     * \return 0 if successfully, otherwise non-0 value is returned.
+     */
+    int getSensorActiveArraySize(camera_coordinate_system_t& arraySize) const;
+
+    /**
+    * \brief Set shading  mode
+    *
+    * \param[in] shadingMode the shading mode related parameters
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int setShadingMode(camera_shading_mode_t shadingMode);
+
+    /**
+    * \brief Get shading  mode
+    *
+    * \param[out] shadingMode the shading mode related parameters, 0:OFF 1:FAST 2:HIGH_QUALITY
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int getShadingMode(camera_shading_mode_t& shadingMode) const;
+
+    /**
+    * \brief Set statistics lens shading map mode
+    *
+    * \param[in] lensShadingMapMode the lens shading map mode related parameters
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int setLensShadingMapMode(camera_lens_shading_map_mode_type_t lensShadingMapMode);
+
+    /**
+    * \brief Get statistics lens shading map mode
+    *
+    * \param[out] lensShadingMapMode the lens shading map mode related parameters, 0:OFF 1:ON
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int getLensShadingMapMode(camera_lens_shading_map_mode_type_t &lensShadingMapMode) const;
+
+    /**
+    * \brief Set lens shading map
+    *
+    * \param[in] lensShadingMap the lens shading map
+    * \param[in] lensShadingMapSize lensShadingMap's size
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int setLensShadingMap(const float *lensShadingMap, size_t lensShadingMapSize);
+
+    /**
+    * \brief Get lens shading map
+    *
+    * \param[out] lensShadingMap the lens shading map
+    * \param[out] lensShadingMapSize the lens shading map's size
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int getLensShadingMap(float **lensShadingMap, size_t &lensShadingMapSize) const;
+
+    /**
+    * \brief Get lens shading map size
+    *
+    * \param[out] arraySize the lens shading map size related parameters
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int getLensInfoShadingMapSize(camera_coordinate_t &shadingMapSize) const;
+
+    /*
+    * \brief Set tonemap mode
+    *
+    * \param[in] camera_tonemap_mode_t& mode
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int setTonemapMode(camera_tonemap_mode_t mode);
+
+    /**
+    * \brief Get tonemap mode
+    *
+    * \param[out] camera_tonemap_mode_t& mode
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int getTonemapMode(camera_tonemap_mode_t& mode) const;
+
+    /**
+    * \brief Get supported tonemap modes
+    *
+    * \param[out] vector<camera_tonemap_mode_t>& tonemapModes
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int getSupportedTonemapMode(std::vector<camera_tonemap_mode_t>& tonemapModes) const;
+
+    /**
+    * \brief Set the type of tonemap preset curve
+    *
+    * \param[in] camera_tonemap_preset_curve_t type
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int setTonemapPresetCurve(camera_tonemap_preset_curve_t type);
+
+    /**
+    * \brief Get tonemap gamma
+    *
+    * \param[out] camera_tonemap_preset_curve_t& type
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int getTonemapPresetCurve(camera_tonemap_preset_curve_t& type) const;
+
+    /**
+    * \brief Set tonemap gamma
+    *
+    * \param[in] float gamma
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int setTonemapGamma(float gamma);
+
+    /**
+    * \brief Get tonemap gamma
+    *
+    * \param[out] float& gamma
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int getTonemapGamma(float& gamma) const;
+
+    /**
+    * \brief Get number of tonemap curve points
+    *
+    * \param[out] int32_t& number
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int getTonemapMaxCurvePoints(int32_t& number) const;
+
+    /**
+    * \brief Set tonemap curves
+    *
+    * \param[in] const camera_tonemap_curves_t& curve
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int setTonemapCurves(const camera_tonemap_curves_t& curves);
+
+    /**
+    * \brief Get tonemap curves
+    *
+    * \param[out] camera_tonemap_curves_t& curve
+    *
+    * \return 0 if successfully, otherwise non-0 value is returned.
+    */
+    int getTonemapCurves(camera_tonemap_curves_t& curves) const;
+
+    /**
+     * \brief Set user request id
+     *
+     * \param[in] user request id
+     *
+     * \return 0 if successfully, otherwise non-0 value is returned.
+     */
+    int setUserRequestId(int32_t userRequestId);
+
+    /**
+     * \brief Get user request id
+     *
+     * \param[out] user request id
+     *
+     * \return 0 if successfully, otherwise non-0 value is returned.
+     */
+    int getUserRequestId(int32_t& userRequestId) const;
+
+    /**
+     * \brief Set capture intent
+     *
+     * \param[in] capture intent mode
+     *
+     * \return 0 if successfully, otherwise non-0 value is returned.
+     */
+    int setCaptureIntent(uint8_t captureIntent);
+
+    /**
+     * \brief Get capture intent
+     *
+     * \param[out] capture intent mode
+     *
+     * \return 0 if successfully, otherwise non-0 value is returned.
+     */
+    int getCaptureIntent(uint8_t& captureIntent) const;
+
+private:
+    friend class ParameterHelper;
+    void* mData; // The internal data to save the all of the parameters.
+}; // class Parameters
+/*******************End of Camera Parameters Definition**********************/
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/include/cameranvm.h b/camera/hal/intel/ipu6/include/cameranvm.h
new file mode 100644
index 000000000000..64cbbf2af24d
--- /dev/null
+++ b/camera/hal/intel/ipu6/include/cameranvm.h
@@ -0,0 +1,112 @@
+/*
+ * Copyright 2012-2017 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/*!
+ * \file cameranvm.h
+ * \brief Definitions of NVM creator functions.
+*/
+
+#ifndef CAMERANVM_H_
+#define CAMERANVM_H_
+
+#include "ia_types.h"
+#include "ia_nvm.h"
+
+// macro for memcpy
+#ifndef MEMCPY_S
+#define NVM_MIN(a, b) ((a) < (b) ? (a) : (b))
+#define MEMCPY_S(dest, dmax, src, smax) memcpy((dest), (src), NVM_MIN((size_t)(dmax), (size_t)(smax)))
+#endif
+
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+/*!
+ * \brief NVM parsing status codes.
+ */
+typedef enum
+{
+    nvm_error_none,            /*!< No error. */
+    nvm_error_internal,        /*!< Parser internal failure (not enough memory). */
+    nvm_error_no_data,         /*!< NULL pointer as input or not enough input data. */
+    nvm_error_af,              /*!< Error parsing AF parameters. */
+    nvm_error_lsc,             /*!< Error parsing AF parameters. (lsc dimensions, lsc data) */
+    nvm_error_awb,             /*!< Error parsing AWB parameters (sensitivities, n_lights). */
+    nvm_error_crc,             /*!< CRC check error. */
+    nvm_error_not_implemented, /*!< Parser for given data type has not been implemented. */
+    nvm_error_version,         /*!< invalid version. */
+    nvm_status_intel_format    /*!< The NVM data is in Intel default format. */
+} nvm_error;
+
+typedef struct
+{
+    int lsc_color_temperature;             /*!< color temperature of shading table calibrate. as ct/100 */
+    uint16_t *lsc_tables[IA_NVM_NUM_CHANNELS]; /*!< LSC table for Ch1, Ch2, Ch3 and Ch4. */
+} nvm_lsc;
+
+typedef struct
+{
+    uint8_t lsc_width;                         /*!< Width of LSC tables */
+    uint8_t lsc_height;                        /*!< Height of LSC tables. */
+    nvm_lsc *lsc[2];                           /*!< LSC tables 1. */
+} nvm_data;
+
+/*!
+ * \brief Creates Intel specified NVM data from various NVM data.
+ * Detection of NVM data is done based on camera ID string. Created NVM
+ * data buffer must be deleted with function cameranvm_delete.
+ * \param[in]     camera_name           String identifying NVM format from various cameras.
+ * \param[in]     input_nvm_data        NVM data from camera module.
+ * \param[in]     input_nvm_motor_data  NVM data from camera lens motor EEPROM.
+ * \param[out]    output_nvm_data       NVM data converted into Intel specified format.
+ * \return                              Error code from NVM creation.
+ */
+nvm_error
+cameranvm_create(const char *camera_name,
+                 const ia_binary_data *input_nvm_data,
+                 const ia_binary_data *input_motor_nvm_data,
+                 ia_binary_data **output_nvm_data);
+
+
+/*!
+ * \brief Creates Intel specified NVM data from NVM data in google format.
+ * Detection of NVM data is done based on camera ID string. NVM data in
+ * google format should be converted into Intel format, and Created NVM
+ * data buffer must be deleted with function cameranvm_delete.
+ * \param[in]     lsc  lsc tables in google format.
+ * \param[out]    output_nvm_data       NVM data converted into Intel specified format.
+ * \return                              Error code from NVM creation.
+ */
+nvm_error
+cameranvm_convert(const nvm_data *input_nvm_data,
+                 ia_binary_data **output_nvm_data);
+
+/*!
+ * \brief Deletes NVM data buffer created with function cameranvm_create.
+ * This function only frees the allocated buffer and clears the parameters
+ * in the given structure.
+ * \param[in] nvm_data  Previously created NVM data buffer.
+ */
+void
+cameranvm_delete(ia_binary_data *nvm_data);
+
+#ifdef __cplusplus
+}
+#endif
+
+#endif /* CAMERANVM_H_ */
diff --git a/camera/hal/intel/ipu6/include/linux/ipu-isys.h b/camera/hal/intel/ipu6/include/linux/ipu-isys.h
new file mode 100644
index 000000000000..7ecf35c49040
--- /dev/null
+++ b/camera/hal/intel/ipu6/include/linux/ipu-isys.h
@@ -0,0 +1,51 @@
+/****************************************************************************
+ * Copyright (C) 2019 Intel Corporation.
+ ****************************************************************************
+ ***
+ ***   This header was automatically generated from a Linux kernel header
+ ***   of the same name, to make information necessary for userspace to
+ ***   call into the kernel available to libc.  It contains only constants,
+ ***   structures, and macros generated from the original header, and thus,
+ ***   contains no copyrightable information.
+ ***
+ ***   To edit the content of this header, modify the corresponding
+ ***   source file (e.g. under external/kernel-headers/original/) then
+ ***   run bionic/libc/kernel/tools/update_all.py
+ ***
+ ***   Any manual change here will be lost the next time this script will
+ ***   be run. You've been warned!
+ ***
+ ****************************************************************************
+ ****************************************************************************/
+#ifndef UAPI_LINUX_IPU_ISYS_H
+#define UAPI_LINUX_IPU_ISYS_H
+#define V4L2_CID_IPU_BASE (V4L2_CID_USER_BASE + 0x1080)
+#define V4L2_CID_IPU_ISA_EN (V4L2_CID_IPU_BASE + 1)
+/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */
+#define V4L2_CID_IPU_STORE_CSI2_HEADER (V4L2_CID_IPU_BASE + 2)
+#define V4L2_CID_IPU_ISYS_COMPRESSION  (V4L2_CID_IPU_BASE + 3)
+#define V4L2_IPU_ISA_EN_BLC (1 << 0)
+#define V4L2_IPU_ISA_EN_LSC (1 << 1)
+#define V4L2_IPU_ISA_EN_DPC (1 << 2)
+/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */
+#define V4L2_IPU_ISA_EN_SCALER (1 << 3)
+#define V4L2_IPU_ISA_EN_AWB (1 << 4)
+#define V4L2_IPU_ISA_EN_AF (1 << 5)
+#define V4L2_IPU_ISA_EN_AE (1 << 6)
+/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */
+#define NR_OF_IPU_ISA_CFG 7
+#define V4L2_FMT_IPU_ISA_CFG v4l2_fourcc('i', 'p', '4', 'c')
+#define V4L2_FMT_IPU_ISYS_META v4l2_fourcc('i', 'p', '4', 'm')
+#ifdef IPU_OTF_SUPPORT
+/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */
+struct ipu_frame_counter {
+  uint32_t frame_counter;
+  uint32_t index;
+} __attribute__((packed));
+/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */
+#define VIDIOC_IPU_SET_LINK_ID _IOWR('v', BASE_VIDIOC_PRIVATE + 1, uint8_t)
+#define VIDIOC_IPU_SET_FRAME_COUNTER _IOWR('v', BASE_VIDIOC_PRIVATE + 2, struct ipu_frame_counter)
+#endif
+#define VIDIOC_IPU_GET_DRIVER_VERSION _IOWR('v', BASE_VIDIOC_PRIVATE + 3, uint32_t)
+/* WARNING: DO NOT EDIT, AUTO-GENERATED CODE - SEE TOP FOR INSTRUCTIONS */
+#endif
diff --git a/camera/hal/intel/ipu6/include/utils/ScopedAtrace.h b/camera/hal/intel/ipu6/include/utils/ScopedAtrace.h
new file mode 100644
index 000000000000..043f60f0743e
--- /dev/null
+++ b/camera/hal/intel/ipu6/include/utils/ScopedAtrace.h
@@ -0,0 +1,73 @@
+/*
+ * Copyright (C) 2015-2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <unistd.h>
+
+namespace icamera {
+
+/**
+ * Following macros PERF_CAMERA_ATRACE_XXX() can be called in the function
+ * we are scoping. Environment variable "camPerf" is need to be set as "16"
+ * or "128", to enalbe atrace profiling:
+ * 1. When "16" is set, ATRACE with level of CAMERA_DEBUG_LOG_ATRACE_OS is
+ * enalbed.
+ * 2. When "128" is set, ATRACE with level of CAMERA_DEBUG_LOG_ATRACE_IMAGING
+ * is enabled.
+ */
+class ScopedAtrace {
+      public:
+          ScopedAtrace(const int level, const char* func, const char* tag,
+                       const char* note = NULL, long value = -1,
+                       const char* note2 = NULL, int value2 = -1,
+                       const char* note3 = NULL, int value3 = -1);
+          ~ScopedAtrace();
+          static void setTraceLevel(int);
+      private:
+          bool mEnableAtraceEnd;
+};
+
+#define CAMERA_DEBUG_LOG_ATRACE_OS (1<<4)
+#define CAMERA_DEBUG_LOG_ATRACE_IMAGING (1<<7)
+
+#define PERF_CAMERA_ATRACE() ScopedAtrace atrace(CAMERA_DEBUG_LOG_ATRACE_OS, \
+                                                 __func__, LOG_TAG);
+#define PERF_CAMERA_ATRACE_PARAM1(note, value) \
+            ScopedAtrace atrace(CAMERA_DEBUG_LOG_ATRACE_OS, __func__, \
+                                LOG_TAG, note, value);
+#define PERF_CAMERA_ATRACE_PARAM2(note, value, note2, value2) \
+            ScopedAtrace atrace(CAMERA_DEBUG_LOG_ATRACE_OS, __func__, LOG_TAG, \
+                                note, value, note2, value2);
+#define PERF_CAMERA_ATRACE_PARAM3(note, value, note2, value2, note3, value3) \
+            ScopedAtrace atrace(CAMERA_DEBUG_LOG_ATRACE_OS, __func__, LOG_TAG, \
+                                note, value, note2, value2, note3, value3);
+
+#define PERF_CAMERA_ATRACE_IMAGING() \
+            ScopedAtrace atrace(CAMERA_DEBUG_LOG_ATRACE_IMAGING, __func__, \
+                                LOG_TAG);
+#define PERF_CAMERA_ATRACE_PARAM1_IMAGING(note, value) \
+            ScopedAtrace atrace(CAMERA_DEBUG_LOG_ATRACE_IMAGING, __func__, \
+                                LOG_TAG, note, value);
+#define PERF_CAMERA_ATRACE_PARAM2_IMAGING(note, value, note2, value2) \
+            ScopedAtrace atrace(CAMERA_DEBUG_LOG_ATRACE_IMAGING, __func__, \
+                                LOG_TAG, note, value, note2, value2);
+#define PERF_CAMERA_ATRACE_PARAM3_IMAGING(note, value, note2, value2, note3, \
+                                          value3) \
+            ScopedAtrace atrace(CAMERA_DEBUG_LOG_ATRACE_IMAGING, __func__, \
+                                LOG_TAG, note, value, note2, value2, note3, \
+                                value3);
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelAiq.cpp b/camera/hal/intel/ipu6/modules/algowrapper/IntelAiq.cpp
new file mode 100644
index 000000000000..9b4b900cac0b
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelAiq.cpp
@@ -0,0 +1,131 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelAiq"
+
+#include "modules/algowrapper/IntelAiq.h"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelAiq::IntelAiq() : mAiq(nullptr) {
+    LOG2("@%s", __func__);
+}
+
+IntelAiq::~IntelAiq() {
+    LOG2("@%s", __func__);
+}
+
+ia_aiq* IntelAiq::init(const ia_binary_data* aiqbData, const ia_binary_data* nvmData,
+                       const ia_binary_data* aiqdData, unsigned int statsMaxWidth,
+                       unsigned int statsMaxHeight, unsigned int maxNumStatsIn, ia_cmc_t* cmc,
+                       ia_mkn* mkn) {
+    LOG2("@%s, aiqbData:%p, nvmData:%p, aiqdData:%p", __func__, aiqbData, nvmData, aiqdData);
+
+    mAiq = ia_aiq_init(aiqbData, nvmData, aiqdData, statsMaxWidth, statsMaxHeight, maxNumStatsIn,
+                       cmc, mkn);
+
+    return mAiq;
+}
+
+ia_err IntelAiq::aeRun(const ia_aiq_ae_input_params* inputParams, ia_aiq_ae_results** results) {
+    LOG2("@%s, inputParams:%p, results:%p", __func__, inputParams, results);
+    CheckError(!mAiq, ia_err_argument, "@%s, mAiq is nullptr", __func__);
+    CheckError(!inputParams, ia_err_argument, "@%s, inputParams is nullptr", __func__);
+    CheckError(!results, ia_err_argument, "@%s, results is nullptr", __func__);
+
+    return ia_aiq_ae_run(mAiq, inputParams, results);
+}
+
+ia_err IntelAiq::afRun(const ia_aiq_af_input_params* inputParams, ia_aiq_af_results** results) {
+    LOG2("@%s, inputParams:%p, results:%p", __func__, inputParams, results);
+    CheckError(!mAiq, ia_err_argument, "@%s, mAiq is nullptr", __func__);
+    CheckError(!inputParams, ia_err_argument, "@%s, inputParams is nullptr", __func__);
+    CheckError(!results, ia_err_argument, "@%s, results is nullptr", __func__);
+
+    return ia_aiq_af_run(mAiq, inputParams, results);
+}
+
+ia_err IntelAiq::awbRun(const ia_aiq_awb_input_params* inputParams, ia_aiq_awb_results** results) {
+    LOG2("@%s, inputParams:%p, results:%p", __func__, inputParams, results);
+    CheckError(!mAiq, ia_err_argument, "@%s, mAiq is nullptr", __func__);
+    CheckError(!inputParams, ia_err_argument, "@%s, inputParams is nullptr", __func__);
+    CheckError(!results, ia_err_argument, "@%s, results is nullptr", __func__);
+
+    return ia_aiq_awb_run(mAiq, inputParams, results);
+}
+
+ia_err IntelAiq::gbceRun(const ia_aiq_gbce_input_params* inputParams,
+                         ia_aiq_gbce_results** results) {
+    LOG2("@%s, inputParams:%p, results:%p", __func__, inputParams, results);
+    CheckError(!mAiq, ia_err_argument, "@%s, mAiq is nullptr", __func__);
+    CheckError(!inputParams, ia_err_argument, "@%s, inputParams is nullptr", __func__);
+    CheckError(!results, ia_err_argument, "@%s, results is nullptr", __func__);
+
+    return ia_aiq_gbce_run(mAiq, inputParams, results);
+}
+
+ia_err IntelAiq::paRunV1(const ia_aiq_pa_input_params* inputParams,
+                         ia_aiq_pa_results_v1** results) {
+    LOG2("@%s, inputParams:%p, results:%p", __func__, inputParams, results);
+    CheckError(!mAiq, ia_err_argument, "@%s, mAiq is nullptr", __func__);
+    CheckError(!inputParams, ia_err_argument, "@%s, inputParams is nullptr", __func__);
+    CheckError(!results, ia_err_argument, "@%s, results is nullptr", __func__);
+
+    return ia_aiq_pa_run_v1(mAiq, inputParams, results);
+}
+
+ia_err IntelAiq::saRunV2(const ia_aiq_sa_input_params_v1* inputParams,
+                         ia_aiq_sa_results_v1** results) {
+    LOG2("@%s, inputParams:%p, results:%p", __func__, inputParams, results);
+    CheckError(!mAiq, ia_err_argument, "@%s, mAiq is nullptr", __func__);
+    CheckError(!inputParams, ia_err_argument, "@%s, inputParams is nullptr", __func__);
+    CheckError(!results, ia_err_argument, "@%s, results is nullptr", __func__);
+
+    return ia_aiq_sa_run_v2(mAiq, inputParams, results);
+}
+
+ia_err IntelAiq::statisticsSetV4(const ia_aiq_statistics_input_params_v4* inputParams) {
+    LOG2("@%s, inputParams:%p", __func__, inputParams);
+    CheckError(!mAiq, ia_err_argument, "@%s, mAiq is nullptr", __func__);
+    CheckError(!inputParams, ia_err_argument, "@%s, inputParams is nullptr", __func__);
+
+    return ia_aiq_statistics_set_v4(mAiq, inputParams);
+}
+
+ia_err IntelAiq::getAiqdData(ia_binary_data* outData) {
+    LOG2("@%s, outData:%p", __func__, outData);
+    CheckError(!mAiq, ia_err_argument, "@%s, mAiq is nullptr", __func__);
+    CheckError(!outData, ia_err_argument, "@%s, outData is nullptr", __func__);
+
+    return ia_aiq_get_aiqd_data(mAiq, outData);
+}
+
+void IntelAiq::deinit() {
+    LOG2("@%s", __func__);
+    CheckError(!mAiq, VOID_VALUE, "@%s, mAiq is nullptr", __func__);
+
+    ia_aiq_deinit(mAiq);
+}
+
+void IntelAiq::getVersion(std::string* version) {
+    LOG2("@%s", __func__);
+
+    *version = std::string(ia_aiq_get_version());
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelAiq.h b/camera/hal/intel/ipu6/modules/algowrapper/IntelAiq.h
new file mode 100644
index 000000000000..7fd89a30b214
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelAiq.h
@@ -0,0 +1,49 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_aiq.h>
+#include <ia_types.h>
+
+#include <string>
+
+namespace icamera {
+class IntelAiq {
+ public:
+    IntelAiq();
+    virtual ~IntelAiq();
+
+    // the return pointer (ia_aiq*) is just valid in the sandboxing process.
+    ia_aiq* init(const ia_binary_data* aiqbData, const ia_binary_data* nvmData,
+                 const ia_binary_data* aiqdData, unsigned int statsMaxWidth,
+                 unsigned int statsMaxHeight, unsigned int maxNumStatsIn, ia_cmc_t* cmc,
+                 ia_mkn* mkn);
+    ia_err aeRun(const ia_aiq_ae_input_params* inputParams, ia_aiq_ae_results** results);
+    ia_err afRun(const ia_aiq_af_input_params* inputParams, ia_aiq_af_results** results);
+    ia_err awbRun(const ia_aiq_awb_input_params* inputParams, ia_aiq_awb_results** results);
+    ia_err gbceRun(const ia_aiq_gbce_input_params* inputParams, ia_aiq_gbce_results** results);
+    ia_err paRunV1(const ia_aiq_pa_input_params* inputParams, ia_aiq_pa_results_v1** results);
+    ia_err saRunV2(const ia_aiq_sa_input_params_v1* inputParams, ia_aiq_sa_results_v1** results);
+    ia_err statisticsSetV4(const ia_aiq_statistics_input_params_v4* inputParams);
+    ia_err getAiqdData(ia_binary_data* outData);
+    void deinit();
+    void getVersion(std::string* version);
+
+ private:
+    ia_aiq* mAiq;
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelCmc.cpp b/camera/hal/intel/ipu6/modules/algowrapper/IntelCmc.cpp
new file mode 100644
index 000000000000..fa86fb1bb7ff
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelCmc.cpp
@@ -0,0 +1,62 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelCmc"
+
+#include "modules/algowrapper/IntelCmc.h"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelCmc::IntelCmc() : mHandle(nullptr) {
+    LOG1("@%s", __func__);
+}
+
+IntelCmc::~IntelCmc() {
+    LOG1("@%s", __func__);
+}
+
+bool IntelCmc::init(const ia_binary_data* aiqbData, const ia_binary_data* nvmData) {
+    LOG1("@%s, aiqbData:%p, nvmData:%p", __func__, aiqbData, nvmData);
+    CheckError(!aiqbData, false, "aiqbData is nullptr");
+    CheckError(nvmData, false, "nvmData is not nullptr");  // it doesn't support nvmData currently.
+
+    mHandle = ia_cmc_parser_init_v1(aiqbData, nvmData);
+    LOG1("@%s, mHandle:%p", __func__, mHandle);
+
+    return true;
+}
+
+ia_cmc_t* IntelCmc::getCmc() const {
+    LOG1("@%s, mHandle:%p", __func__, mHandle);
+
+    return mHandle;
+}
+
+uintptr_t IntelCmc::getCmcHandle() const {
+    LOG1("@%s", __func__);
+
+    return reinterpret_cast<uintptr_t>(mHandle);
+}
+
+void IntelCmc::deinit() {
+    LOG1("@%s", __func__);
+    CheckError(!mHandle, VOID_VALUE, "mHandle is nullptr");
+
+    ia_cmc_parser_deinit(mHandle);
+}
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelCmc.h b/camera/hal/intel/ipu6/modules/algowrapper/IntelCmc.h
new file mode 100644
index 000000000000..3a338d1d1936
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelCmc.h
@@ -0,0 +1,38 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_cmc_parser.h>
+#include <ia_cmc_types.h>
+
+namespace icamera {
+class IntelCmc {
+ public:
+    IntelCmc();
+    ~IntelCmc();
+
+    bool init(const ia_binary_data* aiqbData, const ia_binary_data* nvmData);
+
+    ia_cmc_t* getCmc() const;
+    uintptr_t getCmcHandle() const;
+
+    void deinit();
+
+ private:
+    ia_cmc_t* mHandle;
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelDvs.cpp b/camera/hal/intel/ipu6/modules/algowrapper/IntelDvs.cpp
new file mode 100644
index 000000000000..4b066173d89e
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelDvs.cpp
@@ -0,0 +1,186 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelDvs"
+
+#include "modules/algowrapper/IntelDvs.h"
+
+#include "AiqUtils.h"
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelDvs::IntelDvs() {
+    LOG1("@%s", __func__);
+}
+
+IntelDvs::~IntelDvs() {
+    LOG1("@%s", __func__);
+}
+
+ia_err IntelDvs::init(const ia_binary_data& aiqTuningBinary, const ia_cmc_t* cmc,
+                      ia_dvs_state** dvsHandle) {
+    LOG1("@%s", __func__);
+    CheckError(!dvsHandle, ia_err_none, "@%s, dvsHandle is nullptr", __func__);
+
+    ia_err err = ia_dvs_init(dvsHandle, &aiqTuningBinary, cmc);
+    CheckError(err != ia_err_none, ia_err_general, "@%s, Failed to init dvs lib", __func__);
+    return err;
+}
+
+void IntelDvs::deinit(ia_dvs_state* dvsHandle) {
+    LOG1("@%s", __func__);
+    CheckError(!dvsHandle, VOID_VALUE, "@%s, dvsHandle is nullptr", __func__);
+
+    ia_dvs_deinit(dvsHandle);
+}
+
+ia_err IntelDvs::config(ia_dvs_state* dvsHandle, ia_dvs_configuration* config, float zoomRatio) {
+    LOG1("@%s", __func__);
+    CheckError(!dvsHandle, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+    CheckError(!config, ia_err_general, "@%s, config is nullptr", __func__);
+
+    ia_err err = ia_dvs_config(dvsHandle, config, zoomRatio);
+    CheckError(err != ia_err_none, err, "@%s, ia_dvs_config fails", __func__);
+    return err;
+}
+
+ia_err IntelDvs::setNonBlankRatio(ia_dvs_state* dvsHandle, float nonBlankingRatio) {
+    LOG1("@%s", __func__);
+    CheckError(!dvsHandle, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+
+    ia_err err = ia_dvs_set_non_blank_ratio(dvsHandle, nonBlankingRatio);
+    CheckError(err != ia_err_none, err, "@%s, ia_dvs_set_non_blank_ratio fails", __func__);
+    return err;
+}
+
+ia_err IntelDvs::setDigitalZoomMode(ia_dvs_state* dvsHandle, ia_dvs_zoom_mode zoomMode) {
+    LOG1("@%s", __func__);
+    CheckError(!dvsHandle, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+
+    ia_err err = ia_dvs_set_digital_zoom_mode(dvsHandle, zoomMode);
+    CheckError(err != ia_err_none, err, "@%s, ia_dvs_set_digital_zoom_mode fails", __func__);
+    return err;
+}
+
+ia_err IntelDvs::setDigitalZoomRegion(ia_dvs_state* dvsHandle, ia_rectangle* zoomRegion) {
+    LOG1("@%s", __func__);
+    CheckError(!dvsHandle, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+    CheckError(!zoomRegion, ia_err_general, "@%s, zoomRegion is nullptr", __func__);
+
+    ia_err err = ia_dvs_set_digital_zoom_region(dvsHandle, zoomRegion);
+    CheckError(err != ia_err_none, err, "@%s, ia_dvs_set_digital_zoom_region fails", __func__);
+    return err;
+}
+
+ia_err IntelDvs::setDigitalZoomCoordinate(ia_dvs_state* dvsHandle, ia_coordinate* zoomCoordinate) {
+    LOG1("@%s", __func__);
+    CheckError(!dvsHandle, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+    CheckError(!zoomCoordinate, ia_err_general, "@%s, zoomCoordinate is nullptr", __func__);
+
+    ia_err err = ia_dvs_set_digital_zoom_coordinate(dvsHandle, zoomCoordinate);
+    CheckError(err != ia_err_none, err, "@%s, ia_dvs_set_digital_zoom_coordinate fails", __func__);
+    return err;
+}
+
+ia_err IntelDvs::setDigitalZoomMagnitude(ia_dvs_state* dvsHandle, float zoomRatio) {
+    LOG1("@%s", __func__);
+    CheckError(!dvsHandle, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+
+    ia_err err = ia_dvs_set_digital_zoom_magnitude(dvsHandle, zoomRatio);
+    CheckError(err != ia_err_none, err, "@%s, ia_dvs_set_digital_zoom_magnitude fails", __func__);
+    return err;
+}
+
+void IntelDvs::freeMorphTable(ia_dvs_state* dvsHandle, ia_dvs_morph_table* morphTable) {
+    LOG1("@%s, dvsHandle:%p, morphTable:%p", __func__, dvsHandle, morphTable);
+    CheckError(!morphTable, VOID_VALUE, "@%s, morphTable is nullptr", __func__);
+
+    ia_dvs_free_morph_table(morphTable);
+}
+
+ia_dvs_morph_table* IntelDvs::allocateMorphTalbe(ia_dvs_state* dvsHandle) {
+    LOG1("@%s", __func__);
+    CheckError(!dvsHandle, nullptr, "@%s, dvsHandle is nullptr", __func__);
+
+    ia_dvs_morph_table* morphTable = nullptr;
+    ia_err err = ia_dvs_allocate_morph_table(dvsHandle, &morphTable);
+    CheckError((!morphTable || err != ia_err_none), nullptr,
+               "@%s, ia_dvs_allocate_morph_table fails", __func__);
+    return morphTable;
+}
+
+int IntelDvs::getMorphTable(ia_dvs_state* dvsHandle, ia_dvs_morph_table* morphTable) {
+    LOG2("@%s", __func__);
+    CheckError(!dvsHandle, UNKNOWN_ERROR, "@%s, dvsHandle is nullptr", __func__);
+    CheckError(!morphTable, UNKNOWN_ERROR, "@%s, morphTable is nullptr", __func__);
+
+    ia_err err = ia_dvs_get_morph_table(dvsHandle, morphTable);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, ia_dvs_get_morph_table fails, err:%d",
+               __func__, err);
+
+    return OK;
+}
+
+int IntelDvs::getMorphTable(ia_dvs_state* dvsHandle, ia_dvs_morph_table* morphTable,
+                            DvsResult* result) {
+    LOG2("@%s", __func__);
+
+    int ret = getMorphTable(dvsHandle, morphTable);
+    CheckError(ret != OK, UNKNOWN_ERROR, "@%s, getMorphTable fails", __func__);
+
+    ret = DvsResult::deepCopyDvsResults(*morphTable, &result->mMorphTable);
+    CheckError(ret != OK, UNKNOWN_ERROR, "@%s, deepCopyDvsResults fails", __func__);
+
+    return OK;
+}
+
+ia_err IntelDvs::setStatistics(ia_dvs_state* dvsHandle, const ia_dvs_statistics* statistics,
+                               const ia_aiq_ae_results* aeResults,
+                               const ia_aiq_af_results* afResults,
+                               const ia_aiq_sensor_events* sensorEvents, uint64_t frameReadoutStart,
+                               uint64_t frameReadoutEnd) {
+    LOG1("@%s", __func__);
+    CheckError(!dvsHandle, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+
+    ia_err err = ia_dvs_set_statistics(dvsHandle, statistics, aeResults, afResults, sensorEvents,
+                                       frameReadoutStart, frameReadoutEnd);
+    CheckError(err != ia_err_none, ia_err_general, "@%s, ia_dvs_set_statistics fails", __func__);
+    return err;
+}
+
+ia_err IntelDvs::execute(ia_dvs_state* dvsHandle, uint16_t focusPosition) {
+    LOG1("@%s", __func__);
+    CheckError(!dvsHandle, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+
+    ia_err err = ia_dvs_execute(dvsHandle, focusPosition);
+    CheckError(err != ia_err_none, ia_err_general, "@%s, ia_dvs_execute fails", __func__);
+    return err;
+}
+
+ia_err IntelDvs::getImageTransformation(ia_dvs_state* dvsHandle,
+                                        ia_dvs_image_transformation* imageTransformation) {
+    LOG1("@%s", __func__);
+    CheckError(!dvsHandle, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+    CheckError(!imageTransformation, ia_err_general, "@%s, imageTransformation is nullptr",
+               __func__);
+
+    ia_err err = ia_dvs_get_image_transformation(dvsHandle, imageTransformation);
+    CheckError(err != ia_err_none, ia_err_general, "@%s, ia_dvs_get_image_transformation fails",
+               __func__);
+    return err;
+}
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelDvs.h b/camera/hal/intel/ipu6/modules/algowrapper/IntelDvs.h
new file mode 100644
index 000000000000..f31b391aa72b
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelDvs.h
@@ -0,0 +1,55 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_dvs.h>
+#include <ia_dvs_types.h>
+#include <ia_isp_bxt.h>
+
+#include "CameraEvent.h"
+#include "DvsResult.h"
+#include "iutils/Errors.h"
+#include "iutils/Thread.h"
+
+namespace icamera {
+class IntelDvs {
+ public:
+    IntelDvs();
+    ~IntelDvs();
+
+    ia_err init(const ia_binary_data& aiqTuningBinary, const ia_cmc_t* cmc,
+                ia_dvs_state** dvsHandle);
+    void deinit(ia_dvs_state* dvsHandle);
+    ia_err config(ia_dvs_state* dvsHandle, ia_dvs_configuration* config, float zoomRatio);
+    ia_err setNonBlankRatio(ia_dvs_state* dvsHandle, float nonBlankingRatio);
+    ia_err setDigitalZoomMode(ia_dvs_state* dvsHandle, ia_dvs_zoom_mode zoomMode);
+    ia_err setDigitalZoomRegion(ia_dvs_state* dvsHandle, ia_rectangle* zoomRegion);
+    ia_err setDigitalZoomCoordinate(ia_dvs_state* dvsHandle, ia_coordinate* zoomCoordinate);
+    ia_err setDigitalZoomMagnitude(ia_dvs_state* dvsHandle, float zoomRatio);
+    void freeMorphTable(ia_dvs_state* dvsHandle, ia_dvs_morph_table* morphTable);
+    ia_dvs_morph_table* allocateMorphTalbe(ia_dvs_state* dvsHandle);
+    int getMorphTable(ia_dvs_state* dvsHandle, ia_dvs_morph_table* morphTable);
+    int getMorphTable(ia_dvs_state* dvsHandle, ia_dvs_morph_table* morphTable, DvsResult* result);
+    ia_err setStatistics(ia_dvs_state* dvsHandle, const ia_dvs_statistics* statistics,
+                         const ia_aiq_ae_results* aeResults, const ia_aiq_af_results* afResults,
+                         const ia_aiq_sensor_events* sensorEvents, uint64_t frameReadoutStart,
+                         uint64_t frameReadoutEnd);
+    ia_err execute(ia_dvs_state* dvsHandle, uint16_t focusPosition);
+    ia_err getImageTransformation(ia_dvs_state* dvsHandle,
+                                  ia_dvs_image_transformation* imageTransformation);
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelFaceDetection.cpp b/camera/hal/intel/ipu6/modules/algowrapper/IntelFaceDetection.cpp
new file mode 100644
index 000000000000..78fb986f4dae
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelFaceDetection.cpp
@@ -0,0 +1,141 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelFaceDetection"
+#include "modules/algowrapper/IntelFaceDetection.h"
+
+#include <string.h>
+
+#include <algorithm>
+
+#include "AiqUtils.h"
+#include "PlatformData.h"
+#include "iutils/CameraLog.h"
+
+namespace icamera {
+IntelFaceDetection::IntelFaceDetection() : mFDHandle(nullptr), mMaxFacesNum(0) {
+    LOG1("@%s", __func__);
+}
+
+IntelFaceDetection::~IntelFaceDetection() {
+    LOG1("@%s", __func__);
+}
+
+status_t IntelFaceDetection::init(FaceDetectionInitParams* pData, int dataSize) {
+    LOG1("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(!pData, UNKNOWN_ERROR, "pData is nullptr");
+    CheckError(dataSize < static_cast<int>(sizeof(FaceDetectionInitParams)), UNKNOWN_ERROR,
+               "buffer is small");
+
+    mMaxFacesNum = std::min(pData->max_face_num, static_cast<unsigned int>(MAX_FACES_DETECTABLE));
+    LOG2("@%s, mMaxFacesNum:%d", __func__, mMaxFacesNum);
+
+    pvl_err faceRet = pvl_face_detection_create(nullptr, &mFDHandle);
+    if (faceRet == pvl_success) {
+        return OK;
+    }
+
+    LOGE("@%s, faceRet:%d", __func__, faceRet);
+    return UNKNOWN_ERROR;
+}
+
+status_t IntelFaceDetection::deinit() {
+    LOG1("@%s", __func__);
+
+    if (mFDHandle) {
+        pvl_face_detection_destroy(mFDHandle);
+        mFDHandle = nullptr;
+    }
+    return OK;
+}
+
+void IntelFaceDetection::convertCoordinate(int faceId, int width, int height, const pvl_rect& src,
+                                           pvl_rect* dst) {
+    CheckError(!dst, VOID_VALUE, "dst is nullptr");
+
+    const camera_coordinate_system_t iaCoordinate = {IA_COORDINATE_LEFT, IA_COORDINATE_TOP,
+                                                     IA_COORDINATE_RIGHT, IA_COORDINATE_BOTTOM};
+    const camera_coordinate_system_t faceCoordinate = {0, 0, width, height};
+
+    camera_coordinate_t topLeft =
+        AiqUtils::convertCoordinateSystem(faceCoordinate, iaCoordinate, {src.left, src.top});
+    camera_coordinate_t bottomRight =
+        AiqUtils::convertCoordinateSystem(faceCoordinate, iaCoordinate, {src.right, src.bottom});
+
+    *dst = {topLeft.x, topLeft.y, bottomRight.x, bottomRight.y};
+    LOG2("@%s, face:%d, dst left:%d, top:%d, right:%d, bottom:%d", __func__, faceId, dst->left,
+         dst->top, dst->right, dst->bottom);
+}
+
+FaceDetectionRunParams* IntelFaceDetection::prepareRunBuffer(unsigned int index) {
+    LOG1("@%s", __func__);
+    CheckError(index >= MAX_STORE_FACE_DATA_BUF_NUM, nullptr, "@%s, index is error %d", __func__,
+               index);
+    CheckError(!mFDHandle, nullptr, "mFDHandle is nullptr");
+
+    return &mMemRunBufs[index];
+}
+
+status_t IntelFaceDetection::run(pvl_image* pImage, FaceDetectionResult* fdResults) {
+    LOG1("@%s, pImage:%p", __func__, pImage);
+    CheckError(!pImage, UNKNOWN_ERROR, "pData is nullptr");
+    CheckError(!mFDHandle, UNKNOWN_ERROR, "mFDHandle is nullptr");
+
+    int32_t fdRet =
+        pvl_face_detection_run_in_preview(mFDHandle, pImage, fdResults->faceResults, mMaxFacesNum);
+    fdResults->faceNum = (fdRet > 0) ? fdRet : 0;
+    LOG1("@%s, fdRet:%d, detected face number:%d, w:%d, h:%d", __func__, fdRet, fdResults->faceNum,
+         pImage->width, pImage->height);
+    for (int i = 0; i < fdResults->faceNum; i++) {
+        LOG2("@%s, face:%d rect, left:%d, top:%d, right:%d, bottom:%d", __func__, i,
+             fdResults->faceResults[i].rect.left, fdResults->faceResults[i].rect.top,
+             fdResults->faceResults[i].rect.right, fdResults->faceResults[i].rect.bottom);
+        LOG2("@%s, confidence:%d, rip_angle:%d, rop_angle:%d, tracking_id:%d", __func__,
+             fdResults->faceResults[i].confidence, fdResults->faceResults[i].rip_angle,
+             fdResults->faceResults[i].rop_angle, fdResults->faceResults[i].tracking_id);
+    }
+
+    for (int i = 0; i < fdResults->faceNum; i++) {
+        convertCoordinate(i, pImage->width, pImage->height, fdResults->faceResults[i].rect,
+                          &fdResults->faceResults[i].rect);
+    }
+
+    return OK;
+}
+
+status_t IntelFaceDetection::run(FaceDetectionRunParams* fdRunParams, int dataSize, void* addr) {
+    LOG1("@%s, fdRunParams:%p, dataSize:%d, addr:%p", __func__, fdRunParams, dataSize, addr);
+    CheckError(!fdRunParams, UNKNOWN_ERROR, "pData is nullptr");
+    CheckError(dataSize < static_cast<int>(sizeof(FaceDetectionRunParams)), UNKNOWN_ERROR,
+               "buffer is small");
+    CheckError(!mFDHandle, UNKNOWN_ERROR, "mFDHandle is nullptr");
+
+    pvl_image image;
+    image.size = fdRunParams->size;
+    image.width = fdRunParams->width;
+    image.height = fdRunParams->height;
+    image.format = fdRunParams->format;
+    image.stride = fdRunParams->stride;
+    image.rotation = fdRunParams->rotation;
+    if (addr) {
+        image.data = const_cast<uint8_t*>(static_cast<uint8_t*>(addr));
+    } else {
+        image.data = const_cast<uint8_t*>(fdRunParams->data);
+    }
+
+    return run(&image, &fdRunParams->results);
+}
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelFaceDetection.h b/camera/hal/intel/ipu6/modules/algowrapper/IntelFaceDetection.h
new file mode 100644
index 000000000000..da738efa5a96
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelFaceDetection.h
@@ -0,0 +1,44 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+#include <memory>
+
+#include "FaceBase.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+class IntelFaceDetection {
+ public:
+    IntelFaceDetection();
+    ~IntelFaceDetection();
+
+    status_t init(FaceDetectionInitParams* initData, int dataSize);
+    status_t deinit();
+    status_t run(pvl_image* pImage, FaceDetectionResult* fdResults);
+    status_t run(FaceDetectionRunParams* fdRunParams, int dataSize, void* addr = nullptr);
+    FaceDetectionRunParams* prepareRunBuffer(unsigned int index);
+
+ private:
+    pvl_face_detection* mFDHandle;
+    unsigned int mMaxFacesNum;
+    FaceDetectionRunParams mMemRunBufs[MAX_STORE_FACE_DATA_BUF_NUM];
+    void convertCoordinate(int faceId, int width, int height, const pvl_rect& src, pvl_rect* dst);
+
+    DISALLOW_COPY_AND_ASSIGN(IntelFaceDetection);
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelIspParamAdaptor.cpp b/camera/hal/intel/ipu6/modules/algowrapper/IntelIspParamAdaptor.cpp
new file mode 100644
index 000000000000..84c92ed9bf80
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelIspParamAdaptor.cpp
@@ -0,0 +1,140 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelIspParamAdaptor"
+
+#include "modules/algowrapper/IntelIspParamAdaptor.h"
+
+#include "iutils/CameraLog.h"
+
+namespace icamera {
+
+IntelIspParamAdaptor::IntelIspParamAdaptor() {
+    LOG2("@%s", __func__);
+}
+
+IntelIspParamAdaptor::~IntelIspParamAdaptor() {
+    LOG2("@%s", __func__);
+}
+
+ia_isp_bxt* IntelIspParamAdaptor::init(const ia_binary_data* ispData, const ia_cmc_t* iaCmc,
+                                       unsigned int maxStatsWidth, unsigned int maxStatsHeight,
+                                       unsigned int maxNumStatsIn, ia_mkn* iaMkn) {
+    LOG2("@%s", __func__);
+    CheckError(!ispData || !iaCmc, nullptr, "%s, No CPF or CMC data", __func__);
+    LOG1("%s, ispData size: %d, pointer: %p, max width: %u, max height: %u", __func__,
+         ispData->size, ispData->data, maxStatsWidth, maxStatsHeight);
+
+    return ia_isp_bxt_init(ispData, iaCmc, maxStatsWidth, maxStatsHeight, maxNumStatsIn, iaMkn);
+}
+
+void IntelIspParamAdaptor::deInit(ia_isp_bxt* ispBxtHandle) {
+    LOG2("@%s", __func__);
+    CheckError(!ispBxtHandle, VOID_VALUE, "%s, ispBxtHandle is nullptr", __func__);
+
+    ia_isp_bxt_deinit(ispBxtHandle);
+}
+
+int IntelIspParamAdaptor::getPalDataSize(ia_isp_bxt_program_group* programGroup) {
+    LOG2("@%s", __func__);
+    CheckError(!programGroup, -1, "%s programGroup is nullptr", __func__);
+
+    return ia_isp_bxt_get_output_size(programGroup);
+}
+
+void IntelIspParamAdaptor::freePalBuffer(void* addr) {
+    LOG2("@%s addr: %p", __func__, addr);
+    free(addr);
+}
+
+void* IntelIspParamAdaptor::allocatePalBuffer(int streamId, int index, int palDataSize) {
+    LOG2("@%s index: %d, streamId: %d, size: %d", __func__, index, streamId, palDataSize);
+
+    return calloc(1, palDataSize);
+}
+
+status_t IntelIspParamAdaptor::runPal(ia_isp_bxt* ispBxtHandle,
+                                      const ia_isp_bxt_input_params_v2* inputParams,
+                                      ia_binary_data* outputData) {
+    LOG2("@%s", __func__);
+    CheckError((!ispBxtHandle || !inputParams), UNKNOWN_ERROR,
+               "%s, ispBxtHandle or inputParams is nullptr", __func__);
+    CheckError((!outputData || !outputData->data || outputData->size <= 0), UNKNOWN_ERROR,
+               "%s, Wrong pal data buffer", __func__);
+
+    ia_err ret = ia_isp_bxt_run_v2(ispBxtHandle, inputParams, outputData);
+    CheckError(ret != ia_err_none, UNKNOWN_ERROR, "%s, isp parameters adaptor run failed %d",
+               __func__, ret);
+    LOG1("%s, The pal result size: %d", __func__, outputData->size);
+
+    return OK;
+}
+
+status_t IntelIspParamAdaptor::queryAndConvertStats(ia_isp_bxt* ispBxtHandle,
+                                                    ConvertInputParam* inputParams,
+                                                    ConvertResult* result) {
+    LOG2("@%s", __func__);
+    CheckError(!ispBxtHandle, UNKNOWN_ERROR, "%s, ispBxtHandle is nullptr", __func__);
+    CheckError(!inputParams || !result, UNKNOWN_ERROR, "%s, inputParams or result nullptr",
+               __func__);
+    CheckError(!inputParams->dvsReso || !inputParams->aeResults, UNKNOWN_ERROR,
+               "%s, inputParams or result nullptr", __func__);
+    CheckError((!inputParams->statsBuffer || !inputParams->statsBuffer->data ||
+                inputParams->statsBuffer->size <= 0),
+               UNKNOWN_ERROR, "%s, Wrong statistics buffer", __func__);
+    CheckError(!result->queryResults, UNKNOWN_ERROR, "%s, queryResults is nullptr", __func__);
+
+    ia_err ret =
+        ia_isp_bxt_statistics_query(ispBxtHandle, inputParams->statsBuffer, result->queryResults);
+    CheckError(ret != ia_err_none, UNKNOWN_ERROR, "%s, Query statistice failed %d", __func__, ret);
+
+    // Decode DVS statistics
+    if (result->queryResults->dvs_stats) {
+        if (inputParams->dvsReso->width == 0 || inputParams->dvsReso->height == 0) {
+            LOGW("%s, The gdc resolution for DVS isn't correct", __func__);
+        } else {
+            ret = ia_isp_bxt_statistics_convert_dvs_from_binary(
+                ispBxtHandle, inputParams->statsBuffer, inputParams->dvsReso->width,
+                inputParams->dvsReso->height, &(result->dvsStats));
+            CheckWarning((ret != ia_err_none || !result->dvsStats), UNKNOWN_ERROR,
+                         "%s, Failed to convert DVS statistics %d", __func__, ret);
+            LOG3A("%s, DVS stat vector_count: %u", __func__, result->dvsStats->vector_count);
+        }
+    }
+
+    // Decode psa rgbs and af statistics
+    if (result->queryResults->rgbs_grid && result->queryResults->af_grid &&
+        !inputParams->multiExpo) {
+        ret = ia_isp_bxt_statistics_convert_awb_from_binary_v3(
+            ispBxtHandle, inputParams->statsBuffer, nullptr, inputParams->aeResults,
+            inputParams->bcompResult, result->rgbsGrid, nullptr);
+
+        ia_aiq_rgbs_grid* rgbs = *(result->rgbsGrid);
+        CheckWarning((ret != ia_err_none || !rgbs), UNKNOWN_ERROR,
+                     "%s, Failed to convert psa RGBS statistics %d", __func__, ret);
+        LOG3A("%s, RGBS stat grid %dx%d", __func__, rgbs->grid_width, rgbs->grid_height);
+
+        ret = ia_isp_bxt_statistics_convert_af_from_binary(ispBxtHandle, inputParams->statsBuffer,
+                                                           &(result->afGrid));
+        CheckWarning((ret != ia_err_none || !result->afGrid), UNKNOWN_ERROR,
+                     "%s, Failed to convert psa AF statistics %d", __func__, ret);
+        LOG3A("%s, AF stat grid %dx%d", __func__, result->afGrid->grid_width,
+              result->afGrid->grid_height);
+    }
+
+    return OK;
+}
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelIspParamAdaptor.h b/camera/hal/intel/ipu6/modules/algowrapper/IntelIspParamAdaptor.h
new file mode 100644
index 000000000000..564fe37a95fa
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelIspParamAdaptor.h
@@ -0,0 +1,44 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+#include "modules/algowrapper/StatsTypes.h"
+
+namespace icamera {
+
+class IntelIspParamAdaptor {
+ public:
+    IntelIspParamAdaptor();
+    virtual ~IntelIspParamAdaptor();
+
+    ia_isp_bxt* init(const ia_binary_data* ispData, const ia_cmc_t* iaCmc,
+                     unsigned int maxStatsWidth, unsigned int maxStatsHeight,
+                     unsigned int maxNumStatsIn, ia_mkn* iaMkn);
+    void deInit(ia_isp_bxt* ispBxtHandle);
+    int getPalDataSize(ia_isp_bxt_program_group* programGroup);
+
+    void* allocatePalBuffer(int streamId, int index, int palDataSize);
+    void freePalBuffer(void* addr);
+    status_t runPal(ia_isp_bxt* ispBxtHandle, const ia_isp_bxt_input_params_v2* inputParams,
+                    ia_binary_data* outputData);
+    status_t queryAndConvertStats(ia_isp_bxt* ispBxtHandle, ConvertInputParam* inputParams,
+                                  ConvertResult* result);
+};
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelLard.cpp b/camera/hal/intel/ipu6/modules/algowrapper/IntelLard.cpp
new file mode 100644
index 000000000000..084a6859fcee
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelLard.cpp
@@ -0,0 +1,71 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelLard"
+
+#include "modules/algowrapper/IntelLard.h"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelLard::IntelLard() {
+    LOG1("@%s", __func__);
+}
+
+IntelLard::~IntelLard() {
+    LOG1("@%s", __func__);
+}
+
+ia_lard* IntelLard::init(const ia_binary_data* lard_data_ptr) {
+    LOG1("@%s", __func__);
+    CheckError(!lard_data_ptr, nullptr, "lard_data_ptr is nullptr");
+    LOG1("@%s, lard_data_ptr, data:%p, size:%d", __func__, lard_data_ptr->data,
+         lard_data_ptr->size);
+
+    ia_lard* lard = ia_lard_init(lard_data_ptr);
+    LOG1("@%s, lard:%p", __func__, lard);
+
+    return lard;
+}
+
+ia_err IntelLard::getTagList(ia_lard* ia_lard_ptr, unsigned int mode_tag, unsigned int* num_tags,
+                             const unsigned int** tags) {
+    LOG1("@%s", __func__);
+    CheckError(!ia_lard_ptr, ia_err_general, "ia_lard_ptr is nullptr");
+    CheckError(!num_tags, ia_err_general, "num_tags is nullptr");
+    CheckError(!tags, ia_err_general, "tags is nullptr");
+
+    return ia_lard_get_tag_list(ia_lard_ptr, mode_tag, num_tags, tags);
+}
+
+ia_err IntelLard::run(ia_lard* ia_lard_ptr, ia_lard_input_params* lard_input_params_ptr,
+                      ia_lard_results** lard_results_ptr) {
+    LOG1("@%s", __func__);
+    CheckError(!ia_lard_ptr, ia_err_general, "ia_lard_ptr is nullptr");
+    CheckError(!lard_input_params_ptr, ia_err_general, "lard_input_params_ptr is nullptr");
+    CheckError(!lard_results_ptr, ia_err_general, "lard_results_ptr is nullptr");
+
+    return ia_lard_run(ia_lard_ptr, lard_input_params_ptr, lard_results_ptr);
+}
+
+void IntelLard::deinit(ia_lard* ia_lard_ptr) {
+    LOG1("@%s", __func__);
+    CheckError(!ia_lard_ptr, VOID_VALUE, "ia_lard_ptr is nullptr");
+
+    ia_lard_deinit(ia_lard_ptr);
+}
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelLard.h b/camera/hal/intel/ipu6/modules/algowrapper/IntelLard.h
new file mode 100644
index 000000000000..fa9d99d13098
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelLard.h
@@ -0,0 +1,34 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_lard.h>
+
+namespace icamera {
+class IntelLard {
+ public:
+    IntelLard();
+    ~IntelLard();
+
+    ia_lard* init(const ia_binary_data* lard_data_ptr);
+    ia_err getTagList(ia_lard* ia_lard_ptr, unsigned int mode_tag, unsigned int* num_tags,
+                      const unsigned int** tags);
+    ia_err run(ia_lard* ia_lard_ptr, ia_lard_input_params* lard_input_params_ptr,
+               ia_lard_results** lard_results_ptr);
+    void deinit(ia_lard* ia_lard_ptr);
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelLtm.cpp b/camera/hal/intel/ipu6/modules/algowrapper/IntelLtm.cpp
new file mode 100644
index 000000000000..dc4102c36c2a
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelLtm.cpp
@@ -0,0 +1,64 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelLtm"
+
+#include "modules/algowrapper/IntelLtm.h"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelLtm::IntelLtm() {
+    LOG1("@%s", __func__);
+}
+
+IntelLtm::~IntelLtm() {
+    LOG1("@%s", __func__);
+}
+
+ia_ltm* IntelLtm::init(const ia_binary_data* lard_data_ptr, ia_mkn* mkn) {
+    LOG1("%s", __func__);
+    CheckError(!lard_data_ptr, nullptr, "@%s, lard_data_ptr is null", __func__);
+
+    ia_ltm* ltm = ia_ltm_init(lard_data_ptr, mkn);
+    CheckError(!ltm, nullptr, "@%s, ia_ltm_init fails", __func__);
+
+    return ltm;
+}
+
+void IntelLtm::deinit(ia_ltm* ltm) {
+    LOG1("%s", __func__);
+    CheckError(!ltm, VOID_VALUE, "@%s, ltm is null", __func__);
+
+    ia_ltm_deinit(ltm);
+}
+
+ia_err IntelLtm::run(ia_ltm* ltm, const ia_ltm_input_params* inputParams,
+                     ia_ltm_results** ltmResults, ia_ltm_drc_params** drcResults) {
+    LOG1("%s", __func__);
+    CheckError(!ltm, ia_err_general, "@%s, ltm is null", __func__);
+    CheckError(!inputParams, ia_err_general, "@%s, inputParams is null", __func__);
+    CheckError(!ltmResults, ia_err_general, "@%s, ltmResults is null", __func__);
+    CheckError(!drcResults, ia_err_general, "@%s, drcResults is null", __func__);
+
+    ia_err ret = ia_ltm_run(ltm, inputParams, ltmResults, drcResults);
+
+    return ret;
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelLtm.h b/camera/hal/intel/ipu6/modules/algowrapper/IntelLtm.h
new file mode 100644
index 000000000000..f6f280b93ed5
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelLtm.h
@@ -0,0 +1,32 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_ltm.h>
+
+namespace icamera {
+class IntelLtm {
+ public:
+    IntelLtm();
+    virtual ~IntelLtm();
+
+    ia_ltm* init(const ia_binary_data* lard_data_ptr, ia_mkn* mkn);
+    ia_err run(ia_ltm* ltm, const ia_ltm_input_params* inputParams, ia_ltm_results** ltmResults,
+               ia_ltm_drc_params** drcResults);
+    void deinit(ia_ltm* ltm);
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelMkn.cpp b/camera/hal/intel/ipu6/modules/algowrapper/IntelMkn.cpp
new file mode 100644
index 000000000000..041c277c0380
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelMkn.cpp
@@ -0,0 +1,69 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelMkn"
+
+#include "modules/algowrapper/IntelMkn.h"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelMkn::IntelMkn() {
+    LOG1("%s", __func__);
+}
+
+IntelMkn::~IntelMkn() {
+    LOG1("%s", __func__);
+}
+
+ia_mkn* IntelMkn::init(ia_mkn_config_bits mkn_config_bits, size_t mkn_section_1_size,
+                       size_t mkn_section_2_size) {
+    LOG1("%s", __func__);
+
+    return ia_mkn_init(mkn_config_bits, mkn_section_1_size, mkn_section_2_size);
+}
+
+int IntelMkn::enable(ia_mkn* pMkn, bool enable_data_collection) {
+    LOG1("%s", __func__);
+    CheckError(!pMkn, BAD_VALUE, "@%s, pMkn is null", __func__);
+
+    ia_err err = ia_mkn_enable(pMkn, enable_data_collection);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, call ia_mkn_enable() fails", __func__);
+
+    return OK;
+}
+
+void IntelMkn::deinit(ia_mkn* pMkn) {
+    LOG1("%s", __func__);
+    CheckError(!pMkn, VOID_VALUE, "@%s, pMkn is null", __func__);
+
+    ia_mkn_uninit(pMkn);
+}
+
+int IntelMkn::prepare(ia_mkn* pMkn, ia_mkn_trg data_target, ia_binary_data* binaryData) {
+    LOG1("%s", __func__);
+    CheckError(!pMkn, BAD_VALUE, "@%s, pMkn is null", __func__);
+
+    *binaryData = ia_mkn_prepare(pMkn, data_target);
+    CheckError(binaryData->size == 0 || binaryData->data == nullptr, NO_MEMORY,
+               "@%s, binaryData->size:%d, binaryData->data:%p, error!", __func__, binaryData->size,
+               binaryData->data);
+
+    return OK;
+}
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelMkn.h b/camera/hal/intel/ipu6/modules/algowrapper/IntelMkn.h
new file mode 100644
index 000000000000..72133f4c1ffc
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelMkn.h
@@ -0,0 +1,35 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_mkn_encoder.h>
+#include <ia_mkn_types.h>
+
+namespace icamera {
+class IntelMkn {
+ public:
+    IntelMkn();
+    ~IntelMkn();
+
+    ia_mkn* init(ia_mkn_config_bits mkn_config_bits, size_t mkn_section_1_size,
+                 size_t mkn_section_2_size);
+    void deinit(ia_mkn* pMkn);
+    int prepare(ia_mkn* pMkn, ia_mkn_trg data_target, ia_binary_data* binaryData);
+    int enable(ia_mkn* pMkn, bool enable_data_collection);
+};
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelPGParam.cpp b/camera/hal/intel/ipu6/modules/algowrapper/IntelPGParam.cpp
new file mode 100644
index 000000000000..ca254361cc32
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelPGParam.cpp
@@ -0,0 +1,1196 @@
+/*
+ * Copyright (C) 2018-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelPGParam"
+
+#include "modules/algowrapper/IntelPGParam.h"
+
+#include <algorithm>
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+IntelPGParam::IntelPGParam(int pgId)
+        : mPgId(pgId),
+          mTerminalCount(0),
+          mFragmentCount(0),
+          mFragmentDesc(nullptr),
+          mFragmentConfig(nullptr),
+          mP2pHandle(nullptr),
+          mPgManifest(nullptr),
+          mProcessGroup(nullptr),
+          mProgramControlInitTerminalIndex(-1),
+          mProcessGroupMemory(nullptr) {
+    CLEAR(mP2pCacheBuffer);
+    CLEAR(mPgReqs);
+    CLEAR(mParamPayloads);
+}
+
+IntelPGParam::~IntelPGParam() {
+    if (mFragmentDesc) {
+        delete[] mFragmentDesc;
+    }
+    if (mFragmentConfig) {
+        delete mFragmentConfig;
+    }
+
+    for (int i = 0; i < mTerminalCount; i++) {
+        if (mPgReqs.terminals[i].kernelOrder) {
+            delete[] mPgReqs.terminals[i].kernelOrder;
+        }
+    }
+
+    destroyPayloads();
+    destroyPGBuffer();
+}
+
+int IntelPGParam::init(ia_p2p_platform_t platform, const PgConfiguration& pgConfig) {
+    mP2pHandle = ia_p2p_init(platform);
+    CheckError(!mP2pHandle, UNKNOWN_ERROR, "ia_p2p_init has failed");
+
+    mP2pCacheBuffer.size = ia_p2p_get_cache_buffer_size(mP2pHandle);
+    mP2pCacheBuffer.data = CIPR::callocMemory(1, mP2pCacheBuffer.size);
+    LOG1("%s: mP2pCacheBuffer.size=%d", __func__, mP2pCacheBuffer.size);
+    CheckError(!mP2pCacheBuffer.data, UNKNOWN_ERROR, "Failed to allocate P2P cache buffer.");
+
+    mPgManifest = pgConfig.pgManifest;
+    mDisableDataTermials = pgConfig.disableDataTermials;
+    mTerminalCount = ia_css_program_group_manifest_get_terminal_count(mPgManifest);
+
+    mFragmentCount = pgConfig.fragmentCount;
+    mInputMainFrame = pgConfig.inputMainFrame;
+    mOutputMainFrame = pgConfig.outputMainFrame;
+
+    return OK;
+}
+
+int IntelPGParam::calcFragmentDescriptors(int fragmentCount, const PgFrameDesc& inputMainFrame,
+                                          const PgFrameDesc& outputMainFrame,
+                                          const ia_css_rbm_t* rbm) {
+    if (mFragmentDesc) {
+        delete[] mFragmentDesc;
+        mFragmentDesc = nullptr;
+    }
+
+    delete mFragmentConfig;
+    mFragmentConfig = nullptr;
+
+    mFragmentDesc = new ia_p2p_fragment_desc[FRAG_TERM_TYPE_COUNT * fragmentCount];
+    memset(mFragmentDesc, 0x0, sizeof(ia_p2p_fragment_desc) * FRAG_TERM_TYPE_COUNT * fragmentCount);
+    if (fragmentCount <= 1) {
+        ia_p2p_fragment_desc desc;
+        desc.fragment_width = inputMainFrame.width;
+        desc.fragment_height = inputMainFrame.height;
+        desc.fragment_start_x = 0;
+        desc.fragment_start_y = 0;
+        for (int i = 0; i < FRAG_TERM_TYPE_COUNT; i++) {
+            mFragmentDesc[i] = desc;
+        }
+    }
+
+    mFragmentConfig = new ia_p2p_fragment_configuration_t;
+    CLEAR(*mFragmentConfig);
+    int ret = ia_p2p_calculate_fragments_rbm(mP2pHandle, mPgId, (unsigned int)fragmentCount, rbm,
+                                             nullptr,  // only for IA_P2P_PLATFORM_IPU6 now
+                                             mFragmentConfig);
+
+    dumpFragmentDesc(fragmentCount);
+    return ret;
+}
+
+static int kernel_id_ffs(ia_css_kernel_bitmap_t bitmap) {
+    int n = 0;
+    if (ia_css_is_kernel_bitmap_empty(bitmap)) return -1;
+    while (!ia_css_is_kernel_bitmap_set(bitmap, (unsigned int)n)) n++;
+    return n;
+}
+
+int IntelPGParam::getFragmentDescriptors(int descCount, ia_p2p_fragment_desc* descs) {
+    CheckError(descCount < mTerminalCount * mFragmentCount, BAD_VALUE, "descCount is small",
+               descCount);
+
+    int descLen = sizeof(ia_p2p_fragment_desc) * mFragmentCount;
+    int terminalCount = ia_css_process_group_get_terminal_count(mProcessGroup);
+    for (int i = 0; i < terminalCount; i++) {
+        ia_css_terminal_t* terminal = ia_css_process_group_get_terminal(mProcessGroup, i);
+        CheckError(!terminal, BAD_VALUE, "terminal is nullptr");
+
+        int termIdx = terminal->tm_index;
+        if ((mPgReqs.terminals[termIdx].type != IA_CSS_TERMINAL_TYPE_DATA_OUT) &&
+            (mPgReqs.terminals[termIdx].type != IA_CSS_TERMINAL_TYPE_DATA_IN)) {
+            continue;
+        }
+
+        if (mFragmentConfig) {
+            int kernelId = kernel_id_ffs(mPgReqs.terminals[termIdx].kernelBitmap);
+            CheckError((kernelId < 0 || kernelId >= IA_CSS_KERNEL_BITMAP_BITS), -1,
+                       "error terminal %d", termIdx);
+            MEMCPY_S(&descs[termIdx * mFragmentCount], descLen,
+                     mFragmentConfig->pixel_fragment_descs[kernelId], descLen);
+            LOG2("PG %d: Terminal %d: selected fragment desc (<%d,%d> %dx%d) with kernel id %d",
+                 mPgId, termIdx, descs[termIdx].fragment_start_x, descs[termIdx].fragment_start_y,
+                 descs[termIdx].fragment_width, descs[termIdx].fragment_height, kernelId);
+        } else {
+            /* PG uses legacy fragment calculation logic */
+            MEMCPY_S(&descs[termIdx * mFragmentCount], descLen,
+                     mPgReqs.terminals[termIdx].fragment_descs, descLen);
+            LOG2("PG %d: Terminal %d: selected legacy fragment descriptor (<%d,%d> %dx%d)", mPgId,
+                 termIdx, descs[termIdx].fragment_start_x, descs[termIdx].fragment_start_y,
+                 descs[termIdx].fragment_width, descs[termIdx].fragment_height);
+        }
+    }
+    return mFragmentCount;
+}
+
+int IntelPGParam::prepare(const ia_binary_data* ipuParameters, const ia_css_rbm_t* rbm,
+                          ia_css_kernel_bitmap_t* bitmap, uint32_t* maxStatsSize) {
+    CheckError(ipuParameters == nullptr || bitmap == nullptr, BAD_VALUE,
+               "The input paramter is nullptr.");
+
+    ia_css_terminal_type_t terminalType;
+    int8_t termIndex;
+    int kernelId = 0;
+
+    ia_err err = ia_p2p_parse(mP2pHandle, ipuParameters, mP2pCacheBuffer.data);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "Failed to parse PAL data.");
+
+    int ret = calcFragmentDescriptors(mFragmentCount, mInputMainFrame, mOutputMainFrame, rbm);
+    CheckError(ret != OK, ret, "Failed to calc fragment desc.");
+
+    int outputDataTerminalCount = FRAG_TERM_TYPE_OUTPUT_START;
+    for (termIndex = 0; termIndex < mTerminalCount; termIndex++) {
+        ia_css_terminal_manifest_t* terminalManifest =
+            ia_css_program_group_manifest_get_term_mnfst(mPgManifest, (unsigned int)termIndex);
+        CheckError(!terminalManifest, css_err_internal, "No terminal manifest for terminal %d",
+                   termIndex);
+
+        terminalType = ia_css_terminal_manifest_get_type(terminalManifest);
+        mPgReqs.terminals[termIndex].type = terminalType;
+        mPgReqs.terminals[termIndex].kernelOrder = nullptr;
+        size_t kernelInfoSize = PSYS_MAX_KERNELS_PER_PG * sizeof(IpuPgTerminalKernelInfo);
+
+        switch (terminalType) {
+            case IA_CSS_TERMINAL_TYPE_PARAM_CACHED_IN:
+            case IA_CSS_TERMINAL_TYPE_PARAM_CACHED_OUT: {
+                ia_css_param_terminal_manifest_t* paramMani =
+                    reinterpret_cast<ia_css_param_terminal_manifest_t*>(terminalManifest);
+                mPgReqs.terminals[termIndex].kernelOrder =
+                    new IpuPgTerminalKernelInfo[PSYS_MAX_KERNELS_PER_PG];
+                memset(mPgReqs.terminals[termIndex].kernelOrder, UINT8_MAX, kernelInfoSize);
+                ret = getKernelOrderForParamCachedInTerm(paramMani,
+                                                         mPgReqs.terminals[termIndex].kernelOrder);
+                CheckError(ret != css_err_none, ret, "getKernelOrderForParamCachedInTerm failed");
+                break;
+            }
+            case IA_CSS_TERMINAL_TYPE_PROGRAM: {
+                ia_css_program_terminal_manifest_t* proMani =
+                    reinterpret_cast<ia_css_program_terminal_manifest_t*>(terminalManifest);
+                mPgReqs.terminals[termIndex].kernelOrder =
+                    new IpuPgTerminalKernelInfo[PSYS_MAX_KERNELS_PER_PG];
+                memset(mPgReqs.terminals[termIndex].kernelOrder, UINT8_MAX, kernelInfoSize);
+                ret =
+                    getKernelOrderForProgramTerm(proMani, mPgReqs.terminals[termIndex].kernelOrder);
+                CheckError(ret != css_err_none, ret, "getKernelOrderForProgramTerm failed");
+                break;
+            }
+            case IA_CSS_TERMINAL_TYPE_DATA_IN: {
+                ia_css_data_terminal_manifest_t* dataMani =
+                    reinterpret_cast<ia_css_data_terminal_manifest_t*>(terminalManifest);
+                /**
+                 *Save the kernel bitmaps so that it can later be determined
+                 * whether the terminals are disabled or not.
+                 */
+                mPgReqs.terminals[termIndex].kernelBitmap =
+                    ia_css_data_terminal_manifest_get_kernel_bitmap(dataMani);
+                if (!mFragmentConfig) {
+                    mPgReqs.terminals[termIndex].fragment_descs =
+                        &mFragmentDesc[FRAG_TERM_TYPE_INPUT * mFragmentCount];
+                }
+                break;
+            }
+            case IA_CSS_TERMINAL_TYPE_DATA_OUT: {
+                ia_css_data_terminal_manifest_t* dataMani =
+                    reinterpret_cast<ia_css_data_terminal_manifest_t*>(terminalManifest);
+                /**
+                 * Save the kernel bitmaps so that it can later be determined
+                 * whether the terminals are disabled or not.
+                 */
+                mPgReqs.terminals[termIndex].kernelBitmap =
+                    ia_css_data_terminal_manifest_get_kernel_bitmap(dataMani);
+                if (!mFragmentConfig) {
+                    mPgReqs.terminals[termIndex].fragment_descs =
+                        &mFragmentDesc[outputDataTerminalCount * mFragmentCount];
+                    outputDataTerminalCount++;
+                }
+                break;
+            }
+            case IA_CSS_TERMINAL_TYPE_PARAM_SPATIAL_IN:
+            case IA_CSS_TERMINAL_TYPE_PARAM_SPATIAL_OUT: {
+                ia_css_spatial_param_terminal_manifest_t* paramMani =
+                    reinterpret_cast<ia_css_spatial_param_terminal_manifest_t*>(terminalManifest);
+                kernelId = (int32_t)(paramMani->kernel_id);
+                mPgReqs.terminals[termIndex].kernelBitmap =
+                    ia_css_kernel_bit_mask((uint32_t)kernelId);
+                break;
+            }
+            case IA_CSS_TERMINAL_TYPE_PROGRAM_CONTROL_INIT:
+                /* Calculate the payload later when the final bitmask is known,
+                 * in imaging_pipe_ctrl_identify_property, where it is actually
+                 * needed. Just save here the index to the terminal. */
+                mProgramControlInitTerminalIndex = termIndex;
+                break;
+            default:
+                break;
+        }
+    }
+
+    mPgReqs.terminalCount = mTerminalCount;
+
+    ia_css_kernel_bitmap_t kernelBitmap = ia_p2p_get_kernel_bitmap(mP2pHandle, mPgId);
+    kernelBitmap = ia_css_kernel_bitmap_intersection(
+        kernelBitmap, ia_css_program_group_manifest_get_kernel_bitmap(mPgManifest));
+
+    while (!ia_css_is_kernel_bitmap_empty(kernelBitmap)) {
+        kernelId = getKernelIdByBitmap(kernelBitmap);
+        CheckError((kernelId < 0 || kernelId >= PSYS_MAX_KERNELS_PER_PG), ia_err_internal,
+                   "kernelId is out of range! %d", kernelId);
+
+        /* Get terminal requirements */
+        ret = ia_p2p_get_kernel_terminal_requirements(mP2pHandle, mPgId, (uint32_t)kernelId,
+                                                      &mKernel.mSections[kernelId]);
+        CheckError(ret != ia_err_none, ret, "%s: failed to get requirements for pg %d kernel %d",
+                   __func__, mPgId, kernelId);
+
+        /* Get payload descriptor */
+        ret = ia_p2p_get_kernel_payload_desc(
+            mP2pHandle, mPgId, (uint32_t)kernelId,
+#if defined(IPU_SYSVER_IPU6) && defined(UNIFIED_PROG_TERM_FRAG_DESC)
+            1,
+#else
+            mFragmentCount,
+#endif
+            (mFragmentConfig ? mFragmentConfig->pixel_fragment_descs[kernelId]
+                             : &mFragmentDesc[FRAG_TERM_TYPE_INPUT * mFragmentCount]),
+            &mKernel.mPayloads[kernelId]);
+        CheckError(ret != ia_err_none, ret, "%s: failed to get payload for pg %d kernel %d, ret %d",
+                   __func__, mPgId, kernelId, ret);
+
+        uint8_t kernelOrder = 0;
+        termIndex = -1;
+        if (mKernel.mSections[kernelId].param_in_section_count) {
+            terminalType = IA_CSS_TERMINAL_TYPE_PARAM_CACHED_IN;
+
+            /* P2P assumes single CACHED IN, cumulate to first */
+            termIndex = terminalEnumerateByType(&mPgReqs, terminalType, 0);
+            CheckError(termIndex < 0, ia_err_internal, "No PARAM_CACHED_IN according to manifest!");
+            if (isKernelIdInKernelOrder(&mPgReqs, termIndex, kernelId, &kernelOrder)) {
+                if (mPgReqs.terminals[termIndex].kernelOrder[kernelOrder].sections !=
+                    mKernel.mSections[kernelId].param_in_section_count) {
+                    LOGW("%s: p2p cached in section count differs (kernel_id:%i p2p:%d vs pg:%d)",
+                         __func__, kernelId, mKernel.mSections[kernelId].param_in_section_count,
+                         mPgReqs.terminals[termIndex].kernelOrder[kernelOrder].sections);
+                    /* Overwrite P2P requirements with manifest */
+                    mKernel.mSections[kernelId].param_in_section_count =
+                        mPgReqs.terminals[termIndex].kernelOrder[kernelOrder].sections;
+                    mKernel.mPayloads[kernelId].param_in_payload_size =
+                        std::max(mKernel.mPayloads[kernelId].param_in_payload_size,
+                                 mPgReqs.terminals[termIndex].kernelOrder[kernelOrder].size);
+                    mPgReqs.terminals[termIndex].kernelOrder[kernelOrder].initialize = true;
+                }
+                processTerminalKernelRequirements(&mPgReqs, termIndex, terminalType, kernelId);
+            }
+        }
+
+        if (mKernel.mSections[kernelId].param_out_section_count_per_fragment) {
+            terminalType = IA_CSS_TERMINAL_TYPE_PARAM_CACHED_OUT;
+            for (termIndex = 0; termIndex < mTerminalCount; termIndex++) {
+                if (mPgReqs.terminals[termIndex].type != terminalType) {
+                    continue;
+                }
+
+                if (isKernelIdInKernelOrder(&mPgReqs, termIndex, kernelId, nullptr)) {
+                    processTerminalKernelRequirements(&mPgReqs, termIndex, terminalType, kernelId);
+                }
+            }
+        }
+
+        if (mKernel.mSections[kernelId].program_section_count_per_fragment) {
+            terminalType = IA_CSS_TERMINAL_TYPE_PROGRAM;
+            for (termIndex = 0; termIndex < mTerminalCount; termIndex++) {
+                if (mPgReqs.terminals[termIndex].type != terminalType) {
+                    continue;
+                }
+                if (isKernelIdInKernelOrder(&mPgReqs, termIndex, kernelId, &kernelOrder)) {
+                    if (mPgReqs.terminals[termIndex].kernelOrder[kernelOrder].sections !=
+                        mKernel.mSections[kernelId].program_section_count_per_fragment) {
+                        LOGW("%s: p2p program section count differs (kernel_id:%i p2p:%d vs pg:%d)",
+                             __func__, kernelId,
+                             mKernel.mSections[kernelId].program_section_count_per_fragment,
+                             mPgReqs.terminals[termIndex].kernelOrder[kernelOrder].sections);
+                        /* Overwrite P2P requirements with manifest */
+                        mKernel.mSections[kernelId].program_section_count_per_fragment =
+                            mPgReqs.terminals[termIndex].kernelOrder[kernelOrder].sections;
+                        mKernel.mPayloads[kernelId].program_payload_size =
+                            std::max(mKernel.mPayloads[kernelId].program_payload_size,
+                                     mPgReqs.terminals[termIndex].kernelOrder[kernelOrder].size);
+                        mPgReqs.terminals[termIndex].kernelOrder[kernelOrder].initialize = true;
+                    }
+                    processTerminalKernelRequirements(&mPgReqs, termIndex, terminalType, kernelId);
+                }
+            }
+        }
+
+        /* P2P assumes each spatial kernel parameter has its own terminal */
+        if (mKernel.mSections[kernelId].spatial_param_in_section_count) {
+            terminalType = IA_CSS_TERMINAL_TYPE_PARAM_SPATIAL_IN;
+            termIndex = terminalEnumerateByBitmap(&mPgReqs, terminalType,
+                                                  ia_css_kernel_bit_mask((uint32_t)kernelId));
+            if (termIndex < 0) {
+                LOG1("%s: No PARAM_SPATIAL_IN for kernel id %d according to manifest!", __func__,
+                     kernelId);
+            } else if (isKernelIdInKernelOrder(&mPgReqs, termIndex, kernelId, nullptr)) {
+                mPgReqs.terminals[termIndex].sectionCount +=
+                    mKernel.mSections[kernelId].spatial_param_in_section_count;
+                mPgReqs.terminals[termIndex].payloadSize +=
+                    mKernel.mPayloads[kernelId].spatial_param_in_payload_size;
+                mPgReqs.terminals[termIndex].kernelBitmap =
+                    ia_css_kernel_bit_mask((uint32_t)kernelId);
+            }
+        }
+
+        if (mKernel.mSections[kernelId].spatial_param_out_section_count) {
+            terminalType = IA_CSS_TERMINAL_TYPE_PARAM_SPATIAL_OUT;
+            termIndex = terminalEnumerateByBitmap(&mPgReqs, terminalType,
+                                                  ia_css_kernel_bit_mask((uint32_t)kernelId));
+            if (termIndex < 0) {
+                LOG1("%s: No PARAM_SPATIAL_OUT for kernel id %d according to manifest!", __func__,
+                     kernelId);
+            } else if (isKernelIdInKernelOrder(&mPgReqs, termIndex, kernelId, nullptr)) {
+                mPgReqs.terminals[termIndex].sectionCount +=
+                    mKernel.mSections[kernelId].spatial_param_out_section_count;
+                mPgReqs.terminals[termIndex].payloadSize +=
+                    mKernel.mPayloads[kernelId].spatial_param_out_payload_size;
+                mPgReqs.terminals[termIndex].kernelBitmap =
+                    ia_css_kernel_bit_mask((uint32_t)kernelId);
+            }
+        }
+
+        kernelBitmap = ia_css_kernel_bitmap_unset(kernelBitmap, (uint32_t)kernelId);
+    }
+
+    /* get all kernel bits back */
+    kernelBitmap = ia_css_program_group_manifest_get_kernel_bitmap(mPgManifest);
+
+    /* get disabled kernels from p2p and remove them */
+    kernelBitmap = ia_css_kernel_bitmap_intersection(
+        kernelBitmap,
+        ia_css_kernel_bitmap_complement(ia_p2p_get_kernel_disable_bitmap(mP2pHandle, mPgId)));
+
+    /* get disabled data terminal kernels and remove them */
+    for (auto& item : mDisableDataTermials) {
+        ia_css_terminal_manifest_t* terminalManifest =
+            ia_css_program_group_manifest_get_term_mnfst(mPgManifest, (unsigned int)item);
+        ia_css_kernel_bitmap_t dataTerminalKernelBitmap =
+            ia_css_data_terminal_manifest_get_kernel_bitmap(
+                reinterpret_cast<ia_css_data_terminal_manifest_t*>(terminalManifest));
+        kernelBitmap = ia_css_kernel_bitmap_intersection(
+            kernelBitmap, ia_css_kernel_bitmap_complement(dataTerminalKernelBitmap));
+    }
+
+    /* disable params terminals which payload size are zero */
+    ret = disableZeroSizedTerminals(&kernelBitmap);
+    CheckError(ret != OK, ret, "%s: failed to disable zero size terminals", __func__);
+
+    *bitmap = kernelBitmap;
+
+    if (maxStatsSize) *maxStatsSize = ia_p2p_get_statistics_buffer_size(mP2pHandle);
+    return ret;
+}
+
+void* IntelPGParam::allocatePGBuffer(int pgSize) {
+    destroyPGBuffer();
+    void* memory = CIPR::mallocAlignedMemory(PAGE_ALIGN(pgSize), CIPR::getPageSize());
+    mProcessGroupMemory = reinterpret_cast<ia_css_process_group_t*>(memory);
+    return mProcessGroupMemory;
+}
+
+void IntelPGParam::destroyPGBuffer() {
+    if (mProcessGroupMemory) {
+        CIPR::freeMemory(mProcessGroupMemory);
+        mProcessGroupMemory = nullptr;
+    }
+}
+
+int IntelPGParam::setPGAndPrepareProgram(ia_css_process_group_t* pg) {
+    CheckError(!pg, UNKNOWN_ERROR, "input pg nullptr!");
+    mProcessGroup = pg;
+
+    int ret = OK;
+    int terminalCount = ia_css_process_group_get_terminal_count(mProcessGroup);
+    for (int i = 0; i < terminalCount; i++) {
+        ia_css_terminal_t* terminal = ia_css_process_group_get_terminal(mProcessGroup, i);
+        CheckError(!terminal, UNKNOWN_ERROR, "failed to get terminal");
+        int termIdx = terminal->tm_index;
+
+        if (mPgReqs.terminals[termIdx].type == IA_CSS_TERMINAL_TYPE_PROGRAM_CONTROL_INIT) {
+            unsigned int payloadSize = 0;
+            ret = pg_control_init_get_payload_size(pg, &payloadSize);
+            CheckError(ret != OK, UNKNOWN_ERROR, "call pg_control_init_get_payload_size fail");
+            mPgReqs.terminals[termIdx].payloadSize = payloadSize;
+
+            ret = pg_control_init_terminal_init(
+                mProcessGroup, reinterpret_cast<ia_css_program_control_init_terminal_t*>(terminal));
+            CheckError(ret != ia_err_none, ret, "Failed to call pg_control_init_terminal_init.");
+        }
+
+        if (mPgReqs.terminals[termIdx].type == IA_CSS_TERMINAL_TYPE_PROGRAM) {
+            if (mFragmentConfig) {
+                ret = ia_p2p_program_terminal_init_v2(
+                    mP2pHandle, mPgId, mFragmentConfig,
+                    reinterpret_cast<ia_css_program_terminal_t*>(terminal));
+            } else {
+                ret = ia_p2p_program_terminal_init(
+                    mP2pHandle, mPgId, mFragmentCount, mFragmentDesc,
+                    reinterpret_cast<ia_css_program_terminal_t*>(terminal));
+            }
+            CheckError(ret != ia_err_none, ret, "Failed to init program terminal.");
+        }
+    }
+    return OK;
+}
+
+int IntelPGParam::getPayloadSizes(int payloadCount, ia_binary_data* payloads) {
+    CheckError(payloadCount < mTerminalCount || !payloads, BAD_VALUE, "Can't get payload sizes!");
+    for (int termIdx = 0; termIdx < mTerminalCount; termIdx++) {
+        payloads[termIdx].size = mPgReqs.terminals[termIdx].payloadSize;
+    }
+    return mTerminalCount;
+}
+
+int IntelPGParam::allocatePayloads(int payloadCount, ia_binary_data* payloads) {
+    destroyPayloads();
+    int count = getPayloadSizes(mTerminalCount, mParamPayloads);
+    CheckError(payloadCount < count || !payloads, 0, "payloads error");
+
+    for (int idx = 0; idx < count; idx++) {
+        if (mParamPayloads[idx].size == 0) {
+            continue;
+        }
+        mParamPayloads[idx].data =
+            CIPR::mallocAlignedMemory(PAGE_ALIGN(mParamPayloads[idx].size), CIPR::getPageSize());
+    }
+    MEMCPY_S(payloads, count * sizeof(ia_binary_data), mParamPayloads,
+             count * sizeof(ia_binary_data));
+
+    return count;
+}
+
+void IntelPGParam::destroyPayloads() {
+    for (int termIdx = 0; termIdx < mTerminalCount; termIdx++) {
+        if (mParamPayloads[termIdx].data) {
+            CIPR::freeMemory(mParamPayloads[termIdx].data);
+            mParamPayloads[termIdx].data = nullptr;
+        }
+    }
+    CLEAR(mParamPayloads);
+}
+
+int IntelPGParam::updatePALAndEncode(const ia_binary_data* ipuParams, int payloadCount,
+                                     ia_binary_data* payloads) {
+    ia_err err = ia_p2p_parse(mP2pHandle, ipuParams, mP2pCacheBuffer.data);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "Failed to parse PAL data.");
+
+    CheckError(!payloads, UNKNOWN_ERROR, "no payloads for encode.");
+    CheckError(payloadCount < mTerminalCount, UNKNOWN_ERROR, "small payload count %d, should be %d",
+               payloadCount, mTerminalCount);
+    CheckError(!mProcessGroup, INVALID_OPERATION, "Can't encode due to null pg.");
+
+    int ret = OK;
+    int terminalCount = ia_css_process_group_get_terminal_count(mProcessGroup);
+    ia_css_terminal_t* programControlInitTerminal = nullptr;
+    for (int i = 0; i < terminalCount; i++) {
+        ia_css_terminal_t* terminal = ia_css_process_group_get_terminal(mProcessGroup, i);
+        CheckError(!terminal, UNKNOWN_ERROR, "failed to get terminal");
+        if (!payloads[terminal->tm_index].data) {
+            continue;
+        }
+
+        // Encode that terminal at last
+        if (terminal->tm_index == mProgramControlInitTerminalIndex) {
+            programControlInitTerminal = terminal;
+            continue;
+        }
+
+        ret = encodeTerminal(terminal, payloads[terminal->tm_index]);
+        CheckError(ret != OK, ret, "Failed to encode for terminal %d.", terminal->tm_index);
+    }
+    if (programControlInitTerminal) {
+        ret = encodeTerminal(programControlInitTerminal,
+                             payloads[programControlInitTerminal->tm_index]);
+        CheckError(ret != OK, ret, "Failed to encode for program control init terminal %d.",
+                   programControlInitTerminal->tm_index);
+    }
+
+    return ret;
+}
+
+int IntelPGParam::encodeTerminal(ia_css_terminal_t* terminal, ia_binary_data payload) {
+    int ret = OK;
+
+    int terminalIndex = terminal->tm_index;
+    if (mPgReqs.terminals[terminalIndex].type == IA_CSS_TERMINAL_TYPE_PROGRAM_CONTROL_INIT) {
+        unsigned int kupSize = 0;
+        ret = ia_p2p_get_kernel_user_parameter_size(mP2pHandle, mPgId, mFragmentCount, &kupSize);
+        CheckError(ret != ia_err_none, ret,
+                   "Failed to call ia_p2p_get_kernel_user_parameter_size.");
+        if (kupSize != mPgReqs.terminals[terminalIndex].userParamSize) {
+            mPgReqs.terminals[terminalIndex].userParamSize = kupSize;
+            mPgReqs.terminals[terminalIndex].userParamAddress =
+                std::unique_ptr<uint8_t[]>(new uint8_t[kupSize]);
+        }
+
+        if (mFragmentConfig) {
+            ret = ia_p2p_get_kernel_user_parameters_v2(
+                mP2pHandle, mPgId, mFragmentCount, mFragmentConfig,
+                mPgReqs.terminals[terminalIndex].userParamAddress.get());
+            CheckError(ret != ia_err_none, ret,
+                       "Failed to call ia_p2p_get_kernel_user_parameters_v2.");
+        } else {
+            ret = ia_p2p_get_kernel_user_parameters(
+                mP2pHandle, mPgId, mFragmentCount, mFragmentDesc,
+                mPgReqs.terminals[terminalIndex].userParamAddress.get());
+            CheckError(ret != ia_err_none, ret,
+                       "Failed to call ia_p2p_get_kernel_user_parameters.");
+        }
+
+        ia_css_kernel_user_param_t* userParam = reinterpret_cast<ia_css_kernel_user_param_t*>(
+            mPgReqs.terminals[terminalIndex].userParamAddress.get());
+        ret = pg_control_init_fill_payload(mProcessGroup, userParam, payload.data);
+        CheckError(ret != ia_err_none, ret, "Failed to call pg_control_init_fill_payload.");
+        return ret;
+    }
+
+    ia_css_kernel_bitmap_t kernelBitmap = mPgReqs.terminals[terminalIndex].kernelBitmap;
+    uint16_t kernelId = 0;
+    uint8_t kernelIndex = 0;
+    unsigned int curSection = 0;
+    unsigned int curOffset = 0;
+    ia_p2p_payload_desc tmpPayloadDesc;
+    while (!ia_css_is_kernel_bitmap_empty(kernelBitmap)) {
+        /* Use specific ordering of kernels when available */
+        if (mPgReqs.terminals[terminalIndex].kernelOrder) {
+            kernelId = mPgReqs.terminals[terminalIndex].kernelOrder[kernelIndex++].id;
+            if (kernelId >= PSYS_MAX_KERNELS_PER_PG) {
+                /* All the kernels have now been encoded. */
+                break;
+            }
+            /* Initialize parameter payload for current kernel with zeros in
+             * case P2P has reported less sections for the kernel */
+            if (mPgReqs.terminals[terminalIndex].kernelOrder[kernelIndex - 1].initialize) {
+                LOG2("%s: initializing kernel %d payload in terminal %d (offset:%d, size:%d)",
+                     __func__, kernelId, terminalIndex, curOffset,
+                     mPgReqs.terminals[terminalIndex].kernelOrder[kernelIndex - 1].size);
+                unsigned char* start = reinterpret_cast<unsigned char*>(payload.data);
+                memset(start + curOffset, 0,
+                       mPgReqs.terminals[terminalIndex].kernelOrder[kernelIndex - 1].size);
+            }
+        } else {
+            kernelId = getKernelIdByBitmap(kernelBitmap);
+        }
+
+        /* Sanity check sections sizes and return the size to be used */
+        css_err_t result = payloadSectionSizeSanityTest(&tmpPayloadDesc, kernelId, terminalIndex,
+                                                        curOffset, payload.size);
+        CheckError((result != css_err_none), UNKNOWN_ERROR,
+                   "Failed sanity check of terminal payload sizes");
+
+        switch (mPgReqs.terminals[terminalIndex].type) {
+            case IA_CSS_TERMINAL_TYPE_PARAM_CACHED_IN:
+                ret = ia_p2p_param_in_terminal_encode(
+                    mP2pHandle, mPgId, kernelId,
+                    reinterpret_cast<ia_css_param_terminal_t*>(terminal), curSection,
+                    reinterpret_cast<uint8_t*>(payload.data), payload.size, curOffset);
+                CheckError(ret != ia_err_none, ret, "Failed to encode param in terminal.");
+
+                curSection += mKernel.mSections[kernelId].param_in_section_count;
+                curOffset += tmpPayloadDesc.param_in_payload_size;
+                break;
+
+            case IA_CSS_TERMINAL_TYPE_PARAM_CACHED_OUT:
+                ret = ia_p2p_param_out_terminal_prepare(
+                    mP2pHandle, mPgId, kernelId, mFragmentCount,
+                    (mFragmentConfig ? mFragmentConfig->pixel_fragment_descs[kernelId]
+                                     : mFragmentDesc),
+                    reinterpret_cast<ia_css_param_terminal_t*>(terminal), curSection,
+                    mPgReqs.terminals[terminalIndex].sectionCount, payload.size, curOffset);
+                CheckError(ret != ia_err_none, ret, "Failed to prepare param out terminal.");
+
+                curSection += mKernel.mSections[kernelId].param_out_section_count_per_fragment;
+                curOffset += tmpPayloadDesc.param_out_payload_size;
+                break;
+
+            case IA_CSS_TERMINAL_TYPE_PROGRAM:
+#if defined(IPU_SYSVER_IPU6) && defined(UNIFIED_PROG_TERM_FRAG_DESC)
+                reinterpret_cast<ia_css_program_terminal_t*>(terminal)->payload_fragment_stride =
+                    mPgReqs.terminals[terminalIndex].payloadSize / mFragmentCount;
+#endif
+                ret = ia_p2p_program_terminal_encode(
+                    mP2pHandle, mPgId, kernelId, mFragmentCount,
+                    (mFragmentConfig ? mFragmentConfig->pixel_fragment_descs[kernelId]
+                                     : mFragmentDesc),
+                    reinterpret_cast<ia_css_program_terminal_t*>(terminal), curSection,
+                    mPgReqs.terminals[terminalIndex].sectionCount,
+                    reinterpret_cast<uint8_t*>(payload.data), payload.size, curOffset);
+                CheckError(ret != ia_err_none, ret, "Failed to encode program terminal.");
+
+                curSection += mKernel.mSections[kernelId].program_section_count_per_fragment;
+                curOffset += tmpPayloadDesc.program_payload_size;
+                break;
+
+            case IA_CSS_TERMINAL_TYPE_PARAM_SPATIAL_IN:
+                /* TODO: ensure program terminal gets encoded first */
+                ret = ia_p2p_spatial_param_in_terminal_encode(
+                    mP2pHandle, mPgId, kernelId, mFragmentCount,
+                    (mFragmentConfig ? mFragmentConfig->pixel_fragment_descs[kernelId]
+                                     : mFragmentDesc),
+                    reinterpret_cast<ia_css_spatial_param_terminal_t*>(terminal), curSection,
+                    reinterpret_cast<uint8_t*>(payload.data), payload.size, curOffset);
+                CheckError(ret != ia_err_none, ret, "Failed to encode spatial in terminal.");
+
+                curOffset += tmpPayloadDesc.spatial_param_in_payload_size;
+                curSection += mKernel.mSections[kernelId].spatial_param_in_section_count;
+                break;
+
+            case IA_CSS_TERMINAL_TYPE_PARAM_SPATIAL_OUT:
+                ret = ia_p2p_spatial_param_out_terminal_prepare(
+                    mP2pHandle, mPgId, kernelId, mFragmentCount,
+                    (mFragmentConfig ? mFragmentConfig->pixel_fragment_descs[kernelId]
+                                     : mFragmentDesc),
+                    reinterpret_cast<ia_css_spatial_param_terminal_t*>(terminal), curSection,
+                    payload.size, curOffset);
+                CheckError(ret != ia_err_none, ret, "Failed to prepare spatial out terminal.");
+
+                curOffset += tmpPayloadDesc.spatial_param_out_payload_size;
+                curSection += mKernel.mSections[kernelId].spatial_param_out_section_count;
+                break;
+
+            case IA_CSS_TERMINAL_TYPE_DATA_IN:
+            case IA_CSS_TERMINAL_TYPE_DATA_OUT:
+                /* No encode done for frame terminals */
+                break;
+
+            default:
+                LOGE("%s: terminal type %d encode not implemented", __func__,
+                     mPgReqs.terminals[terminalIndex].type);
+                return UNKNOWN_ERROR;
+        }
+
+        if (!mPgReqs.terminals[terminalIndex].kernelOrder) {
+            kernelBitmap = ia_css_kernel_bitmap_unset(kernelBitmap, kernelId);
+        }
+    }
+
+    return ret;
+}
+
+int IntelPGParam::decode(int payloadCount, ia_binary_data* payload, ia_binary_data* statistics) {
+    CheckError(!mProcessGroup, INVALID_OPERATION, "Can't decode due to null pg.");
+    CheckError(!payload, INVALID_OPERATION, "nullptr payload.");
+
+    if (statistics && statistics->data) {
+        ia_p2p_set_statistics_buffer(mP2pHandle, statistics->data);
+    }
+    int ret = OK;
+    int terminalCount = ia_css_process_group_get_terminal_count(mProcessGroup);
+    for (int i = 0; i < terminalCount; i++) {
+        ia_css_terminal_t* terminal = ia_css_process_group_get_terminal(mProcessGroup, i);
+        CheckError(!terminal, UNKNOWN_ERROR, "failed to get terminal");
+        if ((terminal->terminal_type != IA_CSS_TERMINAL_TYPE_PARAM_CACHED_OUT) &&
+            (terminal->terminal_type != IA_CSS_TERMINAL_TYPE_PARAM_SPATIAL_OUT)) {
+            continue;
+        }
+        CheckError(terminal->tm_index >= payloadCount, UNKNOWN_ERROR,
+                   "no payload for term %d decoding", terminal->tm_index);
+        ret = decodeTerminal(terminal, payload[terminal->tm_index]);
+        CheckError(ret != OK, ret, "%s, call p2p decode fail", __func__);
+    }
+
+    return serializeDecodeCache(statistics);
+}
+
+int IntelPGParam::decodeTerminal(ia_css_terminal_t* terminal, ia_binary_data payload) {
+    int ret = OK;
+    int terminalIndex = terminal->tm_index;
+    unsigned int currentSection = 0;
+    int kernelIndex = 0;
+    uint16_t kernelId;
+    ia_css_kernel_bitmap_t kernelBitmap = mPgReqs.terminals[terminal->tm_index].kernelBitmap;
+    while (!ia_css_is_kernel_bitmap_empty(kernelBitmap)) {
+        /* Use specific ordering of kernels when available */
+        if (mPgReqs.terminals[terminalIndex].kernelOrder) {
+            kernelId = mPgReqs.terminals[terminalIndex].kernelOrder[kernelIndex++].id;
+            CheckError(kernelId >= PSYS_MAX_KERNELS_PER_PG, css_err_internal,
+                       "%s: Kernel bitmap for terminal %d covers more kernels than in manifest",
+                       __func__, terminalIndex);
+        } else {
+            kernelId = getKernelIdByBitmap(kernelBitmap);
+        }
+
+        switch (mPgReqs.terminals[terminalIndex].type) {
+            case IA_CSS_TERMINAL_TYPE_PARAM_CACHED_OUT:
+                ret = ia_p2p_param_out_terminal_decode(
+                    mP2pHandle, mPgId, kernelId, mFragmentCount,
+                    reinterpret_cast<ia_css_param_terminal_t*>(terminal), currentSection,
+                    mPgReqs.terminals[terminalIndex].sectionCount,
+                    reinterpret_cast<unsigned char*>(payload.data), payload.size);
+                currentSection += mKernel.mSections[kernelId].param_out_section_count_per_fragment;
+                break;
+            case IA_CSS_TERMINAL_TYPE_PARAM_SPATIAL_OUT:
+                ret = ia_p2p_spatial_param_out_terminal_decode_v2(
+                    mP2pHandle, mPgId, kernelId, mFragmentCount,
+                    (mFragmentConfig ? mFragmentConfig->pixel_fragment_descs[kernelId]
+                                     : mFragmentDesc),
+                    reinterpret_cast<ia_css_spatial_param_terminal_t*>(terminal), currentSection,
+                    (unsigned char*)payload.data, payload.size, mP2pCacheBuffer.data);
+                currentSection += mKernel.mSections[kernelId].spatial_param_out_section_count;
+                break;
+            default:
+                LOGE("%s: terminal type %d decode not implemented", __func__,
+                     mPgReqs.terminals[terminalIndex].type);
+                return UNKNOWN_ERROR;
+        }
+
+        CheckError(ret != ia_err_none, ret, "%s: failed to decode terminal %d", __func__,
+                   terminalIndex);
+        kernelBitmap = ia_css_kernel_bitmap_unset(kernelBitmap, kernelId);
+    }
+
+    return ret;
+}
+
+int IntelPGParam::serializeDecodeCache(ia_binary_data* result) {
+    CheckError(!result, UNKNOWN_ERROR, "The statistics buffer is nullptr");
+
+    ia_err ia_ret = ia_p2p_serialize_statistics(mP2pHandle, result, nullptr);
+    CheckError(ia_ret != ia_err_none, UNKNOWN_ERROR, "Serializ statistics fail");
+
+    return OK;
+}
+
+void IntelPGParam::deinit() {
+    ia_p2p_deinit(mP2pHandle);
+    if (mP2pCacheBuffer.data) {
+        CIPR::freeMemory(mP2pCacheBuffer.data);
+    }
+}
+
+int IntelPGParam::getKernelIdByBitmap(ia_css_kernel_bitmap_t bitmap) {
+    int kernelId = 0;
+    CheckError(ia_css_is_kernel_bitmap_empty(bitmap), BAD_VALUE, "The bitmap is empty");
+    while (!ia_css_is_kernel_bitmap_set(bitmap, (unsigned int)kernelId)) {
+        kernelId++;
+    }
+
+    return kernelId;
+}
+
+ia_css_kernel_bitmap_t IntelPGParam::getCachedTerminalKernelBitmap(
+    ia_css_param_terminal_manifest_t* manifest) {
+    ia_css_kernel_bitmap_t kernelBitmap = ia_css_kernel_bitmap_clear();
+    unsigned int section, sectionCount;
+
+    /* Loop through all the sections in manifest and put the kernel ids into the kernel bitmap. */
+    sectionCount = manifest->param_manifest_section_desc_count;
+    for (section = 0; section < sectionCount; section++) {
+        ia_css_param_manifest_section_desc_t* desc =
+            ia_css_param_terminal_manifest_get_prm_sct_desc(manifest, section);
+        CheckError(!desc, kernelBitmap, "failed to get desc");
+        int index = ia_css_param_manifest_section_desc_get_kernel_id(desc);
+        kernelBitmap = ia_css_kernel_bitmap_set(kernelBitmap, index);
+    }
+
+    return kernelBitmap;
+}
+
+ia_css_kernel_bitmap_t IntelPGParam::getProgramTerminalKernelBitmap(
+    ia_css_program_terminal_manifest_t* manifest) {
+    ia_css_kernel_bitmap_t kernelBitmap = ia_css_kernel_bitmap_clear();
+    unsigned int section, sectionCount;
+
+    /* Loop through all the sections in manifest and put the kernel ids into the kernel bitmap. */
+    sectionCount = manifest->fragment_param_manifest_section_desc_count;
+    for (section = 0; section < sectionCount; section++) {
+        ia_css_fragment_param_manifest_section_desc_t* desc =
+            ia_css_program_terminal_manifest_get_frgmnt_prm_sct_desc(manifest, section);
+        CheckError(!desc, kernelBitmap, "failed to get desc");
+        int index = ia_css_fragment_param_manifest_section_desc_get_kernel_id(desc);
+        kernelBitmap = ia_css_kernel_bitmap_set(kernelBitmap, index);
+    }
+
+    return kernelBitmap;
+}
+
+int IntelPGParam::disableZeroSizedTerminals(ia_css_kernel_bitmap_t* kernelBitmap) {
+    int ret = OK;
+    ia_css_terminal_type_t terminalType;
+    ia_css_kernel_bitmap_t terminalKernelsBitmap = ia_css_kernel_bitmap_clear();
+    ia_css_kernel_bitmap_t disabledTerminalKernelsBitmap = ia_css_kernel_bitmap_clear();
+    for (int i = 0; i < mTerminalCount; i++) {
+        terminalKernelsBitmap = ia_css_kernel_bitmap_clear();
+        unsigned int payloadSize = mPgReqs.terminals[i].payloadSize;
+        ia_css_terminal_manifest_t* manifest =
+            ia_css_program_group_manifest_get_term_mnfst(mPgManifest, i);
+        terminalType = ia_css_terminal_manifest_get_type(manifest);
+
+        if (payloadSize == 0) {
+            switch (terminalType) {
+                case IA_CSS_TERMINAL_TYPE_PARAM_SPATIAL_IN:
+                case IA_CSS_TERMINAL_TYPE_PARAM_SPATIAL_OUT: {
+                    /* Spatial terminals are only associated to a single kernel. */
+                    ia_css_spatial_param_terminal_manifest_t* paramMani =
+                        reinterpret_cast<ia_css_spatial_param_terminal_manifest_t*>(manifest);
+                    terminalKernelsBitmap =
+                        ia_css_kernel_bitmap_set(terminalKernelsBitmap, paramMani->kernel_id);
+                    break;
+                }
+                case IA_CSS_TERMINAL_TYPE_PARAM_CACHED_IN:
+                case IA_CSS_TERMINAL_TYPE_PARAM_CACHED_OUT: {
+                    ia_css_param_terminal_manifest_t* paramMani =
+                        reinterpret_cast<ia_css_param_terminal_manifest_t*>(manifest);
+                    terminalKernelsBitmap = getCachedTerminalKernelBitmap(paramMani);
+                    break;
+                }
+                case IA_CSS_TERMINAL_TYPE_PROGRAM: {
+                    ia_css_program_terminal_manifest_t* proMani =
+                        reinterpret_cast<ia_css_program_terminal_manifest_t*>(manifest);
+                    terminalKernelsBitmap = getProgramTerminalKernelBitmap(proMani);
+                    break;
+                }
+                case IA_CSS_TERMINAL_TYPE_PROGRAM_CONTROL_INIT:
+                    LOG1("%s: program control init terminal is always enabled.", __func__);
+                    break;
+                default:
+                    break;
+            }
+            disabledTerminalKernelsBitmap =
+                ia_css_kernel_bitmap_union(disabledTerminalKernelsBitmap, terminalKernelsBitmap);
+        }
+    }
+
+    *kernelBitmap = ia_css_kernel_bitmap_intersection(
+        *kernelBitmap, ia_css_kernel_bitmap_complement(disabledTerminalKernelsBitmap));
+    return ret;
+}
+
+css_err_t IntelPGParam::getKernelOrderForParamCachedInTerm(
+    ia_css_param_terminal_manifest_t* terminalManifest, IpuPgTerminalKernelInfo* kernelOrder) {
+    CheckError((!terminalManifest || !kernelOrder), ia_err_argument, "No manifest or order info");
+
+    uint16_t sectionCount = terminalManifest->param_manifest_section_desc_count;
+    CheckError(sectionCount == 0, css_err_argument, "No static sections in manifest");
+    uint8_t kernelCount = 0;
+
+    for (uint16_t section = 0; section < sectionCount; section++) {
+        ia_css_param_manifest_section_desc_t* param =
+            ia_css_param_terminal_manifest_get_prm_sct_desc(terminalManifest, section);
+        CheckError(!param, css_err_internal, "Failed to get param from terminal manifest!");
+
+        /* there is agreement that sections of the same kernel are
+         * encoded in a row. Here, skipping sections of the same kernel
+         * based on this assumption.
+         */
+        /* info: Indication of the kernel this parameter belongs to,
+         * may stand for mem_type, region and kernel_id for ipu6
+         */
+        int index = ia_css_param_manifest_section_desc_get_kernel_id(param);
+        if (kernelCount > 0 && kernelOrder[kernelCount - 1].id == index) {
+            ++kernelOrder[kernelCount - 1].sections;
+            kernelOrder[kernelCount - 1].size += param->max_mem_size;
+            continue;
+        }
+        kernelOrder[kernelCount].id = (uint8_t)index;
+        kernelOrder[kernelCount].sections = 1;
+        kernelOrder[kernelCount].size = param->max_mem_size;
+        kernelOrder[kernelCount].initialize = false;
+        kernelCount++;
+    }
+
+    return css_err_none;
+}
+
+css_err_t IntelPGParam::getKernelOrderForProgramTerm(
+    ia_css_program_terminal_manifest_t* terminalManifest, IpuPgTerminalKernelInfo* kernelOrder) {
+    CheckError((!terminalManifest || !kernelOrder), css_err_argument, "No manifest or order info");
+    uint16_t sectionCount = terminalManifest->fragment_param_manifest_section_desc_count;
+    CheckError(sectionCount == 0, ia_err_internal, "No static sections in manifest");
+    uint8_t kernelCount = 0;
+
+    for (uint16_t section = 0; section < sectionCount; section++) {
+        ia_css_fragment_param_manifest_section_desc_t* param =
+            ia_css_program_terminal_manifest_get_frgmnt_prm_sct_desc(terminalManifest, section);
+        CheckError(!param, css_err_internal, "Failed to get param from terminal manifest!");
+
+        /* there is agreement that sections of the same kernel are
+         * encoded in a row. Here, skipping sections of the same kernel
+         * based on this assumption.
+         */
+        /* info: Indication of the kernel this parameter belongs to,
+         * may stand for mem_type, region and kernel_id for ipu6
+         */
+        int index = ia_css_fragment_param_manifest_section_desc_get_kernel_id(param);
+        if (kernelCount > 0 && kernelOrder[kernelCount - 1].id == index) {
+            ++kernelOrder[kernelCount - 1].sections;
+            kernelOrder[kernelCount - 1].size += param->max_mem_size;
+            continue;
+        }
+        kernelOrder[kernelCount].id = (uint8_t)index;
+        kernelOrder[kernelCount].sections = 1;
+        kernelOrder[kernelCount].size = param->max_mem_size;
+        kernelOrder[kernelCount].initialize = false;
+        kernelCount++;
+    }
+
+    return css_err_none;
+}
+
+int8_t IntelPGParam::terminalEnumerateByType(IpuPgRequirements* reqs,
+                                             ia_css_terminal_type_t terminalType, uint8_t num) {
+    CheckError(reqs->terminalCount == 0, -1, "%s: no terminals!", __func__);
+
+    for (uint8_t terminal = 0; terminal < reqs->terminalCount; terminal++) {
+        if (reqs->terminals[terminal].type == terminalType) {
+            if (num)
+                num--;
+            else
+                return (int8_t)terminal;
+        }
+    }
+
+    return -1;
+}
+
+int8_t IntelPGParam::terminalEnumerateByBitmap(IpuPgRequirements* reqs,
+                                               ia_css_terminal_type_t terminal_type,
+                                               ia_css_kernel_bitmap_t bitmap) {
+    CheckError(reqs->terminalCount == 0, -1, "%s: no terminals!", __func__);
+
+    for (uint8_t terminal = 0; terminal < reqs->terminalCount; terminal++) {
+        if (reqs->terminals[terminal].type == terminal_type &&
+            ia_css_is_kernel_bitmap_equal(reqs->terminals[terminal].kernelBitmap, bitmap)) {
+            return (int8_t)terminal;
+        }
+    }
+
+    return -1;
+}
+
+bool IntelPGParam::isKernelIdInKernelOrder(IpuPgRequirements* reqs, int8_t termIndex, int kernelId,
+                                           uint8_t* orderedIndex) {
+    /* No kernel order, return true always */
+    if (!reqs->terminals[termIndex].kernelOrder) return true;
+
+    /* Check if the kernel_id can be found from the kernelOrder */
+    for (uint8_t i = 0; i < PSYS_MAX_KERNELS_PER_PG; i++) {
+        if (reqs->terminals[termIndex].kernelOrder[i].id == kernelId) {
+            if (orderedIndex) *orderedIndex = i;
+            return true;
+        }
+    }
+
+    return false;
+}
+
+uint32_t IntelPGParam::getKernelCountFromKernelOrder(IpuPgRequirements* reqs, int8_t termIndex,
+                                                     int kernelId) {
+    if (!reqs->terminals[termIndex].kernelOrder) {
+        /* If no kernel order is present, assuming kernel appears once. */
+        return 1;
+    }
+
+    uint32_t count = 0;
+    for (int i = 0; i < PSYS_MAX_KERNELS_PER_PG; i++) {
+        if (reqs->terminals[termIndex].kernelOrder[i].id == kernelId) {
+            ++count;
+        }
+    }
+
+    return count;
+}
+
+void IntelPGParam::processTerminalKernelRequirements(IpuPgRequirements* reqs, int8_t termIndex,
+                                                     ia_css_terminal_type_t terminalType,
+                                                     int kernelId) {
+    uint32_t kernelCount = getKernelCountFromKernelOrder(reqs, termIndex, kernelId);
+    uint32_t sectionCount = 0, payloadSize = 0;
+#if defined(IPU_SYSVER_IPU6) && defined(UNIFIED_PROG_TERM_FRAG_DESC)
+    uint32_t multiplier = 1;
+#endif
+    for (unsigned int i = 0; i < kernelCount; ++i) {
+        switch (terminalType) {
+            case IA_CSS_TERMINAL_TYPE_PARAM_CACHED_IN:
+                sectionCount = mKernel.mSections[kernelId].param_in_section_count;
+                payloadSize = mKernel.mPayloads[kernelId].param_in_payload_size;
+                break;
+            case IA_CSS_TERMINAL_TYPE_PARAM_CACHED_OUT:
+                sectionCount = mKernel.mSections[kernelId].param_out_section_count_per_fragment;
+#if defined(IPU_SYSVER_IPU6) && defined(UNIFIED_PROG_TERM_FRAG_DESC)
+                payloadSize = mKernel.mPayloads[kernelId].param_out_payload_size * mFragmentCount;
+#else
+                payloadSize = mKernel.mPayloads[kernelId].param_out_payload_size;
+#endif
+                break;
+            case IA_CSS_TERMINAL_TYPE_PROGRAM:
+                sectionCount = mKernel.mSections[kernelId].program_section_count_per_fragment;
+                payloadSize = mKernel.mPayloads[kernelId].program_payload_size;
+#if defined(IPU_SYSVER_IPU6) && defined(UNIFIED_PROG_TERM_FRAG_DESC)
+                multiplier = mFragmentCount;
+#endif
+                break;
+            default:
+                LOG1("%s: terminal type %d encode not implemented", __func__, terminalType);
+                break;
+        }
+        reqs->terminals[termIndex].sectionCount += sectionCount;
+
+#if defined(IPU_SYSVER_IPU6) && defined(UNIFIED_PROG_TERM_FRAG_DESC)
+        reqs->terminals[termIndex].payloadSize += multiplier * payloadSize;
+#else
+        reqs->terminals[termIndex].payloadSize += payloadSize;
+#endif
+
+        mKernel.mPayloadSize = reqs->terminals[termIndex].payloadSize;
+    }
+
+    reqs->terminals[termIndex].kernelBitmap =
+        ia_css_kernel_bitmap_set(reqs->terminals[termIndex].kernelBitmap, (unsigned int)kernelId);
+}
+
+css_err_t IntelPGParam::payloadSectionSizeSanityTest(ia_p2p_payload_desc* current,
+                                                     uint16_t kernelId, uint8_t terminalIndex,
+                                                     uint32_t currentOffset, size_t payloadSize) {
+    size_t nextPayloadSize = 0;
+    ia_p2p_payload_desc init = mKernel.mPayloads[kernelId];
+    /* calculate again the memory requirements for each kernel
+     * and compare it with what we stored at init time. */
+    ia_err ia_ret = ia_p2p_get_kernel_payload_desc(
+        mP2pHandle, mPgId, kernelId,
+#if defined(IPU_SYSVER_IPU6) && defined(UNIFIED_PROG_TERM_FRAG_DESC)
+        1,
+#else
+        mFragmentCount,
+#endif
+        (mFragmentConfig ? mFragmentConfig->pixel_fragment_descs[kernelId] : mFragmentDesc),
+        current);
+    CheckError(ia_ret != ia_err_none, css_err_internal,
+               "Failed to get payload description during sanity check (kernel %d)", kernelId);
+
+    switch (mPgReqs.terminals[terminalIndex].type) {
+        case IA_CSS_TERMINAL_TYPE_PARAM_CACHED_IN:
+            if (current->param_in_payload_size > init.param_in_payload_size) {
+                LOGW(
+                    "%s: param-in section size mismatch in pg[%d] kernel[%d]"
+                    " p2p size %d pg_die size %d",
+                    __func__, mPgId, kernelId, current->param_in_payload_size,
+                    init.param_in_payload_size);
+            } else {
+                current->param_in_payload_size = init.param_in_payload_size;
+            }
+            nextPayloadSize = current->param_in_payload_size;
+            break;
+        case IA_CSS_TERMINAL_TYPE_PARAM_CACHED_OUT:
+            if (current->param_out_payload_size > init.param_out_payload_size) {
+                LOGW(
+                    "%s: param-out section size mismatch in pg[%d] kernel[%d]"
+                    " p2p size %d pg_die size %d",
+                    __func__, mPgId, kernelId, current->param_out_payload_size,
+                    init.param_out_payload_size);
+            } else {
+                current->param_out_payload_size = init.param_out_payload_size;
+            }
+            nextPayloadSize = current->param_out_payload_size;
+            break;
+        case IA_CSS_TERMINAL_TYPE_PROGRAM:
+            if (current->program_payload_size > init.program_payload_size) {
+                LOG1(
+                    "%s: program section size mismatch in pg[%d] kernel[%d]"
+                    " p2p size %d pg_die size %d",
+                    __func__, mPgId, kernelId, current->program_payload_size,
+                    init.program_payload_size);
+            } else {
+                current->program_payload_size = init.program_payload_size;
+            }
+            nextPayloadSize = current->program_payload_size;
+            break;
+        case IA_CSS_TERMINAL_TYPE_PARAM_SPATIAL_IN:
+            if (current->spatial_param_in_payload_size > init.spatial_param_in_payload_size) {
+                LOGW(
+                    "%s: spatial-in section size mismatch in pg[%d] kernel[%d]"
+                    " p2p size %d pg_die size %d",
+                    __func__, mPgId, kernelId, current->spatial_param_in_payload_size,
+                    init.spatial_param_in_payload_size);
+            } else {
+                current->spatial_param_in_payload_size = init.spatial_param_in_payload_size;
+            }
+            nextPayloadSize = current->spatial_param_in_payload_size;
+            break;
+        case IA_CSS_TERMINAL_TYPE_PARAM_SPATIAL_OUT:
+            if (current->spatial_param_out_payload_size > init.spatial_param_out_payload_size) {
+                LOGW(
+                    "%s: spatial-out section size mismatch in pg[%d] kernel[%d]"
+                    " p2p size %d pg_die size %d",
+                    __func__, mPgId, kernelId, current->spatial_param_out_payload_size,
+                    init.spatial_param_out_payload_size);
+            } else {
+                current->spatial_param_out_payload_size = init.spatial_param_out_payload_size;
+            }
+            nextPayloadSize = current->spatial_param_out_payload_size;
+            break;
+        case IA_CSS_TERMINAL_TYPE_DATA_IN:
+        case IA_CSS_TERMINAL_TYPE_DATA_OUT:
+        case IA_CSS_TERMINAL_TYPE_PROGRAM_CONTROL_INIT:
+            /* No check done for frame terminals */
+            break;
+        default:
+            LOGE("%s: terminal type %d payload check not implemented", __func__,
+                 mPgReqs.terminals[terminalIndex].type);
+            return css_err_argument;
+    }
+
+    CheckError(
+        (currentOffset + nextPayloadSize > payloadSize), css_err_nomemory,
+        "pg %d terminal %d payload size small, encoding for kernel %d will exceed size by %u bytes",
+        mPgId, terminalIndex, kernelId, currentOffset + nextPayloadSize - payloadSize);
+    return css_err_none;
+}
+
+void IntelPGParam::dumpFragmentDesc(int fragmentCount) {
+    if (!Log::isDebugLevelEnable(CAMERA_DEBUG_LOG_LEVEL2)) return;
+
+    LOG2("%s: pg %d get frag count %d (new api)", __func__, mPgId, fragmentCount);
+    for (int kernel = 0; kernel < IA_P2P_MAX_KERNELS_PER_PG; kernel++) {
+        for (int frag = 0; frag < fragmentCount; frag++) {
+            LOG2("   kernel %d, frag %d: [%d %d %d %d]", kernel, frag,
+                 mFragmentConfig->pixel_fragment_descs[kernel][frag].fragment_width,
+                 mFragmentConfig->pixel_fragment_descs[kernel][frag].fragment_height,
+                 mFragmentConfig->pixel_fragment_descs[kernel][frag].fragment_start_x,
+                 mFragmentConfig->pixel_fragment_descs[kernel][frag].fragment_start_y);
+        }
+    }
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelPGParam.h b/camera/hal/intel/ipu6/modules/algowrapper/IntelPGParam.h
new file mode 100644
index 000000000000..154704d36c22
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelPGParam.h
@@ -0,0 +1,237 @@
+/*
+ * Copyright (C) 2018-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+extern "C" {
+#include <ia_css_program_group_data.h>
+#include <ia_css_program_group_param.h>
+#include <ia_css_psys_process_group.h>
+#include <ia_css_psys_program_group_manifest.h>
+#include <ia_css_psys_terminal.h>
+#include <ia_css_psys_terminal_manifest.h>
+#include <ia_css_terminal_manifest_types.h>
+#include <ia_css_terminal_types.h>
+#include <ia_isp_bxt.h>
+#include <ia_isp_types.h>
+#include <ia_p2p.h>
+#include <ia_p2p_types.h>
+#include <ia_pal_types_isp_ids_autogen.h>
+#include <pg_control_init_framework.h>
+}
+
+#include <map>
+#include <memory>
+#include <vector>
+
+#include "ia_tools/css_types.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+#include "modules/ia_cipr/include/Utils.h"
+#include "src/core/psysprocessor/PGUtils.h"
+
+namespace icamera {
+
+#define PSYS_MAX_KERNELS_PER_PG IA_CSS_KERNEL_BITMAP_BITS
+
+/**
+ * \class IntelPGParam
+ *
+ * \brief This is a version P2P implementation which is used to encode parameter terminal
+ *        and decode statistic terminal for PSYS pipeline.
+ *
+ * The call sequence as follows:
+ * 1. init();
+ * 2. prepare();
+ * 3. allocatePGBuffer() (optional);
+ * 4. setPGAndPrepareProgram();
+ * 5. allocatePayloads();
+ * 6. getFragmentDescriptors();
+ * 7. loop frame {
+ *      updatePALAndEncode();
+ *      decode();
+ *    }
+ * 8. deinit();
+ */
+class IntelPGParam {
+ public:
+    explicit IntelPGParam(int pgId);
+    ~IntelPGParam();
+
+    /**
+     * Use to init and config P2P handle.
+     */
+    int init(ia_p2p_platform_t platform, const PgConfiguration& Pgconfiguration);
+
+    /**
+     * Query and save the requirement for each terminal, calculate the final kernel bitmap.
+     */
+    int prepare(const ia_binary_data* ipuParameters, const ia_css_rbm_t* rbm,
+                ia_css_kernel_bitmap_t* bitmap, uint32_t* maxStatsSize = nullptr);
+
+    /**
+     * Allocate PG buffer for caller
+     */
+    void* allocatePGBuffer(int pgSize);
+
+    /**
+     * Accept pg outside and init program_control_init terminal.
+     */
+    int setPGAndPrepareProgram(ia_css_process_group_t* pg);
+
+    /**
+     * Allocate payload memory for terminals, and return valid terminals count.
+     */
+    int allocatePayloads(int payloadCount, ia_binary_data* payloads);
+
+    /**
+     * Update PAL and encode payload data for all terminals. Will skip inactive terminals.
+     */
+    int updatePALAndEncode(const ia_binary_data* ipuParams, int payloadCount,
+                           ia_binary_data* payloads);
+
+    /**
+     * Decode payload data for all related terminals.
+     */
+    int decode(int payloadCount, ia_binary_data* payload, ia_binary_data* statistics);
+
+    /**
+     * Use to deinit P2P handle.
+     */
+    void deinit();
+
+    /**
+     * Get fragment descriptors calculated according to PAL data
+     * Called after prepare().
+     */
+    int getFragmentDescriptors(int descCount, ia_p2p_fragment_desc* descs);
+
+    /**
+     * Get payload size for all terminals, and return valid payloads number.
+     */
+    int getPayloadSizes(int payloadCount, ia_binary_data* payloads);
+
+ private:
+    enum FragmentDataTerminalType {
+        FRAG_TERM_TYPE_INPUT = 0,
+        FRAG_TERM_TYPE_OUTPUT_START,  // Mapping to data out terminal in order for all postgdc pgs.
+        FRAG_TERM_TYPE_DISPALY_OUTPUT = FRAG_TERM_TYPE_OUTPUT_START,
+        FRAG_TERM_TYPE_MAIN_OUTPUT,
+        FRAG_TERM_TYPE_PP_OUTPUT,
+        FRAG_TERM_TYPE_COUNT,
+    };
+
+    int mPgId;
+    int mTerminalCount;
+    PgFrameDesc mInputMainFrame;
+    PgFrameDesc mOutputMainFrame;
+
+    uint8_t mFragmentCount;
+    ia_p2p_fragment_desc* mFragmentDesc;
+    ia_p2p_fragment_configuration_t* mFragmentConfig;
+    // for pg fragment with new api:
+    // ia_p2p_calculate_fragments_rbm
+    // Instead of mFragmentDesc
+    ia_p2p_handle mP2pHandle;
+    ia_binary_data mP2pCacheBuffer;
+
+    ia_css_program_group_manifest_t* mPgManifest;
+    std::vector<int> mDisableDataTermials;
+    ia_css_process_group_t* mProcessGroup;
+    int mProgramControlInitTerminalIndex;
+
+    struct IpuPgTerminalKernelInfo {
+        IpuPgTerminalKernelInfo() {}
+        uint8_t id = 0;
+        uint8_t sections = 0;
+        uint32_t size = 0;
+        bool initialize = false;
+    };
+
+    struct IpuPgTerminaRequirements {
+        IpuPgTerminaRequirements() { kernelBitmap = ia_css_kernel_bitmap_clear(); }
+        ia_css_terminal_type_t type = IA_CSS_N_TERMINAL_TYPES;
+        uint32_t payloadSize = 0;
+        ia_css_kernel_bitmap_t kernelBitmap;
+        uint32_t sectionCount = 0;
+        IpuPgTerminalKernelInfo* kernelOrder = nullptr;
+        ia_p2p_fragment_desc* fragment_descs = nullptr;
+
+        // Use for program_control_init
+        uint32_t userParamSize;
+        std::unique_ptr<uint8_t[]> userParamAddress;
+    };
+
+    struct IpuPgRequirements {
+        IpuPgRequirements() {}
+        uint32_t terminalCount = 0;
+        IpuPgTerminaRequirements terminals[IPU_MAX_TERMINAL_COUNT];
+    };
+
+    struct KernelRequirement {
+        KernelRequirement() { mKernelBitmap = ia_css_kernel_bitmap_clear(); }
+        ia_p2p_terminal_requirements_t mSections[PSYS_MAX_KERNELS_PER_PG];
+        ia_p2p_payload_desc mPayloads[PSYS_MAX_KERNELS_PER_PG];
+        int mPayloadSize = 0;
+        ia_css_kernel_bitmap_t mKernelBitmap;
+    };
+
+    KernelRequirement mKernel;
+    IpuPgRequirements mPgReqs;
+
+    // Allocate them here, for sandboxing case (shared memory)
+    ia_binary_data __attribute__((aligned(PG_PAGE_SIZE))) mParamPayloads[IPU_MAX_TERMINAL_COUNT];
+    void* mProcessGroupMemory;
+
+ private:
+    int getKernelIdByBitmap(ia_css_kernel_bitmap_t bitmap);
+    ia_css_kernel_bitmap_t getCachedTerminalKernelBitmap(
+        ia_css_param_terminal_manifest_t* manifest);
+    ia_css_kernel_bitmap_t getProgramTerminalKernelBitmap(
+        ia_css_program_terminal_manifest_t* manifest);
+    int disableZeroSizedTerminals(ia_css_kernel_bitmap_t* kernelBitmap);
+    css_err_t getKernelOrderForProgramTerm(ia_css_program_terminal_manifest_t* terminalManifest,
+                                           IpuPgTerminalKernelInfo* kernelOrder);
+    css_err_t getKernelOrderForParamCachedInTerm(ia_css_param_terminal_manifest_t* terminalManifest,
+                                                 IpuPgTerminalKernelInfo* kernelOrder);
+    int8_t terminalEnumerateByType(IpuPgRequirements* reqs, ia_css_terminal_type_t terminalType,
+                                   uint8_t num);
+    int8_t terminalEnumerateByBitmap(IpuPgRequirements* reqs, ia_css_terminal_type_t terminal_type,
+                                     ia_css_kernel_bitmap_t bitmap);
+    bool isKernelIdInKernelOrder(IpuPgRequirements* reqs, int8_t termIndex, int kernelId,
+                                 uint8_t* orderedIndex);
+    uint32_t getKernelCountFromKernelOrder(IpuPgRequirements* reqs, int8_t termIndex, int kernelId);
+    void processTerminalKernelRequirements(IpuPgRequirements* reqs, int8_t termIndex,
+                                           ia_css_terminal_type_t terminalType, int kernelId);
+    css_err_t payloadSectionSizeSanityTest(ia_p2p_payload_desc* current, uint16_t kernelId,
+                                           uint8_t terminalIndex, uint32_t currentOffset,
+                                           size_t payloadSize);
+
+    int calcFragmentDescriptors(int fragmentCount, const PgFrameDesc& inputMainFrame,
+                                const PgFrameDesc& outputMainFrame, const ia_css_rbm_t* rbm);
+
+    void dumpFragmentDesc(int fragmentCount);
+    int encodeTerminal(ia_css_terminal_t* terminal, ia_binary_data payload);
+    int decodeTerminal(ia_css_terminal_t* terminal, ia_binary_data payload);
+    int serializeDecodeCache(ia_binary_data* result);
+
+    void destroyPayloads();
+    void destroyPGBuffer();
+
+    DISALLOW_COPY_AND_ASSIGN(IntelPGParam);
+};
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelTNR7US.cpp b/camera/hal/intel/ipu6/modules/algowrapper/IntelTNR7US.cpp
new file mode 100644
index 000000000000..38fc2e9af95a
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelTNR7US.cpp
@@ -0,0 +1,184 @@
+/*
+ * Copyright (C) 2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelTNR7US"
+
+#include "modules/algowrapper/IntelTNR7US.h"
+
+#include <string>
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+#define CM_SURFACE_ALIGN_SIZE 0x1000
+
+namespace icamera {
+
+IntelTNR7US::IntelTNR7US()
+        : mWidth(0),
+          mHeight(0),
+          mRefBufferSize(0),
+          mRefBufIndex(0),
+          mTnrParam(nullptr) {
+    LOG1("%s ", __func__);
+    CLEAR(mRefBufAddr);
+    CLEAR(mRefSurface);
+    std::string threadName = "IntelTNR7US";
+    mThread = std::unique_ptr<base::Thread>(new base::Thread(threadName));
+    mThread->Start();
+}
+
+IntelTNR7US::~IntelTNR7US() {
+    LOG1("%s ", __func__);
+    for (auto surface : mCMSurfaceMap) {
+        destroyCMSurface(surface.second);
+    }
+    mCMSurfaceMap.clear();
+}
+
+int IntelTNR7US::init(int width, int height) {
+    LOG1("%s size %dx%d", __func__, width, height);
+    mWidth = width;
+    mHeight = height;
+    return OK;
+}
+
+Tnr7Param* IntelTNR7US::allocTnr7ParamBuf() {
+    LOG1("%s ", __func__);
+    mTnrParam = new Tnr7Param;
+    return mTnrParam;
+}
+
+void* IntelTNR7US::allocCamBuf(uint32_t bufSize, int id) {
+    LOG1("%s id: %d", __func__, id);
+    void* buffer = nullptr;
+    int ret = posix_memalign(&buffer, getpagesize(), bufSize);
+    CheckError(ret != 0, nullptr, "%s, posix_memalign fails, ret:%d", __func__, ret);
+    CmSurface2DUP* surface = createCMSurface(buffer);
+    if (!surface) {
+        ::free(buffer);
+        return nullptr;
+    }
+    mCMSurfaceMap[buffer] = surface;
+    return buffer;
+}
+
+int IntelTNR7US::allocRefBufs(uint32_t bufSize) {
+    mRefBufferSize = bufSize;
+    for (int i = 0; i < TnrRefBufCount; i++) {
+        mRefBufAddr[i] = allocCamBuf(bufSize, MAX_BUFFER_COUNT + i);
+        // will release all buffer in freeAllBufs
+        CheckError(!mRefBufAddr[i], UNKNOWN_ERROR, "@%s, alloc reference buffer fails", __func__);
+    }
+    return OK;
+}
+
+void IntelTNR7US::freeAllBufs() {
+    for (auto surface : mCMSurfaceMap) {
+        ::free(surface.first);
+    }
+    if (mTnrParam) {
+        delete mTnrParam;
+    }
+}
+
+int IntelTNR7US::prepareSurface(void* bufAddr, int size) {
+    CheckError(size < mWidth * mHeight * 3 / 2, UNKNOWN_ERROR, "%s, invalid buffer size:%d",
+               __func__, size);
+    CmSurface2DUP* surface = createCMSurface(bufAddr);
+    CheckError(!surface, UNKNOWN_ERROR, "Failed to create CMSurface");
+    mCMSurfaceMap[bufAddr] = surface;
+
+    return OK;
+}
+
+int IntelTNR7US::runTnrFrame(const void* inBufAddr, void* outBufAddr, uint32_t inBufSize,
+                             uint32_t outBufSize, Tnr7Param* tnrParam, bool isUsrPtr) {
+    /* the outBufAddr can be the reference buffer ptr from client or usr data ptr
+     * isUsrPtr is used to identify the source */
+    if (tnrParam->bc.is_first_frame) {
+        if (isUsrPtr) {
+            // the first frame, tnr copy input buffer as reference buffer for next frame
+            MEMCPY_S(mRefBufAddr[mRefBufIndex], mRefBufferSize, inBufAddr, inBufSize);
+        } else {
+            /* when outBufAddr is from client, it is the reference buffer for next frame */
+            MEMCPY_S(outBufAddr, outBufSize, inBufAddr, inBufSize);
+        }
+    }
+
+    CmSurface2DUP* inSurface = getBufferCMSurface(const_cast<void*>(inBufAddr));
+    CheckError(!inSurface, UNKNOWN_ERROR, "Failed to get CMSurface for input buffer");
+
+    if (!isUsrPtr) {
+        // the outBuf is reference buffer ptr, get the it's surface
+        mRefSurface[mRefBufIndex] = getBufferCMSurface(outBufAddr);
+        CheckError(mRefSurface[mRefBufIndex] == nullptr, UNKNOWN_ERROR,
+                   "Failed to get CMSurface for output buffer");
+    }
+    /* call Tnr api, it will run tnr for the inSurface and store the result in refSurface[0]
+    ** in next frame, the refSurface[0] will be the reference buffer, the tnr keeps a pointer
+    ** to it, and refSurface[1] will used to store the tnr result.
+    */
+    run_tnr7us_frame(mWidth, mHeight, mWidth, inSurface, mRefSurface[mRefBufIndex],
+                     &tnrParam->scale, &tnrParam->ims, &tnrParam->bc, &tnrParam->blend,
+                     tnrParam->bc.is_first_frame);
+
+    if (isUsrPtr) {
+        /* when the outBufAddr is not from the client, then it is usr data,
+         * copy tnr result to it from reference buffer */
+        MEMCPY_S(outBufAddr, outBufSize, mRefBufAddr[mRefBufIndex], mRefBufferSize);
+    }
+    mRefBufIndex = (mRefBufIndex + 1) % TnrRefBufCount;
+
+    return OK;
+}
+
+int IntelTNR7US::asyncParamUpdate(int gain) {
+    LOG1("%s gain: %d", __func__, gain);
+    if (mThread->task_runner()) {
+        mThread->task_runner()->PostTask(
+            FROM_HERE, base::Bind(&IntelTNR7US::handleParamUpdate, base::Unretained(this), gain));
+    }
+    return OK;
+}
+
+void IntelTNR7US::handleParamUpdate(int gain) {
+    LOG1("%s gain: %d", __func__, gain);
+    // gain value is from AE expore digital_gain multi by 1000, to avoid lose accuracy
+    tnr7usParamUpdate(gain);
+}
+
+CmSurface2DUP* IntelTNR7US::getBufferCMSurface(void* bufAddr) {
+    if (mCMSurfaceMap.find(bufAddr) != mCMSurfaceMap.end()) {
+        return mCMSurfaceMap[bufAddr];
+    }
+
+    return nullptr;
+}
+
+CmSurface2DUP* IntelTNR7US::createCMSurface(void* bufAddr) {
+    LOG1("%s ", __func__);
+    CmSurface2DUP* cmSurface = nullptr;
+    int32_t ret = createCmSurface2DUP(mWidth, mHeight, CM_SURFACE_FORMAT_NV12, bufAddr, cmSurface);
+    CheckError(ret != 0, nullptr, "failed to create CmSurface2DUP object");
+    return cmSurface;
+}
+
+int32_t IntelTNR7US::destroyCMSurface(CmSurface2DUP* surface) {
+    LOG1("%s ", __func__);
+    return destroyCMSurface2DUP(surface);
+}
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/IntelTNR7US.h b/camera/hal/intel/ipu6/modules/algowrapper/IntelTNR7US.h
new file mode 100644
index 000000000000..7d3efd77985a
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/IntelTNR7US.h
@@ -0,0 +1,108 @@
+/*
+ * Copyright (C) 2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <base/threading/thread.h>
+
+#include <memory>
+#include <unordered_map>
+extern "C" {
+#include "ia_pal_types_isp_parameters_autogen.h"
+#include "tnr7_ipu3_output_interface.h"
+}
+
+#include "PlatformData.h"
+#include "TNRCommon.h"
+
+/* the cm_rt.h has some build error with current clang build flags
+ * use the ignored setting to ignore these errors, and use
+ * push/pop to make the ignore only take effect on this file */
+#pragma clang diagnostic push
+#pragma clang diagnostic ignored "-Wbitfield-constant-conversion"
+#pragma clang diagnostic ignored "-Wunused-private-field"
+// HANDLE is redefined in cm_rt.h, avoid the redefinition build error
+#define HANDLE cancel_fw_pre_define
+#include "cm_rt.h"
+#pragma clang diagnostic pop
+
+extern int run_tnr7us_frame(int width, int height, int stride, CmSurface2DUP*& inputSurface,
+                            CmSurface2DUP*& outputSurface, tnr_scale_1_0_t* dsPtr_ipu3,
+                            tnr7_ims_1_0_t* imsPtr_ipu3, tnr7_bc_1_0_t* bcPtr_ipu3,
+                            tnr7_blend_1_0_t* blendPtr_ipu3, bool first_frame);
+
+extern int32_t createCmSurface2DUP(uint32_t width, uint32_t height, CM_SURFACE_FORMAT format,
+                                   void* sysMem, CmSurface2DUP*& surface);
+
+extern int32_t destroyCMSurface2DUP(CmSurface2DUP*& surface);
+// update parameters when gain changes
+extern int tnr7usParamUpdate(int gain, bool forceUpdate = true, int type = -1);
+
+namespace icamera {
+
+// IntelTNR7US object is for using Intel GPU tnr(tnr7ultraslim) feature.
+class IntelTNR7US {
+ public:
+    IntelTNR7US();
+    ~IntelTNR7US();
+    int init(int width, int height);
+    /**
+     * call tnr api to calc tnr result
+     *
+     * \param inBufAddr: input image buffer
+     * \param outBufAddr: tnr output
+     * \param isUsrPtr: source of outBufAddr
+     * \param tnrParam: tnr parameters from ISP
+     */
+    int runTnrFrame(const void* inBufAddr, void* outBufAddr, uint32_t inBufSize,
+                    uint32_t outBufSize, Tnr7Param* tnrParam, bool isUsrPtr = true);
+
+    Tnr7Param* allocTnr7ParamBuf();
+    void* allocCamBuf(uint32_t bufSize, int id);
+    int allocRefBufs(uint32_t bufSize);
+    void freeAllBufs();
+    int prepareSurface(void* bufAddr, int size);
+    int asyncParamUpdate(int gain);
+
+ private:
+    /* tnr api use CmSurface2DUP object as data buffer, call this api to create
+     * CmSurface2DUP object from user data buffer */
+    CmSurface2DUP* createCMSurface(void* bufAddr);
+    int32_t destroyCMSurface(CmSurface2DUP* surface);
+    // get the CmSurface object of the bufAddr in mCMSurfaceMap
+    CmSurface2DUP* getBufferCMSurface(void* bufAddr);
+    /* call tnr7us API to update params */
+    void handleParamUpdate(int gain);
+
+ private:
+    int mWidth;
+    int mHeight;
+    // Tnr will create CMSurface for input buffers and cache them in the map
+    std::unordered_map<void*, CmSurface2DUP*> mCMSurfaceMap;
+    static const int TnrRefBufCount = TNR7US_REFERENCE_BUFFER_COUNT;
+    static const int MaxCachedBufferCount = MAX_BUFFER_COUNT + TnrRefBufCount;
+    // use double buffer as tnr reference buffer
+    int mRefBufferSize;
+    // store the reference buffer address allocated by itself, only used in sandboxing mode
+    void* mRefBufAddr[TnrRefBufCount];
+    // CMSurface object of the reference buffer
+    CmSurface2DUP* mRefSurface[TnrRefBufCount];
+    int mRefBufIndex;
+    Tnr7Param* mTnrParam;
+    std::unique_ptr<base::Thread> mThread;
+    DISALLOW_COPY_AND_ASSIGN(IntelTNR7US);
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/StatsTypes.h b/camera/hal/intel/ipu6/modules/algowrapper/StatsTypes.h
new file mode 100644
index 000000000000..4ea0e26d724e
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/StatsTypes.h
@@ -0,0 +1,46 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_aiq_types.h>
+#include <ia_bcomp_types.h>
+#include <ia_dvs_types.h>
+#include <ia_isp_bxt.h>
+#include <ia_isp_bxt_statistics_types.h>
+#include <ia_isp_bxt_types.h>
+
+#include "Parameters.h"
+
+namespace icamera {
+
+#define MAX_EXPOSURES_COUNT 3
+
+struct ConvertInputParam {
+    bool multiExpo;
+    ia_binary_data* statsBuffer;
+    camera_resolution_t* dvsReso;
+    const ia_aiq_ae_results* aeResults;
+    ia_bcomp_results* bcompResult;
+};
+
+struct ConvertResult {
+    ia_isp_bxt_statistics_query_results_t* queryResults;
+    ia_aiq_rgbs_grid* rgbsGrid[MAX_EXPOSURES_COUNT];
+    ia_aiq_af_grid* afGrid;
+    ia_dvs_statistics* dvsStats;
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/graph/GraphConfigImpl.cpp b/camera/hal/intel/ipu6/modules/algowrapper/graph/GraphConfigImpl.cpp
new file mode 100644
index 000000000000..c53cf662a4ae
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/graph/GraphConfigImpl.cpp
@@ -0,0 +1,859 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "GraphConfigImpl"
+
+#include "modules/algowrapper/graph/GraphConfigImpl.h"
+
+#include <GCSSParser.h>
+#include <graph_query_manager.h>
+
+#include <algorithm>
+#include <unordered_map>
+
+#include "FormatUtils.h"
+#include "GraphUtils.h"
+#include "iutils/CameraLog.h"
+
+using GCSS::GCSSParser;
+using GCSS::GraphConfigNode;
+using GCSS::GraphQueryManager;
+using GCSS::ItemUID;
+using std::map;
+using std::shared_ptr;
+using std::string;
+using std::vector;
+
+namespace icamera {
+
+#define DISPERSED_MAX_OUTPUTS 2
+
+Mutex GraphConfigImpl::sLock;
+std::unordered_map<int32_t, GraphConfigNodes*> GraphConfigImpl::mGraphNode;
+
+GraphConfigNodes::GraphConfigNodes() : mDesc(nullptr), mSettings(nullptr) {}
+
+GraphConfigNodes::~GraphConfigNodes() {
+    delete mDesc;
+    delete mSettings;
+}
+
+GraphConfigImpl::GraphConfigImpl()
+        : mCameraId(-1),
+          mGraphQueryManager(nullptr),
+          mConfigMode(CAMERA_STREAM_CONFIGURATION_MODE_NORMAL),
+          mType(COUPLED),
+          mMcId(-1) {}
+
+GraphConfigImpl::GraphConfigImpl(int32_t camId, ConfigMode mode, GraphSettingType type)
+        : mCameraId(camId),
+          mGraphQueryManager(new GraphQueryManager()),
+          mConfigMode(mode),
+          mType(type),
+          mMcId(-1) {
+    AutoMutex lock(sLock);
+
+    GraphConfigNodes* nodes = nullptr;
+    if (mGraphNode.find(camId) != mGraphNode.end()) {
+        nodes = mGraphNode[camId];
+    }
+    CheckError(!nodes, VOID_VALUE, "Failed to allocate Graph Query Manager");
+    mGraphQueryManager->setGraphDescriptor(nodes->mDesc);
+    mGraphQueryManager->setGraphSettings(nodes->mSettings);
+}
+
+GraphConfigImpl::~GraphConfigImpl() {}
+
+/**
+ * Add predefined keys to the map used by the graph config parser.
+ *
+ * This method should only be called once.
+ *
+ * We do this so that the keys we will use in the queries are already defined
+ * and we can create the query objects in a more compact way, by using the
+ * ItemUID initializers.
+ */
+void GraphConfigImpl::addCustomKeyMap() {
+    /**
+     * Initialize the map with custom specific tags found in the
+     * Graph Config XML's
+     */
+#define GCSS_KEY(key, str) std::make_pair(#str, GCSS_KEY_##key),
+    map<string, ia_uid> CUSTOM_GRAPH_KEYS = {
+    #include "custom_gcss_keys.h"
+    };
+#undef GCSS_KEY
+
+    LOG1("Adding %zu custom specific keys to graph config parser", CUSTOM_GRAPH_KEYS.size());
+
+    /*
+     * add custom specific tags so parser can use them
+     */
+    ItemUID::addCustomKeyMap(CUSTOM_GRAPH_KEYS);
+}
+
+/**
+ * Method to parse the XML graph configurations and settings
+ *
+ * Provide the file name with absolute path to this method, and
+ * save the GraphConfigNodes pointer to static area.
+ * This method is currently called once per camera
+ *
+ * \param[in] graphDescFile: name of the graph descriptor file
+ * \param[in] settingsFile: name of the graph settings file
+ */
+status_t GraphConfigImpl::parse(int cameraId, const char* graphDescFile, const char* settingsFile) {
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL1);
+
+    GCSSParser parser;
+    GraphConfigNodes* nodes = new GraphConfigNodes;
+
+    parser.parseGCSSXmlFile(graphDescFile, &nodes->mDesc);
+    if (!nodes->mDesc) {
+        LOGE("Failed to parse graph descriptor from %s", graphDescFile);
+        delete nodes;
+        return UNKNOWN_ERROR;
+    }
+
+    parser.parseGCSSXmlFile(settingsFile, &nodes->mSettings);
+    if (!nodes->mSettings) {
+        LOGE("Failed to parse graph settings from %s", settingsFile);
+        delete nodes;
+        return UNKNOWN_ERROR;
+    }
+
+    AutoMutex lock(sLock);
+    // Destory the old item
+    auto it = mGraphNode.find(cameraId);
+    if (it != mGraphNode.end()) {
+        delete it->second;
+        mGraphNode.erase(it);
+    }
+    mGraphNode[cameraId] = nodes;
+
+    return OK;
+}
+
+/**
+ * Method to parse the XML graph configurations and settings
+ *
+ * Provide the memory address and size of files to this method,
+ * and save the GraphConfigNodes pointer to static area.
+ * This method is currently called once per camera
+ *
+ * \param[in] graphDescData: the memory address for graph descriptor
+ * \param[in] descDataSize: the memory size for graph descriptor
+ * \param[in] settingsData: the memory address for graph settings
+ * \param[in] settingsDataSize: the memory size for graph settings
+ */
+status_t GraphConfigImpl::parse(int cameraId, char* graphDescData, size_t descDataSize,
+                                char* settingsData, size_t settingsDataSize) {
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL1);
+
+    GCSSParser parser;
+    GraphConfigNodes* nodes = new GraphConfigNodes;
+
+    parser.parseGCSSXmlData(graphDescData, descDataSize, &nodes->mDesc);
+    if (!nodes->mDesc) {
+        LOGE("Failed to parse graph descriptor addr: %p, size: %zu", graphDescData, descDataSize);
+        delete nodes;
+        return UNKNOWN_ERROR;
+    }
+
+    parser.parseGCSSXmlData(settingsData, settingsDataSize, &nodes->mSettings);
+    if (!nodes->mSettings) {
+        LOGE("Failed to parse graph settings addr: %p, size: %zu", settingsData, settingsDataSize);
+        delete nodes;
+        return UNKNOWN_ERROR;
+    }
+
+    AutoMutex lock(sLock);
+    // Destory the old item
+    auto it = mGraphNode.find(cameraId);
+    if (it != mGraphNode.end()) {
+        delete it->second;
+        mGraphNode.erase(it);
+    }
+    mGraphNode[cameraId] = nodes;
+
+    return OK;
+}
+
+/*
+ * Release the graph config nodes of all cameras
+ */
+void GraphConfigImpl::releaseGraphNodes() {
+    AutoMutex lock(sLock);
+
+    for (auto& nodes : mGraphNode) {
+        delete nodes.second;
+    }
+    mGraphNode.clear();
+}
+
+/**
+ * Create the query rule for current stream configuration based
+ * on stream list and graph setting type.
+ */
+status_t GraphConfigImpl::createQueryRule(const vector<HalStream*>& activeStreams) {
+    mQuery.clear();
+    mStreamToSinkIdMap.clear();
+
+    int videoIndex = 0, stillIndex = 0;
+    map<GCSS::ItemUID, std::string> videoQuery;
+    map<GCSS::ItemUID, std::string> stillQuery;
+    map<HalStream*, uid_t> videoStreamToSinkIdMap;
+    map<HalStream*, uid_t> stillStreamToSinkIdMap;
+    vector<AndroidGraphConfigKey> videoStreamKeys = {GCSS_KEY_VIDEO0, GCSS_KEY_VIDEO1,
+                                                     GCSS_KEY_VIDEO2};
+    vector<AndroidGraphConfigKey> stillStreamKeys = {GCSS_KEY_STILL0, GCSS_KEY_STILL1,
+                                                     GCSS_KEY_STILL2};
+
+    // Depends on outputs numbers in GC settings
+    int vOutputNum = (mType == DISPERSED) ? DISPERSED_MAX_OUTPUTS : videoStreamKeys.size();
+    int sOutputNum = (mType == DISPERSED) ? DISPERSED_MAX_OUTPUTS : stillStreamKeys.size();
+    for (auto& stream : activeStreams) {
+        CheckError(stream->useCase() == USE_CASE_INPUT, UNKNOWN_ERROR,
+                   "Error: Re-processing not supported with graph config yet.");
+        /*
+         * According to the usage to create the query item
+         */
+        CheckError(videoIndex >= vOutputNum && stillIndex >= sOutputNum, UNKNOWN_ERROR,
+                   "%s: no output for new stream! video %d, still %d", __func__, videoIndex,
+                   stillIndex);
+        bool isVideo = isVideoStream(stream) ? true : false;
+        if (videoIndex < vOutputNum) {
+            isVideo = (isVideo || stillIndex >= sOutputNum);
+        } else {
+            isVideo = false;
+        }
+        AndroidGraphConfigKey key =
+            (isVideo) ? videoStreamKeys[videoIndex++] : stillStreamKeys[stillIndex++];
+        map<HalStream*, uid_t>& streamToSinkId =
+            isVideo ? videoStreamToSinkIdMap : stillStreamToSinkIdMap;
+        map<GCSS::ItemUID, std::string>& query = isVideo ? videoQuery : stillQuery;
+        ItemUID w = {key, GCSS_KEY_WIDTH};
+        ItemUID h = {key, GCSS_KEY_HEIGHT};
+
+        query[w] = std::to_string(stream->width());
+        query[h] = std::to_string(stream->height());
+        streamToSinkId[stream] = key;
+        LOG1("Adding stream %p to map %s", stream, ItemUID::key2str(key));
+    }
+
+    if (mType == COUPLED) {
+        LOG2("Merge the query rule if graph settings type is COUPLED");
+        /*
+         * In this case(graph settings type is COUPLED), we need to merge still and
+         * video together, so there is only one item in mQuery and mStreamToSinkIdMap
+         * and it is used for both still and video pipe
+         */
+
+        // Merge the query rule
+        for (auto& still : stillQuery) {
+            videoQuery.insert(still);
+        }
+
+        // Merge the stream to sink key map
+        for (auto& stillKey : stillStreamToSinkIdMap) {
+            videoStreamToSinkIdMap.insert(stillKey);
+        }
+
+        /*
+         * Add to the query the number of active outputs
+         * The number of active outputs is video + still
+         */
+        ItemUID streamCount = {GCSS_KEY_ACTIVE_OUTPUTS};
+        videoQuery[streamCount] = std::to_string(videoStreamToSinkIdMap.size());
+
+        int useCase = videoQuery.empty() ? 0 : USE_CASE_VIDEO;
+        useCase |= stillQuery.empty() ? 0 : USE_CASE_STILL_CAPTURE;
+        mQuery[useCase] = videoQuery;
+        dumpQuery(useCase, mQuery[useCase]);
+        mStreamToSinkIdMap[useCase] = videoStreamToSinkIdMap;
+    } else {
+        LOG2("Fill each query rule if graph settings type is DISPERSED");
+        /*
+         * In this case(graph settings type is DISPERSED), the query item for still
+         * and video is dispersed, so there are two items in mQuery and mStreamToSinkIdMap
+         * one is for still pipe and the other is for video pipe
+         */
+        ItemUID streamCount = {GCSS_KEY_ACTIVE_OUTPUTS};
+
+        // Add active outputs for video
+        if (!videoStreamToSinkIdMap.empty()) {
+            LOG2("The video output number: %zu", videoStreamToSinkIdMap.size());
+            videoQuery[streamCount] = std::to_string(videoStreamToSinkIdMap.size());
+            mQuery[USE_CASE_VIDEO] = videoQuery;
+            dumpQuery(USE_CASE_VIDEO, mQuery[USE_CASE_VIDEO]);
+            mStreamToSinkIdMap[USE_CASE_VIDEO] = videoStreamToSinkIdMap;
+        }
+
+        // Add active outputs for still
+        if (!stillStreamToSinkIdMap.empty()) {
+            LOG2("The still output number: %zu", stillStreamToSinkIdMap.size());
+            stillQuery[streamCount] = std::to_string(stillStreamToSinkIdMap.size());
+            mQuery[USE_CASE_STILL_CAPTURE] = stillQuery;
+            dumpQuery(USE_CASE_STILL_CAPTURE, mQuery[USE_CASE_STILL_CAPTURE]);
+            mStreamToSinkIdMap[USE_CASE_STILL_CAPTURE] = stillStreamToSinkIdMap;
+        }
+    }
+
+    return OK;
+}
+
+status_t GraphConfigImpl::getRawInputSize(GCSS::IGraphConfig* query, camera_resolution_t* reso) {
+    CheckError(!reso, UNKNOWN_ERROR, "%s, The reso is nullptr", __func__);
+    GCSS::IGraphConfig* result = nullptr;
+    css_err_t ret = mGraphQueryManager->createGraph(query, &result);
+    if (ret != css_err_none) {
+        delete result;
+        return UNKNOWN_ERROR;
+    }
+    CheckError(!result, UNKNOWN_ERROR, "%s, Failed to create the graph", __func__);
+
+    vector<string> isysOutput = {"csi_be:output",
+                                 "csi_be_soc:output"};
+    for (auto& item : isysOutput) {
+        GCSS::IGraphConfig* isysNode = result->getDescendantByString(item.c_str());
+        if (isysNode != nullptr) {
+            GCSS::GraphCameraUtil::getDimensions(isysNode, &(reso->width), &(reso->height));
+            return OK;
+        }
+    }
+
+    LOGE("Error: Couldn't get the resolution in isys output");
+    return UNKNOWN_ERROR;
+}
+
+/*
+ * According to the stream list to query graph setting and create GraphConfigPipe
+ */
+status_t GraphConfigImpl::configStreams(const vector<HalStream*>& activeStreams) {
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL1);
+
+    status_t ret = createQueryRule(activeStreams);
+    CheckError(ret != OK, ret, "Failed to create the query rule");
+
+    mQueryResult.clear();
+    map<int, vector<GCSS::IGraphConfig*> > useCaseToQueryResults;
+
+    LOG2("The mQuery size: %zu", mQuery.size());
+    for (auto& query : mQuery) {
+        mFirstQueryResults.clear();
+        dumpQuery(query.first, query.second);
+        mGraphQueryManager->queryGraphs(query.second, mFirstQueryResults);
+        CheckError(mFirstQueryResults.empty(), BAD_VALUE,
+                   "Failed to query the result, please check the settings xml");
+
+        // select setting from multiple results
+        ret = selectSetting(query.first, &useCaseToQueryResults);
+        CheckError(ret != OK, BAD_VALUE,
+                   "Failed to select the settings for ConfigMode (0x%x)in results", mConfigMode);
+    }
+
+    CheckError(useCaseToQueryResults.empty(), UNKNOWN_ERROR,
+               "%s, There isn't matched result after filtering with first query rule", __func__);
+    // Filter the results with same isys output if there are
+    // multiple items in useCaseToQueryResults map
+    if (useCaseToQueryResults.size() > 1) {
+        ret = OK;
+        bool matchFound = false;
+
+        vector<GCSS::IGraphConfig*>& videoQueryResults = useCaseToQueryResults.at(USE_CASE_VIDEO);
+        vector<GCSS::IGraphConfig*>& stillQueryResults =
+            useCaseToQueryResults.at(USE_CASE_STILL_CAPTURE);
+        CheckError(videoQueryResults.empty() || stillQueryResults.empty(), UNKNOWN_ERROR,
+                   "%s, the still or video query results is empty", __func__);
+
+        // Filter the video and still query results with same isys ouput resolution.
+        for (auto& video : videoQueryResults) {
+            camera_resolution_t videoReso;
+            ret = getRawInputSize(video, &videoReso);
+            CheckError(ret != OK, UNKNOWN_ERROR,
+                       "%s, Failed to get csi ouput resolution for video pipe", __func__);
+            LOG2("Isys output resolution of video pipe: %dx%d", videoReso.width, videoReso.height);
+
+            for (auto& still : stillQueryResults) {
+                camera_resolution_t stillReso;
+                ret = getRawInputSize(still, &stillReso);
+                CheckError(ret != OK, UNKNOWN_ERROR,
+                           "%s, Failed to get csi ouput resolution for still pipe", __func__);
+                LOG2("Isys output resolution for still pipe: %dx%d", stillReso.width,
+                     stillReso.height);
+
+                if (videoReso.width == stillReso.width && videoReso.height == stillReso.height) {
+                    matchFound = true;
+                    mQueryResult[USE_CASE_VIDEO] = video;
+                    mQueryResult[USE_CASE_STILL_CAPTURE] = still;
+                    break;
+                }
+            }
+            if (matchFound) break;
+        }
+
+        CheckError(!matchFound, UNKNOWN_ERROR,
+                   "%s, Failed to find the isys ouput for video and still pipe", __func__);
+    } else {
+        // Use the query result with smallest isys output if there is only video pipe
+        int resultIdx = 0;
+        if (useCaseToQueryResults.begin()->first == USE_CASE_VIDEO) {
+            camera_resolution_t resultReso;
+            getRawInputSize((useCaseToQueryResults.begin()->second)[0], &resultReso);
+            for (size_t idx = 0; idx < (useCaseToQueryResults.begin()->second).size(); idx++) {
+                camera_resolution_t reso;
+                getRawInputSize((useCaseToQueryResults.begin()->second)[idx], &reso);
+                if (resultReso.width > reso.width && resultReso.height > reso.height) {
+                    resultIdx = idx;
+                    resultReso = reso;
+                }
+            }
+        }
+        mQueryResult[useCaseToQueryResults.begin()->first] =
+            useCaseToQueryResults.begin()->second[resultIdx];
+    }
+    CheckError(mQueryResult.empty(), UNKNOWN_ERROR, "%s, Failed to fill the map into mQueryResult",
+               __func__);
+
+    int key = -1;
+    string mcId, opMode;
+    mQueryResult.begin()->second->getValue(GCSS_KEY_KEY, key);
+    mQueryResult.begin()->second->getValue(GCSS_KEY_MC_ID, mcId);
+    mQueryResult.begin()->second->getValue(GCSS_KEY_OP_MODE, opMode);
+    LOG1("CAM[%d]Graph config for pipe: %d SUCCESS, settings id %d, operation mode: %s", mCameraId,
+         mQueryResult.begin()->first, key, opMode.c_str());
+
+    if (mQueryResult.size() > 1) {
+        mQueryResult.rbegin()->second->getValue(GCSS_KEY_KEY, key);
+        mQueryResult.rbegin()->second->getValue(GCSS_KEY_MC_ID, mcId);
+        mQueryResult.rbegin()->second->getValue(GCSS_KEY_OP_MODE, opMode);
+        LOG1("CAM[%d]Graph config for pipe: %d SUCCESS, settings id %d, operation mode: %s",
+             mCameraId, mQueryResult.rbegin()->first, key, opMode.c_str());
+    }
+    mMcId = mcId.empty() ? -1 : stoi(mcId);
+    ret = prepareGraphConfig();
+    CheckError(ret != OK, ret, "%s, Failed to prepare graph config: real ConfigMode: %x", __func__,
+               mConfigMode);
+
+    return OK;
+}
+
+/**
+ * Prepare graph config object
+ *
+ * Use graph query results as a parameter to getGraph. The result will be given
+ * to graph config object.
+ */
+status_t GraphConfigImpl::prepareGraphConfig() {
+    status_t status = OK;
+    mGraphConfigPipe.clear();
+
+    for (auto& query : mQueryResult) {
+        shared_ptr<GraphConfigPipe> graphConfigPipe =
+            std::make_shared<GraphConfigPipe>(query.first);
+        GCSS::IGraphConfig* result = nullptr;
+        css_err_t ret = mGraphQueryManager->createGraph(query.second, &result);
+        if (ret != css_err_none) {
+            delete result;
+            return UNKNOWN_ERROR;
+        }
+        status = graphConfigPipe->prepare(static_cast<GraphConfigNode*>(result),
+                                          mStreamToSinkIdMap[query.first]);
+        if (status != OK) {
+            delete result;
+            LOGE("Failed to prepare the GraphConfigPipe for pipe: %d", query.first);
+            return UNKNOWN_ERROR;
+        }
+        mGraphConfigPipe[query.first] = graphConfigPipe;
+        LOG1("Graph config object prepared");
+    }
+
+    return OK;
+}
+
+/*
+ * Do the secondary filter: configMode and stream format.
+ */
+status_t GraphConfigImpl::selectSetting(
+    int useCase, std::map<int, std::vector<GCSS::IGraphConfig*> >* queryResults) {
+    CheckError(!queryResults, UNKNOWN_ERROR, "%s, The queryResults is nullptr", __func__);
+    string opMode;
+    vector<GCSS::IGraphConfig*> internalQueryResults;
+
+    // Firstly, filter the config mode
+    for (auto& result : mFirstQueryResults) {
+        vector<ConfigMode> cfgModes;
+        result->getValue(GCSS_KEY_OP_MODE, opMode);
+        LOG1("The operation mode str in xml: %s", opMode.c_str());
+
+        CameraUtils::getConfigModeFromString(opMode, cfgModes);
+        LOG1("The query results supports configModes size: %zu", cfgModes.size());
+
+        for (const auto mode : cfgModes) {
+            if (mConfigMode == mode) {
+                internalQueryResults.push_back(result);
+                break;
+            }
+        }
+    }
+    CheckError(internalQueryResults.size() == 0, UNKNOWN_ERROR,
+               "Failed to query the results for configMode: %d", mConfigMode);
+
+    /*
+     * May still have multiple graphs after config mode parsing
+     * Those graphs have same resolution/configMode, but different output formats
+     * Do second graph query with format/bpp as query condition
+     */
+    map<HalStream*, uid_t>& streamToSinkIdMap = mStreamToSinkIdMap[useCase];
+    vector<GCSS::IGraphConfig*> secondQueryResults;
+    if (internalQueryResults.size() > 1) {
+        map<GCSS::ItemUID, std::string> queryItem;
+        for (auto const& item : streamToSinkIdMap) {
+            HalStream* s = item.first;
+            ItemUID formatKey = {(ia_uid)item.second, GCSS_KEY_FORMAT};
+            string fmt = graphconfig::utils::format2string(s->format());
+            queryItem[formatKey] = fmt;
+        }
+
+        LOG1("dumpQuery with format condition");
+        dumpQuery(useCase, queryItem);
+
+        /*
+         * Note: In some projects, there isn't format item in graph setting,
+         * So the result of this query may be empty, and ignore it.
+         */
+        mGraphQueryManager->queryGraphs(queryItem, internalQueryResults, secondQueryResults);
+        LOG2("The query results size: %zu after filtering format", secondQueryResults.size());
+    }
+
+    // Firstly, select the results with fully filtering
+    if (secondQueryResults.size() > 0) {
+        (*queryResults)[useCase] = secondQueryResults;
+    } else {
+        (*queryResults)[useCase] = internalQueryResults;
+    }
+
+    return OK;
+}
+
+status_t GraphConfigImpl::getGraphConfigData(IGraphType::GraphConfigData* data) {
+    // The graph id, csi output and sensor mode must be same if there are two graph config pipes
+    data->graphId = mGraphConfigPipe.begin()->second->getGraphId();
+    mGraphConfigPipe.begin()->second->getCSIOutputResolution(&(data->csiReso));
+
+    data->mcId = mMcId;
+    getGdcKernelSetting(&(data->gdcKernelId), &(data->gdcReso));
+
+    int ret = getPgNames(&(data->pgNames));
+    CheckError(ret != OK, UNKNOWN_ERROR, "%s, Failed to get pg names", __func__);
+    for (auto& pgName : data->pgNames) {
+        IGraphType::PgInfo info;
+        info.pgName = pgName;
+        info.streamId = getStreamIdByPgName(pgName);
+        info.pgId = getPgIdByPgName(pgName);
+        getPgRbmValue(pgName, &info.rbmValue);
+        data->pgInfo.push_back(info);
+    }
+
+    ret = graphGetStreamIds(&(data->streamIds));
+    CheckError(ret != OK, UNKNOWN_ERROR, "%s, Failed to get streamIds", __func__);
+    for (auto& streamId : data->streamIds) {
+        IGraphType::MbrInfo mBr;
+        mBr.streamId = streamId;
+        getMBRData(streamId, &mBr.data);
+        data->mbrInfo.push_back(mBr);
+
+        IGraphType::ProgramGroupInfo info;
+        info.streamId = streamId;
+        info.pgPtr = getProgramGroup(streamId);
+        data->programGroup.push_back(info);
+    }
+
+    return OK;
+}
+
+status_t GraphConfigImpl::getGdcKernelSetting(uint32_t* kernelId,
+                                              ia_isp_bxt_resolution_info_t* resolution) {
+    CheckError(mGraphConfigPipe.empty(), UNKNOWN_ERROR, "%s, the mGraphConfigPipe is empty",
+               __func__);
+    CheckError(!kernelId || !resolution, UNKNOWN_ERROR, "%s, the kernelId or resolution is nullptr",
+               __func__);
+
+    int ret = OK;
+    if (mGraphConfigPipe.size() == 1) {
+        ret = mGraphConfigPipe.begin()->second->getGdcKernelSetting(kernelId, resolution);
+    } else {
+        // Get the information from video pipe firstly
+        shared_ptr<GraphConfigPipe>& videoGraphPipe = mGraphConfigPipe.at(USE_CASE_VIDEO);
+        ret = videoGraphPipe->getGdcKernelSetting(kernelId, resolution);
+        if (ret != OK) {
+            shared_ptr<GraphConfigPipe>& stillGraphPipe =
+                mGraphConfigPipe.at(USE_CASE_STILL_CAPTURE);
+            ret = stillGraphPipe->getGdcKernelSetting(kernelId, resolution);
+        }
+    }
+    LOG2("%s, %s", __func__, ret != OK ? "No gdc resolution" : "Get gdc resolution successfully");
+
+    return OK;
+}
+
+status_t GraphConfigImpl::graphGetStreamIds(std::vector<int32_t>* streamIds) {
+    CheckError(mGraphConfigPipe.empty(), UNKNOWN_ERROR, "%s, the mGraphConfigPipe is empty",
+               __func__);
+    CheckError(!streamIds, UNKNOWN_ERROR, "%s, The streamIds is nullptr", __func__);
+
+    if (mGraphConfigPipe.size() == 1) {
+        mGraphConfigPipe.begin()->second->graphGetStreamIds(streamIds);
+    } else {
+        vector<int32_t> stillStreamIds;
+        shared_ptr<GraphConfigPipe>& videoGraphPipe = mGraphConfigPipe.at(USE_CASE_VIDEO);
+        shared_ptr<GraphConfigPipe>& stillGraphPipe = mGraphConfigPipe.at(USE_CASE_STILL_CAPTURE);
+
+        videoGraphPipe->graphGetStreamIds(streamIds);
+        stillGraphPipe->graphGetStreamIds(&stillStreamIds);
+
+        for (auto& id : stillStreamIds) {
+            if (std::find(streamIds->begin(), streamIds->end(), id) == streamIds->end())
+                streamIds->push_back(id);
+        }
+    }
+
+    CheckError(streamIds->empty(), UNKNOWN_ERROR, "%s, Failed to find any streamIds for all pipes",
+               __func__);
+
+    return OK;
+}
+
+int GraphConfigImpl::getStreamIdByPgName(std::string pgName) {
+    CheckError(mGraphConfigPipe.empty(), -1, "%s, the mGraphConfigPipe is empty", __func__);
+
+    int streamId = -1;
+    for (auto& pipe : mGraphConfigPipe) {
+        streamId = pipe.second->getStreamIdByPgName(pgName);
+        if (streamId != -1) break;
+    }
+    return streamId;
+}
+
+int GraphConfigImpl::getPgIdByPgName(std::string pgName) {
+    CheckError(mGraphConfigPipe.empty(), -1, "%s, the mGraphConfigPipe is empty", __func__);
+
+    int pgId = -1;
+    for (auto& pipe : mGraphConfigPipe) {
+        pgId = pipe.second->getPgIdByPgName(pgName);
+        if (pgId != -1) break;
+    }
+    return pgId;
+}
+
+ia_isp_bxt_program_group* GraphConfigImpl::getProgramGroup(int32_t streamId) {
+    CheckError(mGraphConfigPipe.empty(), nullptr, "%s, the mGraphConfigPipe is empty", __func__);
+
+    if (mGraphConfigPipe.size() == 1) {
+        return mGraphConfigPipe.begin()->second->getProgramGroup(streamId);
+    }
+
+    // Find the streamd id from video graph pipe firstly
+    vector<int32_t> streamIds;
+    shared_ptr<GraphConfigPipe>& videoGraphPipe = mGraphConfigPipe.at(USE_CASE_VIDEO);
+    videoGraphPipe->graphGetStreamIds(&streamIds);
+    if (std::find(streamIds.begin(), streamIds.end(), streamId) != streamIds.end()) {
+        return videoGraphPipe->getProgramGroup(streamId);
+    }
+
+    shared_ptr<GraphConfigPipe>& stillGraphPipe = mGraphConfigPipe.at(USE_CASE_STILL_CAPTURE);
+    return stillGraphPipe->getProgramGroup(streamId);
+}
+
+int GraphConfigImpl::getProgramGroup(std::string pgName,
+                                     ia_isp_bxt_program_group* programGroupForPG) {
+    for (auto& graph : mGraphConfigPipe) {
+        vector<string> pgNames;
+        graph.second->getPgNames(&pgNames);
+        if (std::find(pgNames.begin(), pgNames.end(), pgName) != pgNames.end()) {
+            return graph.second->getProgramGroup(pgName, programGroupForPG);
+        }
+    }
+
+    LOGE("There isn't this pg: %s in all graph config pipes", pgName.c_str());
+    return UNKNOWN_ERROR;
+}
+
+status_t GraphConfigImpl::getMBRData(int32_t streamId, ia_isp_bxt_gdc_limits* data) {
+    CheckError(mGraphConfigPipe.empty(), UNKNOWN_ERROR, "%s, the mGraphConfigPipe is empty",
+               __func__);
+
+    if (mGraphConfigPipe.size() == 1) {
+        return mGraphConfigPipe.begin()->second->getMBRData(streamId, data);
+    }
+
+    // Find the streamd id from video graph pipe firstly
+    vector<int32_t> streamIds;
+    shared_ptr<GraphConfigPipe>& videoGraphPipe = mGraphConfigPipe.at(USE_CASE_VIDEO);
+    videoGraphPipe->graphGetStreamIds(&streamIds);
+    if (std::find(streamIds.begin(), streamIds.end(), streamId) != streamIds.end()) {
+        return videoGraphPipe->getMBRData(streamId, data);
+    }
+
+    shared_ptr<GraphConfigPipe>& stillGraphPipe = mGraphConfigPipe.at(USE_CASE_STILL_CAPTURE);
+    return stillGraphPipe->getMBRData(streamId, data);
+}
+
+status_t GraphConfigImpl::getPgNames(std::vector<std::string>* pgNames) {
+    CheckError(mGraphConfigPipe.empty(), UNKNOWN_ERROR, "%s, the mGraphConfigPipe is empty",
+               __func__);
+
+    if (mGraphConfigPipe.size() == 1) {
+        mGraphConfigPipe.begin()->second->getPgNames(pgNames);
+    } else {
+        vector<string> stillPgNames;
+        shared_ptr<GraphConfigPipe>& videoGraphPipe = mGraphConfigPipe.at(USE_CASE_VIDEO);
+        shared_ptr<GraphConfigPipe>& stillGraphPipe = mGraphConfigPipe.at(USE_CASE_STILL_CAPTURE);
+
+        videoGraphPipe->getPgNames(pgNames);
+        stillGraphPipe->getPgNames(&stillPgNames);
+
+        for (auto& pg : stillPgNames) {
+            if (std::find(pgNames->begin(), pgNames->end(), pg.c_str()) == pgNames->end())
+                pgNames->push_back(pg);
+        }
+    }
+
+    CheckError(pgNames->empty(), UNKNOWN_ERROR, "%s, Failed to get the PG's name for all pipes",
+               __func__);
+
+    return OK;
+}
+
+status_t GraphConfigImpl::pipelineGetConnections(
+    const std::vector<std::string>& pgList, std::vector<IGraphType::ScalerInfo>* scalerInfo,
+    std::vector<IGraphType::PipelineConnection>* confVector) {
+    CheckError(!confVector, UNKNOWN_ERROR, "%s, the confVector is nullptr", __func__);
+    CheckError(mGraphConfigPipe.empty(), UNKNOWN_ERROR, "%s, the mGraphConfigPipe is empty",
+               __func__);
+
+    if (mGraphConfigPipe.size() == 1) {
+        return mGraphConfigPipe.begin()->second->pipelineGetConnections(pgList, scalerInfo,
+                                                                        confVector);
+    }
+
+    vector<IGraphType::PipelineConnection> stillConnVector, videoConnVector;
+    shared_ptr<GraphConfigPipe>& videoGraphPipe = mGraphConfigPipe.at(USE_CASE_VIDEO);
+    shared_ptr<GraphConfigPipe>& stillGraphPipe = mGraphConfigPipe.at(USE_CASE_STILL_CAPTURE);
+
+    std::vector<IGraphType::ScalerInfo> stillScalerInfo, videoScalerInfo;
+    int ret = videoGraphPipe->pipelineGetConnections(pgList, &videoScalerInfo, &videoConnVector);
+    CheckError(ret != OK, UNKNOWN_ERROR, "Failed to get the connetction from video pipe");
+    ret = stillGraphPipe->pipelineGetConnections(pgList, &stillScalerInfo, &stillConnVector);
+    CheckError(ret != OK, UNKNOWN_ERROR, "Failed to get the connetction from still pipe");
+
+    LOG2("The connetction in video: %zu, in still: %zu; the scalera in video: %zu, in still: %uz",
+         videoConnVector.size(), stillConnVector.size(), videoScalerInfo.size(),
+         stillScalerInfo.size());
+
+    if (!stillScalerInfo.empty()) {
+        for (auto& stillScaler : stillScalerInfo) {
+            videoScalerInfo.push_back(stillScaler);
+        }
+    }
+    *scalerInfo = videoScalerInfo;
+
+    if (videoConnVector.empty()) {
+        videoConnVector = stillConnVector;
+    } else {
+        if (stillConnVector.size() > 0) {
+            LOG1("Need to merge the two connetction vector: %zu", stillConnVector.size());
+        }
+        for (auto& stillConn : stillConnVector) {
+            bool sameTerminalId = false;
+            for (auto& conn : videoConnVector) {
+                if (conn.portFormatSettings.terminalId == stillConn.portFormatSettings.terminalId) {
+                    sameTerminalId = true;
+                    if (conn.portFormatSettings.enabled == 0 &&
+                        stillConn.portFormatSettings.enabled == 1)
+                        conn = stillConn;
+                    break;
+                }
+            }
+            if (!sameTerminalId) videoConnVector.push_back(stillConn);
+        }
+    }
+    CheckError(videoConnVector.empty(), UNKNOWN_ERROR,
+               "%s, Failed to get connetctions from graph config pipe", __func__);
+
+    LOG2("dump the final connetction");
+    GraphUtils::dumpConnections(videoConnVector);
+    *confVector = videoConnVector;
+
+    return OK;
+}
+
+status_t GraphConfigImpl::getPgIdForKernel(const uint32_t streamId, const int32_t kernelId,
+                                           int32_t* pgId) {
+    CheckError(!pgId, UNKNOWN_ERROR, "%s, the pgId is nullptr", __func__);
+    CheckError(mGraphConfigPipe.empty(), UNKNOWN_ERROR, "%s, the mGraphConfigPipe is empty",
+               __func__);
+
+    if (mGraphConfigPipe.size() == 1) {
+        return mGraphConfigPipe.begin()->second->getPgIdForKernel(streamId, kernelId, pgId);
+    }
+
+    vector<int32_t> streamIds;
+    shared_ptr<GraphConfigPipe>& videoGraphPipe = mGraphConfigPipe.at(USE_CASE_VIDEO);
+    videoGraphPipe->graphGetStreamIds(&streamIds);
+    if (std::find(streamIds.begin(), streamIds.end(), streamId) != streamIds.end())
+        return videoGraphPipe->getPgIdForKernel(streamId, kernelId, pgId);
+
+    shared_ptr<GraphConfigPipe>& stillGraphPipe = mGraphConfigPipe.at(USE_CASE_STILL_CAPTURE);
+    return stillGraphPipe->getPgIdForKernel(streamId, kernelId, pgId);
+}
+
+status_t GraphConfigImpl::getPgRbmValue(string pgName, IGraphType::StageAttr* stageAttr) {
+    for (auto& graph : mGraphConfigPipe) {
+        vector<string> pgNames;
+        graph.second->getPgNames(&pgNames);
+        if (std::find(pgNames.begin(), pgNames.end(), pgName) != pgNames.end()) {
+            return graph.second->getPgRbmValue(pgName, stageAttr);
+        }
+    }
+
+    LOGE("There isn't this pg: %s in all graph config pipes", pgName.c_str());
+    return UNKNOWN_ERROR;
+}
+
+/******************************************************************************
+ *  HELPER METHODS
+ ******************************************************************************/
+/**
+ * Check the gralloc hint flags and decide whether this stream should be served
+ * by Video Pipe or Still Pipe
+ */
+bool GraphConfigImpl::isVideoStream(HalStream* stream) {
+    if (stream->useCase() == USE_CASE_PREVIEW || stream->useCase() == USE_CASE_VIDEO) return true;
+
+    return false;
+}
+
+void GraphConfigImpl::dumpQuery(int useCase, const map<GCSS::ItemUID, std::string>& query) {
+    map<GCSS::ItemUID, std::string>::const_iterator it;
+    it = query.begin();
+    LOG1("Query Dump --- %d --- Start", useCase);
+    for (; it != query.end(); ++it) {
+        LOG1("item: %s value %s", it->first.toString().c_str(), it->second.c_str());
+    }
+    LOG1("Query Dump --- %d --- End", useCase);
+}
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/graph/GraphConfigImpl.h b/camera/hal/intel/ipu6/modules/algowrapper/graph/GraphConfigImpl.h
new file mode 100644
index 000000000000..9158172f20ec
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/graph/GraphConfigImpl.h
@@ -0,0 +1,183 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <gcss.h>
+
+#include <map>
+#include <memory>
+#include <string>
+#include <unordered_map>
+#include <utility>
+#include <vector>
+
+#include "HalStream.h"
+#include "iutils/Errors.h"
+#include "iutils/Thread.h"
+#include "iutils/Utils.h"
+#include "modules/algowrapper/graph/GraphConfigPipe.h"
+
+namespace icamera {
+
+/**
+ * \enum AndroidGraphConfigKey
+ * List of keys that are Android Specific used in queries of settings by
+ * the GraphConfigImpl.
+ *
+ * The enum should not overlap with the enum of tags already predefined by the
+ * parser, hence the initial offset.
+ */
+#define GCSS_KEY(key, str) GCSS_KEY_##key,
+enum AndroidGraphConfigKey {
+    GCSS_ANDROID_KEY_START = GCSS_KEY_START_CUSTOM_KEYS,
+    #include "custom_gcss_keys.h"
+};
+#undef GCSS_KEY
+
+/**
+ * Static data for graph settings for given sensor. Used to initialize GraphConfigImpl.
+ */
+class GraphConfigNodes {
+ public:
+    GraphConfigNodes();
+    ~GraphConfigNodes();
+
+ public:
+    GCSS::IGraphConfig* mDesc;
+    GCSS::IGraphConfig* mSettings;
+
+ private:
+    // Disable copy constructor and assignment operator
+    DISALLOW_COPY_AND_ASSIGN(GraphConfigNodes);
+};
+
+/**
+ * \class GraphConfigImpl
+ *
+ * Class to wrap over parsing and executing queries on graph settings.
+ * It creates the query rule based on graph settings type and stream list
+ * from user. And filter the final result for current stream config.
+ * It supports COUPLED and DISPERSED
+ *
+ * GraphConfigImpl maintains a GraphConfigPipe map table, each item for one
+ * pipe. And it provides the public APIs to get the useful information which
+ * builds PSYS pipe.
+ *
+ * At camera open, GraphConfigImpl object is created.
+ * In stream config period, GraphConfigImpl creates the GraphConfigPipe map which is
+ * current stream configuration. And HAL can get all the necessary information to
+ * create PSYS pipe.
+ */
+class GraphConfigImpl {
+ public:
+    GraphConfigImpl();
+    GraphConfigImpl(int32_t camId, ConfigMode mode, GraphSettingType type);
+    virtual ~GraphConfigImpl();
+
+    /*
+     * Methods for XML parsing for XML parsing
+     */
+    void addCustomKeyMap();
+    status_t parse(int cameraId, const char* graphDescFile, const char* settingsFile);
+    status_t parse(int cameraId, char* graphDescData, size_t descDataSize, char* settingsData,
+                   size_t settingsDataSize);
+    void releaseGraphNodes();
+
+    // These public methods called by GraphConfig
+    status_t configStreams(const std::vector<HalStream*>& activeStreams);
+    status_t getGraphConfigData(IGraphType::GraphConfigData* data);
+
+    int getProgramGroup(std::string pgName, ia_isp_bxt_program_group* programGroupForPG);
+    status_t getPgIdForKernel(const uint32_t streamId, const int32_t kernelId, int32_t* pgId);
+
+    status_t pipelineGetConnections(const std::vector<std::string>& pgList,
+                                    std::vector<IGraphType::ScalerInfo>* scalerInfo,
+                                    std::vector<IGraphType::PipelineConnection>* confVector);
+
+ private:
+    status_t prepareGraphConfig();
+    bool isVideoStream(HalStream* stream);
+    status_t selectSetting(int useCase,
+                           std::map<int, std::vector<GCSS::IGraphConfig*> >* queryResults);
+    status_t createQueryRule(const std::vector<HalStream*>& activeStreams);
+    status_t getRawInputSize(GCSS::IGraphConfig* query, camera_resolution_t* reso);
+
+    status_t getGdcKernelSetting(uint32_t* kernelId, ia_isp_bxt_resolution_info_t* resolution);
+    status_t graphGetStreamIds(std::vector<int32_t>* streamIds);
+    int getStreamIdByPgName(std::string pgName);
+    int getPgIdByPgName(std::string pgName);
+    ia_isp_bxt_program_group* getProgramGroup(int32_t streamId);
+    status_t getPgRbmValue(std::string pgName, IGraphType::StageAttr* stageAttr);
+    status_t getMBRData(int32_t streamId, ia_isp_bxt_gdc_limits* data);
+    status_t getPgNames(std::vector<std::string>* pgNames);
+
+    // Debug helper
+    void dumpQuery(int useCase, const std::map<GCSS::ItemUID, std::string>& query);
+
+ private:
+    static Mutex sLock;
+    static std::unordered_map<int32_t, GraphConfigNodes*> mGraphNode;
+    /**
+     * Pair of ItemUIDs to store the width and height of a stream
+     * first item is for width, second for height
+     */
+    typedef std::pair<GCSS::ItemUID, GCSS::ItemUID> ResolutionItem;
+
+    int32_t mCameraId;
+    std::unique_ptr<GCSS::GraphQueryManager> mGraphQueryManager;
+    /*
+     * The query interface uses types that are actually STL maps and vectors
+     * to avoid the creation/deletion on the stack for every call we
+     * have them as member variables.
+     * - The first item of mQuery is stream useCase(VIDEO or STILL),
+     * - and the second is an query rule map(GCSS_KEY_, VALUE).
+     */
+    std::map<int, std::map<GCSS::ItemUID, std::string> > mQuery;
+
+    /**
+     * Map to get the virtual sink id from a client stream pointer.
+     * The uid is one of the GCSS keys defined for the virtual sinks, like
+     * GCSS_KEY_VIDEO0 or GCSS_KEY_STILL1
+     * From that we can derive the name using the id to string methods from
+     * ItemUID class
+     *  - The first item is streams useCase(VIDEO or STILL)
+     *  - and the second is the stream to virtual sink map
+     */
+    std::map<int, std::map<HalStream*, uid_t> > mStreamToSinkIdMap;
+
+    /*
+     * This vector is used to store the first query result.
+     * After that we also need to filter the results with configMode,
+     * stream format, and matched isys output resolution.
+     */
+    std::vector<GCSS::IGraphConfig*> mFirstQueryResults;
+
+    // The stream useCase to result map
+    std::map<int, GCSS::IGraphConfig*> mQueryResult;
+
+    // The stream useCase to GraphConfigPipe map
+    std::map<int, std::shared_ptr<GraphConfigPipe> > mGraphConfigPipe;
+
+    ConfigMode mConfigMode;
+    GraphSettingType mType;
+    int mMcId;
+
+    // Disable copy constructor and assignment operator
+    DISALLOW_COPY_AND_ASSIGN(GraphConfigImpl);
+};
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/graph/GraphConfigPipe.cpp b/camera/hal/intel/ipu6/modules/algowrapper/graph/GraphConfigPipe.cpp
new file mode 100644
index 000000000000..6c0c064aaa36
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/graph/GraphConfigPipe.cpp
@@ -0,0 +1,2008 @@
+/*
+ * Copyright (C) 2018-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "GraphConfigPipe"
+
+#include "modules/algowrapper/graph/GraphConfigPipe.h"
+
+#include <GCSSParser.h>
+#include <gcss.h>
+#include <gcss_utils.h>
+#include <ia_pal_types_isp_ids_autogen.h>
+#include <v4l2_device.h>
+
+#include <algorithm>
+
+#include "FormatUtils.h"
+#include "GraphUtils.h"
+#include "iutils/CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+#include "linux/media-bus-format.h"
+
+using GCSS::GraphConfigNode;
+using std::string;
+using std::vector;
+namespace gcu = graphconfig::utils;
+
+namespace icamera {
+#define STRINGIFY1(x) #x
+#define STRINGIFY(x) STRINGIFY1(x)
+
+// TODO: Change the format attribute natively as integer attribute
+#ifndef VIDEO_RECORDING_FORMAT
+#define VIDEO_RECORDING_FORMAT TILE
+#endif
+
+const char* SENSOR_PORT_NAME = "sensor:port_0";
+const char* TPG_PORT_NAME = "tpg:port_0";
+
+#define DB_KERNEL_SIZE 2
+#define PPP_KERNEL_SIZE 2
+#define DS_KERNEL_SIZE 2
+uint32_t dpKernel[DB_KERNEL_SIZE] = {ia_pal_uuid_isp_sc_outputscaler_dp,
+                                     ia_pal_uuid_isp_sc_outputscaler_dp_1_1};
+uint32_t pppKernel[PPP_KERNEL_SIZE] = {ia_pal_uuid_isp_sc_outputscaler_ppp,
+                                       ia_pal_uuid_isp_sc_outputscaler_ppp_1_1};
+uint32_t dsKernel[DS_KERNEL_SIZE] = {ia_pal_uuid_isp_b2i_ds_1_0_0, ia_pal_uuid_isp_b2i_ds_1_0_1};
+
+GraphConfigPipe::GraphConfigPipe(int pipeUseCase)
+        : mSettings(nullptr),
+          mReqId(0),
+          mMetaEnabled(false),
+          mSourceType(SRC_NONE),
+          mPipeUseCase(pipeUseCase) {
+    mCsiOutput = {0, 0};
+}
+
+GraphConfigPipe::~GraphConfigPipe() {
+    fullReset();
+}
+/*
+ * Full reset
+ * This is called whenever we want to reset the whole object. Currently that
+ * is only, when GraphConfigPipe object is destroyed.
+ */
+void GraphConfigPipe::fullReset() {
+    mSourcePortName.clear();
+    mSinkPeerPort.clear();
+    mStreamToSinkIdMap.clear();
+    delete mSettings;
+    mSettings = nullptr;
+    mReqId = 0;
+    mStream2TuningMap.clear();
+    mProgramGroup.clear();
+}
+/*
+ * Reset
+ * This is called per frame
+ */
+void GraphConfigPipe::reset(GraphConfigPipe* me) {
+    if (me != nullptr) {
+        me->mReqId = 0;
+    } else {
+        LOGE("Trying to reset a null GraphConfigPipe - BUG!");
+    }
+}
+
+const GCSS::IGraphConfig* GraphConfigPipe::getInterface(Node* node) const {
+    if (!node) return nullptr;
+
+    return node;
+}
+
+const GCSS::IGraphConfig* GraphConfigPipe::getInterface() const {
+    return mSettings;
+}
+
+int GraphConfigPipe::getGraphId(void) {
+    int graphId = -1;
+    int ret = mSettings->getValue(GCSS_KEY_ID, graphId);
+    if (ret != css_err_none) {
+        LOGE("Failed to get graphId");
+        return BAD_VALUE;
+    }
+
+    LOGG("%s: graphId %d", __func__, graphId);
+    return graphId;
+}
+
+/**
+ * Per frame initialization of graph config.
+ * Updates request id
+ * \param[in] reqId
+ */
+void GraphConfigPipe::init(int32_t reqId) {
+    mReqId = reqId;
+}
+
+/**
+ * Prepare graph config once per stream config.
+ * \param[in] manager
+ * \param[in] settings
+ * \param[in] streamToSinkIdMap
+ * \param[in] active
+ */
+status_t GraphConfigPipe::prepare(Node* settings, const StreamToSinkMap& streamToSinkIdMap) {
+    mSettings = settings;
+    status_t ret = OK;
+
+    if (settings == nullptr) {
+        LOGW("Settings is NULL!! - BUG?");
+        return UNKNOWN_ERROR;
+    }
+
+    ret = analyzeSourceType();
+    if (ret != OK) {
+        LOGE("Failed to analyze source type");
+        return ret;
+    }
+
+    ret = getActiveOutputPorts(streamToSinkIdMap);
+    if (ret != OK) {
+        LOGE("Failed to get output ports");
+        return ret;
+    }
+    // Options should be updated before kernel list generation
+    ret = handleDynamicOptions();
+    if (ret != OK) {
+        LOGE("Failed to update options");
+        return ret;
+    }
+
+    const GCSS::IGraphConfig* gcHandle = getInterface(mSettings);
+    css_err_t status = mGCSSAicUtil.initAicUtils(gcHandle);
+    if (status != css_err_none) {
+        LOGE("Failed to generate kernel list");
+        return UNKNOWN_ERROR;
+    }
+
+    calculateSinkDependencies();
+    storeTuningModes();
+    analyzeCSIOutput();
+
+    return ret;
+}
+
+/**
+ * Store the tuning modes for each stream id into a map that can be used on a
+ * per frame basis.
+ * This method is executed once per stream configuration.
+ * The tuning mode is used by AIC to find the correct tuning tables in CPF.
+ *
+ */
+void GraphConfigPipe::storeTuningModes() {
+    GraphConfigNode::const_iterator it = mSettings->begin();
+    css_err_t ret = css_err_none;
+    GraphConfigNode* result = nullptr;
+    int32_t tuningMode = 0;
+    int32_t streamId = 0;
+    mStream2TuningMap.clear();
+
+    while (it != mSettings->end()) {
+        ret = mSettings->getDescendant(GCSS_KEY_TYPE, "program_group", it, &result);
+        if (ret == css_err_none) {
+            ret = result->getValue(GCSS_KEY_STREAM_ID, streamId);
+            if (ret != css_err_none) {
+                string pgName;
+                // This should  not fail
+                ret = result->getValue(GCSS_KEY_NAME, pgName);
+                LOGW("Failed to find stream id for PG %s", pgName.c_str());
+                continue;
+            }
+            tuningMode = 0;  // default value in case it is not found
+            ret = result->getValue(GCSS_KEY_TUNING_MODE, tuningMode);
+            if (ret != css_err_none) {
+                string pgName;
+                // This should  not fail
+                ret = result->getValue(GCSS_KEY_NAME, pgName);
+                LOG2("There isn't tuning mode for PG %s, defaulting to %d", pgName.c_str(),
+                     tuningMode);
+            }
+            mStream2TuningMap[streamId] = tuningMode;
+        }
+    }
+}
+/**
+ * Retrieve the tuning mode associated with a given stream id.
+ *
+ * The tuning mode is defined by IQ-studio and represent and index to different
+ * set of tuning parameters in the AIQB (a.k.a CPF)
+ *
+ * The tuning mode is an input parameter for AIC.
+ * \param [in] streamId Identifier for the branch (video/still/isa)
+ * \return tuning mode, if stream id is not found defaults to 0
+ */
+int32_t GraphConfigPipe::getTuningMode(int32_t streamId) {
+    auto item = mStream2TuningMap.find(streamId);
+    if (item != mStream2TuningMap.end()) {
+        return item->second;
+    }
+    LOGW("Could not find tuning mode for requested stream id %d", streamId);
+    return 0;
+}
+
+/*
+ * According to the node, analyze the source type:
+ * TPG or sensor
+ */
+status_t GraphConfigPipe::analyzeSourceType() {
+    bool hasSensor = false, hasTPG = false;
+    Node* inputDevNode = nullptr;
+    css_err_t ret = mSettings->getDescendant(GCSS_KEY_SENSOR, &inputDevNode);
+    if (ret == css_err_none) {
+        mSourceType = SRC_SENSOR;
+        mSourcePortName = SENSOR_PORT_NAME;
+        hasSensor = true;
+    } else {
+        LOG1("No sensor node from the graph");
+    }
+
+    ret = mSettings->getDescendant(GCSS_KEY_TPG, &inputDevNode);
+    if (ret == css_err_none) {
+        mSourceType = SRC_TPG;
+        mSourcePortName = TPG_PORT_NAME;
+        hasTPG = true;
+    } else {
+        LOG1("No TPG node from the graph");
+    }
+
+    if (hasTPG == hasSensor) {
+        // failed to analyze source type, reset them
+        mSourceType = SRC_NONE;
+        mSourcePortName.clear();
+        LOGE("Error: Both TPG/Sensor exist or none of TPG/Sensor");
+        return UNKNOWN_ERROR;
+    }
+
+    return OK;
+}
+
+status_t GraphConfigPipe::analyzeCSIOutput() {
+    vector<string> csiBeOutput = {"csi_be:output",
+                                  "csi_be_soc:output"};
+    for (auto& item : csiBeOutput) {
+        GCSS::IGraphConfig* csiBeNode =
+            static_cast<GCSS::IGraphConfig*>(mSettings)->getDescendantByString(item.c_str());
+        if (csiBeNode != nullptr) {
+            GCSS::GraphCameraUtil::getDimensions(csiBeNode, &mCsiOutput.width, &mCsiOutput.height);
+            return OK;
+        }
+    }
+
+    LOGE("Error: Couldn't get CSI-BE node");
+    return UNKNOWN_ERROR;
+}
+
+status_t GraphConfigPipe::getMBRData(int32_t streamId, ia_isp_bxt_gdc_limits* data) {
+    css_err_t ret = mGCSSAicUtil.getMbrData(streamId, data);
+    if (ret == css_err_none) {
+        return OK;
+    } else {
+        LOG2("fail to getMBRData for stream id:%d", streamId);
+        return UNKNOWN_ERROR;
+    }
+}
+/**
+ * Finds the sink nodes and the output port peer. Use streamToSinkIdMap
+ * since we are intrested only in sinks that serve a stream. Takes an
+ * internal copy of streamToSinkIdMap to be used later.
+ *
+ * \param[in] streamToSinkIdMap to get the virtual sink id from a client stream pointer
+ * \return OK in case of success.
+ * \return UNKNOWN_ERROR or BAD_VALUE in case of fail.
+ */
+status_t GraphConfigPipe::getActiveOutputPorts(const StreamToSinkMap& streamToSinkIdMap) {
+    status_t status = OK;
+    css_err_t ret = css_err_none;
+    std::vector<GCSS::IGraphConfig*> sinks;
+
+    mStreamToSinkIdMap.clear();
+    mStreamToSinkIdMap = streamToSinkIdMap;
+    mSinkPeerPort.clear();
+
+    StreamToSinkMap::const_iterator it;
+    it = streamToSinkIdMap.begin();
+
+    for (; it != streamToSinkIdMap.end(); ++it) {
+        sinks.clear();
+        status = GCSS::GraphCameraUtil::graphGetSinksByName(GCSS::ItemUID::key2str(it->second),
+                                                            mSettings, sinks);
+        if (status != OK || sinks.empty()) {
+            string sinkName = GCSS::ItemUID::key2str(it->second);
+            LOGE("Found %zu sinks, expecting 1 for sink %s", sinks.size(), sinkName.c_str());
+            return BAD_VALUE;
+        }
+
+        Node* sink = static_cast<GraphConfigNode*>(sinks[0]);
+        Node* outputPort = nullptr;
+
+        // Get the sinkname for getting the output port
+        string sinkName;
+        ret = sink->getValue(GCSS_KEY_NAME, sinkName);
+        if (ret != css_err_none) {
+            LOGE("Failed to get sink name");
+            return BAD_VALUE;
+        }
+        LOG2("sink name %s", sinkName.c_str());
+
+        int32_t streamId = -1;
+        ret = sink->getValue(GCSS_KEY_STREAM_ID, streamId);
+        if (ret != css_err_none) {
+            LOGE("Failed to get stream id");
+            return BAD_VALUE;
+        }
+        LOG2("stream id %d", streamId);
+
+        outputPort = getOutputPortForSink(sinkName);
+        if (outputPort == nullptr) {
+            LOGE("No output port found for sink");
+            return UNKNOWN_ERROR;
+        }
+
+        LOG2("output port name %s", NODE_NAME(outputPort));
+        mSinkPeerPort[sink] = outputPort;
+    }
+
+    return OK;
+}
+
+string GraphConfigPipe::getNodeName(Node* node) {
+    string nodeName("");
+    if (node == nullptr) {
+        LOGE("Node is NULL");
+        return nodeName;
+    }
+
+    node->getValue(GCSS_KEY_NAME, nodeName);
+    return nodeName;
+}
+
+/**
+ * Finds the output port which is the peer to the sink node.
+ *
+ * Gets root node, and finds the sink with the given name. Use portGetPeer()
+ * to find the output port.
+ * \return GraphConfigNode in case of success.
+ * \return nullptr in case of fail.
+ */
+Node* GraphConfigPipe::getOutputPortForSink(const string& sinkName) {
+    css_err_t ret = css_err_none;
+    status_t retErr = OK;
+    Node* rootNode = nullptr;
+    Node* portNode = nullptr;
+    Node* peerNode = nullptr;
+
+    rootNode = mSettings->getRootNode();
+    if (rootNode == nullptr) {
+        LOGE("Couldn't get root node, BUG!");
+        return nullptr;
+    }
+    ret = rootNode->getDescendantByString(sinkName, &portNode);
+    if (ret != css_err_none) {
+        LOGE("Error getting sink");
+        return nullptr;
+    }
+    retErr = portGetPeer(portNode, &peerNode);
+    if (retErr != OK) {
+        LOGE("Error getting peer");
+        return nullptr;
+    }
+    return peerNode;
+}
+
+/**
+ * Update the option-list to the graph tree.
+ * TODO: Add more options.
+ * \return OK in case of success
+ * \return UNKNOWN_ERROR if graph update failed.
+ */
+status_t GraphConfigPipe::handleDynamicOptions() {
+    status_t status = setPortFormats();
+    if (status != OK) {
+        LOGE("Failed to update metadata");
+        return UNKNOWN_ERROR;
+    }
+
+    // TODO add other options
+    return status;
+}
+
+/**
+ * Returns true if the given node is used to output a video record
+ * stream. The sink name is found and used to find client stream from the
+ * mStreamToSinkIdMap.
+ * Then the video encoder gralloc flag is checked from the stream flags of the
+ * client stream.
+ * \param[in] peer output port to find the sink node of.
+ * \return true if sink port serves a video record stream.
+ * \return false if sink port does not serve a video record stream.
+ */
+bool GraphConfigPipe::isVideoRecordPort(Node* sink) {
+    css_err_t ret = css_err_none;
+    string sinkName;
+    HalStream* clientStream = nullptr;
+
+    if (sink == nullptr) {
+        LOGE("No sink node provided");
+        return false;
+    }
+
+    ret = sink->getValue(GCSS_KEY_NAME, sinkName);
+    if (ret != css_err_none) {
+        LOGE("Failed to get sink name");
+        return false;
+    }
+
+    // Find the client stream for the sink port
+    StreamToSinkMap::iterator it1;
+    it1 = mStreamToSinkIdMap.begin();
+
+    for (; it1 != mStreamToSinkIdMap.end(); ++it1) {
+        if (GCSS::ItemUID::key2str(it1->second) == sinkName) {
+            clientStream = it1->first;
+            break;
+        }
+    }
+
+    if (clientStream == nullptr) {
+        LOGE("Failed to find client stream");
+        return false;
+    }
+
+    if (clientStream->useCase() == USE_CASE_VIDEO) {
+        LOG2("%s is video record port", NODE_NAME(sink));
+        return true;
+    }
+
+    return false;
+}
+
+/**
+ * Takes a stream id, and checks if it exists in the graph.
+ *
+ * \param[in] streamId
+ * \return true if found, false otherwise
+ */
+bool GraphConfigPipe::hasStreamInGraph(int streamId) {
+    status_t status;
+    StreamsVector streamsFound;
+
+    status = graphGetStreamIds(&streamsFound);
+    if (status != OK) return false;
+
+    for (auto id : streamsFound) {
+        if (id == streamId) return true;
+    }
+    return false;
+}
+
+/**
+ * Apply the video recording format for the video record stream handling
+ * output port.
+ * \return OK in case of success
+ * \return UNKNOWN_ERROR if option list apply failed.
+ */
+status_t GraphConfigPipe::setPortFormats() {
+    css_err_t ret = css_err_none;
+    std::map<Node*, Node*>::iterator it;
+    it = mSinkPeerPort.begin();
+
+    for (; it != mSinkPeerPort.end(); ++it) {
+        Node* sink = it->first;
+        Node* peer = it->second;
+        if (!isVideoRecordPort(sink)) continue;
+
+        ret = peer->setValue(GCSS_KEY_FORMAT, STRINGIFY(VIDEO_RECORDING_FORMAT));
+        if (ret != css_err_none) {
+            // If format attribute does not exist, create it.
+            ret = peer->addValue(GCSS_KEY_FORMAT, STRINGIFY(VIDEO_RECORDING_FORMAT));
+            if (ret != css_err_none) {
+                LOGE("Failed to update options for video record port");
+                return UNKNOWN_ERROR;
+            }
+        }
+    }
+
+    return OK;
+}
+
+/**
+ * Returns pointer to kernel list based on given stream id
+ * \param[in] streamId Return kernel list for this stream id
+ * \return ia_isp_bxt_program_group
+ */
+ia_isp_bxt_program_group* GraphConfigPipe::getProgramGroup(int32_t streamId) {
+    if (mProgramGroup.find(streamId) == mProgramGroup.end()) {
+        ia_isp_bxt_program_group programGroup;
+        CLEAR(programGroup);
+        mGCSSAicUtil.getProgramGroup(streamId, programGroup);
+        mProgramGroup[streamId] = programGroup;
+    }
+    LOGG("Dump kernel info for stream %d", streamId);
+    GraphUtils::dumpKernelInfo(mProgramGroup[streamId]);
+
+    return &(mProgramGroup[streamId]);
+}
+
+int GraphConfigPipe::getProgramGroup(string pgName, ia_isp_bxt_program_group* programGroupForPG) {
+    GCSS::NodeIterator iter(mSettings);
+    GCSS::IGraphConfig* pg = iter.iterateByType(GCSS_KEY_PROGRAM_GROUP);
+    std::string name;
+    for (; pg != nullptr; pg = iter.iterateByType(GCSS_KEY_PROGRAM_GROUP)) {
+        css_err_t ret = pg->getValue(GCSS_KEY_NAME, name);
+        CheckError(ret != css_err_none, BAD_VALUE,
+                   "Failed to get the name of an existing PG node, BUG");
+        if (name == pgName) {
+            break;
+        }
+    }
+    CheckError(pg == nullptr, UNKNOWN_ERROR, "Failed to get program groups, BUG");
+
+    mGCSSAicUtil.getProgramGroup(pg, *programGroupForPG);
+
+    LOGG("Dump kernel info for %s", pgName.c_str());
+    GraphUtils::dumpKernelInfo(*programGroupForPG);
+
+    return OK;
+}
+
+status_t GraphConfigPipe::getPgRbmValue(string pgName, IGraphType::StageAttr* stageAttr) {
+    std::string name;
+    GCSS::NodeIterator iter(mSettings);
+    GCSS::IGraphConfig* pg = iter.iterateByType(GCSS_KEY_PROGRAM_GROUP);
+
+    for (; pg != nullptr; pg = iter.iterateByType(GCSS_KEY_PROGRAM_GROUP)) {
+        css_err_t ret = pg->getValue(GCSS_KEY_NAME, name);
+        CheckError(ret != css_err_none, BAD_VALUE, "Failed to get the name of PG node");
+        if (name == pgName) {
+            break;
+        }
+    }
+    CheckError(pg == nullptr, UNKNOWN_ERROR, "Failed to get program groups for PG: %s",
+               pgName.c_str());
+
+    pg = pg->getDescendant(GCSS_KEY_CIPF);
+    if (pg == nullptr) return NAME_NOT_FOUND;
+
+    string rbmString;
+    css_err_t ret = pg->getValue(GCSS_KEY_RBM, rbmString);
+    if (ret != css_err_none) return NAME_NOT_FOUND;
+
+    GCSS::GraphCameraUtil mGCSSCameraUtil;
+    stageAttr->rbm = mGCSSCameraUtil.numString2binary(rbmString, &stageAttr->rbm_bytes);
+    CheckError(!stageAttr->rbm, NO_MEMORY, "%s get rbm value: %s", __func__, rbmString.c_str());
+
+    return OK;
+}
+
+status_t GraphConfigPipe::getScalerKernelResolutionRatio(uint32_t* kenerArray, uint32_t sizeArray,
+                                                         float* widthRatio, float* heightRatio) {
+    CheckError(!kenerArray, UNKNOWN_ERROR, "%s the array is null", __func__);
+    CheckError(!widthRatio || !heightRatio, UNKNOWN_ERROR, "%s widthRatio or heightRatio is null",
+               __func__);
+
+    const ia_isp_bxt_resolution_info_t* resolutionInfo;
+    resolutionInfo = getScalerKernelResolutionInfo(kenerArray, sizeArray);
+    CheckError(!resolutionInfo, UNKNOWN_ERROR, "%s getScalerKernelResolutionInfo fails", __func__);
+
+    *widthRatio = 1.0;
+    *heightRatio = 1.0;
+    const ia_rectangle* input_crop = &resolutionInfo->input_crop;
+    const ia_rectangle* output_crop = &resolutionInfo->output_crop;
+    if (((resolutionInfo->input_width != resolutionInfo->output_width) ||
+         (resolutionInfo->input_height != resolutionInfo->output_height)) &&
+        ((input_crop->left == 0) && (input_crop->top == 0) && (input_crop->right == 0) &&
+         (input_crop->bottom == 0)) &&
+        ((output_crop->left == 0) && (output_crop->top == 0) && (output_crop->right == 0) &&
+         (output_crop->bottom == 0))) {
+        *widthRatio = static_cast<float>(resolutionInfo->input_width) /
+                      static_cast<float>(resolutionInfo->output_width);
+        *heightRatio = static_cast<float>(resolutionInfo->input_height) /
+                       static_cast<float>(resolutionInfo->output_height);
+        LOG2("%s, width:%d-%d; height:%d-%d", __func__, resolutionInfo->input_width,
+             resolutionInfo->output_width, resolutionInfo->input_height,
+             resolutionInfo->output_height);
+    }
+    return OK;
+}
+
+const ia_isp_bxt_resolution_info_t* GraphConfigPipe::getScalerKernelResolutionInfo(
+    uint32_t* kenerArray, uint32_t sizeArray) {
+    CheckError(!kenerArray, nullptr, "%s the array is null", __func__);
+
+    std::vector<int32_t> streamIds;
+    // Get all stream IDs
+    status_t ret = graphGetStreamIds(&streamIds);
+    CheckWarning((ret != OK || streamIds.empty()), nullptr, "Failed to get the PG streamIds");
+
+    uint32_t kernelId = kenerArray[0];
+    int32_t streamIdTmp = VIDEO_STREAM_ID;
+
+    bool hasVideo = false;
+    for (auto streamId : streamIds) {
+        for (uint32_t i = 0; i < sizeArray; i++) {
+            if (isKernelInStream(streamId, kenerArray[i])) {
+                LOG2("%s, found outputscaler %u from stream %d", __func__, kenerArray[i], streamId);
+                streamIdTmp = streamId;
+                kernelId = kenerArray[i];
+                if (streamId == VIDEO_STREAM_ID) hasVideo = true;
+
+                break;
+            }
+        }
+        if (hasVideo) break;
+    }
+
+    // Get resolution as per above kernel and stream
+    return getKernelResolutionInfo(streamIdTmp, kernelId);
+}
+
+const ia_isp_bxt_resolution_info_t* GraphConfigPipe::getGdcKernelResolutionInfo(
+    uint32_t* kernelId) {
+    CheckError(!kernelId, nullptr, "%s the kernelId is nullptr", __func__);
+
+    std::vector<int32_t> streamIds;
+    // Get all stream IDs
+    status_t ret = graphGetStreamIds(&streamIds);
+    CheckWarning((ret != OK || streamIds.empty()), nullptr, "Failed to get the PG streamIds");
+
+    *kernelId = ia_pal_uuid_isp_gdc3;
+    LOG1("%s, initalize gdc version 3 as default", __func__);
+    int32_t gdcStreamId = VIDEO_STREAM_ID;
+    LOG1("%s, initalize gdc video stream as default", __func__);
+
+    // Check video stream for gdc version firstly,
+    // in case more than one stream contain gdc kernel.
+    bool hasVideoGdc = false;
+    for (auto streamId : streamIds) {
+        if (isKernelInStream(streamId, ia_pal_uuid_isp_gdc3_1)) {
+            LOG1("%s, found gdc3_1 from stream %d", __func__, streamId);
+            gdcStreamId = streamId;
+            *kernelId = ia_pal_uuid_isp_gdc3_1;
+            if (streamId == VIDEO_STREAM_ID) hasVideoGdc = true;
+        } else if (isKernelInStream(streamId, ia_pal_uuid_isp_gdc3)) {
+            LOG1("%s, found gdc3 from stream %d", __func__, streamId);
+            gdcStreamId = streamId;
+            *kernelId = ia_pal_uuid_isp_gdc3;
+            if (streamId == VIDEO_STREAM_ID) hasVideoGdc = true;
+        } else if (isKernelInStream(streamId, ia_pal_uuid_isp_gdc3_1_1)) {
+            LOG1("%s, found gdc3_1_1 from stream %d", __func__, streamId);
+            gdcStreamId = streamId;
+            *kernelId = ia_pal_uuid_isp_gdc3_1_1;
+            if (streamId == VIDEO_STREAM_ID) hasVideoGdc = true;
+        } else if (isKernelInStream(streamId, ia_pal_uuid_isp_gdc5)) {
+            LOG1("%s, found gdc5 from stream %d", __func__, streamId);
+            gdcStreamId = streamId;
+            *kernelId = ia_pal_uuid_isp_gdc5;
+            if (streamId == VIDEO_STREAM_ID) hasVideoGdc = true;
+        }
+        if (hasVideoGdc) break;
+    }
+
+    // Get resolution as per above kernel and stream
+    return getKernelResolutionInfo(gdcStreamId, *kernelId);
+}
+
+status_t GraphConfigPipe::getGdcKernelSetting(uint32_t* kernelId,
+                                              ia_isp_bxt_resolution_info_t* resolution) {
+    CheckError(!kernelId || !resolution, UNKNOWN_ERROR, "%s, the kernelId or resolution is nullptr",
+               __func__);
+
+    // Get resolution as per above kernel and stream
+    const ia_isp_bxt_resolution_info_t* gdcResolution = getGdcKernelResolutionInfo(kernelId);
+    CheckWarning(!gdcResolution, NO_ENTRY, "Couldn't get the GDC resolution in current pipe: %d",
+                 mPipeUseCase);
+
+    *resolution = *gdcResolution;
+
+    LOGG("%s: kernel %d, inResolution %dx%d, outResolution %dx%d", __func__, *kernelId,
+         resolution->input_width, resolution->input_height, resolution->output_width,
+         resolution->output_height);
+
+    LOGG("%s: kernel %d, inputCrop %d,%d,%d,%d; outputCrop %d,%d,%d,%d", __func__, *kernelId,
+         resolution->input_crop.left, resolution->input_crop.top, resolution->input_crop.right,
+         resolution->input_crop.bottom, resolution->output_crop.left, resolution->output_crop.top,
+         resolution->output_crop.right, resolution->output_crop.bottom);
+
+    return OK;
+}
+
+const ia_isp_bxt_resolution_info_t* GraphConfigPipe::getKernelResolutionInfo(uint32_t streamId,
+                                                                             uint32_t kernelId) {
+    ia_isp_bxt_program_group* programGroup = getProgramGroup(streamId);
+    if (programGroup == nullptr) {
+        return nullptr;
+    }
+
+    for (unsigned int i = 0; i < programGroup->kernel_count; i++) {
+        if (programGroup->run_kernels[i].stream_id == streamId &&
+            programGroup->run_kernels[i].kernel_uuid == kernelId) {
+            return programGroup->run_kernels[i].resolution_info;
+        }
+    }
+
+    return nullptr;
+}
+
+/**
+ * check whether the kernel is in this stream
+ *
+ * \param[in] streamId stream id.
+ * \param[in] kernelId kernel id.
+ * \param[out] whether the kernel in this stream
+ * \return true the kernel is in this stream
+ * \return false the kernel isn't in this stream.
+ *
+ */
+bool GraphConfigPipe::isKernelInStream(uint32_t streamId, uint32_t kernelId) {
+    ia_isp_bxt_program_group* programGroup = getProgramGroup(streamId);
+    if (programGroup == nullptr) {
+        return false;
+    }
+    for (unsigned int i = 0; i < programGroup->kernel_count; i++) {
+        if (programGroup->run_kernels[i].kernel_uuid == kernelId) {
+            return true;
+        }
+    }
+
+    return false;
+}
+
+/**
+ * get program group id for some kernel
+ *
+ * \param[in] streamId stream id.
+ * \param[in] kernelId kernel pal uuid.
+ * \param[out] program group id that contain this kernel with the same stream id
+ * \return error if can't find the kernel id in any ot the PGs in this stream
+ */
+status_t GraphConfigPipe::getPgIdForKernel(const uint32_t streamId, const int32_t kernelId,
+                                           int32_t* pgId) {
+    CheckError(!pgId, UNKNOWN_ERROR, "%s, the pgId is nullptr", __func__);
+    css_err_t ret = css_err_none;
+    status_t retErr;
+    NodesPtrVector programGroups;
+
+    // Get all program groups with the stream id
+    retErr = streamGetProgramGroups(streamId, &programGroups);
+    if (retErr != OK) {
+        LOGE("ERROR: couldn't get program groups");
+        return retErr;
+    }
+
+    // Go through all the program groups with the selected streamID
+    for (auto& ndVec : programGroups) {
+        /* Iterate through program group nodes, find kernel and get the PG id */
+        GCSS::GraphConfigItem::const_iterator it = ndVec->begin();
+        while (it != ndVec->end()) {
+            Node* kernelNode = nullptr;
+            // Look for kernel with the requested uuid
+            ret = ndVec->getDescendant(GCSS_KEY_PAL_UUID, kernelId, it, &kernelNode);
+            if (ret != css_err_none) continue;
+
+            ret = ndVec->getValue(GCSS_KEY_PG_ID, *pgId);
+            if (ret == css_err_none) {
+                LOG2("got the pgid:%d for kernel id:%d in stream:%d", *pgId, kernelId, streamId);
+                return OK;
+            }
+            LOGE("ERROR: Couldn't get pg id for kernel %d", kernelId);
+            return BAD_VALUE;
+        }
+    }
+    LOG2("%s: kernel id %d is not found in stream %d", __func__, kernelId, streamId);
+    return BAD_VALUE;
+}
+
+/**
+ * This method creates SinkDependency structure for every active sink found in
+ * the graph. These structs allow quick access to information that is required
+ * by other methods.
+ * Active sinks are the ones that have a connection to an active port.
+ * This list of active sinks(mSinkPeerPort) has to be filled before this method
+ * is executed.
+ * For every virtual sink we store the name (as a key) and the terminal id of
+ * the input port of the stream associated with that stream. This input port
+ * will be the destination of the buffers from the capture unit.
+ *
+ * This method is used during init()
+ * If we would have different settings per frame then this would be enough
+ * to detect the active ISA nodes, but we are not there yet. we are still using
+ * the base graph settings every frame.
+ */
+void GraphConfigPipe::calculateSinkDependencies() {
+    status_t status = OK;
+    GCSS::IGraphConfig* streamInputPort = nullptr;
+    std::string sinkName;
+    SinkDependency aSinkDependency;
+    ia_uid stageId;  // not needed
+    mSinkDependencies.clear();
+    mIsaOutputPort2StreamId.clear();
+    std::map<Node*, Node*>::iterator sinkIter = mSinkPeerPort.begin();
+
+    for (; sinkIter != mSinkPeerPort.end(); ++sinkIter) {
+        sinkIter->first->getValue(GCSS_KEY_NAME, sinkName);
+        aSinkDependency.sinkGCKey = GCSS::ItemUID::str2key(sinkName);
+        sinkIter->first->getValue(GCSS_KEY_STREAM_ID, aSinkDependency.streamId);
+        status = GCSS::GraphCameraUtil::getInputPort(GCSS_KEY_STREAM_ID, aSinkDependency.streamId,
+                                                     mSettings, &streamInputPort);
+        if (status != OK) {
+            LOGE("Failed to get input port for stream %d associated to sink %s",
+                 aSinkDependency.streamId, sinkName.c_str());
+            continue;
+        }
+        status = GCSS::GraphCameraUtil::portGetFourCCInfo(streamInputPort, stageId,
+                                                          aSinkDependency.streamInputPortId);
+        if (status != OK) {
+            LOGE("Failed to get stream %d input port 4CC code", aSinkDependency.streamId);
+            continue;
+        }
+        GCSS::IGraphConfig* temp = nullptr;
+        status = GCSS::GraphCameraUtil::portGetPeer(streamInputPort, &temp);
+        if (status != OK) {
+            LOGE("fail to get peer for the port(%s)",
+                 GCSS::GraphCameraUtil::print(streamInputPort).c_str());
+            continue;
+        }
+        aSinkDependency.peer = static_cast<Node*>(temp);
+        LOG2("Adding dependency %s stream id %d", sinkName.c_str(), aSinkDependency.streamId);
+        mSinkDependencies.push_back(aSinkDependency);
+
+        // get the output port of capture unit
+        status = GCSS::GraphCameraUtil::portGetPeer(streamInputPort, &temp);
+        if (status != OK) {
+            LOGE("Fail to get isa output port for sink %s", sinkName.c_str());
+            continue;
+        }
+        Node* isaOutPutPort = static_cast<Node*>(temp);
+        std::string fullName;
+        status = portGetFullName(isaOutPutPort, &fullName);
+        if (status != OK) {
+            LOGE("Fail to get isa output port name");
+            continue;
+        }
+        int32_t streamId = portGetStreamId(isaOutPutPort);
+        if (streamId != -1 &&
+            mIsaOutputPort2StreamId.find(fullName) == mIsaOutputPort2StreamId.end())
+            mIsaOutputPort2StreamId[fullName] = streamId;
+    }
+}
+
+/**
+ * This method is used by the GC Manager that has access to the request
+ * to inform us of what are the active sinks.
+ * Using the sink dependency information we can then know which ISA ports
+ * are active for this GC.
+ *
+ * Once we have different settings per request then we can incorporate this
+ * method into calculateSinkDependencies.
+ *
+ * \param[in] activeSinks Vector with GCSS_KEY's of the active sinks in a
+ *                        request
+ */
+void GraphConfigPipe::setActiveSinks(const std::vector<uid_t>& activeSinks) {
+    mIsaActiveDestinations.clear();
+    uid_t activeDest = 0;
+
+    for (auto sink : activeSinks) {
+        for (auto& dependency : mSinkDependencies) {
+            if (dependency.sinkGCKey == sink) {
+                activeDest = dependency.streamInputPortId;
+                mIsaActiveDestinations[activeDest] = activeDest;
+            }
+        }
+    }
+}
+
+/**
+ * This method is used by the GC Manager that has access to the request
+ * to inform us of what will the stream id be used.
+ * Using the sink dependency information we can then know which stream ids
+ * are active for this GC.
+ *
+ * Once we have different settings per request then we can incorporate this
+ * method into calculateSinkDependencies.
+ *
+ * \param[in] activeSinks Vector with GCSS_KEY's of the active sinks in a
+ *                        request
+ */
+void GraphConfigPipe::setActiveStreamId(const std::vector<uid_t>& activeSinks) {
+    mActiveStreamId.clear();
+    int32_t activeStreamId = 0;
+
+    for (auto sink : activeSinks) {
+        for (auto& dependency : mSinkDependencies) {
+            if (dependency.sinkGCKey == sink) {
+                activeStreamId = dependency.streamId;
+                mActiveStreamId.insert(activeStreamId);
+
+                // get peer's stream Id
+                activeStreamId = portGetStreamId(dependency.peer);
+                if (activeStreamId == -1) {
+                    LOGE("fail to get the stream id for peer port");
+                    continue;
+                }
+                if (mActiveStreamId.find(activeStreamId) == mActiveStreamId.end())
+                    mActiveStreamId.insert(activeStreamId);
+            }
+        }
+    }
+}
+
+int GraphConfigPipe::getStreamIdByPgName(string pgName) {
+    css_err_t ret = ia_err_none;
+    string foundPgName = "invalid";
+    GraphConfigNode* programGroup = nullptr;
+    GraphConfigNode::const_iterator it = mSettings->begin();
+
+    while (it != mSettings->end()) {
+        programGroup = nullptr;
+        ret = mSettings->getDescendant(GCSS_KEY_TYPE, "program_group", it, &programGroup);
+        if (ret != ia_err_none || programGroup == nullptr) {
+            continue;
+        }
+
+        ret = programGroup->getValue(GCSS_KEY_NAME, foundPgName);
+        if (ret != ia_err_none) {
+            LOGW("%s, failed to get pg name in program group", __func__);
+            continue;
+        }
+
+        if (foundPgName == pgName) break;
+    }
+
+    if (foundPgName != pgName || !programGroup) {
+        LOG2("No matched PG found in pipeUseCase: %d", mPipeUseCase);
+        return -1;
+    }
+
+    int streamId = -1;
+    ret = programGroup->getValue(GCSS_KEY_STREAM_ID, streamId);
+    CheckError(ret != ia_err_none, -1, "Get streamId failed by name:%s, pipeUseCase: %d",
+               pgName.c_str(), mPipeUseCase);
+
+    LOGG("%s: streamId %d", __func__, streamId);
+    return streamId;
+}
+
+status_t GraphConfigPipe::getPgNames(std::vector<string>* pgNames) {
+    css_err_t ret = css_err_none;
+    GraphConfigNode::const_iterator it = mSettings->begin();
+    GraphConfigNode* programGroup = nullptr;
+
+    while (it != mSettings->end()) {
+        programGroup = nullptr;
+        ret = mSettings->getDescendant(GCSS_KEY_TYPE, "program_group", it, &programGroup);
+        if (ret != css_err_none || programGroup == nullptr) {
+            continue;
+        }
+
+        string foundPgName;
+        ret = programGroup->getValue(GCSS_KEY_NAME, foundPgName);
+        if (ret != css_err_none) {
+            LOGW("%s, failed to get pg name in program group", __func__);
+            continue;
+        }
+
+        pgNames->push_back(foundPgName);
+    }
+
+    return OK;
+}
+
+bool GraphConfigPipe::containPgs(std::vector<string> pgNames) {
+    std::vector<string> allPgNames;
+    getPgNames(&allPgNames);
+
+    for (auto& name : pgNames) {
+        if (std::find(allPgNames.begin(), allPgNames.end(), name.c_str()) == allPgNames.end())
+            return false;
+    }
+    return true;
+}
+
+int GraphConfigPipe::getPgIdByPgName(string pgName) {
+    css_err_t ret = css_err_none;
+    GraphConfigNode::const_iterator it = mSettings->begin();
+    GraphConfigNode* programGroup = nullptr;
+    bool foundMatchedPg = false;
+
+    while (it != mSettings->end()) {
+        programGroup = nullptr;
+        ret = mSettings->getDescendant(GCSS_KEY_TYPE, "program_group", it, &programGroup);
+        if (ret != css_err_none || programGroup == nullptr) {
+            continue;
+        }
+
+        string foundPgName;
+        ret = programGroup->getValue(GCSS_KEY_NAME, foundPgName);
+        if (ret != css_err_none) {
+            LOGW("%s, failed to get pg name in program group", __func__);
+            continue;
+        }
+
+        if (foundPgName == pgName) {
+            foundMatchedPg = true;
+            break;
+        }
+    }
+
+    if (!foundMatchedPg) {
+        LOG2("No matched PG found, pgName: %s, pipeUseCase: %d", pgName.c_str(), mPipeUseCase);
+        return -1;
+    }
+
+    const GCSS::IGraphConfig* gc = getInterface(programGroup);
+    CheckError(gc == nullptr, -1, "%s, Failed to get graph config interface", __func__);
+
+    int pgId = -1;
+    ret = gc->getValue(GCSS_KEY_PG_ID, pgId);
+    CheckError(ret != css_err_none, -1, "Get PG ID failed with:%d", ret);
+
+    LOGG("%s: pgName %s, pgId %d", __func__, pgName.c_str(), pgId);
+    return pgId;
+}
+
+status_t GraphConfigPipe::getProgramGroupsByName(const std::vector<std::string>& pgNames,
+                                                 NodesPtrVector* programGroups) {
+    CheckError(!programGroups, UNKNOWN_ERROR, "%s, The programGroups is nullptr", __func__);
+    css_err_t ret = css_err_none;
+    GraphConfigNode* result;
+    NodesPtrVector allProgramGroups;
+    string foundPgName;
+
+    GraphConfigNode::const_iterator it = mSettings->begin();
+
+    while (it != mSettings->end()) {
+        ret = mSettings->getDescendant(GCSS_KEY_TYPE, "program_group", it, &result);
+        if (ret == css_err_none) allProgramGroups.push_back(result);
+    }
+
+    CheckError(allProgramGroups.empty(), UNKNOWN_ERROR,
+               "%s, doesn't find any PG in current pipe: %d", __func__, mPipeUseCase);
+    for (auto& ndVec : allProgramGroups) {
+        ret = ndVec->getValue(GCSS_KEY_NAME, foundPgName);
+        if (ret != css_err_none) {
+            LOGE("%s, failed to get pg name in program group", __func__);
+            continue;
+        }
+
+        for (auto& name : pgNames) {
+            if (foundPgName.find(name) != string::npos) {
+                programGroups->push_back(ndVec);
+            }
+        }
+    }
+    if (programGroups->empty()) {
+        LOG2("%s, doesn't find the matched pg in current pipe: %d", __func__, mPipeUseCase);
+    }
+
+    return OK;
+}
+
+status_t GraphConfigPipe::pipelineGetConnections(
+    const std::vector<std::string>& pgList, std::vector<IGraphType::ScalerInfo>* scalerInfo,
+    std::vector<IGraphType::PipelineConnection>* confVector) {
+    CheckError(!confVector, UNKNOWN_ERROR, "%s, the confVector is nullptr", __func__);
+
+    NodesPtrVector programGroups;
+    NodesPtrVector alreadyConnectedPorts;
+    Node* peerPort = nullptr;
+    Node* port = nullptr;
+    IGraphType::PipelineConnection aConnection;
+    std::map<Node*, IGraphType::PipelineConnection> edgePort2Connection;
+
+    status_t status = getProgramGroupsByName(pgList, &programGroups);
+    CheckError(status != OK, status, "%s, failed to get program groups, BUG", __func__);
+
+    for (size_t i = 0; i < programGroups.size(); i++) {
+        Node::const_iterator it = programGroups[i]->begin();
+
+        while (it != programGroups[i]->end()) {
+            css_err_t ret = programGroups[i]->getDescendant(GCSS_KEY_TYPE, "port", it, &port);
+            if (ret != css_err_none) continue;
+
+            // port for private terminal, no need to connect
+            int priv = 0;
+            ret = port->getValue(GCSS_KEY_PRIVATE, priv);
+            if (ret == css_err_none && priv) continue;
+
+            /*
+             * Since we are iterating through the ports
+             * check if this port is already connected to avoid setting
+             * the connection twice
+             */
+            if (std::find(alreadyConnectedPorts.begin(), alreadyConnectedPorts.end(), port) !=
+                alreadyConnectedPorts.end()) {
+                continue;
+            }
+            LOG1("Configuring Port from PG[%zu] in line:%d", i, __LINE__);
+
+            string contentType;
+            ret = port->getValue(GCSS_KEY_CONTENT_TYPE, contentType);
+            if (ret == css_err_none && contentType != "pixel_data") {
+                LOG2("%s skipped content type %s", NODE_NAME(port), contentType.c_str());
+                continue;
+            }
+
+            status = portGetFormat(port, &(aConnection.portFormatSettings));
+            if (status != OK) {
+                LOGE("Failed to get port format info in port from PG[%zu]", i);
+                return BAD_VALUE;
+            }
+            if (aConnection.portFormatSettings.enabled == 0) {
+                LOG1("Port from PG[%zu] disabled", i);
+                status = portGetOwner(port, &(aConnection.connectionConfig));
+                CheckError((status != OK), BAD_VALUE, "Failed to get ownerfor port from PG[%zu]",
+                           i);
+                confVector->push_back(aConnection);
+                continue;
+            } else {
+                LOG1("Port: 0x%x format(%dx%d)fourcc: %s bpl: %d bpp: %d",
+                     aConnection.portFormatSettings.terminalId,
+                     aConnection.portFormatSettings.width, aConnection.portFormatSettings.height,
+                     CameraUtils::fourcc2String(aConnection.portFormatSettings.fourcc).c_str(),
+                     aConnection.portFormatSettings.bpl, aConnection.portFormatSettings.bpp);
+            }
+
+            /*
+             * for each port get the connection info and pass it
+             * to the pipeline object
+             */
+            status = portGetConnection(port, &(aConnection.connectionConfig), &peerPort);
+            if (status != OK) {
+                LOGE("Failed to create connection info in port from PG[%zu]", i);
+                return BAD_VALUE;
+            }
+
+            aConnection.hasEdgePort = false;
+            if (isPipeEdgePort(port)) {
+                int32_t direction = portGetDirection(port);
+                if (direction == GraphConfigPipe::PORT_DIRECTION_INPUT) {
+                    aConnection.connectionConfig.mConnectionType = IGraphType::connection_type_push;
+                } else {
+                    HalStream* clientStream = nullptr;
+                    status = portGetClientStream(peerPort, &clientStream);
+                    CheckError(status != OK, UNKNOWN_ERROR,
+                               "Failed to find client stream for v-sink");
+                    aConnection.stream = clientStream;
+                    if (clientStream != nullptr) {
+                        edgePort2Connection[port] = aConnection;
+                    }
+                }
+                aConnection.hasEdgePort = true;
+            }
+            confVector->push_back(aConnection);
+            alreadyConnectedPorts.push_back(port);
+            alreadyConnectedPorts.push_back(peerPort);
+        }
+    }
+
+    getScalerByStreamId(edgePort2Connection, scalerInfo);
+    GraphUtils::dumpConnections(*confVector);
+
+    return status;
+}
+
+status_t GraphConfigPipe::getScalerByStreamId(
+    std::map<Node*, IGraphType::PipelineConnection> edgePort2Connection,
+    std::vector<IGraphType::ScalerInfo>* scalerInfo) {
+    if (edgePort2Connection.empty()) {
+        return OK;
+    }
+    CheckError(!scalerInfo, UNKNOWN_ERROR, "%s, scalerInfo is nullptr", __func__);
+
+    for (auto it = edgePort2Connection.begin(); it != edgePort2Connection.end(); ++it) {
+        const char* portName;
+        bool mpFLag = false;
+        bool dpFlag = false;
+        bool pppFlag = false;
+        float scalerW = 1;
+        float scalerH = 1;
+
+        IGraphType::PipelineConnection connection = it->second;
+        portName = NODE_NAME(it->first);
+        CheckError(!connection.stream, UNKNOWN_ERROR, "%s, connection.stream is null.", __func__);
+        int32_t streamId = connection.stream->streamId();
+        LOG2("%s, streamId:%d, portName:%s", __func__, streamId, portName);
+
+        if (!strcmp("main", portName)) {
+            mpFLag = true;
+        } else if (!strcmp("display", portName)) {
+            dpFlag = true;
+        } else if (!strcmp("postproc", portName)) {
+            pppFlag = true;
+        }
+        if (!mpFLag && !dpFlag && !pppFlag) continue;
+
+        // if port name is main, the value of osW and osH are 1.
+        float osW = 1;
+        float osH = 1;
+        if (dpFlag) {
+            (void)getScalerKernelResolutionRatio(dpKernel, DB_KERNEL_SIZE, &osW, &osH);
+            LOG2("%s, dp ratio, osW:%f, osH:%f", __func__, osW, osH);
+        } else if (pppFlag) {
+            (void)getScalerKernelResolutionRatio(pppKernel, PPP_KERNEL_SIZE, &osW, &osH);
+            LOG2("%s, ppp ratio, osW:%f, osH:%f", __func__, osW, osH);
+        }
+
+        uint32_t kernelId;
+        float gdcScalerW = 1;
+        float gdcScalerH = 1;
+        const ia_isp_bxt_resolution_info_t* gdcResolution = getGdcKernelResolutionInfo(&kernelId);
+        if ((gdcResolution) && ((gdcResolution->input_width != gdcResolution->output_width) ||
+                                (gdcResolution->input_height != gdcResolution->output_height))) {
+            const ia_rectangle* input_crop = &gdcResolution->input_crop;
+            const ia_rectangle* output_crop = &gdcResolution->output_crop;
+            if (((input_crop->left == 0) && (input_crop->top == 0) && (input_crop->right == 0) &&
+                 (input_crop->bottom == 0)) &&
+                ((output_crop->left == 0) && (output_crop->top == 0) && (output_crop->right == 0) &&
+                 (output_crop->bottom == 0))) {
+                gdcScalerW = static_cast<float>(gdcResolution->input_width) /
+                             static_cast<float>(gdcResolution->output_width);
+                gdcScalerH = static_cast<float>(gdcResolution->input_height) /
+                             static_cast<float>(gdcResolution->output_height);
+            }
+        }
+        LOG2("%s, gdc ratio, gdcScalerW:%f, gdcScalerH:%f", __func__, gdcScalerW, gdcScalerH);
+
+        float b2iDsW = 1;
+        float b2iDsH = 1;
+        (void)getScalerKernelResolutionRatio(dsKernel, DS_KERNEL_SIZE, &b2iDsW, &b2iDsH);
+        LOG2("%s, b2iDs ratio, b2iDsW:%f, b2iDsH:%f", __func__, b2iDsW, b2iDsH);
+
+        scalerW = osW * gdcScalerW * b2iDsW;
+        scalerH = osH * gdcScalerH * b2iDsH;
+        scalerInfo->push_back({streamId, scalerW, scalerH});
+        LOG2("%s, streamId:%d, scalerW:%f, scalerH:%f", __func__, streamId, scalerW, scalerH);
+    }
+
+    return OK;
+}
+
+status_t GraphConfigPipe::portGetOwner(Node* port, IGraphType::ConnectionConfig* connectionInfo) {
+    int32_t direction = PORT_DIRECTION_INPUT;
+    css_err_t ret = port->getValue(GCSS_KEY_DIRECTION, direction);
+    CheckError((ret != css_err_none), BAD_VALUE, "Failed to get port direction");
+
+    /*
+     * Default to pull, it will be amended later,
+     * Iterations are not used
+     */
+    connectionInfo->mConnectionType = IGraphType::connection_type_pull;
+    connectionInfo->mSinkIteration = 0;
+    connectionInfo->mSourceIteration = 0;
+
+    status_t status = OK;
+    if (direction == PORT_DIRECTION_INPUT) {
+        // input port is the sink in a connection
+        status = GCSS::GraphCameraUtil::portGetFourCCInfo(*port, connectionInfo->mSinkStage,
+                                                          connectionInfo->mSinkTerminal);
+        CheckError((status != OK), BAD_VALUE, "Failed to create fourcc info for sink port");
+    } else {
+        // output port is the source in a connection
+        status = GCSS::GraphCameraUtil::portGetFourCCInfo(*port, connectionInfo->mSourceStage,
+                                                          connectionInfo->mSourceTerminal);
+        CheckError((status != OK), BAD_VALUE, "Failed to create fourcc info for source port");
+    }
+    return status;
+}
+
+/**
+ * Query the connection info structs for a given pipeline defined by
+ * stream id.
+ *
+ * \param[in] sinkName to be used as key to get pipeline connections
+ * \param[out] stream id connect with sink
+ * \param[out] connections for pipeline configuation
+ * \return OK in case of success.
+ * \return UNKNOWN_ERROR or BAD_VALUE in case of fail.
+ * \if sinkName is not supported, NAME_NOT_FOUND is returned.
+ * \sink name support list as below defined in graph_descriptor.xml
+ * \<sink name="video0"/>
+ * \<sink name="video1"/>
+ * \<sink name="video2"/>
+ * \<sink name="still0"/>
+ * \<sink name="still1"/>
+ * \<sink name="still2"/>
+ * \<sink name="raw"/>
+ */
+status_t GraphConfigPipe::pipelineGetConnections(
+    const std::string& sinkName, int* streamId,
+    std::vector<IGraphType::PipelineConnection>* confVector) {
+    CheckError(!streamId, UNKNOWN_ERROR, "the streamId is nullptr");
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL2);
+
+    std::vector<GCSS::IGraphConfig*> sinks;
+    NodesPtrVector programGroups;
+    NodesPtrVector alreadyConnectedPorts;
+    Node* peerPort = nullptr;
+    Node* port = nullptr;
+    IGraphType::PipelineConnection aConnection;
+
+    alreadyConnectedPorts.clear();
+    status_t status = GCSS::GraphCameraUtil::graphGetSinksByName(sinkName, mSettings, sinks);
+    if (status != OK || sinks.empty()) {
+        LOGD("No %s sinks in graph", sinkName.c_str());
+        return NAME_NOT_FOUND;
+    }
+
+    status = sinks[0]->getValue(GCSS_KEY_STREAM_ID, *streamId);
+    if (status != css_err_none) {
+        LOGE("Sink node lacks stream id attribute - fix your config");
+        return BAD_VALUE;
+    }
+
+    status = streamGetProgramGroups(*streamId, &programGroups);
+    if (status != OK || programGroups.empty()) {
+        LOGE("No Program groups associated with stream id %d", *streamId);
+        return BAD_VALUE;
+    }
+
+    for (size_t i = 0; i < programGroups.size(); i++) {
+        Node::const_iterator it = programGroups[i]->begin();
+
+        while (it != programGroups[i]->end()) {
+            css_err_t ret = programGroups[i]->getDescendant(GCSS_KEY_TYPE, "port", it, &port);
+            if (ret != css_err_none) continue;
+
+            // port for private terminal, no need to connect
+            int priv = 0;
+            ret = port->getValue(GCSS_KEY_PRIVATE, priv);
+            if (ret == css_err_none && priv) continue;
+
+            /*
+             * Since we are iterating through the ports
+             * check if this port is already connected to avoid setting
+             * the connection twice
+             */
+            if (std::find(alreadyConnectedPorts.begin(), alreadyConnectedPorts.end(), port) !=
+                alreadyConnectedPorts.end()) {
+                continue;
+            }
+            LOG1("Configuring Port from PG[%zu] in line:%d", i, __LINE__);
+
+            string contentType;
+            ret = port->getValue(GCSS_KEY_CONTENT_TYPE, contentType);
+            if (ret == css_err_none && contentType != "pixel_data") {
+                LOG2("%s skipped content type %s", NODE_NAME(port), contentType.c_str());
+                continue;
+            }
+
+            status = portGetFormat(port, &(aConnection.portFormatSettings));
+            if (status != OK) {
+                LOGE(
+                    "Failed to get port format info in port from PG[%zu] "
+                    "from stream id %d",
+                    i, *streamId);
+                return BAD_VALUE;
+            }
+            if (aConnection.portFormatSettings.enabled == 0) {
+                LOG1("Port from PG[%zu] from stream id %d disabled", i, *streamId);
+                confVector->push_back(aConnection);
+                continue;
+            } else {
+                LOG1("Port: 0x%x format(%dx%d)fourcc: %s bpl: %d bpp: %d",
+                     aConnection.portFormatSettings.terminalId,
+                     aConnection.portFormatSettings.width, aConnection.portFormatSettings.height,
+                     CameraUtils::fourcc2String(aConnection.portFormatSettings.fourcc).c_str(),
+                     aConnection.portFormatSettings.bpl, aConnection.portFormatSettings.bpp);
+            }
+
+            /*
+             * for each port get the connection info and pass it
+             * to the pipeline object
+             */
+            status = portGetConnection(port, &(aConnection.connectionConfig), &peerPort);
+            if (status != OK) {
+                LOGE(
+                    "Failed to create connection info in port from PG[%zu]"
+                    "from stream id %d",
+                    i, *streamId);
+                return BAD_VALUE;
+            }
+
+            aConnection.hasEdgePort = false;
+            if (isPipeEdgePort(port)) {
+                int32_t direction = portGetDirection(port);
+                if (direction == GraphConfigPipe::PORT_DIRECTION_INPUT) {
+                    aConnection.connectionConfig.mConnectionType = IGraphType::connection_type_push;
+                } else {
+                    HalStream* clientStream = nullptr;
+                    status = portGetClientStream(peerPort, &clientStream);
+                    if (status != OK) {
+                        LOGE("Failed to find client stream for v-sink");
+                        return UNKNOWN_ERROR;
+                    }
+                    aConnection.stream = clientStream;
+                }
+                aConnection.hasEdgePort = true;
+            }
+            confVector->push_back(aConnection);
+            alreadyConnectedPorts.push_back(port);
+            alreadyConnectedPorts.push_back(peerPort);
+        }
+    }
+
+    return OK;
+}
+
+/**
+ * Find distinct stream ids from the graph and return them in a vector.
+ * \param streamIds Vector to be populated with stream ids.
+ */
+status_t GraphConfigPipe::graphGetStreamIds(StreamsVector* streamIds) {
+    CheckError(!streamIds, UNKNOWN_ERROR, "%s, The streamIds is nullptr", __func__);
+    GraphConfigNode* result;
+    int32_t streamId = -1;
+    css_err_t ret;
+
+    GraphConfigNode::const_iterator it = mSettings->begin();
+    while (it != mSettings->end()) {
+        bool found = false;
+        // Find all program groups
+        ret = mSettings->getDescendant(GCSS_KEY_TYPE, "program_group", it, &result);
+        if (ret != css_err_none) continue;
+
+        ret = result->getValue(GCSS_KEY_STREAM_ID, streamId);
+        if (ret != css_err_none) continue;
+
+        // If stream id is not yet in vector, add it
+        StreamsVector::iterator ite = streamIds->begin();
+        for (; ite != streamIds->end(); ++ite) {
+            if (streamId == *ite) {
+                found = true;
+                break;
+            }
+        }
+        if (found) continue;
+
+        streamIds->push_back(streamId);
+    }
+
+    LOGG("%s: stream IDs size %d", __func__, streamIds->size());
+    return OK;
+}
+
+int32_t GraphConfigPipe::portGetStreamId(Node* port) {
+    css_err_t ret = css_err_none;
+    Node* ancestor = nullptr;
+    int32_t streamId = -1;
+
+    if (port == nullptr) {
+        LOGE("Invalid Node, cannot get the port stream id");
+        return -1;
+    }
+    ret = port->getAncestor(&ancestor);
+    if (ret != css_err_none) {
+        LOGE("Failed to get port's ancestor");
+        return -1;
+    }
+
+    ret = ancestor->getValue(GCSS_KEY_STREAM_ID, streamId);
+    if (ret != css_err_none) {
+        LOGE("Failed to get stream ID");
+        return -1;
+    }
+    return streamId;
+}
+
+/**
+ * Retrieve a list of program groups that belong to a  given stream id.
+ * Iterates through the graph configuration storing the program groups
+ * that match this stream id into the provided vector.
+ *
+ * \param[in] streamId Id of the stream to match.
+ * \param[out] programGroups Vector with the nodes that match the criteria.
+ */
+status_t GraphConfigPipe::streamGetProgramGroups(int32_t streamId, NodesPtrVector* programGroups) {
+    CheckError(!programGroups, UNKNOWN_ERROR, "%s, The programGroups is nullptr", __func__);
+    css_err_t ret = css_err_none;
+    GraphConfigNode* result;
+    NodesPtrVector allProgramGroups;
+    int32_t streamIdFound = -1;
+
+    GraphConfigNode::const_iterator it = mSettings->begin();
+
+    while (it != mSettings->end()) {
+        ret = mSettings->getDescendant(GCSS_KEY_TYPE, "program_group", it, &result);
+        if (ret == css_err_none) allProgramGroups.push_back(result);
+    }
+
+    if (allProgramGroups.empty()) {
+        LOGE(
+            "Failed to find any PG's for stream id %d"
+            " BUG(check graph config file)",
+            streamId);
+        return UNKNOWN_ERROR;
+    }
+
+    for (auto& pg : allProgramGroups) {
+        ret = pg->getValue(GCSS_KEY_STREAM_ID, streamIdFound);
+        if ((ret == css_err_none) && (streamIdFound == streamId)) {
+            programGroups->push_back(pg);
+        }
+    }
+
+    return OK;
+}
+
+/**
+ * Retrieve the graph config node of the port that is connected to a given port.
+ *
+ * \param[in] port Node with the info of the port that we want to find its peer.
+ * \param[out] peer Pointer to a node where the peer node reference will be
+ *                  stored
+ * \return OK
+ * \return INVALID_OPERATION if the port is disabled.
+ * \return BAD_VALUE if any of the graph settings is incorrect.
+ * \return NO_ENTRY if including "private" attribute which is used for private
+ *         terminal.
+ */
+status_t GraphConfigPipe::portGetPeer(Node* port, Node** peer) {
+    css_err_t ret = css_err_none;
+    int32_t enabled = 1, priv = 0;
+    string peerName;
+
+    if (port == nullptr || peer == nullptr) {
+        LOGE("Invalid Node, cannot get the peer port");
+        return BAD_VALUE;
+    }
+    ret = port->getValue(GCSS_KEY_ENABLED, enabled);
+    if (ret == css_err_none && !enabled) {
+        LOG1("This port is disabled, keep on getting the connection");
+        return INVALID_OPERATION;
+    }
+
+    // port for private terminal, no connection needed
+    ret = port->getValue(GCSS_KEY_PRIVATE, priv);
+    if (ret == css_err_none && priv) {
+        LOG2("NO_ENTRY due to key PRIVATE:%d", priv);
+        return NO_ENTRY;
+    }
+
+    ret = port->getValue(GCSS_KEY_PEER, peerName);
+    if (ret != css_err_none) {
+        LOGE("Error getting peer attribute");
+        return BAD_VALUE;
+    }
+    ret = mSettings->getDescendantByString(peerName, peer);
+    if (ret != css_err_none) {
+        LOGE("Failed to find peer by name %s", peerName.c_str());
+        return BAD_VALUE;
+    }
+    return OK;
+}
+
+/**
+ * Generate the connection configuration information for a given port.
+ *
+ * This connection configuration  information is required by CIPF to build
+ * the pipeline
+ *
+ * \param[in] port Pointer to the port node
+ * \param[out] connectionInfo point to the connection info object
+ * \param[out] peerPort Reference to the peer port
+ * \return OK in case of success
+ * \return BAD_VALUE in case of error while retrieving the information.
+ * \return INVALID_OPERATION in case of the port being disabled.
+ */
+status_t GraphConfigPipe::portGetConnection(Node* port,
+                                            IGraphType::ConnectionConfig* connectionInfo,
+                                            Node** peerPort) {
+    int32_t direction = PORT_DIRECTION_INPUT;
+
+    status_t status = portGetPeer(port, peerPort);
+    if (status == NO_ENTRY) {
+        LOG2("port for private terminal, no peer");
+        *peerPort = nullptr;
+    } else if (status != OK) {
+        if (status == INVALID_OPERATION) {
+            LOGE("Port %s disabled, cannot get the connection", getNodeName(port).c_str());
+        } else {
+            LOGE("Failed to get the peer port for port %s", getNodeName(port).c_str());
+        }
+        return status;
+    }
+
+    css_err_t ret = port->getValue(GCSS_KEY_DIRECTION, direction);
+    if (ret != css_err_none) {
+        LOGE("Failed to get port direction");
+        return BAD_VALUE;
+    }
+
+    /*
+     * Default to pull, it will be amended later,
+     * Iterations are not used
+     */
+    connectionInfo->mConnectionType = IGraphType::connection_type_pull;
+    connectionInfo->mSinkIteration = 0;
+    connectionInfo->mSourceIteration = 0;
+
+    if (direction == PORT_DIRECTION_INPUT) {
+        // input port is the sink in a connection
+        status = GCSS::GraphCameraUtil::portGetFourCCInfo(port, connectionInfo->mSinkStage,
+                                                          connectionInfo->mSinkTerminal);
+        if (status != OK) {
+            LOGE("Failed to create fourcc info for sink port");
+            return BAD_VALUE;
+        }
+        if (*peerPort != nullptr && !portIsVirtual(*peerPort)) {
+            status = GCSS::GraphCameraUtil::portGetFourCCInfo(
+                *peerPort, connectionInfo->mSourceStage, connectionInfo->mSourceTerminal);
+            if (status != OK) {
+                LOGE("Failed to create fourcc info for source port");
+                return BAD_VALUE;
+            }
+        } else {
+            connectionInfo->mSourceStage = 0;
+            connectionInfo->mSourceTerminal = 0;
+        }
+    } else {
+        // output port is the source in a connection
+        status = GCSS::GraphCameraUtil::portGetFourCCInfo(port, connectionInfo->mSourceStage,
+                                                          connectionInfo->mSourceTerminal);
+        if (status != OK) {
+            LOGE("Failed to create fourcc info for source port");
+            return BAD_VALUE;
+        }
+
+        if (*peerPort != nullptr && !portIsVirtual(*peerPort)) {
+            status = GCSS::GraphCameraUtil::portGetFourCCInfo(*peerPort, connectionInfo->mSinkStage,
+                                                              connectionInfo->mSinkTerminal);
+            if (status != OK) {
+                LOGE("Failed to create fourcc info for sink port");
+                return BAD_VALUE;
+            }
+            /**
+             * Because all the connections are used for frame flow , so
+             * create one implicit rule which sets the mSinkTerminal as
+             * same as mSourceTerminal to handle the parameter or
+             * hanging port. And then exclude this connection when binding
+             * the port of executor.
+             */
+        } else if (*peerPort != nullptr && portIsVirtual(*peerPort) &&
+                   getNodeName(*peerPort).find(getNodeName(port)) != string::npos) {
+            LOG2("%s, found one hanging port: %s, peer port: %s", __func__,
+                 getNodeName(port).c_str(), getNodeName(*peerPort).c_str());
+            connectionInfo->mSinkStage = 0;
+            connectionInfo->mSinkTerminal = connectionInfo->mSourceTerminal;
+        } else {
+            connectionInfo->mSinkStage = 0;
+            connectionInfo->mSinkTerminal = 0;
+        }
+    }
+
+    return status;
+}
+
+/**
+ * Retrieve the format information of a port
+ * if the port doesn't have any format set, it gets the format from the peer
+ * port (i.e. the port connected to this one)
+ *
+ * \param[in] port Port to query the format.
+ * \param[out] format Format settings for this port.
+ */
+status_t GraphConfigPipe::portGetFormat(Node* port, IGraphType::PortFormatSettings* format) {
+    GraphConfigNode* peerNode = nullptr;  // The peer port node
+    GraphConfigNode* tmpNode = port;      // The port node node we are interrogating
+    css_err_t ret = css_err_none;
+    ia_uid stageId;  // ignored
+
+    if (port == nullptr) {
+        LOGE("Invalid parameter, could not get port format");
+        return BAD_VALUE;
+    }
+
+    ret = port->getValue(GCSS_KEY_ENABLED, format->enabled);
+    if (ret != css_err_none) {
+        // if not present by default is enabled
+        format->enabled = 1;
+    }
+
+    status_t status =
+        GCSS::GraphCameraUtil::portGetFourCCInfo(tmpNode, stageId, format->terminalId);
+    if (status != OK) {
+        LOGE("Could not get port uid");
+        return INVALID_OPERATION;
+    }
+
+    // if disabled there is no need to query the format
+    if (format->enabled == 0) {
+        return OK;
+    }
+
+    format->width = 0;
+    format->height = 0;
+
+    ret = port->getValue(GCSS_KEY_WIDTH, format->width);
+    if (ret != css_err_none) {
+        /*
+         * It could be the port configuration is not in settings, that is normal
+         * it means that we need to ask the format from the peer.
+         */
+        status = portGetPeer(port, &peerNode);
+        if (status != OK) {
+            LOGE("Could not find peer port - Fix your graph");
+            return BAD_VALUE;
+        }
+
+        tmpNode = peerNode;
+
+        ret = tmpNode->getValue(GCSS_KEY_WIDTH, format->width);
+        if (ret != css_err_none) {
+            LOGE("Could not find port format info: width (from peer)");
+            return BAD_VALUE;
+        }
+    }
+
+    ret = tmpNode->getValue(GCSS_KEY_HEIGHT, format->height);
+    if (ret != css_err_none) {
+        LOGE("Could not find port format info: height");
+        return BAD_VALUE;
+    }
+
+    string fourccFormat;
+    ret = tmpNode->getValue(GCSS_KEY_FORMAT, fourccFormat);
+    if (ret != css_err_none) {
+        LOGE("Could not find port format info: fourcc");
+        return BAD_VALUE;
+    }
+
+    const char* pFormat = fourccFormat.c_str();
+    format->fourcc = CameraUtils::string2IaFourccCode(pFormat);
+    format->bpl = gcu::getBpl(format->fourcc, format->width);
+    LOG2("bpl set to %d for %s", format->bpl, fourccFormat.c_str());
+
+    // if settings are specifying bpl, owerwrite the calculated one
+    int bplFromSettings = 0;
+    ret = tmpNode->getValue(GCSS_KEY_BYTES_PER_LINE, bplFromSettings);
+    if (ret == css_err_none) {
+        LOG2("Overwriting bpl(%d) from settings %d", format->bpl, bplFromSettings);
+        format->bpl = bplFromSettings;
+    }
+
+    format->bpp = gcu::getBppFromCommon(format->fourcc);
+
+    return OK;
+}
+
+/**
+ * Return the port direction
+ *
+ * \param[in] port Reference to port Graph node
+ * \return 0 if it is an input port
+ * \return 1 if it is an output port
+ */
+int32_t GraphConfigPipe::portGetDirection(Node* port) {
+    int32_t direction = 0;
+    css_err_t ret = port->getValue(GCSS_KEY_DIRECTION, direction);
+    if (ret != css_err_none) {
+        LOGE("Failed to retrieve port direction, default to input");
+    }
+
+    return direction;
+}
+
+/**
+ * Return the port full name
+ * The port full name is made out from:
+ * - the name program group it belongs to
+ * - the name of the port
+ * separated by ":"
+ *
+ * \param[in] port Reference to port Graph node
+ * \param[out] fullName reference to a string to store the full name
+ *
+ * \return OK if everything went fine.
+ * \return BAD_VALUE if any of the graph queries failed.
+ */
+status_t GraphConfigPipe::portGetFullName(Node* port, string* fullName) {
+    CheckError(!fullName, UNKNOWN_ERROR, "%s, the fullName is nullptr", __func__);
+    string portName, ancestorName;
+    Node* ancestor;
+    css_err_t ret = css_err_none;
+
+    if (port == nullptr) {
+        LOGE("Invalid parameter, could not get port full name");
+        return BAD_VALUE;
+    }
+
+    ret = port->getAncestor(&ancestor);
+    if (ret != css_err_none) {
+        LOGE("Failed to retrieve port ancestor");
+        return BAD_VALUE;
+    }
+    ret = ancestor->getValue(GCSS_KEY_NAME, ancestorName);
+    if (ret != css_err_none) {
+        LOGE("Failed to get ancestor name for port");
+        port->dumpNodeTree(port, 1);
+        return BAD_VALUE;
+    }
+
+    ret = port->getValue(GCSS_KEY_NAME, portName);
+    if (ret != css_err_none) {
+        LOGE("Failed to retrieve port name");
+        return BAD_VALUE;
+    }
+
+    *fullName = ancestorName + ":" + portName;
+    return OK;
+}
+
+/**
+ * Perform a reverse lookup on the map that associates client streams to
+ * virtual sinks.
+ *
+ * This method is used during pipeline configuration to find a stream associated
+ * with the id (GCSS key) of the virtual sink
+ *
+ * \param[in] vPortId GCSS key representing one of the virtual sinks in the
+ *                    graph, like GCSS_KEY_VIDEO1
+ * \return nullptr if not found
+ * \return pointer to the client stream associated with that virtual sink.
+ */
+HalStream* GraphConfigPipe::getHalStreamByVirtualId(uid_t vPortId) {
+    for (auto& it : mStreamToSinkIdMap) {
+        if (it.second == vPortId) {
+            return it.first;
+        }
+    }
+
+    return nullptr;
+}
+
+/**
+ * Return true if the port is a virtual port, this is the end point
+ * of the graph.
+ * Virtual ports are the nodes of type sink.
+ *
+ * \param[in] port Reference to port Graph node
+ * \return true if it is a virtual port
+ * \return false if it is not a virtual port
+ */
+bool GraphConfigPipe::portIsVirtual(Node* port) {
+    string type;
+    css_err_t ret = port->getValue(GCSS_KEY_TYPE, type);
+    if (ret != css_err_none) {
+        LOGE("Failed to retrieve port type, default to input");
+    }
+
+    return (type == string("sink"));
+}
+
+/**
+ * retrieve the pointer to the client stream associated with a virtual sink
+ *
+ * I.e. access the mapping done at stream config time between the pointers
+ * to camera3_stream_t and the names video0, video1, still0 etc...
+ *
+ * \param[in] port Node to the virtual sink (with name videoX or stillX etc..)
+ * \param[out] stream Pointer to the client stream associated with that virtual
+ *                    sink.
+ * \return OK
+ * \return BAD_VALUE in case of invalid parameters (null pointers)
+ * \return INVALID_OPERATION in case the Node is not a virtual sink.
+ */
+status_t GraphConfigPipe::portGetClientStream(Node* port, HalStream** stream) {
+    if (!port || !stream) {
+        LOGE("Could not get client stream - bad parameters");
+        return BAD_VALUE;
+    }
+
+    if (!portIsVirtual(port)) {
+        LOGE("Trying to find the client stream from a non virtual port");
+        return INVALID_OPERATION;
+    }
+
+    string portName;
+    css_err_t ret = port->getValue(GCSS_KEY_NAME, portName);
+    if (ret != css_err_none) {
+        LOGE("Failed to get name for port");
+        port->dumpNodeTree(port, 1);
+        return BAD_VALUE;
+    }
+
+    uid_t vPortId = GCSS::ItemUID::str2key(portName);
+    *stream = getHalStreamByVirtualId(vPortId);
+
+    return OK;
+}
+
+/**
+ * A port is at the edge of the video stream (pipeline) if its peer's stream id is 0 or -1,
+ * or if its peer is a virtual sink.
+ *
+ * Here we check for both conditions and return true if this port is at either
+ * edge of a pipeline
+ */
+bool GraphConfigPipe::isPipeEdgePort(Node* port) {
+    CheckError(!port, false, "%s, the port is nullptr", __func__);
+    Node* peer = nullptr;
+    Node* peerAncestor = nullptr;
+    int32_t streamId = -1;
+    int32_t peerStreamId = -1;
+    string peerType;
+
+    int32_t portDirection = portGetDirection(port);
+
+    status_t status = portGetPeer(port, &peer);
+    if (status == INVALID_OPERATION) {
+        LOG1("port is disabled, so it is an edge port");
+        return true;
+    }
+    if (status != OK) {
+        LOGE("Failed to create fourcc info for source port");
+        return false;
+    }
+
+    streamId = portGetStreamId(port);
+    if (streamId < 0) return false;
+    /*
+     * get the stream id of the peer port
+     * we also check the ancestor for that. If the peer is a virtual sink then
+     * it does not have ancestor.
+     */
+    if (!portIsVirtual(peer)) {
+        css_err_t ret = peer->getAncestor(&peerAncestor);
+        if (ret != css_err_none) {
+            LOGE("Failed to get peer's ancestor");
+            return false;
+        }
+        ret = peerAncestor->getValue(GCSS_KEY_STREAM_ID, peerStreamId);
+        if (ret != css_err_none) {
+            LOGE("Failed to get stream ID of peer PG");
+            return false;
+        }
+        /*
+         * Retrieve the type of node the peer ancestor is. It could be is not a
+         * program group node but a sink or hw block
+         */
+        peerAncestor->getValue(GCSS_KEY_TYPE, peerType);
+    }
+
+    LOG1("%s port direction: %d, port stream id:%d, peer stream id:%d", __func__, portDirection,
+         streamId, peerStreamId);
+
+    bool isEdge = false;
+    if (portDirection == GraphConfigPipe::PORT_DIRECTION_INPUT) {
+        /*
+         *  input port,
+         *  if the peer is a source or hw block then it is on the edge,
+         *  or its stream id is 0 or -1.
+         */
+        isEdge = (peerType == string("hw") || peerStreamId == 0 || peerStreamId == -1);
+    } else {
+        /*
+         *  output port,
+         *  if the peer is a virtual port, or its stream id is 0 or -1,
+         *  then it is on the edge,
+         */
+        isEdge = (portIsVirtual(peer) || peerStreamId == 0 || peerStreamId == -1);
+    }
+
+    return isEdge;
+}
+
+void GraphConfigPipe::dumpSettings() {
+    mSettings->dumpNodeTree(mSettings, 2);
+}
+
+GraphConfigPipe::Rectangle::Rectangle() : w(0), h(0), t(0), l(0) {}
+GraphConfigPipe::SubdevPad::SubdevPad() : Rectangle(), mbusFormat(0) {}
+GraphConfigPipe::SourceNodeInfo::SourceNodeInfo() : metadataEnabled(false), interlaced(0) {}
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/algowrapper/graph/GraphConfigPipe.h b/camera/hal/intel/ipu6/modules/algowrapper/graph/GraphConfigPipe.h
new file mode 100644
index 000000000000..b9a044713c94
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/algowrapper/graph/GraphConfigPipe.h
@@ -0,0 +1,313 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <gcss.h>
+#include <gcss_aic_utils.h>
+#include <ia_aiq.h>
+#include <ia_isp_bxt_types.h>
+
+#include <map>
+#include <memory>
+#include <set>
+#include <string>
+#include <vector>
+
+#include "iutils/CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+#include "src/platformdata/gc/IGraphConfig.h"
+
+#define NODE_NAME(x) (getNodeName(x).c_str())
+
+namespace icamera {
+
+/**
+ * Stream id associated with the ISA PG that runs on Psys.
+ */
+static const int32_t PSYS_ISA_STREAM_ID = 60002;
+
+/**
+ * \class SinkDependency
+ *
+ * This class is a container for sink dependency information for each virtual sink.
+ * This information is useful to determine the connections that preceded the
+ * virtual sink.
+ * We do not go all the way up to the sensor (we could), we just store the
+ * terminal id of the input port of the pipeline that serves a particular sink
+ * (i.e. the input port of the video pipe or still pipe)
+ */
+class SinkDependency {
+ public:
+    SinkDependency() : sinkGCKey(0), streamId(-1), streamInputPortId(0), peer(nullptr) {}
+
+    uid_t sinkGCKey;             /**< GCSS_KEY that represents a sink, like GCSS_KEY_VIDEO1 */
+    int32_t streamId;            /**< (a.k.a pipeline id) linked to this sink (ex 60000) */
+    uid_t streamInputPortId;     /**< 4CC code of that terminal */
+    GCSS::GraphConfigNode* peer; /**< pointer to peer of this sink */
+};
+/**
+ * \class GraphConfigPipe
+ *
+ * Reference and accessor to pipe configuration for specific request.
+ *
+ * In general case, at sream-config time there are multiple possible graphs.
+ * Per each request there is additional intent that can narrow down the
+ * possibilities to single graph settings: the GraphConfigPipe object.
+ *
+ * This class is instantiated by \class GraphConfigManager for each request,
+ * and passed around HAL (control unit, capture unit, processing unit) via
+ * shared pointers. The objects are read-only and owned by GCM.
+ */
+class GraphConfigPipe {
+ public:
+    typedef std::vector<int32_t> StreamsVector;
+    typedef std::map<int32_t, int32_t> StreamsMap;
+    typedef std::map<HalStream*, uid_t> StreamToSinkMap;
+    static const int32_t PORT_DIRECTION_INPUT = 0;
+    static const int32_t PORT_DIRECTION_OUTPUT = 1;
+
+ public:
+    explicit GraphConfigPipe(int pipeUseCase);
+    ~GraphConfigPipe();
+
+    void init(int32_t reqId);
+    int getGraphId(void);
+    void setActiveSinks(const std::vector<uid_t>& activeSinks);
+    void setActiveStreamId(const std::vector<uid_t>& activeSinks);
+    /*
+     * Convert Node to GraphConfig interface
+     */
+    const GCSS::IGraphConfig* getInterface(Node* node) const;
+    const GCSS::IGraphConfig* getInterface() const;
+    ia_isp_bxt_program_group* getProgramGroup(int32_t streamId);
+    int getProgramGroup(std::string pgName, ia_isp_bxt_program_group* programGroupForPG);
+    status_t getGdcKernelSetting(uint32_t* kernelId, ia_isp_bxt_resolution_info_t* resolution);
+    const ia_isp_bxt_resolution_info_t* getKernelResolutionInfo(uint32_t streamId,
+                                                                uint32_t kernelId);
+    bool hasStreamInGraph(int streamId);
+    bool isKernelInStream(uint32_t streamId, uint32_t kernelId);
+    status_t getPgIdForKernel(const uint32_t streamId, const int32_t kernelId, int32_t* pgId);
+    int32_t getTuningMode(int32_t streamId);
+    status_t getMBRData(int32_t streamId, ia_isp_bxt_gdc_limits* data);
+    status_t prepare(Node* settings, const StreamToSinkMap& streamToSinkIdMap);
+    bool containPgs(std::vector<std::string> pgNames);
+
+    /*
+     * Find distinct stream ids from the graph
+     */
+    status_t graphGetStreamIds(std::vector<int32_t>* streamIds);
+    /*
+     * Sink Interrogation methods
+     */
+    int32_t portGetStreamId(Node* port);
+    /*
+     * Stream Interrogation methods
+     */
+    status_t streamGetProgramGroups(int32_t streamId, NodesPtrVector* programGroups);
+    /*
+     * Port Interrogation methods
+     */
+    status_t portGetFullName(Node* port, std::string* fullName);
+    status_t portGetPeer(Node* port, Node** peer);
+    status_t portGetFormat(Node* port, IGraphType::PortFormatSettings* format);
+    status_t portGetConnection(Node* port, IGraphType::ConnectionConfig* connectionInfo,
+                               Node** peerPort);
+    status_t portGetClientStream(Node* port, HalStream** stream);
+    int32_t portGetDirection(Node* port);
+    bool portIsVirtual(Node* port);
+    bool isPipeEdgePort(Node* port);  // TODO: should be renamed as portIsEdgePort
+
+    bool getSensorEmbeddedMetadataEnabled() const { return mMetaEnabled; }
+    /*
+     * Pipeline connection support
+     */
+    status_t pipelineGetConnections(const std::string& sinkName, int* streamId,
+                                    std::vector<IGraphType::PipelineConnection>* confVector);
+    status_t portGetOwner(Node* port, IGraphType::ConnectionConfig* connectionInfo);
+    status_t pipelineGetConnections(const std::vector<std::string>& pgList,
+                                    std::vector<IGraphType::ScalerInfo>* scalerInfo,
+                                    std::vector<IGraphType::PipelineConnection>* confVector);
+
+    status_t getPgNames(std::vector<std::string>* pgNames);
+    status_t getPgRbmValue(std::string pgName, IGraphType::StageAttr* stageAttr);
+
+    /**
+     * Get PG id by given PG name.
+     * -1 will be returned if cannot find the valid PG id.
+     */
+    int getPgIdByPgName(std::string pgName);
+
+    /**
+     * Get PG streamId by given PG name.
+     * -1 will be returned if cannot find the valid PG id.
+     */
+    int getStreamIdByPgName(std::string pgName);
+    /*
+     * re-cycler static method
+     */
+    static void reset(GraphConfigPipe* me);
+    void fullReset();
+    /*
+     * Debugging support
+     */
+    void dumpSettings();
+    void dumpKernels(int32_t streamId);
+    std::string getNodeName(Node* node);
+    void getCSIOutputResolution(camera_resolution_t* reso) { *reso = mCsiOutput; }
+
+ private:
+    struct ResolutionMemPool {
+        std::vector<ia_isp_bxt_resolution_info_t*> resHistorys;
+        std::vector<ia_isp_bxt_resolution_info_t*> resInfos;
+    };
+    /* Helper structures to access Sensor Node information easily */
+    class Rectangle {
+     public:
+        Rectangle();
+        int32_t w; /*<! width */
+        int32_t h; /*<! height */
+        int32_t t; /*<! top */
+        int32_t l; /*<! left */
+    };
+    class SubdevPad : public Rectangle {
+     public:
+        SubdevPad();
+        int32_t mbusFormat;
+    };
+    struct BinFactor {
+        int32_t h;
+        int32_t v;
+    };
+    struct ScaleFactor {
+        int32_t num;
+        int32_t denom;
+    };
+    union RcFactor {  // Resolution Changing factor
+        BinFactor bin;
+        ScaleFactor scale;
+    };
+    struct SubdevInfo {
+        std::string name;
+        SubdevPad in;
+        SubdevPad out;
+        RcFactor factor;
+        SubdevInfo() { CLEAR(factor); }
+    };
+    class SourceNodeInfo {
+     public:
+        SourceNodeInfo();
+        std::string name;
+        std::string i2cAddress;
+        std::string modeId;
+        bool metadataEnabled;
+        std::string csiPort;
+        std::string nativeBayer;
+        SubdevInfo tpg;
+        SubdevInfo pa;
+        SubdevInfo binner;
+        SubdevInfo scaler;
+        SubdevPad output;
+        int32_t interlaced;
+        std::string verticalFlip;
+        std::string horizontalFlip;
+        std::string link_freq;
+    };
+    status_t analyzeSourceType();
+    status_t analyzeCSIOutput();
+    void calculateSinkDependencies();
+    void storeTuningModes();
+    HalStream* getHalStreamByVirtualId(uid_t vPortId);
+
+    // Format options methods
+    status_t getActiveOutputPorts(const StreamToSinkMap& streamToSinkIdMap);
+    Node* getOutputPortForSink(const std::string& sinkName);
+    status_t getSinkFormatOptions();
+    status_t handleDynamicOptions();
+    status_t setPortFormats();
+    bool isVideoRecordPort(Node* sink);
+    status_t getProgramGroupsByName(const std::vector<std::string>& pgNames,
+                                    NodesPtrVector* programGroups);
+    const ia_isp_bxt_resolution_info_t* getGdcKernelResolutionInfo(uint32_t* kernelId);
+    const ia_isp_bxt_resolution_info_t* getScalerKernelResolutionInfo(uint32_t* kenerArray,
+                                                                      uint32_t sizeArray);
+    status_t getScalerKernelResolutionRatio(uint32_t* kenerArray, uint32_t sizeArray,
+                                            float* widthRatio, float* heightRatio);
+    status_t getScalerByStreamId(
+        std::map<Node*, IGraphType::PipelineConnection> edgePort2Connection,
+        std::vector<IGraphType::ScalerInfo>* scalerInfo);
+
+ private:
+    GCSS::GraphConfigNode* mSettings;
+    int32_t mReqId;
+    std::map<int32_t, ia_isp_bxt_program_group> mProgramGroup;
+    GCSS::BxtAicUtils mGCSSAicUtil;
+
+    bool mMetaEnabled;  // indicates if the specific sensor provides sensor
+                        // embedded metadata
+    enum SourceType {
+        SRC_NONE = 0,
+        SRC_SENSOR,
+        SRC_TPG,
+    };
+    SourceType mSourceType;
+    camera_resolution_t mCsiOutput;
+    std::string mSourcePortName;  // Sensor or TPG port name
+
+    /**
+     * pre-computed state done *per request*.
+     * This map holds the terminal id's of the ISA's peer ports (this is
+     * the terminal id's of the input port of the video or still pipe)
+     * that are required to fulfill a request.
+     * Ideally this gets initialized during init() call.
+     * But for now the GcManager will set it via a private method.
+     * we use a map so that we can handle the case when a request has 2 buffers
+     * that are generated from the same pipe.
+     */
+    std::map<uid_t, uid_t> mIsaActiveDestinations;
+    std::set<int32_t> mActiveStreamId;
+    /**
+     * vector holding one structure per virtual sink that stores the stream id
+     * (pipeline id) associated with it and the terminal id of the input port
+     * of that stream.
+     * This vector is updated once per stream config.
+     */
+    std::vector<SinkDependency> mSinkDependencies;
+    /**
+     * vector holding the peers to the sink nodes. Map contains pairs of
+     * {sink, peer}.
+     * This map is filled at stream config time.
+     */
+    std::map<Node*, Node*> mSinkPeerPort;
+    /**
+     *copy of the map provided from GraphConfigManager to be used internally.
+     */
+    StreamToSinkMap mStreamToSinkIdMap;
+    std::map<std::string, int32_t> mIsaOutputPort2StreamId;
+    /**
+     * Map of tuning modes per stream id
+     * Key: stream id
+     * Value: tuning mode
+     */
+    std::map<int32_t, int32_t> mStream2TuningMap;
+    int mPipeUseCase;
+
+    // Disable copy constructor and assignment operator
+    DISALLOW_COPY_AND_ASSIGN(GraphConfigPipe);
+};
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/memory/Camera3BufferPool.cpp b/camera/hal/intel/ipu6/modules/memory/Camera3BufferPool.cpp
new file mode 100644
index 000000000000..40b81822132b
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/memory/Camera3BufferPool.cpp
@@ -0,0 +1,127 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Camera3BufferPool"
+
+#include "Camera3BufferPool.h"
+
+#include "HALv3Utils.h"
+#include "iutils/Utils.h"
+
+namespace camera3 {
+
+Camera3BufferPool::Camera3BufferPool() {
+    LOG1("@%s", __func__);
+}
+
+Camera3BufferPool::~Camera3BufferPool() {
+    LOG1("@%s", __func__);
+    destroyBufferPool();
+}
+
+// Create the buffer pool with HEAP buffer
+icamera::status_t Camera3BufferPool::createBufferPool(int cameraId, uint32_t numBufs,
+                                                      const icamera::stream_t& stream) {
+    LOG1("@%s number of buffers:%d", __func__, numBufs);
+    std::lock_guard<std::mutex> l(mLock);
+    mBuffers.clear();
+
+    for (uint32_t i = 0; i < numBufs; i++) {
+        std::shared_ptr<Camera3Buffer> buffer = MemoryUtils::allocateHeapBuffer(
+            stream.width, stream.height, stream.stride, stream.format, cameraId, stream.size);
+        if (!buffer) {
+            mBuffers.clear();
+            LOGE("failed to alloc %d internal buffers", i);
+            return icamera::NO_MEMORY;
+        }
+
+        // Initialize the buffer status to free
+        mBuffers[buffer] = false;
+    }
+
+    return icamera::OK;
+}
+
+// Create the buffer pool with GFX handle buffer
+icamera::status_t Camera3BufferPool::createBufferPool(int cameraId, uint32_t numBufs, int width,
+                                                      int height, int gfxFmt, int usage) {
+    LOG1("@%s number of buffers:%d", __func__, numBufs);
+    std::lock_guard<std::mutex> l(mLock);
+    mBuffers.clear();
+
+    for (uint32_t i = 0; i < numBufs; i++) {
+        std::shared_ptr<Camera3Buffer> buffer =
+            MemoryUtils::allocateHandleBuffer(width, height, gfxFmt, usage, cameraId);
+        if (!buffer || buffer->lock() != icamera::OK) {
+            mBuffers.clear();
+            LOGE("failed to alloc %d internal buffers", i);
+            return icamera::NO_MEMORY;
+        }
+
+        // Initialize the buffer status to free
+        mBuffers[buffer] = false;
+    }
+
+    return icamera::OK;
+}
+
+void Camera3BufferPool::destroyBufferPool() {
+    LOG1("@%s Internal buffers size:%zu", __func__, mBuffers.size());
+
+    std::lock_guard<std::mutex> l(mLock);
+    mBuffers.clear();
+}
+
+std::shared_ptr<Camera3Buffer> Camera3BufferPool::acquireBuffer() {
+    std::lock_guard<std::mutex> l(mLock);
+    for (auto& buf : mBuffers) {
+        if (!buf.second) {
+            buf.second = true;
+            LOG2("%s addr:%p", __func__, buf.first->data());
+            return buf.first;
+        }
+    }
+
+    LOGE("%s all the internal buffers are busy", __func__);
+    return nullptr;
+}
+
+void Camera3BufferPool::returnBuffer(std::shared_ptr<Camera3Buffer> buffer) {
+    std::lock_guard<std::mutex> l(mLock);
+    for (auto& buf : mBuffers) {
+        if (buf.second && buf.first == buffer) {
+            LOG2("%s addr:%p", __func__, buffer->data());
+            buf.second = false;
+            return;
+        }
+    }
+
+    LOGE("%s, the internal buffer addr:%p not found", __func__, buffer->data());
+}
+
+std::shared_ptr<Camera3Buffer> Camera3BufferPool::findBuffer(void* memAddr) {
+    std::lock_guard<std::mutex> l(mLock);
+    for (auto& buf : mBuffers) {
+        if (buf.second && buf.first->data() == memAddr) {
+            LOG2("%s addr:%p", __func__, memAddr);
+            return buf.first;
+        }
+    }
+
+    LOGE("%s, Failed to find the internal buffer addr: %p", __func__, memAddr);
+    return nullptr;
+}
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/modules/memory/Camera3BufferPool.h b/camera/hal/intel/ipu6/modules/memory/Camera3BufferPool.h
new file mode 100644
index 000000000000..e08bdc9b7136
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/memory/Camera3BufferPool.h
@@ -0,0 +1,53 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <mutex>
+#include <unordered_map>
+
+#include "Camera3Buffer.h"
+
+namespace camera3 {
+
+/**
+ * \class Camera3BufferPool
+ *
+ * This class is used to manage a memory pool based on Camera3Buffer
+ * It needs to follow the calling sequence:
+ * createBufferPool -> acquireBuffer -> findBuffer -> returnBuffer
+ */
+class Camera3BufferPool {
+ public:
+    Camera3BufferPool();
+    ~Camera3BufferPool();
+
+    icamera::status_t createBufferPool(int cameraId, uint32_t numBufs,
+                                       const icamera::stream_t& stream);
+    icamera::status_t createBufferPool(int cameraId, uint32_t numBufs, int width, int height,
+                                       int gfxFmt, int usage);
+
+    void destroyBufferPool();
+    std::shared_ptr<Camera3Buffer> acquireBuffer();
+    void returnBuffer(std::shared_ptr<Camera3Buffer> buffer);
+    std::shared_ptr<Camera3Buffer> findBuffer(void* memAddr);
+
+ private:
+    std::unordered_map<std::shared_ptr<Camera3Buffer>, bool> mBuffers;
+    // first: camera3Buffer, second: true as buffer in used
+    std::mutex mLock;  // lock the mBuffers
+};
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/modules/memory/chrome/Camera3Buffer.cpp b/camera/hal/intel/ipu6/modules/memory/chrome/Camera3Buffer.cpp
new file mode 100644
index 000000000000..7be88eb6d2ae
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/memory/chrome/Camera3Buffer.cpp
@@ -0,0 +1,527 @@
+/*
+ * Copyright (C) 2013-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Camera3Buffer"
+
+#include "Camera3Buffer.h"
+
+#include <sync/sync.h>
+#include <sys/mman.h>
+#include <unistd.h>
+
+#include "Camera3Stream.h"
+#include "HALv3Utils.h"
+#include "iutils/CameraDump.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+using namespace icamera;
+
+namespace camera3 {
+////////////////////////////////////////////////////////////////////
+// PUBLIC METHODS
+////////////////////////////////////////////////////////////////////
+
+/**
+ * Camera3Buffer
+ *
+ * Default constructor
+ * This constructor is used when we pre-allocate the Camera3Buffer object
+ * The initialization will be done as a second stage wit the method
+ * init(), where we initialize the wrapper with the gralloc buffer provided by
+ * the framework
+ */
+Camera3Buffer::Camera3Buffer()
+        : mFormat(0),
+          mInit(false),
+          mLocked(false),
+          mType(BUF_TYPE_HANDLE),
+          mHandlePtr(nullptr),
+          mCameraId(0),
+          mRegistered(false),
+          mGbmBufferManager(nullptr) {
+    CLEAR(mHalBuffer);
+    mHalBuffer.dmafd = -1;
+    LOG1("%s default constructor for buf %p", __func__, this);
+}
+
+/**
+ * Camera3Buffer
+ *
+ * Constructor for buffers allocated using MemoryUtils::allocateHeapBuffer
+ *
+ * \param w [IN] width
+ * \param h [IN] height
+ * \param s [IN] stride
+ * \param v4l2fmt [IN] V4l2 format
+ * \param usrPtr [IN] Data pointer
+ * \param cameraId [IN] id of camera being used
+ * \param dataSizeOverride [IN] buffer size input. Default is 0 and frameSize()
+                                is used in that case.
+ */
+Camera3Buffer::Camera3Buffer(int w, int h, int stride, int v4l2fmt, void* usrPtr, int cameraId,
+                             int dataSizeOverride)
+        : mFormat(0),
+          mInit(false),
+          mLocked(true),
+          mType(BUF_TYPE_MALLOC),
+          mHandlePtr(nullptr),
+          mCameraId(cameraId)
+
+{
+    LOG1("%s create malloc camera buffer %p", __func__, this);
+
+    CLEAR(mHalBuffer);
+    mHalBuffer.s.format = v4l2fmt;
+    mHalBuffer.s.width = w;
+    mHalBuffer.s.height = h;
+    mHalBuffer.s.stride = stride;
+    mHalBuffer.s.memType = V4L2_MEMORY_USERPTR;
+    mHalBuffer.flags = camera_buffer_flags_t::BUFFER_FLAG_SW_WRITE;
+    mHalBuffer.dmafd = -1;
+
+    if (usrPtr != nullptr) {
+        mHalBuffer.addr = usrPtr;
+        mInit = true;
+        mHalBuffer.s.size =
+            dataSizeOverride ? dataSizeOverride : CameraUtils::getFrameSize(v4l2fmt, stride, h);
+    } else {
+        LOGE("Tried to initialize a buffer with nullptr ptr!!");
+    }
+}
+
+Camera3Buffer::~Camera3Buffer() {
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL1);
+
+    if (mInit) {
+        switch (mType) {
+            case BUF_TYPE_MALLOC:
+                free(mHalBuffer.addr);
+                mHalBuffer.addr = nullptr;
+                break;
+            case BUF_TYPE_MMAP:
+                if (mHalBuffer.addr != nullptr) munmap(mHalBuffer.addr, mHalBuffer.s.size);
+                mHalBuffer.addr = nullptr;
+                mHalBuffer.s.size = 0;
+                close(mHalBuffer.dmafd);
+                break;
+            case BUF_TYPE_HANDLE:
+                // Allocated by the HAL
+                if (!(mUserBuffer.stream)) {
+                    LOG1("release internal buffer");
+                    // For HAL buffer, need to unlock before free it
+                    unlock();
+                    mGbmBufferManager->Free(mHandle);
+                }
+                break;
+            default:
+                break;
+        }
+    }
+    LOG1("%s destroying buf %p", __func__, this);
+}
+
+/**
+ * init
+ *
+ * Construct to wrap a camera3_stream_buffer
+ *
+ * \param aBuffer [IN] camera3_stream_buffer buffer
+ */
+icamera::status_t Camera3Buffer::init(const camera3_stream_buffer* aBuffer, int cameraId) {
+    mType = BUF_TYPE_HANDLE;
+    mGbmBufferManager = cros::CameraBufferManager::GetInstance();
+    mHandle = *aBuffer->buffer;
+    mHandlePtr = aBuffer->buffer;
+    mHalBuffer.s.width = aBuffer->stream->width;
+    mHalBuffer.s.height = aBuffer->stream->height;
+    mFormat = aBuffer->stream->format;
+    mHalBuffer.s.memType =
+        IS_ZSL_USAGE(aBuffer->stream->usage) ? V4L2_MEMORY_USERPTR : V4L2_MEMORY_DMABUF;
+    mHalBuffer.s.format = mGbmBufferManager->GetV4L2PixelFormat(mHandle);
+    // Use actual width from platform native handle for stride
+    mHalBuffer.s.stride = mGbmBufferManager->GetPlaneStride(*aBuffer->buffer, 0);
+    mHalBuffer.s.size =
+        CameraUtils::getFrameSize(mHalBuffer.s.format, mHalBuffer.s.stride, mHalBuffer.s.height);
+    mHalBuffer.flags = camera_buffer_flags_t::BUFFER_FLAG_SW_WRITE;
+    mLocked = false;
+    mInit = true;
+    mHalBuffer.addr = nullptr;
+    mUserBuffer = *aBuffer;
+    mUserBuffer.release_fence = -1;
+    mCameraId = cameraId;
+    LOG2("@%s, mHandle:%p, mFormat:%d, width:%d, height:%d, stride:%d, size: %d", __func__, mHandle,
+         mFormat, mHalBuffer.s.width, mHalBuffer.s.height, mHalBuffer.s.stride, mHalBuffer.s.size);
+
+    if (mHandle == nullptr) {
+        LOGE("@%s: invalid buffer handle", __func__);
+        mUserBuffer.status = CAMERA3_BUFFER_STATUS_ERROR;
+        return BAD_VALUE;
+    }
+
+    mHalBuffer.dmafd = mHandle->data[0];
+    int ret = registerBuffer();
+    if (ret) {
+        mUserBuffer.status = CAMERA3_BUFFER_STATUS_ERROR;
+        return UNKNOWN_ERROR;
+    }
+
+    /* TODO: add some consistency checks here and return an error */
+    return icamera::OK;
+}
+
+icamera::status_t Camera3Buffer::init(const camera3_stream_t* stream, buffer_handle_t handle,
+                                      int cameraId) {
+    mType = BUF_TYPE_HANDLE;
+    mGbmBufferManager = cros::CameraBufferManager::GetInstance();
+    mHandle = handle;
+    mHalBuffer.s.width = stream->width;
+    mHalBuffer.s.height = stream->height;
+    mFormat = stream->format;
+    mHalBuffer.s.memType = V4L2_MEMORY_USERPTR;
+    mHalBuffer.s.format = mGbmBufferManager->GetV4L2PixelFormat(mHandle);
+    // Use actual width from platform native handle for stride
+    mHalBuffer.s.stride = mGbmBufferManager->GetPlaneStride(handle, 0);
+    mHalBuffer.s.size = 0;
+    mHalBuffer.flags = camera_buffer_flags_t::BUFFER_FLAG_SW_WRITE;
+    mLocked = false;
+    mInit = true;
+    mHalBuffer.addr = nullptr;
+    mCameraId = cameraId;
+    LOG2("@%s, mHandle:%p, mFormat:%d, width:%d, height:%d, stride:%d", __func__, mHandle, mFormat,
+         mHalBuffer.s.width, mHalBuffer.s.height, mHalBuffer.s.stride);
+
+    return icamera::OK;
+}
+
+icamera::status_t Camera3Buffer::deinit() {
+    return deregisterBuffer();
+}
+
+icamera::status_t Camera3Buffer::waitOnAcquireFence() {
+    const int BUFFER_READY = -1;
+    if (mUserBuffer.acquire_fence != BUFFER_READY) {
+        LOG2("%s: Fence in HAL is %d", __func__, mUserBuffer.acquire_fence);
+        const int WAIT_TIME_OUT_MS = 300;
+        int ret = sync_wait(mUserBuffer.acquire_fence, WAIT_TIME_OUT_MS);
+        if (ret) {
+            mUserBuffer.release_fence = mUserBuffer.acquire_fence;
+            mUserBuffer.acquire_fence = -1;
+            mUserBuffer.status = CAMERA3_BUFFER_STATUS_ERROR;
+            LOGE("Buffer sync_wait fail!");
+            return TIMED_OUT;
+        } else {
+            close(mUserBuffer.acquire_fence);
+        }
+        mUserBuffer.acquire_fence = BUFFER_READY;
+    }
+
+    return icamera::OK;
+}
+
+/**
+ * getFence
+ *
+ * return the fecne to request result
+ */
+icamera::status_t Camera3Buffer::getFence(camera3_stream_buffer* buf) {
+    if (!buf) return BAD_VALUE;
+
+    buf->acquire_fence = mUserBuffer.acquire_fence;
+    buf->release_fence = mUserBuffer.release_fence;
+
+    return icamera::OK;
+}
+
+icamera::status_t Camera3Buffer::registerBuffer() {
+    int ret = mGbmBufferManager->Register(mHandle);
+    if (ret) {
+        LOGE("@%s: call Register fail, mHandle:%p, ret:%d", __func__, mHandle, ret);
+        return UNKNOWN_ERROR;
+    }
+
+    mRegistered = true;
+    return icamera::OK;
+}
+
+icamera::status_t Camera3Buffer::deregisterBuffer() {
+    if (mRegistered) {
+        int ret = mGbmBufferManager->Deregister(mHandle);
+        if (ret) {
+            LOGE("@%s: call Deregister fail, mHandle:%p, ret:%d", __func__, mHandle, ret);
+            return UNKNOWN_ERROR;
+        }
+        mRegistered = false;
+    }
+
+    return icamera::OK;
+}
+
+/**
+ * lock
+ *
+ * lock the gralloc buffer with specified flags
+ *
+ * \param aBuffer [IN] int flags
+ */
+icamera::status_t Camera3Buffer::lock(int flags) {
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL1);
+    mHalBuffer.addr = nullptr;
+    mHalBuffer.s.size = 0;
+    int ret = 0;
+    uint32_t planeNum = mGbmBufferManager->GetNumPlanes(mHandle);
+    LOG2("@%s, planeNum:%d, mHandle:%p, mFormat:%d", __func__, planeNum, mHandle, mFormat);
+
+    if (planeNum == 1) {
+        void* data = nullptr;
+        ret = (mFormat == HAL_PIXEL_FORMAT_BLOB)
+                  ? mGbmBufferManager->Lock(mHandle, 0, 0, 0, mHalBuffer.s.stride, 1, &data)
+                  : mGbmBufferManager->Lock(mHandle, 0, 0, 0, mHalBuffer.s.width,
+                                            mHalBuffer.s.height, &data);
+        mHalBuffer.addr = data;
+    } else if (planeNum > 1) {
+        struct android_ycbcr ycbrData;
+        ret = mGbmBufferManager->LockYCbCr(mHandle, 0, 0, 0, mHalBuffer.s.width,
+                                           mHalBuffer.s.height, &ycbrData);
+        mHalBuffer.addr = ycbrData.y;
+    } else {
+        LOGE("ERROR @%s: planeNum is 0", __func__);
+        return UNKNOWN_ERROR;
+    }
+
+    CheckError(ret, UNKNOWN_ERROR, "@%s: Failed to lock buffer, mHandle:%p planeNum: %d", __func__,
+               mHandle, planeNum);
+
+    for (uint32_t i = 0; i < planeNum; i++) {
+        mHalBuffer.s.size += mGbmBufferManager->GetPlaneSize(mHandle, i);
+    }
+
+    LOG2("@%s, addr:%p, size:%d", __func__, mHalBuffer.addr, mHalBuffer.s.size);
+    CheckError(!mHalBuffer.s.size, UNKNOWN_ERROR, "ERROR @%s: Failed to GetPlaneSize, it's 0",
+               __func__);
+
+    mLocked = true;
+
+    return icamera::OK;
+}
+
+icamera::status_t Camera3Buffer::lock() {
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL1);
+    icamera::status_t status;
+    int lockMode;
+
+    if (!mInit) {
+        LOGE("@%s: Error: Cannot lock now this buffer, not initialized", __func__);
+        return INVALID_OPERATION;
+    }
+
+    if (mType != BUF_TYPE_HANDLE) {
+        mLocked = true;
+        return icamera::OK;
+    }
+
+    if (mLocked) {
+        LOGE("@%s: Error: Cannot lock buffer, already locked", __func__);
+        return INVALID_OPERATION;
+    }
+
+    int usage = 0x20003;  // TODO: hard code the usage, fix to get the usage from stream later
+    lockMode = usage & (GRALLOC_USAGE_SW_READ_MASK | GRALLOC_USAGE_SW_WRITE_MASK |
+                        GRALLOC_USAGE_HW_CAMERA_MASK);
+    if (!lockMode) {
+        LOGW("@%s:trying to lock a buffer with no flags", __func__);
+        return INVALID_OPERATION;
+    }
+
+    status = lock(lockMode);
+    if (status != icamera::OK) {
+        mUserBuffer.status = CAMERA3_BUFFER_STATUS_ERROR;
+    }
+
+    return status;
+}
+
+icamera::status_t Camera3Buffer::unlock() {
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL1);
+    if (mLocked && mType != BUF_TYPE_HANDLE) {
+        mLocked = false;
+        return icamera::OK;
+    }
+
+    if (mLocked) {
+        LOG2("@%s, mHandle:%p, mFormat:%d", __func__, mHandle, mFormat);
+        int ret = mGbmBufferManager->Unlock(mHandle);
+        if (ret) {
+            LOGE("@%s: call Unlock fail, mHandle:%p, ret:%d", __func__, mHandle, ret);
+            return UNKNOWN_ERROR;
+        }
+
+        mLocked = false;
+        return icamera::OK;
+    }
+    LOG1("@%s:trying to unlock a buffer that is not locked", __func__);
+    return INVALID_OPERATION;
+}
+
+void Camera3Buffer::dump() {
+    if (mInit) {
+        LOG1("Buffer dump: handle %p: locked :%d: dataPtr:%p", static_cast<void*>(&mHandle),
+             mLocked, mHalBuffer.addr);
+    } else {
+        LOG1("Buffer dump: Buffer not initialized");
+    }
+}
+
+void Camera3Buffer::dumpImage(int frameNumber, const int type, int format) {
+    if (!CameraDump::isDumpTypeEnable(type)) {
+        return;
+    }
+
+    dumpImage(mHalBuffer.addr, frameNumber, mHalBuffer.s.size, mHalBuffer.s.width,
+              mHalBuffer.s.height, format);
+}
+
+void Camera3Buffer::dumpImage(const void* data, int frameNumber, const int size, int width,
+                              int height, int format) const {
+#ifdef DUMP_IMAGE
+    static unsigned int count = 0;
+    count++;
+
+    std::string fileName(gDumpPath);
+    fileName += "dump_" + std::to_string(width) + "x" + std::to_string(height) + "_frame#" +
+                std::to_string(count) + "_req#" + std::to_string(frameNumber) + "." +
+                CameraUtils::format2string(format).c_str();
+    LOG2("%s filename is %s", __func__, fileName.data());
+
+    FILE* fp = fopen(fileName.data(), "w+");
+    if (fp == nullptr) {
+        LOGE("open file failed");
+        return;
+    }
+    LOG1("Begin write image %s", fileName.data());
+
+    if ((fwrite(data, size, 1, fp)) != 1)
+        LOGW("Error or short count writing %d bytes to %s", size, fileName.data());
+    fclose(fp);
+#endif
+}
+
+int Camera3Buffer::v4L2Fmt2GFXFmt(int v4l2Fmt) {
+    int gfxFmt = -1;
+
+    switch (v4l2Fmt) {
+        case V4L2_PIX_FMT_JPEG:
+            gfxFmt = HAL_PIXEL_FORMAT_BLOB;
+            break;
+        case V4L2_PIX_FMT_SBGGR8:
+        case V4L2_PIX_FMT_SRGGB8:
+        case V4L2_PIX_FMT_SGRBG8:
+        case V4L2_PIX_FMT_SRGGB10:
+        case V4L2_PIX_FMT_SGRBG10:
+        case V4L2_PIX_FMT_SGRBG12:
+        case V4L2_PIX_FMT_SBGGR10:
+        case V4L2_PIX_FMT_SBGGR10P:
+        case V4L2_PIX_FMT_SGBRG10P:
+        case V4L2_PIX_FMT_SGRBG10P:
+        case V4L2_PIX_FMT_SRGGB10P:
+        case V4L2_PIX_FMT_SBGGR12:
+        case V4L2_PIX_FMT_SGBRG12:
+        case V4L2_PIX_FMT_SRGGB12:
+            gfxFmt = HAL_PIXEL_FORMAT_RAW16;
+            break;
+        case V4L2_PIX_FMT_YVU420:
+            gfxFmt = HAL_PIXEL_FORMAT_YV12;
+            break;
+        case V4L2_PIX_FMT_NV21:
+            gfxFmt = HAL_PIXEL_FORMAT_YCrCb_420_SP;
+            break;
+        case V4L2_PIX_FMT_NV12:
+            LOGW("Current there is no gfx format for V4L2_PIX_FMT_NV12.");
+            break;
+        case V4L2_PIX_FMT_YUYV:
+            gfxFmt = HAL_PIXEL_FORMAT_YCbCr_422_I;
+            break;
+        default:
+            LOGE("%s: no gfx format for v4l2 0x%x, %s!", __func__, v4l2Fmt,
+                 CameraUtils::format2string(v4l2Fmt).c_str());
+            break;
+    }
+
+    return gfxFmt;
+}
+
+/**
+ * Utility methods to allocate Camera3Buffer from HEAP or Gfx memory
+ */
+namespace MemoryUtils {
+
+/**
+ * Allocates the memory needed to store the image described by the parameters
+ * passed during construction
+ */
+std::shared_ptr<Camera3Buffer> allocateHeapBuffer(int w, int h, int stride, int v4l2Fmt,
+                                                  int cameraId, int dataSizeOverride) {
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL1);
+    void* dataPtr;
+
+    int dataSize = dataSizeOverride ? PAGE_ALIGN(dataSizeOverride)
+                                    : CameraUtils::getFrameSize(v4l2Fmt, stride, h);
+    LOG1("@%s, dataSize:%d", __func__, dataSize);
+
+    int ret = posix_memalign(&dataPtr, sysconf(_SC_PAGESIZE), dataSize);
+    if (dataPtr == nullptr || ret != 0) {
+        LOGE("Could not allocate heap camera buffer of size %d", dataSize);
+        return nullptr;
+    }
+
+    return std::shared_ptr<Camera3Buffer>(
+        new Camera3Buffer(w, h, stride, v4l2Fmt, dataPtr, cameraId, dataSize));
+}
+
+/**
+ * Allocates internal GBM buffer
+ */
+std::shared_ptr<Camera3Buffer> allocateHandleBuffer(int w, int h, int gfxFmt, int usage,
+                                                    int cameraId) {
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL1);
+    cros::CameraBufferManager* bufManager = cros::CameraBufferManager::GetInstance();
+    buffer_handle_t handle;
+    uint32_t stride = 0;
+
+    LOG1("%s, [wxh] = [%dx%d], format 0x%x, usage 0x%x", __func__, w, h, gfxFmt, usage);
+    int ret = bufManager->Allocate(w, h, gfxFmt, usage, cros::GRALLOC, &handle, &stride);
+    if (ret != 0) {
+        LOGE("Allocate handle failed! %d", ret);
+        return nullptr;
+    }
+
+    std::shared_ptr<Camera3Buffer> buffer(new Camera3Buffer());
+    camera3_stream_t stream{};
+    stream.width = w;
+    stream.height = h;
+    stream.format = gfxFmt;
+    stream.usage = usage;
+    ret = buffer->init(&stream, handle, cameraId);
+    if (ret != icamera::OK) {
+        // buffer handle will free in Camera3Buffer destructure function
+        return nullptr;
+    }
+
+    return buffer;
+}
+
+}  // namespace MemoryUtils
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/modules/memory/chrome/Camera3Buffer.h b/camera/hal/intel/ipu6/modules/memory/chrome/Camera3Buffer.h
new file mode 100644
index 000000000000..2fc59af592ff
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/memory/chrome/Camera3Buffer.h
@@ -0,0 +1,159 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <cros-camera/camera_buffer_manager.h>
+#include <hardware/camera3.h>
+
+#include <memory>
+
+#include "Parameters.h"
+#include "iutils/Errors.h"
+
+namespace camera3 {
+
+// Forward declaration to  avoid extra include
+class Camera3Stream;
+
+enum Camera3BufferType {
+    BUF_TYPE_HANDLE,
+    BUF_TYPE_MALLOC,
+    BUF_TYPE_MMAP,
+};
+
+/**
+ * \class Camera3Buffer
+ *
+ * This class is the buffer abstraction in the HAL. It can store buffers
+ * provided by the framework or buffers allocated by the HAL.
+ * Allocation in the HAL can be done via gralloc, malloc or mmap
+ * in case of mmap the memory cannot be freed
+ */
+class Camera3Buffer {
+ public:
+    /**
+     * default constructor
+     * Used for buffers coming from the framework. The wrapper is initialized
+     * using the method init
+     */
+    Camera3Buffer();
+
+    /**
+     * no need to delete a buffer since it is RefBase'd. Buffer will be deleted
+     * when no reference to it exist.
+     */
+    ~Camera3Buffer();
+
+    /**
+     * constructor for the HAL-allocated buffer
+     * These are used via the utility methods in the MemoryUtils namespace
+     */
+    Camera3Buffer(int w, int h, int stride, int v4l2fmt, void* usrPtr, int cameraId,
+                  int dataSizeOverride = 0);
+
+    /**
+     * initialization for the wrapper around the framework buffers
+     */
+    icamera::status_t init(const camera3_stream_buffer* aBuffer, int cameraId);
+
+    /**
+     * initialization for the fake framework buffer (allocated by the HAL)
+     */
+    icamera::status_t init(const camera3_stream_t* stream, buffer_handle_t buffer, int cameraId);
+    /**
+     * deinitialization for the wrapper around the framework buffers
+     */
+    icamera::status_t deinit();
+
+    /**
+     * dump functions
+     */
+    void dump();
+    void dumpImage(int frameNumber, const int type, int format);
+    void dumpImage(const void* data, int frameNumber, const int size, int width, int height,
+                   int format) const;
+
+    /**
+     * lock the buffer and get buffer addr from handle
+     */
+    icamera::status_t lock();
+    icamera::status_t unlock();
+
+    /**
+     * Fence
+     */
+    icamera::status_t waitOnAcquireFence();
+    icamera::status_t getFence(camera3_stream_buffer* buf);
+
+    /**
+     * Convert the GFX format to v4l2 format
+     */
+    int v4L2Fmt2GFXFmt(int v4l2Fmt);
+
+    /**
+     * APIs for getting private member
+     */
+    int width() { return mHalBuffer.s.width; }
+    int height() { return mHalBuffer.s.height; }
+    int stride() { return mHalBuffer.s.stride; }
+    unsigned int size() { return mHalBuffer.s.size; }
+    int v4l2Fmt() { return mHalBuffer.s.format; }
+    void* data() { return mHalBuffer.addr; }
+    uint64_t getTimeStamp() { return mHalBuffer.timestamp; }
+    void setTimeStamp(uint64_t timestamp) { mHalBuffer.timestamp = timestamp; }
+    int format() { return mFormat; }
+    buffer_handle_t* getBufferHandle() { return mHandlePtr; }
+    bool isLocked() const { return mLocked; }
+    int dmaBufFd() { return mType == BUF_TYPE_HANDLE ? mHandle->data[0] : mHalBuffer.dmafd; }
+    int status() { return mUserBuffer.status; }
+    camera3_stream_t* getStream() { return mUserBuffer.stream; }
+    icamera::camera_buffer_t getHalBuffer() { return mHalBuffer; }
+
+ private:
+    icamera::status_t lock(int flags);
+    icamera::status_t registerBuffer();
+    icamera::status_t deregisterBuffer();
+
+ private:
+    /*!< Original structure passed by request */
+    camera3_stream_buffer_t mUserBuffer = {0, 0, 0, -1, -1};
+    int mFormat;  /*!<  HAL PIXEL fmt */
+    bool mInit;   /*!< Boolean to check the integrity of the
+                       buffer when it is created*/
+    bool mLocked; /*!< Use to track the lock status */
+    Camera3BufferType mType;
+
+    buffer_handle_t mHandle = {};
+    buffer_handle_t* mHandlePtr;
+    int mCameraId;
+    icamera::camera_buffer_t mHalBuffer;
+
+ private:
+    bool mRegistered; /*!< Use to track the buffer register status */
+    cros::CameraBufferManager* mGbmBufferManager;
+};
+
+namespace MemoryUtils {
+
+std::shared_ptr<Camera3Buffer> allocateHeapBuffer(int w, int h, int stride, int v4l2Fmt,
+                                                  int cameraId, int dataSizeOverride = 0);
+
+std::shared_ptr<Camera3Buffer> allocateHandleBuffer(int w, int h, int gfxFmt, int usage,
+                                                    int cameraId);
+};  // namespace MemoryUtils
+
+}  // namespace camera3
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCCommon.cpp b/camera/hal/intel/ipu6/modules/sandboxing/IPCCommon.cpp
new file mode 100644
index 000000000000..edbd5998cf93
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCCommon.cpp
@@ -0,0 +1,116 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "modules/sandboxing/IPCCommon.h"
+
+#include <iostream>
+#include <string>
+
+namespace icamera {
+const char* IntelAlgoIpcCmdToString(IPC_CMD cmd) {
+    static const char* gIpcCmdMapping[] = {"unknown",
+                                           "IPC_LARD_INIT",
+                                           "IPC_LARD_GET_TAG_LIST",
+                                           "IPC_LARD_RUN",
+                                           "IPC_LARD_DEINIT",
+                                           "IPC_FD_INIT",
+                                           "IPC_FD_RUN",
+                                           "IPC_FD_DEINIT",
+                                           "IPC_GRAPH_ADD_KEY",
+                                           "IPC_GRAPH_PARSE",
+                                           "IPC_GRAPH_RELEASE_NODES",
+                                           "IPC_GRAPH_CONFIG_STREAMS",
+                                           "IPC_GRAPH_GET_CONFIG_DATA",
+                                           "IPC_GRAPH_GET_CONNECTION",
+                                           "IPC_GRAPH_GET_PG_ID",
+                                           "IPC_CMC_INIT",
+                                           "IPC_CMC_DEINIT",
+                                           "IPC_MKN_INIT",
+                                           "IPC_MKN_ENABLE",
+                                           "IPC_MKN_PREPARE",
+                                           "IPC_MKN_DEINIT",
+                                           "IPC_LTM_INIT",
+                                           "IPC_LTM_RUN",
+                                           "IPC_LTM_DEINIT",
+                                           "IPC_AIQ_INIT",
+                                           "IPC_AIQ_AE_RUN",
+                                           "IPC_AIQ_AF_RUN",
+                                           "IPC_AIQ_AWB_RUN",
+                                           "IPC_AIQ_GBCE_RUN",
+                                           "IPC_AIQ_PA_RUN_V1",
+                                           "IPC_AIQ_SA_RUN_V2",
+                                           "IPC_AIQ_STATISTICS_SET_V4",
+                                           "IPC_AIQ_GET_AIQD_DATA",
+                                           "IPC_AIQ_DEINIT",
+                                           "IPC_AIQ_GET_VERSION",
+                                           "IPC_DVS_INIT",
+                                           "IPC_DVS_CONFIG",
+                                           "IPC_DVS_SET_NONE_BLANK_RATION",
+                                           "IPC_DVS_SET_DIGITAL_ZOOM_MODE",
+                                           "IPC_DVS_SET_DIGITAL_ZOOM_REGION",
+                                           "IPC_DVS_SET_DIGITAL_ZOOM_COORDINATE",
+                                           "IPC_DVS_SET_DIGITAL_ZOOM_MAGNITUDE",
+                                           "IPC_DVS_FREE_MORPH_TABLE",
+                                           "IPC_DVS_ALLOCATE_MORPH_TABLE",
+                                           "IPC_DVS_GET_MORPH_TABLE",
+                                           "IPC_DVS_SET_STATISTICS",
+                                           "IPC_DVS_EXECUTE",
+                                           "IPC_DVS_GET_IMAGE_TRANSFORMATION",
+                                           "IPC_DVS_DEINIT",
+                                           "IPC_ISP_ADAPTOR_INIT",
+                                           "IPC_ISP_ADAPTOR_DEINIT",
+                                           "IPC_ISP_GET_PAL_SIZE",
+                                           "IPC_ISP_CONVERT_STATS",
+                                           "IPC_ISP_RUN_PAL",
+                                           "IPC_PG_PARAM_INIT",
+                                           "IPC_PG_PARAM_PREPARE",
+                                           "IPC_PG_PARAM_ALLOCATE_PG",
+                                           "IPC_PG_PARAM_GET_FRAG_DESCS",
+                                           "IPC_PG_PARAM_PREPARE_PROGRAM",
+                                           "IPC_PG_PARAM_ALLOCATE_PAYLOADS",
+                                           "IPC_PG_PARAM_ENCODE",
+                                           "IPC_PG_PARAM_DECODE",
+                                           "IPC_PG_PARAM_DEINIT",
+                                           "IPC_GPU_TNR_INIT",
+                                           "IPC_GPU_TNR_PREPARE_SURFACE",
+                                           "IPC_GPU_TNR_RUN_FRAME",
+                                           "IPC_GPU_TNR_PARAM_UPDATE",
+                                           "IPC_GPU_TNR_DEINIT"};
+
+    unsigned int num = sizeof(gIpcCmdMapping) / sizeof(gIpcCmdMapping[0]);
+    return cmd < num ? gIpcCmdMapping[cmd] : gIpcCmdMapping[0];
+}
+
+IPC_GROUP IntelAlgoIpcCmdToGroup(IPC_CMD cmd) {
+    IPC_GROUP group = IPC_GROUP_CPU_OTHER;
+    if (cmd >= IPC_AIQ_INIT && cmd <= IPC_AIQ_GET_VERSION) {
+        group = IPC_GROUP_AIQ;
+    } else if (cmd >= IPC_ISP_ADAPTOR_INIT && cmd <= IPC_ISP_RUN_PAL) {
+        group = IPC_GROUP_PAL;
+    } else if (cmd >= IPC_PG_PARAM_INIT && cmd <= IPC_PG_PARAM_DEINIT) {
+        group = IPC_GROUP_PSYS;
+    } else if (cmd >= IPC_FD_INIT && cmd <= IPC_FD_DEINIT) {
+        group = IPC_GROUP_FD;
+    } else if (cmd < IPC_GPU_TNR_INIT) {
+        group = IPC_GROUP_CPU_OTHER;
+    } else {
+        group = IPC_GROUP_GPU;
+    }
+
+    return group;
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCCommon.h b/camera/hal/intel/ipu6/modules/sandboxing/IPCCommon.h
new file mode 100644
index 000000000000..72dc0158c6e8
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCCommon.h
@@ -0,0 +1,125 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_aiq_types.h>
+#include <ia_cmc_types.h>
+#include <ia_types.h>
+
+#include "iutils/Errors.h"
+
+namespace icamera {
+#define IPC_MATCHING_KEY 0x56  // the value is randomly chosen
+#define IPC_REQUEST_HEADER_USED_NUM 1
+#define SHM_NAME "shm"
+
+enum IPC_CMD {
+    // CPU IPC COMMANDS
+    IPC_LARD_INIT = 1,
+    IPC_LARD_GET_TAG_LIST,
+    IPC_LARD_RUN,
+    IPC_LARD_DEINIT,
+    IPC_FD_INIT,
+    IPC_FD_RUN,
+    IPC_FD_DEINIT,
+    IPC_GRAPH_ADD_KEY,
+    IPC_GRAPH_PARSE,
+    IPC_GRAPH_RELEASE_NODES,
+    IPC_GRAPH_CONFIG_STREAMS,
+    IPC_GRAPH_GET_CONFIG_DATA,
+    IPC_GRAPH_GET_CONNECTION,
+    IPC_GRAPH_GET_PG_ID,
+    IPC_CMC_INIT,
+    IPC_CMC_DEINIT,
+    IPC_MKN_INIT,
+    IPC_MKN_ENABLE,
+    IPC_MKN_PREPARE,
+    IPC_MKN_DEINIT,
+    IPC_LTM_INIT,
+    IPC_LTM_RUN,
+    IPC_LTM_DEINIT,
+    IPC_AIQ_INIT,
+    IPC_AIQ_AE_RUN,
+    IPC_AIQ_AF_RUN,
+    IPC_AIQ_AWB_RUN,
+    IPC_AIQ_GBCE_RUN,
+    IPC_AIQ_PA_RUN_V1,
+    IPC_AIQ_SA_RUN_V2,
+    IPC_AIQ_STATISTICS_SET_V4,
+    IPC_AIQ_GET_AIQD_DATA,
+    IPC_AIQ_DEINIT,
+    IPC_AIQ_GET_VERSION,
+    IPC_DVS_INIT,
+    IPC_DVS_CONFIG,
+    IPC_DVS_SET_NONE_BLANK_RATION,
+    IPC_DVS_SET_DIGITAL_ZOOM_MODE,
+    IPC_DVS_SET_DIGITAL_ZOOM_REGION,
+    IPC_DVS_SET_DIGITAL_ZOOM_COORDINATE,
+    IPC_DVS_SET_DIGITAL_ZOOM_MAGNITUDE,
+    IPC_DVS_FREE_MORPH_TABLE,
+    IPC_DVS_ALLOCATE_MORPH_TABLE,
+    IPC_DVS_GET_MORPH_TABLE,
+    IPC_DVS_SET_STATISTICS,
+    IPC_DVS_EXECUTE,
+    IPC_DVS_GET_IMAGE_TRANSFORMATION,
+    IPC_DVS_DEINIT,
+    IPC_ISP_ADAPTOR_INIT,
+    IPC_ISP_ADAPTOR_DEINIT,
+    IPC_ISP_GET_PAL_SIZE,
+    IPC_ISP_CONVERT_STATS,
+    IPC_ISP_RUN_PAL,
+    IPC_PG_PARAM_INIT,
+    IPC_PG_PARAM_PREPARE,
+    IPC_PG_PARAM_ALLOCATE_PG,
+    IPC_PG_PARAM_GET_FRAG_DESCS,
+    IPC_PG_PARAM_PREPARE_PROGRAM,
+    IPC_PG_PARAM_ALLOCATE_PAYLOADS,
+    IPC_PG_PARAM_ENCODE,
+    IPC_PG_PARAM_DECODE,
+    IPC_PG_PARAM_DEINIT,
+    // GPU IPC COMMANDS
+    IPC_GPU_TNR_INIT,
+    IPC_GPU_TNR_PREPARE_SURFACE,
+    IPC_GPU_TNR_RUN_FRAME,
+    IPC_GPU_TNR_PARAM_UPDATE,
+    IPC_GPU_TNR_DEINIT
+};
+
+#define MAX_IA_BINARY_DATA_SIZE 800000
+struct ia_binary_data_mod {
+    unsigned int size;
+    char data[MAX_IA_BINARY_DATA_SIZE];
+};
+
+const char* IntelAlgoIpcCmdToString(IPC_CMD cmd);
+
+enum IPC_GROUP {
+    // IPC command group for cpu
+    IPC_GROUP_AIQ,
+    IPC_GROUP_PAL,
+    IPC_GROUP_PSYS,
+    IPC_GROUP_FD,
+    IPC_GROUP_CPU_OTHER,
+    // IPU command group for gpu
+    IPC_GROUP_GPU,
+};
+#define IPC_GROUP_NUM (IPC_GROUP_GPU + 1)
+#define IPC_CPU_GROUP_NUM (IPC_GROUP_CPU_OTHER + 1)
+#define IPC_GPU_GROUP_NUM (IPC_GROUP_NUM - IPC_GROUP_GPU)
+
+IPC_GROUP IntelAlgoIpcCmdToGroup(IPC_CMD cmd);
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCGraphConfig.cpp b/camera/hal/intel/ipu6/modules/sandboxing/IPCGraphConfig.cpp
new file mode 100644
index 000000000000..3903ae6ab51d
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCGraphConfig.cpp
@@ -0,0 +1,490 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IPC_GRAPH_CONFIG"
+
+#include "modules/sandboxing/IPCGraphConfig.h"
+
+#include <sys/stat.h>
+
+#include <memory>
+
+#include "iutils/CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IPCGraphConfig::IPCGraphConfig() {
+    LOGIPC("@%s", __func__);
+}
+
+IPCGraphConfig::~IPCGraphConfig() {
+    LOGIPC("@%s", __func__);
+}
+
+status_t IPCGraphConfig::readDataFromXml(const char* fileName, char* dataPtr, size_t* dataSize,
+                                         int maxSize) {
+    CheckError(!dataSize || !fileName || !dataPtr, UNKNOWN_ERROR, "%s, invalid argument", __func__);
+
+    struct stat statBuf;
+    int ret = stat(fileName, &statBuf);
+    CheckError((ret != 0), UNKNOWN_ERROR, "Failed to query the size of file: %s!", fileName);
+    CheckError(statBuf.st_size > maxSize, BAD_VALUE, "The memory size less than file size: %s",
+               fileName);
+
+    *dataSize = static_cast<size_t>(statBuf.st_size);
+    LOGIPC("%s, fileName: %s, size: %zu", __func__, fileName, *dataSize);
+
+    FILE* file = fopen(fileName, "rb");
+    CheckError(!file, NAME_NOT_FOUND, "%s, Failed to open file: %s", __func__, fileName);
+
+    size_t len = fread(dataPtr, 1, *dataSize, file);
+    fclose(file);
+
+    CheckError((len != *dataSize), UNKNOWN_ERROR, "%s, Failed to read data from file: %s", __func__,
+               fileName);
+
+    return OK;
+}
+
+bool IPCGraphConfig::clientFlattenParse(void* pData, uint32_t size, int cameraId,
+                                        const char* graphDescFile, const char* settingsFile) {
+    LOGIPC("@%s, pData:%p, size: %zu, cameraId: %d", __func__, pData, size, cameraId);
+
+    CheckError(!pData || !graphDescFile || !settingsFile, false,
+               "@%s, pData, graphDescFile or settingsFile is nullptr", __func__);
+    CheckError(size < sizeof(GraphParseParams), false, "@%s, buffer is small", __func__);
+
+    GraphParseParams* params = static_cast<GraphParseParams*>(pData);
+    CLEAR(*params);
+
+    params->cameraId = cameraId;
+    int ret =
+        readDataFromXml(graphDescFile, params->GD, &(params->gdSize), MAX_GRAPH_DESCRIPTOR_SIZE);
+    CheckError(ret != OK, false, "Failed to read the graph descriptor file: %s", graphDescFile);
+
+    ret = readDataFromXml(settingsFile, params->GS, &(params->gsSize), MAX_GRAPH_SETTINGS_SIZE);
+    CheckError(ret != OK, false, "Failed to read the graph settings file: %s", settingsFile);
+
+    return true;
+}
+
+bool IPCGraphConfig::serverUnflattenParse(void* pData, uint32_t size,
+                                          GraphParseParams** parseParam) {
+    LOGIPC("@%s, pData:%p, size: %zu", __func__, pData, size);
+
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(GraphParseParams), false, "@%s, buffer is small", __func__);
+    CheckError(!parseParam, false, "@%s, parseParam is nullptr", __func__);
+
+    GraphParseParams* params = static_cast<GraphParseParams*>(pData);
+    *parseParam = params;
+
+    return true;
+}
+
+bool IPCGraphConfig::clientFlattenConfigStreams(void* pData, uint32_t size, GraphBaseInfo info,
+                                                GraphSettingType type,
+                                                const std::vector<HalStream*>& streams) {
+    LOGIPC("@%s, pData:%p, cameraId: %d, configMode: %d", __func__, pData, info.cameraId,
+           info.configMode);
+
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(GraphConfigStreamParams), false, "@%s, buffer is small", __func__);
+    CheckError(streams.empty(), false, "@%s, stream vector is empty", __func__);
+
+    GraphConfigStreamParams* params = static_cast<GraphConfigStreamParams*>(pData);
+    CLEAR(*params);
+
+    params->baseInfo = info;
+    params->type = type;
+    for (size_t i = 0; i < streams.size(); ++i) {
+        params->streamCfg[i] = *(streams[i]);
+        params->streamPriv[i] = *(static_cast<stream_t*>(streams[i]->mPrivate));
+        params->streamNum++;
+    }
+
+    return true;
+}
+
+bool IPCGraphConfig::serverUnflattenConfigStreams(void* pData, uint32_t size, GraphBaseInfo* info,
+                                                  GraphSettingType* type,
+                                                  std::vector<HalStream*>* streams) {
+    LOGIPC("@%s, pData:%p, size: %zu", __func__, pData, size);
+
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(GraphConfigStreamParams), false, "@%s, buffer is small", __func__);
+    CheckError(!info || !type || !streams, false, "%s, the info, type or streams is nullptr",
+               __func__);
+
+    GraphConfigStreamParams* params = static_cast<GraphConfigStreamParams*>(pData);
+
+    *info = params->baseInfo;
+    *type = params->type;
+    for (uint32_t i = 0; i < params->streamNum; ++i) {
+        params->streamCfg[i].mPrivate = static_cast<void*>(&(params->streamPriv[i]));
+        streams->push_back(&(params->streamCfg[i]));
+    }
+
+    return true;
+}
+
+bool IPCGraphConfig::clientFlattenGetGraphData(void* pData, uint32_t size, GraphBaseInfo info) {
+    LOGIPC("@%s, pData:%p, cameraId: %d, configMode: %d", __func__, pData, info.cameraId,
+           info.configMode);
+
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(GraphGetDataParams), false, "@%s, buffer is small", __func__);
+
+    GraphGetDataParams* params = static_cast<GraphGetDataParams*>(pData);
+    CLEAR(*params);
+
+    params->baseInfo = info;
+
+    return true;
+}
+
+bool IPCGraphConfig::serverUnflattenGetGraphData(void* pData, uint32_t size, GraphBaseInfo* info) {
+    LOGIPC("@%s, pData:%p, size: %zu", __func__, pData, size);
+
+    CheckError(!pData || !info, false, "@%s, pData or info is nullptr", __func__);
+    CheckError(size < sizeof(GraphGetDataParams), false, "@%s, buffer is small", __func__);
+
+    GraphGetDataParams* params = static_cast<GraphGetDataParams*>(pData);
+    *info = params->baseInfo;
+
+    return true;
+}
+
+bool IPCGraphConfig::serverFlattenGetGraphData(void* pData, uint32_t size,
+                                               IGraphType::GraphConfigData graphData) {
+    LOGIPC("@%s, pData:%p, size: %zu", __func__, pData, size);
+
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(GraphGetDataParams), false, "@%s, buffer is small", __func__);
+
+    GraphGetDataParams* params = static_cast<GraphGetDataParams*>(pData);
+
+    params->mcId = graphData.mcId;
+    params->graphId = graphData.graphId;
+    params->gdcKernelId = graphData.gdcKernelId;
+
+    params->csiReso = graphData.csiReso;
+    params->gdcReso = graphData.gdcReso;
+
+    LOGIPC("%s, mcId: %d, graphId: %d, gdcKernelId: %d", __func__, params->mcId, params->graphId,
+           params->gdcKernelId);
+
+    params->streamIdNum = graphData.streamIds.size();
+    for (size_t i = 0; i < graphData.streamIds.size(); ++i) {
+        params->streamIdData[i] = graphData.streamIds[i];
+    }
+
+    params->pgInfoNum = graphData.pgInfo.size();
+    for (size_t i = 0; i < graphData.pgInfo.size(); ++i) {
+        size_t len = graphData.pgInfo[i].pgName.copy(params->pgInfoData[i].pgName, MAX_NAME_LENGTH);
+        params->pgInfoData[i].pgName[len] = '\0';
+        params->pgInfoData[i].pgId = graphData.pgInfo[i].pgId;
+        params->pgInfoData[i].streamId = graphData.pgInfo[i].streamId;
+        params->pgInfoData[i].rbmByte = graphData.pgInfo[i].rbmValue.rbm_bytes;
+
+        if (params->pgInfoData[i].rbmByte > 0) {
+            MEMCPY_S(params->pgInfoData[i].rbmData, MAX_NAME_LENGTH,
+                     graphData.pgInfo[i].rbmValue.rbm, graphData.pgInfo[i].rbmValue.rbm_bytes);
+        }
+    }
+
+    params->mBrInfoNum = graphData.mbrInfo.size();
+    for (size_t i = 0; i < graphData.mbrInfo.size(); ++i) {
+        params->mBrInfoData[i].streamId = graphData.mbrInfo[i].streamId;
+        params->mBrInfoData[i].mBrData = graphData.mbrInfo[i].data;
+    }
+
+    params->pgNamesNum = graphData.pgNames.size();
+    for (size_t i = 0; i < params->pgNamesNum; ++i) {
+        size_t len = graphData.pgNames[i].copy(params->pgNames[i], MAX_NAME_LENGTH);
+        params->pgNames[i][len] = '\0';
+    }
+
+    params->kernelArrayNum = graphData.programGroup.size();
+    for (size_t i = 0; i < params->kernelArrayNum; ++i) {
+        params->kernelArray[i].streamId = graphData.programGroup[i].streamId;
+        ia_isp_bxt_program_group* pgPtr = graphData.programGroup[i].pgPtr;
+        params->kernelArray[i].group = *pgPtr;
+
+        for (unsigned int j = 0; j < params->kernelArray[i].group.kernel_count; ++j) {
+            params->kernelArray[i].runKernels[j] = pgPtr->run_kernels[j];
+            if (pgPtr->run_kernels[j].resolution_info) {
+                params->kernelArray[i].resoInfo[j] = *(pgPtr->run_kernels[j].resolution_info);
+                params->kernelArray[i].runKernels[j].resolution_info =
+                    &(params->kernelArray[i].resoInfo[j]);
+            } else {
+                params->kernelArray[i].runKernels[j].resolution_info = nullptr;
+            }
+
+            if (pgPtr->run_kernels[j].resolution_history) {
+                params->kernelArray[i].resoHistory[j] = *(pgPtr->run_kernels[j].resolution_history);
+                params->kernelArray[i].runKernels[j].resolution_history =
+                    &(params->kernelArray[i].resoHistory[j]);
+            } else {
+                params->kernelArray[i].runKernels[j].resolution_history = nullptr;
+            }
+        }
+        params->kernelArray[i].group.run_kernels = params->kernelArray[i].runKernels;
+
+        if (pgPtr->pipe) {
+            params->kernelArray[i].pipeInfo = *(pgPtr->pipe);
+            params->kernelArray[i].group.pipe = &(params->kernelArray[i].pipeInfo);
+        } else {
+            params->kernelArray[i].group.pipe = nullptr;
+        }
+    }
+
+    return true;
+}
+
+bool IPCGraphConfig::clientUnflattenGetGraphData(void* pData, uint32_t size,
+                                                 IGraphType::GraphConfigData* graphData) {
+    LOGIPC("@%s, pData:%p, size: %zu", __func__, pData, size);
+
+    CheckError(!pData || !graphData, false, "@%s, pData or graphData is nullptr", __func__);
+    CheckError(size < sizeof(GraphGetDataParams), false, "@%s, buffer is small", __func__);
+
+    GraphGetDataParams* params = static_cast<GraphGetDataParams*>(pData);
+
+    graphData->mcId = params->mcId;
+    graphData->graphId = params->graphId;
+    graphData->gdcKernelId = params->gdcKernelId;
+
+    graphData->csiReso = params->csiReso;
+    graphData->gdcReso = params->gdcReso;
+
+    LOGIPC("%s, mcId: %d, graphId: %d, gdcKernelId: %d", __func__, params->mcId, params->graphId,
+           params->gdcKernelId);
+
+    for (size_t i = 0; i < params->streamIdNum; ++i) {
+        graphData->streamIds.push_back(params->streamIdData[i]);
+    }
+
+    for (size_t i = 0; i < params->pgInfoNum; ++i) {
+        IGraphType::PgInfo info;
+        info.pgName = params->pgInfoData[i].pgName;
+        info.pgId = params->pgInfoData[i].pgId;
+        info.streamId = params->pgInfoData[i].streamId;
+        if (params->pgInfoData[i].rbmByte > 0) {
+            info.rbmValue.rbm_bytes = params->pgInfoData[i].rbmByte;
+            info.rbmValue.rbm = params->pgInfoData[i].rbmData;
+        } else {
+            info.rbmValue.rbm = nullptr;
+            info.rbmValue.rbm_bytes = 0;
+        }
+        graphData->pgInfo.push_back(info);
+    }
+
+    for (size_t i = 0; i < params->mBrInfoNum; ++i) {
+        IGraphType::MbrInfo info;
+        info.streamId = params->mBrInfoData[i].streamId;
+        info.data = params->mBrInfoData[i].mBrData;
+        graphData->mbrInfo.push_back(info);
+    }
+
+    for (size_t i = 0; i < params->pgNamesNum; ++i) {
+        graphData->pgNames.push_back(params->pgNames[i]);
+    }
+
+    for (size_t i = 0; i < params->kernelArrayNum; ++i) {
+        IGraphType::ProgramGroupInfo info;
+
+        info.streamId = params->kernelArray[i].streamId;
+        info.pgPtr = &(params->kernelArray[i].group);
+
+        info.pgPtr->run_kernels = params->kernelArray[i].runKernels;
+        for (unsigned j = 0; j < info.pgPtr->kernel_count; ++j) {
+            if (params->kernelArray[i].runKernels[j].resolution_info) {
+                info.pgPtr->run_kernels[j].resolution_info = &(params->kernelArray[i].resoInfo[j]);
+            } else {
+                info.pgPtr->run_kernels[j].resolution_info = nullptr;
+            }
+
+            if (params->kernelArray[i].runKernels[j].resolution_history) {
+                info.pgPtr->run_kernels[j].resolution_history =
+                    &(params->kernelArray[i].resoHistory[j]);
+            } else {
+                info.pgPtr->run_kernels[j].resolution_history = nullptr;
+            }
+        }
+
+        if (params->kernelArray[i].group.pipe) {
+            info.pgPtr->pipe = &(params->kernelArray[i].pipeInfo);
+        } else {
+            info.pgPtr->pipe = nullptr;
+        }
+
+        graphData->programGroup.push_back(info);
+    }
+
+    return true;
+}
+
+bool IPCGraphConfig::clientFlattenGetPgId(void* pData, uint32_t size, GraphBaseInfo info,
+                                          const int streamId, const int kernelId) {
+    LOGIPC("@%s, pData:%p, size:%u, cameraId :%d, config mode:%d, streamId: %d, kernelId: %d",
+           __func__, pData, size, info.cameraId, info.configMode, streamId, kernelId);
+
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(GraphGetPgIdParams), false, "@%s, buffer is small", __func__);
+
+    GraphGetPgIdParams* params = static_cast<GraphGetPgIdParams*>(pData);
+    CLEAR(*params);
+
+    params->baseInfo = info;
+    params->streamId = streamId;
+    params->kernelId = kernelId;
+
+    return true;
+}
+
+bool IPCGraphConfig::serverUnFlattenGetPgId(void* pData, uint32_t size, GraphBaseInfo* info,
+                                            uint32_t* streamId, int32_t* kernelId) {
+    LOGIPC("@%s, pData:%p, size:%u", __func__, pData, size);
+
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(GraphGetPgIdParams), false, "@%s, buffer is small", __func__);
+    CheckError(!info || !streamId || !kernelId, false, "@%s, info, streamId or kernelId is nullptr",
+               __func__);
+
+    GraphGetPgIdParams* params = static_cast<GraphGetPgIdParams*>(pData);
+
+    *info = params->baseInfo;
+    *streamId = params->streamId;
+    *kernelId = params->kernelId;
+
+    return true;
+}
+
+bool IPCGraphConfig::serverFlattenGetPgId(void* pData, uint32_t size, int32_t pgId) {
+    LOGIPC("@%s, pData:%p, size:%u, pgId: %d", __func__, pData, size, pgId);
+
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(GraphGetPgIdParams), false, "@%s, buffer is small", __func__);
+
+    GraphGetPgIdParams* params = static_cast<GraphGetPgIdParams*>(pData);
+    params->pgId = pgId;
+
+    return true;
+}
+
+bool IPCGraphConfig::clientUnFlattenGetPgId(void* pData, uint32_t size, int32_t* pgId) {
+    LOGIPC("@%s, pData:%p, size:%u", __func__, pData, size);
+
+    CheckError(!pData || !pgId, false, "@%s, pData or pgId is nullptr", __func__);
+    CheckError(size < sizeof(GraphGetPgIdParams), false, "@%s, buffer is small", __func__);
+
+    GraphGetPgIdParams* params = static_cast<GraphGetPgIdParams*>(pData);
+    *pgId = params->pgId;
+
+    return true;
+}
+
+bool IPCGraphConfig::clientFlattenGetConnection(void* pData, uint32_t size, GraphBaseInfo info,
+                                                const std::vector<std::string>& pgList) {
+    LOGIPC("@%s, pData:%p, size:%u, cameraId :%d, config mode:%d, pgName size: %zu", __func__,
+           pData, size, info.cameraId, info.configMode, pgList.size());
+
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(GraphGetConnectionParams), false, "@%s, buffer is small", __func__);
+
+    GraphGetConnectionParams* params = static_cast<GraphGetConnectionParams*>(pData);
+    CLEAR(*params);
+
+    params->baseInfo = info;
+    params->pgListNum = pgList.size();
+    for (size_t i = 0; i < pgList.size(); ++i) {
+        size_t len = pgList[i].copy(params->pgList[i], MAX_NAME_LENGTH);
+        params->pgList[i][len] = '\0';
+    }
+
+    return true;
+}
+
+bool IPCGraphConfig::serverUnFlattenGetConnection(void* pData, uint32_t size, GraphBaseInfo* info,
+                                                  std::vector<std::string>* pgList) {
+    LOGIPC("@%s, pData:%p, size:%u", __func__, pData, size);
+
+    CheckError(size < sizeof(GraphGetConnectionParams), false, "@%s, buffer is small", __func__);
+    CheckError(!pData || !info || !pgList, false, "@%s, pData, info or pgList is nullptr",
+               __func__);
+
+    GraphGetConnectionParams* params = static_cast<GraphGetConnectionParams*>(pData);
+
+    *info = params->baseInfo;
+    for (size_t i = 0; i < params->pgListNum; ++i) {
+        pgList->push_back(params->pgList[i]);
+    }
+
+    return true;
+}
+
+bool IPCGraphConfig::serverFlattenGetConnection(
+    void* pData, uint32_t size, const std::vector<IGraphType::ScalerInfo>& scalerInfo,
+    const std::vector<IGraphType::PipelineConnection>& confVector) {
+    LOGIPC("@%s, pData:%p, size:%u", __func__, pData, size);
+
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(GraphGetConnectionParams), false, "@%s, buffer is small", __func__);
+
+    GraphGetConnectionParams* params = static_cast<GraphGetConnectionParams*>(pData);
+
+    params->connectionArraySize = confVector.size();
+    for (size_t i = 0; i < confVector.size(); ++i) {
+        params->connectionArray[i].connection = confVector[i];
+        if (params->connectionArray[i].connection.stream) {
+            params->connectionArray[i].stream = *(confVector[i].stream);
+        }
+    }
+
+    params->scalerInfoNum = scalerInfo.size();
+    for (size_t i = 0; i < scalerInfo.size(); ++i) {
+        params->scalerInfoArray[i] = scalerInfo[i];
+    }
+
+    return true;
+}
+
+bool IPCGraphConfig::clientUnFlattenGetConnection(
+    void* pData, uint32_t size, std::vector<IGraphType::ScalerInfo>* scalerInfo,
+    std::vector<IGraphType::PipelineConnection>* confVector) {
+    LOGIPC("@%s, pData:%p, size:%u", __func__, pData, size);
+
+    CheckError(!pData || !confVector, false, "@%s, pData or confVector is nullptr", __func__);
+    CheckError(size < sizeof(GraphGetConnectionParams), false, "@%s, buffer is small", __func__);
+
+    GraphGetConnectionParams* params = static_cast<GraphGetConnectionParams*>(pData);
+
+    for (size_t i = 0; i < params->connectionArraySize; ++i) {
+        if (params->connectionArray[i].connection.stream) {
+            params->connectionArray[i].connection.stream = &(params->connectionArray[i].stream);
+        }
+        confVector->push_back(params->connectionArray[i].connection);
+    }
+
+    for (size_t i = 0; i < params->scalerInfoNum; ++i) {
+        scalerInfo->push_back(params->scalerInfoArray[i]);
+    }
+
+    return true;
+}
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCGraphConfig.h b/camera/hal/intel/ipu6/modules/sandboxing/IPCGraphConfig.h
new file mode 100644
index 000000000000..08fdce465fd1
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCGraphConfig.h
@@ -0,0 +1,171 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <string>
+#include <vector>
+
+#include "modules/sandboxing/IPCCommon.h"
+#include "src/platformdata/gc/IGraphConfig.h"
+
+namespace icamera {
+
+#define MAX_STREAM (4)                             // Max stream number
+#define MAX_GRAPH_SETTINGS_SIZE (2 * 1024 * 1024)  // Max graph settings file size
+#define MAX_GRAPH_DESCRIPTOR_SIZE (512 * 1024)     // Max graph descriptor file size
+#define MAX_PG_NUMBER (10)                         // Max pg number
+#define MAX_NAME_LENGTH (128)                      // Max length for name
+#define MAX_CONNECTION_COUNT (64)                  // Max connection count
+#define MAX_STREAM_KERNEL_COUNT (500)              // Max kernels info per one stream
+
+struct GraphBaseInfo {
+    int cameraId;
+    ConfigMode configMode;
+
+    bool operator<(const GraphBaseInfo& b) const {
+        return (cameraId < b.cameraId) ? true : (configMode < b.configMode ? true : false);
+    }
+};
+
+struct GraphPgInfo {
+    char pgName[MAX_NAME_LENGTH];
+    uint32_t pgId;
+    int streamId;
+    uint32_t rbmByte;
+    char rbmData[MAX_NAME_LENGTH];
+};
+
+struct GraphMbrInfo {
+    int32_t streamId;
+    ia_isp_bxt_gdc_limits mBrData;
+};
+
+struct GraphKernelArray {
+    int32_t streamId;
+    ia_isp_bxt_program_group group;
+    ia_isp_bxt_run_kernels_t runKernels[MAX_STREAM_KERNEL_COUNT];
+    ia_isp_bxt_resolution_info_t resoInfo[MAX_STREAM_KERNEL_COUNT];
+    ia_isp_bxt_resolution_info_t resoHistory[MAX_STREAM_KERNEL_COUNT];
+    ia_isp_bxt_pipe_t pipeInfo;
+};
+
+struct GraphParseParams {
+    int cameraId;
+    size_t gdSize;
+    char GD[MAX_GRAPH_DESCRIPTOR_SIZE];
+    size_t gsSize;
+    char GS[MAX_GRAPH_SETTINGS_SIZE];
+};
+
+struct GraphConfigStreamParams {
+    GraphBaseInfo baseInfo;
+    GraphSettingType type;
+    uint32_t streamNum;
+    HalStream streamCfg[MAX_STREAM];
+    stream_t streamPriv[MAX_STREAM];
+};
+
+struct GraphGetDataParams {
+    GraphBaseInfo baseInfo;
+
+    int mcId;
+    int graphId;
+    uint32_t gdcKernelId;
+
+    camera_resolution_t csiReso;
+    ia_isp_bxt_resolution_info_t gdcReso;
+
+    uint32_t streamIdNum;
+    int32_t streamIdData[MAX_STREAM];
+
+    uint32_t pgInfoNum;
+    GraphPgInfo pgInfoData[MAX_PG_NUMBER];
+
+    uint32_t mBrInfoNum;
+    GraphMbrInfo mBrInfoData[MAX_STREAM];
+
+    uint32_t pgNamesNum;
+    char pgNames[MAX_PG_NUMBER][MAX_NAME_LENGTH];
+
+    uint32_t kernelArrayNum;
+    GraphKernelArray kernelArray[MAX_STREAM];
+};
+
+struct GraphGetPgIdParams {
+    GraphBaseInfo baseInfo;
+    uint32_t streamId;
+    int32_t kernelId;
+    int32_t pgId;
+};
+
+struct GraphConnection {
+    IGraphType::PipelineConnection connection;
+    HalStream stream;
+};
+
+struct GraphGetConnectionParams {
+    GraphBaseInfo baseInfo;
+    uint32_t pgListNum;
+    char pgList[MAX_PG_NUMBER][MAX_NAME_LENGTH];
+    uint32_t connectionArraySize;
+    GraphConnection connectionArray[MAX_CONNECTION_COUNT];
+    uint32_t scalerInfoNum;
+    IGraphType::ScalerInfo scalerInfoArray[MAX_STREAM];
+};
+
+class IPCGraphConfig {
+ public:
+    IPCGraphConfig();
+    virtual ~IPCGraphConfig();
+
+    bool clientFlattenParse(void* pData, uint32_t size, int cameraId, const char* graphDescFile,
+                            const char* settingsFile);
+    bool serverUnflattenParse(void* pData, uint32_t size, GraphParseParams** parseParam);
+    bool clientFlattenConfigStreams(void* pData, uint32_t size, GraphBaseInfo info,
+                                    GraphSettingType type, const std::vector<HalStream*>& streams);
+    bool serverUnflattenConfigStreams(void* pData, uint32_t size, GraphBaseInfo* info,
+                                      GraphSettingType* type, std::vector<HalStream*>* streams);
+    bool clientFlattenGetGraphData(void* pData, uint32_t size, GraphBaseInfo info);
+    bool serverUnflattenGetGraphData(void* pData, uint32_t size, GraphBaseInfo* info);
+    bool serverFlattenGetGraphData(void* pData, uint32_t size,
+                                   IGraphType::GraphConfigData graphData);
+    bool clientUnflattenGetGraphData(void* pData, uint32_t size,
+                                     IGraphType::GraphConfigData* graphData);
+    bool clientFlattenGetPgId(void* pData, uint32_t size, GraphBaseInfo info, const int streamId,
+                              const int kernelId);
+    bool serverUnFlattenGetPgId(void* pData, uint32_t size, GraphBaseInfo* info, uint32_t* streamId,
+                                int32_t* kernelId);
+    bool serverFlattenGetPgId(void* pData, uint32_t size, int32_t pgId);
+    bool clientUnFlattenGetPgId(void* pData, uint32_t size, int32_t* pgId);
+    bool clientFlattenGetConnection(void* pData, uint32_t size, GraphBaseInfo info,
+                                    const std::vector<std::string>& pgList);
+    bool serverUnFlattenGetConnection(void* pData, uint32_t size, GraphBaseInfo* info,
+                                      std::vector<std::string>* pgList);
+    bool serverFlattenGetConnection(void* pData, uint32_t size,
+                                    const std::vector<IGraphType::ScalerInfo>& scalerInfo,
+                                    const std::vector<IGraphType::PipelineConnection>& confVector);
+    bool clientUnFlattenGetConnection(void* pData, uint32_t size,
+                                      std::vector<IGraphType::ScalerInfo>* scalerInfo,
+                                      std::vector<IGraphType::PipelineConnection>* confVector);
+
+ private:
+    status_t readDataFromXml(const char* fileName, char* dataPtr, size_t* dataSize, int maxSize);
+
+    // Disable copy constructor and assignment operator
+    DISALLOW_COPY_AND_ASSIGN(IPCGraphConfig);
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelAiq.cpp b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelAiq.cpp
new file mode 100644
index 000000000000..b532e3b90b28
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelAiq.cpp
@@ -0,0 +1,1121 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IPCIntelAiq"
+
+#include "modules/sandboxing/IPCIntelAiq.h"
+
+#include <ia_types.h>
+
+namespace icamera {
+IPCIntelAiq::IPCIntelAiq() {
+    LOGIPC("@%s", __func__);
+}
+
+IPCIntelAiq::~IPCIntelAiq() {
+    LOGIPC("@%s", __func__);
+}
+
+// init
+bool IPCIntelAiq::clientFlattenInit(const ia_binary_data* aiqbData, const ia_binary_data* nvmData,
+                                    const ia_binary_data* aiqdData, unsigned int statsMaxWidth,
+                                    unsigned int statsMaxHeight, unsigned int maxNumStatsIn,
+                                    uintptr_t cmc, uintptr_t mkn, uint8_t* pData,
+                                    unsigned int size) {
+    LOGIPC("@%s, aiqbData:%p, nvmData:%p, aiqdData:%p", __func__, aiqbData, nvmData, aiqdData);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+
+    uint8_t* ptr = pData;
+    memset(ptr, 0, size);
+
+    aiq_init_params* params = reinterpret_cast<aiq_init_params*>(ptr);
+    params->aiqb_size = aiqbData ? aiqbData->size : 0;
+    params->nvm_size = nvmData ? nvmData->size : 0;
+    params->aiqd_size = aiqdData ? aiqdData->size : 0;
+    params->stats_max_width = statsMaxWidth;
+    params->stats_max_height = statsMaxHeight;
+    params->max_num_stats_in = maxNumStatsIn;
+    params->ia_mkn = mkn;
+    params->cmcRemoteHandle = cmc;
+
+    ptr += sizeof(aiq_init_params);
+    if (aiqbData) {
+        MEMCPY_S(ptr, aiqbData->size, aiqbData->data, aiqbData->size);
+    }
+
+    ptr += params->aiqb_size;
+    if (nvmData) {
+        MEMCPY_S(ptr, nvmData->size, nvmData->data, nvmData->size);
+    }
+
+    ptr += params->nvm_size;
+    if (aiqdData) {
+        MEMCPY_S(ptr, aiqdData->size, aiqdData->data, aiqdData->size);
+    }
+
+    return true;
+}
+
+bool IPCIntelAiq::serverUnflattenInit(const void* pData, int dataSize, ia_binary_data* aiqbData,
+                                      ia_binary_data* nvmData, ia_binary_data* aiqdData) {
+    LOGIPC("@%s, pData:%p, dataSize:%d, aiqbData:%p, nvmData:%p, aiqdData:%p", __func__, pData,
+           dataSize, aiqbData, nvmData, aiqdData);
+    CheckError(dataSize < sizeof(aiq_init_params), false, "@%s, buffer is small", __func__);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(!aiqbData, false, "@%s, aiqbData is nullptr", __func__);
+    CheckError(!nvmData, false, "@%s, nvmData is nullptr", __func__);
+    CheckError(!aiqdData, false, "@%s, aiqdData is nullptr", __func__);
+
+    const aiq_init_params* params = static_cast<const aiq_init_params*>(pData);
+
+    LOGIPC("@%s, aiqb_size:%d, nvm_size:%d, aiqd_size:%d", __func__, params->aiqb_size,
+           params->nvm_size, params->aiqd_size);
+
+    int totalMemSize =
+        sizeof(aiq_init_params) + params->aiqb_size + params->nvm_size + params->aiqd_size;
+    CheckError(dataSize < totalMemSize, false, "@%s, dataSize:%d is too small", __func__, dataSize);
+
+    const aiq_init_params* p = static_cast<const aiq_init_params*>(pData) + 1;
+    uint8_t* ptr = reinterpret_cast<uint8_t*>(const_cast<aiq_init_params*>(p));
+    aiqbData->size = params->aiqb_size;
+    aiqbData->data = aiqbData->size > 0 ? ptr : nullptr;
+
+    ptr += params->aiqb_size;
+    nvmData->size = params->nvm_size;
+    nvmData->data = nvmData->size > 0 ? ptr : nullptr;
+
+    ptr += params->nvm_size;
+    aiqdData->size = params->aiqd_size;
+    aiqdData->data = aiqdData->size > 0 ? ptr : nullptr;
+
+    return true;
+}
+
+// ae
+bool IPCIntelAiq::clientFlattenAe(uintptr_t aiq, const ia_aiq_ae_input_params& inParams,
+                                  ae_run_params* params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(reinterpret_cast<ia_aiq*>(aiq) == nullptr, false, "@%s, aiq is nullptr", __func__);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    params->aiq_handle = aiq;
+
+    params->base = inParams;
+    const ia_aiq_ae_input_params* base = &params->base;
+
+    if (base->aec_features) {
+        params->aec_features = *inParams.aec_features;
+    }
+
+    if (base->exposure_coordinate) {
+        params->exposure_coordinate = *inParams.exposure_coordinate;
+    }
+
+    if (base->exposure_window) {
+        params->exposure_window = *inParams.exposure_window;
+    }
+
+    if (inParams.num_exposures > 1) {
+        LOGE("@%s, BUG: num_exposures:%d > 1. Copying only first.", __func__,
+             inParams.num_exposures);
+    }
+
+    if (base->sensor_descriptor) {
+        params->sensor_descriptor = *inParams.sensor_descriptor;
+    }
+
+    if (base->manual_exposure_time_us) {
+        params->manual_exposure_time_us = *inParams.manual_exposure_time_us;
+    }
+
+    if (base->manual_analog_gain) {
+        params->manual_analog_gain = *inParams.manual_analog_gain;
+    }
+
+    if (base->manual_iso) {
+        params->manual_iso = *inParams.manual_iso;
+    }
+
+    if (base->manual_limits) {
+        params->manual_limits = *inParams.manual_limits;
+    }
+
+    return true;
+}
+
+bool IPCIntelAiq::serverUnflattenAe(ae_run_params* inParams, ia_aiq_ae_input_params** params) {
+    LOGIPC("@%s, inParams:%p, params:%p", __func__, inParams, params);
+    CheckError(!inParams, false, "@%s, inParams is nullptr", __func__);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    ia_aiq_ae_input_params* base = &inParams->base;
+    if (base->aec_features) {
+        base->aec_features = &inParams->aec_features;
+    }
+
+    if (base->exposure_coordinate) {
+        base->exposure_coordinate = &inParams->exposure_coordinate;
+    }
+
+    if (base->exposure_window) {
+        base->exposure_window = &inParams->exposure_window;
+    }
+
+    if (base->sensor_descriptor) {
+        base->sensor_descriptor = &inParams->sensor_descriptor;
+    }
+
+    if (base->manual_exposure_time_us) {
+        base->manual_exposure_time_us = &inParams->manual_exposure_time_us;
+    }
+
+    if (base->manual_analog_gain) {
+        base->manual_analog_gain = &inParams->manual_analog_gain;
+    }
+
+    if (base->manual_iso) {
+        base->manual_iso = &inParams->manual_iso;
+    }
+
+    if (base->manual_limits) {
+        base->manual_limits = &inParams->manual_limits;
+    }
+
+    *params = base;
+
+    return true;
+}
+
+bool IPCIntelAiq::clientUnflattenAe(ae_run_params* params, ia_aiq_ae_results** results) {
+    LOGIPC("@%s, params:%p, results:%p", __func__, params, results);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+    CheckError(!results, false, "@%s, results is nullptr", __func__);
+
+    ae_run_params_results* res = &params->res;
+    bool ret = unflattenAeResults(res);
+    CheckError((ret == false), false, "@%s, unflattenAeResults fails", __func__);
+
+    *results = &res->base;
+
+    return true;
+}
+
+bool IPCIntelAiq::serverFlattenAe(const ia_aiq_ae_results& aeResults, ae_run_params* params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    bool ret = flattenAeResults(aeResults, &params->res);
+    CheckError(ret == false, false, "@%s, flattenAeResults fails", __func__);
+
+    return true;
+}
+
+bool IPCIntelAiq::flattenAeResults(const ia_aiq_ae_results& aeResults, ae_run_params_results* res) {
+    LOGIPC("@%s, res:%p", __func__, res);
+    CheckError(!res, false, "@%s, res is nullptr", __func__);
+
+    res->base = aeResults;
+    const ia_aiq_ae_results* base = &res->base;
+
+    if (base->exposures && base->num_exposures > 0) {
+        CheckError(base->num_exposures > MAX_NUM_EXPOSURES, false,
+                   "@%s, base->num_exposures:% is too big", __func__, base->num_exposures);
+
+        for (unsigned int i = 0; i < base->num_exposures; i++) {
+            res->exposures[i] = aeResults.exposures[i];
+
+            if (res->exposures[i].exposure) {
+                res->exposure[i] = *aeResults.exposures[i].exposure;
+            }
+            if (res->exposures[i].sensor_exposure) {
+                res->sensor_exposure[i] = *aeResults.exposures[i].sensor_exposure;
+            }
+            if (res->exposures[i].exposure_plan_ids) {
+                CheckError(base->exposures->num_exposure_plan > MAX_NUM_OF_EXPOSURE_PLANS, false,
+                           "@%s, base->exposures->num_exposure_plan:% is too big", __func__,
+                           base->exposures->num_exposure_plan);
+                MEMCPY_S(res->exposure_plan_ids[i],
+                         sizeof(unsigned int) * MAX_NUM_OF_EXPOSURE_PLANS,
+                         aeResults.exposures[i].exposure_plan_ids,
+                         sizeof(unsigned int) * aeResults.exposures[i].num_exposure_plan);
+            }
+        }
+    }
+
+    if (base->weight_grid) {
+        res->weight_grid = *aeResults.weight_grid;
+
+        if (res->weight_grid.weights) {
+            unsigned int gridElements =
+                aeResults.weight_grid->width * aeResults.weight_grid->height;
+            gridElements = CLIP(gridElements, MAX_SIZE_WEIGHT_GRID, 1);
+            MEMCPY_S(res->weights, sizeof(res->weights), aeResults.weight_grid->weights,
+                     gridElements * sizeof(unsigned char));
+        }
+    }
+
+    if (base->flashes) {
+        // Valgrind will give warning from here in the first round. It should be fine.
+        if (aeResults.num_flashes > 0) {
+            MEMCPY_S(res->flashes, sizeof(res->flashes), aeResults.flashes,
+                     MAX_NUM_FLASHES * sizeof(ia_aiq_flash_parameters));
+        }
+    }
+
+    if (base->aperture_control) {
+        res->aperture_control = *aeResults.aperture_control;
+    }
+
+    return true;
+}
+
+bool IPCIntelAiq::unflattenAeResults(ae_run_params_results* res) {
+    LOGIPC("@%s, res:%p", __func__, res);
+    CheckError(!res, false, "@%s, res is nullptr", __func__);
+
+    ia_aiq_ae_results* base = &res->base;
+
+    if (base->exposures) {
+        base->exposures = res->exposures;
+
+        CheckError(base->num_exposures > MAX_NUM_EXPOSURES, false,
+                   "@%s, base->num_exposures:% is too big", __func__, base->num_exposures);
+
+        for (unsigned int i = 0; i < base->num_exposures; i++) {
+            if (base->exposures[i].exposure) {
+                base->exposures[i].exposure = &res->exposure[i];
+            }
+            if (base->exposures[i].sensor_exposure) {
+                base->exposures[i].sensor_exposure = &res->sensor_exposure[i];
+            }
+            if (base->exposures[i].exposure_plan_ids) {
+                base->exposures[i].exposure_plan_ids = res->exposure_plan_ids[i];
+            }
+        }
+    }
+
+    if (base->weight_grid) {
+        base->weight_grid = &res->weight_grid;
+        if (base->weight_grid->weights) {
+            base->weight_grid->weights = res->weights;
+        }
+    }
+
+    if (base->flashes) {
+        base->flashes = res->flashes;
+    }
+
+    if (base->aperture_control) {
+        base->aperture_control = &res->aperture_control;
+    }
+
+    return true;
+}
+
+// af
+bool IPCIntelAiq::clientFlattenAf(uintptr_t aiq, const ia_aiq_af_input_params& inParams,
+                                  af_run_params* params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(reinterpret_cast<ia_aiq*>(aiq) == nullptr, false, "@%s, aiq is nullptr", __func__);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    params->aiq_handle = aiq;
+
+    params->base = inParams;
+    ia_aiq_af_input_params* base = &params->base;
+    if (base->focus_rect) {
+        params->focus_rect = *inParams.focus_rect;
+    }
+    if (base->manual_focus_parameters) {
+        params->manual_focus_parameters = *inParams.manual_focus_parameters;
+    }
+
+    return true;
+}
+
+bool IPCIntelAiq::clientUnflattenAf(const af_run_params& params, ia_aiq_af_results** results) {
+    LOGIPC("@%s, results:%p", __func__, results);
+    CheckError(!results, false, "@%s, results is nullptr", __func__);
+
+    *results = const_cast<ia_aiq_af_results*>(&params.results);
+
+    return true;
+}
+
+bool IPCIntelAiq::serverUnflattenAf(af_run_params* inParams, ia_aiq_af_input_params** params) {
+    LOGIPC("@%s, inParams:%p, params:%p", __func__, inParams, params);
+    CheckError(!inParams, false, "@%s, inParams is nullptr", __func__);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    ia_aiq_af_input_params* base = &inParams->base;
+    if (base->focus_rect) {
+        base->focus_rect = &inParams->focus_rect;
+    }
+    if (base->manual_focus_parameters) {
+        base->manual_focus_parameters = &inParams->manual_focus_parameters;
+    }
+
+    *params = base;
+
+    return true;
+}
+
+bool IPCIntelAiq::serverFlattenAf(const ia_aiq_af_results& afResults, af_run_params* params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    ia_aiq_af_results* results = &params->results;
+    *results = afResults;
+
+    LOGIPC("af results->status:%d", results->status);
+    LOGIPC("af results->current_focus_distance:%d", results->current_focus_distance);
+    LOGIPC("af results->next_lens_position:%d", results->next_lens_position);
+    LOGIPC("af results->lens_driver_action:%d", results->lens_driver_action);
+    LOGIPC("af results->use_af_assist:%d", results->use_af_assist);
+    LOGIPC("af results->final_lens_position_reached:%d", results->final_lens_position_reached);
+
+    return true;
+}
+
+// awb
+bool IPCIntelAiq::clientFlattenAwb(uintptr_t aiq, const ia_aiq_awb_input_params& inParams,
+                                   awb_run_params* params) {
+    LOGIPC("@%s, aiq:0x%, params:%p", __func__, aiq, params);
+    CheckError(reinterpret_cast<ia_aiq*>(aiq) == nullptr, false, "@%s, aiq is nullptr", __func__);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    params->aiq_handle = aiq;
+
+    params->base = inParams;
+    const ia_aiq_awb_input_params* base = &params->base;
+
+    if (base->manual_cct_range) {
+        params->manual_cct_range = *inParams.manual_cct_range;
+    }
+
+    if (base->manual_white_coordinate) {
+        params->manual_white_coordinate = *inParams.manual_white_coordinate;
+    }
+
+    return true;
+}
+
+bool IPCIntelAiq::clientUnflattenAwb(const awb_run_params& inParams, ia_aiq_awb_results** results) {
+    LOGIPC("@%s, results:%p", __func__, results);
+    CheckError(!results, false, "@%s, results is nullptr", __func__);
+
+    *results = const_cast<ia_aiq_awb_results*>(&inParams.results);
+
+    return true;
+}
+
+bool IPCIntelAiq::serverUnflattenAwb(awb_run_params* inParams, ia_aiq_awb_input_params** params) {
+    LOGIPC("@%s, inParams:%p, params:%p", __func__, inParams, params);
+    CheckError(!inParams, false, "@%s, inParams is nullptr", __func__);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    ia_aiq_awb_input_params* base = &inParams->base;
+
+    if (base->manual_cct_range) {
+        base->manual_cct_range = &inParams->manual_cct_range;
+    }
+
+    if (base->manual_white_coordinate) {
+        base->manual_white_coordinate = &inParams->manual_white_coordinate;
+    }
+
+    LOGIPC("@%s, manual_cct_range:%p, manual_white_coordinate:%p", __func__, base->manual_cct_range,
+           base->manual_white_coordinate);
+
+    *params = base;
+
+    return true;
+}
+
+bool IPCIntelAiq::serverFlattenAwb(const ia_aiq_awb_results& awbResults, awb_run_params* params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    ia_aiq_awb_results* results = &params->results;
+    *results = awbResults;
+
+    LOGIPC("awb results->accurate_r_per_g:%f", results->accurate_r_per_g);
+    LOGIPC("awb results->accurate_b_per_g:%f", results->accurate_b_per_g);
+    LOGIPC("awb results->final_r_per_g:%f", results->final_r_per_g);
+    LOGIPC("awb results->final_b_per_g:%f", results->final_b_per_g);
+    LOGIPC("awb results->cct_estimate:%d", results->cct_estimate);
+    LOGIPC("awb results->distance_from_convergence:%f", results->distance_from_convergence);
+
+    return true;
+}
+
+// gbce
+bool IPCIntelAiq::clientFlattenGbce(uintptr_t aiq, const ia_aiq_gbce_input_params& inParams,
+                                    gbce_run_params* params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(reinterpret_cast<ia_aiq*>(aiq) == nullptr, false, "@%s, aiq is nullptr", __func__);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    params->aiq_handle = aiq;
+    params->base = inParams;
+
+    return true;
+}
+
+bool IPCIntelAiq::clientUnflattenGbce(gbce_run_params* params, ia_aiq_gbce_results** results) {
+    LOGIPC("@%s, params:%p, results:%p", __func__, params, results);
+    CheckError(!results, false, "@%s, results is nullptr", __func__);
+
+    gbce_results_params* res = &params->res;
+    bool ret = unflattenGbceResults(res);
+    CheckError(!ret, false, "@%s, unflattenGbceResults fails", __func__);
+
+    *results = &res->base;
+
+    return true;
+}
+
+bool IPCIntelAiq::serverFlattenGbce(const ia_aiq_gbce_results& gbceResults,
+                                    gbce_run_params* params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    return flattenGbceResults(gbceResults, &params->res);
+}
+
+bool IPCIntelAiq::flattenGbceResults(const ia_aiq_gbce_results& gbceResults,
+                                     gbce_results_params* res) {
+    LOGIPC("@%s, res:%p", __func__, res);
+    CheckError(!res, false, "@%s, res is nullptr", __func__);
+
+    size_t size = gbceResults.gamma_lut_size * sizeof(*gbceResults.g_gamma_lut);
+    LOGIPC("@%s, gamma_lut_size:%d, size:%zu, tone_map_lut_size:%d", __func__,
+           gbceResults.gamma_lut_size, size, gbceResults.tone_map_lut_size);
+
+    res->base = gbceResults;
+
+    const ia_aiq_gbce_results* base = &res->base;
+
+    if (base->r_gamma_lut) {
+        MEMCPY_S(res->r_gamma_lut, sizeof(res->r_gamma_lut), gbceResults.r_gamma_lut, size);
+    }
+
+    if (base->b_gamma_lut) {
+        MEMCPY_S(res->b_gamma_lut, sizeof(res->b_gamma_lut), gbceResults.b_gamma_lut, size);
+    }
+
+    if (base->g_gamma_lut) {
+        MEMCPY_S(res->g_gamma_lut, sizeof(res->g_gamma_lut), gbceResults.g_gamma_lut, size);
+    }
+
+    if (base->tone_map_lut) {
+        MEMCPY_S(res->tone_map_lut, sizeof(res->tone_map_lut), gbceResults.tone_map_lut,
+                 gbceResults.tone_map_lut_size * sizeof(*gbceResults.tone_map_lut));
+    }
+
+    return true;
+}
+
+bool IPCIntelAiq::unflattenGbceResults(gbce_results_params* res) {
+    LOGIPC("@%s, res:%p", __func__, res);
+    CheckError(!res, false, "@%s, res is nullptr", __func__);
+
+    ia_aiq_gbce_results* base = &res->base;
+
+    LOGIPC("@%s, gamma_lut_size:%d", __func__, base->gamma_lut_size);
+    LOGIPC("@%s, tone_map_lut_size:%d", __func__, base->tone_map_lut_size);
+    CheckError(base->gamma_lut_size > MAX_NUM_GAMMA_LUTS, false,
+               "@%s, gamma_lut_size:%d is too big", __func__, base->gamma_lut_size);
+    CheckError((base->tone_map_lut_size > MAX_NUM_TOME_MAP_LUTS), false,
+               "@%s, tone_map_lut_size:%d is too big", __func__, base->tone_map_lut_size);
+
+    if (base->r_gamma_lut) {
+        base->r_gamma_lut = res->r_gamma_lut;
+    }
+
+    if (base->b_gamma_lut) {
+        base->b_gamma_lut = res->b_gamma_lut;
+    }
+
+    if (base->g_gamma_lut) {
+        base->g_gamma_lut = res->g_gamma_lut;
+    }
+
+    if (base->tone_map_lut) {
+        base->tone_map_lut = res->tone_map_lut;
+    }
+
+    return true;
+}
+
+// pa
+bool IPCIntelAiq::clientFlattenPaV1(uintptr_t aiq, const ia_aiq_pa_input_params& inParams,
+                                    pa_run_v1_params* params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(reinterpret_cast<ia_aiq*>(aiq) == nullptr, false, "@%s, aiq is nullptr", __func__);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    params->aiq_handle = aiq;
+
+    params->base = inParams;
+    ia_aiq_pa_input_params* base = &params->base;
+
+    if (base->awb_results) {
+        params->awb_results = *base->awb_results;
+    }
+
+    if (base->exposure_params) {
+        params->exposure_params = *base->exposure_params;
+    }
+
+    if (base->color_gains) {
+        params->color_gains = *base->color_gains;
+    }
+
+    return true;
+}
+
+bool IPCIntelAiq::clientUnflattenPaV1(pa_run_v1_params* params, ia_aiq_pa_results_v1** results) {
+    LOGIPC("@%s, params:%p, results:%p", __func__, params, results);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+    CheckError(!results, false, "@%s, results is nullptr", __func__);
+
+    pa_run_params_results_v1* res = &params->res;
+    bool ret = unflattenPaResultsV1(res);
+    CheckError((ret == false), false, "@%s, unflattenPaResultsV1 fails", __func__);
+
+    *results = &res->base;
+
+    return true;
+}
+
+bool IPCIntelAiq::serverUnflattenPaV1(pa_run_v1_params* inParams, ia_aiq_pa_input_params** params) {
+    LOGIPC("@%s, inParams:%p, params:%p", __func__, inParams, params);
+    CheckError(!inParams, false, "@%s, inParams is nullptr", __func__);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    ia_aiq_pa_input_params* base = &inParams->base;
+
+    if (base->awb_results) {
+        base->awb_results = &inParams->awb_results;
+    }
+
+    if (base->exposure_params) {
+        base->exposure_params = &inParams->exposure_params;
+    }
+
+    if (base->color_gains) {
+        base->color_gains = &inParams->color_gains;
+    }
+
+    *params = base;
+
+    return true;
+}
+
+bool IPCIntelAiq::serverFlattenPaV1(const ia_aiq_pa_results_v1& paResults,
+                                    pa_run_v1_params* params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    bool ret = flattenPaResultsV1(paResults, &params->res);
+    CheckError(ret == false, false, "@%s, flattenPaResultsV1 fails", __func__);
+
+    return true;
+}
+
+bool IPCIntelAiq::flattenPaResultsV1(const ia_aiq_pa_results_v1& paResults,
+                                     pa_run_params_results_v1* res) {
+    LOGIPC("@%s, res:%p", __func__, res);
+    CheckError(!res, false, "@%s, res is nullptr", __func__);
+
+    res->base = paResults;
+    ia_aiq_pa_results_v1* base = &res->base;
+
+    ia_aiq_color_channels_lut* linearization = &base->linearization;
+    CheckError((MAX_NUM_LUTS < linearization->size), false, "@%s, linearization:%d is too big",
+               __func__, linearization->size);
+    if (linearization->gr) {
+        MEMCPY_S(res->gr, sizeof(res->gr), linearization->gr,
+                 sizeof(*linearization->gr) * linearization->size);
+    }
+    if (linearization->r) {
+        MEMCPY_S(res->r, sizeof(res->r), linearization->r,
+                 sizeof(*linearization->r) * linearization->size);
+    }
+    if (linearization->b) {
+        MEMCPY_S(res->b, sizeof(res->b), linearization->b,
+                 sizeof(*linearization->b) * linearization->size);
+    }
+    if (linearization->gb) {
+        MEMCPY_S(res->gb, sizeof(res->gb), linearization->gb,
+                 sizeof(*linearization->gb) * linearization->size);
+    }
+
+    ia_aiq_advanced_ccm_t* preferred_acm = base->preferred_acm;
+    if (preferred_acm) {
+        CheckError((MAX_SECTOR_COUNT < preferred_acm->sector_count), false,
+                   "@%s, sector_count:%d is too big", __func__, preferred_acm->sector_count);
+
+        res->preferred_acm = *preferred_acm;
+
+        if (preferred_acm->hue_of_sectors) {
+            MEMCPY_S(res->hue_of_sectors, sizeof(res->hue_of_sectors),
+                     preferred_acm->hue_of_sectors,
+                     sizeof(*preferred_acm->hue_of_sectors) * preferred_acm->sector_count);
+        }
+
+        if (preferred_acm->advanced_color_conversion_matrices) {
+            MEMCPY_S(res->advanced_color_conversion_matrices,
+                     sizeof(res->advanced_color_conversion_matrices),
+                     preferred_acm->advanced_color_conversion_matrices,
+                     (sizeof(*preferred_acm->advanced_color_conversion_matrices) *
+                      preferred_acm->sector_count));
+        }
+    }
+
+    ia_aiq_ir_weight_t* ir_weight = base->ir_weight;
+    if (ir_weight) {
+        res->ir_weight = *ir_weight;
+
+        if (ir_weight->ir_weight_grid_R) {
+            MEMCPY_S(res->ir_weight_grid_R, sizeof(res->ir_weight_grid_R),
+                     ir_weight->ir_weight_grid_R,
+                     sizeof(*ir_weight->ir_weight_grid_R) * ir_weight->height * ir_weight->width);
+        }
+
+        if (ir_weight->ir_weight_grid_G) {
+            MEMCPY_S(res->ir_weight_grid_G, sizeof(res->ir_weight_grid_G),
+                     ir_weight->ir_weight_grid_G,
+                     sizeof(*ir_weight->ir_weight_grid_G) * ir_weight->height * ir_weight->width);
+        }
+
+        if (ir_weight->ir_weight_grid_B) {
+            MEMCPY_S(res->ir_weight_grid_B, sizeof(res->ir_weight_grid_B),
+                     ir_weight->ir_weight_grid_B,
+                     sizeof(*ir_weight->ir_weight_grid_B) * ir_weight->height * ir_weight->width);
+        }
+    }
+
+    ia_aiq_rgbir_t* rgbir = base->rgbir;
+    if (rgbir) {
+        res->rgbir = *rgbir;
+
+        CheckError(rgbir->n_models > MAX_NUM_IR_MODES, false, "@%s, rgbir->n_models:%d is too big",
+                   __func__, rgbir->n_models);
+
+        if (rgbir->models && rgbir->n_models > 0) {
+            for (unsigned int i = 0; i < rgbir->n_models; i++) {
+                res->models[i] = rgbir->models[i];
+            }
+        }
+    }
+
+    return true;
+}
+
+bool IPCIntelAiq::unflattenPaResultsV1(pa_run_params_results_v1* res) {
+    LOGIPC("@%s, res:%p", __func__, res);
+    CheckError(!res, false, "@%s, res is nullptr", __func__);
+
+    ia_aiq_pa_results_v1* base = &res->base;
+
+    if (base->linearization.gr) {
+        base->linearization.gr = res->gr;
+    }
+    if (base->linearization.r) {
+        base->linearization.r = res->r;
+    }
+    if (base->linearization.b) {
+        base->linearization.b = res->b;
+    }
+    if (base->linearization.gb) {
+        base->linearization.gb = res->gb;
+    }
+
+    if (base->preferred_acm) {
+        base->preferred_acm = &res->preferred_acm;
+
+        if (base->preferred_acm->hue_of_sectors) {
+            base->preferred_acm->hue_of_sectors = res->hue_of_sectors;
+        }
+
+        if (base->preferred_acm->advanced_color_conversion_matrices) {
+            base->preferred_acm->advanced_color_conversion_matrices =
+                static_cast<float(*)[3][3]>(res->advanced_color_conversion_matrices);
+        }
+    }
+
+    if (base->ir_weight) {
+        base->ir_weight = &res->ir_weight;
+
+        if (base->ir_weight->ir_weight_grid_R) {
+            base->ir_weight->ir_weight_grid_R = res->ir_weight_grid_R;
+        }
+        if (base->ir_weight->ir_weight_grid_G) {
+            base->ir_weight->ir_weight_grid_G = res->ir_weight_grid_G;
+        }
+        if (base->ir_weight->ir_weight_grid_B) {
+            base->ir_weight->ir_weight_grid_B = res->ir_weight_grid_B;
+        }
+    }
+
+    if (base->rgbir) {
+        base->rgbir = &res->rgbir;
+
+        ia_aiq_rgbir_t* resRgbir = &res->rgbir;
+        if (resRgbir->models && resRgbir->n_models > 0) {
+            resRgbir->models = res->models;
+        }
+    }
+
+    return true;
+}
+
+// sa
+bool IPCIntelAiq::clientFlattenSaV2(uintptr_t aiq, const ia_aiq_sa_input_params_v1& inParams,
+                                    sa_run_v2_params* params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(reinterpret_cast<ia_aiq*>(aiq) == nullptr, false, "@%s, aiq is nullptr", __func__);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    params->aiq_handle = aiq;
+
+    params->base = inParams;
+    const ia_aiq_sa_input_params_v1* base = &params->base;
+
+    if (base->sensor_frame_params) {
+        params->sensor_frame_params = *inParams.sensor_frame_params;
+    }
+
+    if (base->awb_results) {
+        params->awb_results = *inParams.awb_results;
+    }
+
+    return true;
+}
+
+bool IPCIntelAiq::clientUnflattenSaV2(sa_run_v2_params* params, ia_aiq_sa_results_v1** results) {
+    LOGIPC("@%s, params:%p, results:%p", __func__, params, results);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+    CheckError(!results, false, "@%s, results is nullptr", __func__);
+
+    sa_run_v2_params_results* res = &params->res;
+    bool ret = unflattenSaResultsV2(res);
+    CheckError(ret == false, false, "@%s, unflattenSaResultsV2 fails", __func__);
+
+    *results = &res->base;
+
+    return true;
+}
+
+bool IPCIntelAiq::serverUnflattenSaV2(const sa_run_v2_params& inParams,
+                                      ia_aiq_sa_input_params_v1** params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    ia_aiq_sa_input_params_v1* base = const_cast<ia_aiq_sa_input_params_v1*>(&inParams.base);
+
+    if (base->sensor_frame_params) {
+        base->sensor_frame_params = const_cast<ia_aiq_frame_params*>(&inParams.sensor_frame_params);
+    }
+
+    if (base->awb_results) {
+        base->awb_results = const_cast<ia_aiq_awb_results*>(&inParams.awb_results);
+    }
+
+    *params = base;
+
+    return true;
+}
+
+bool IPCIntelAiq::serverFlattenSaV2(const ia_aiq_sa_results_v1& saResults,
+                                    sa_run_v2_params* params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(!params, false, "@%s, results is nullptr", __func__);
+
+    bool ret = flattenSaResultsV2(saResults, &params->res);
+    CheckError(ret == false, false, "@%s, flattenSaResultsV2 fails", __func__);
+
+    return true;
+}
+
+bool IPCIntelAiq::flattenSaResultsV2(const ia_aiq_sa_results_v1& saResults,
+                                     sa_run_v2_params_results* res) {
+    LOGIPC("@%s, res:%p", __func__, res);
+    CheckError(!res, false, "@%s, res is nullptr", __func__);
+
+    res->base = saResults;
+    ia_aiq_sa_results_v1* base = &res->base;
+
+    LOGIPC("sa_results: width:%d, height:%d, lsc_update:%d", base->width, base->height,
+           base->lsc_update);
+
+    if (base->width && base->height) {
+        size_t size = base->width * base->height * sizeof(unsigned short);
+        for (int i = 0; i < LSC_MAX_BAYER_ORDER_NUM; i++) {
+            for (int j = 0; j < LSC_MAX_BAYER_ORDER_NUM; j++) {
+                lsc_grid_content* lgc = &res->lsc_grid[i][j];
+                if (base->lsc_grid[i][j]) {
+                    MEMCPY_S(lgc->content, sizeof(lgc->content), base->lsc_grid[i][j], size);
+                }
+            }
+        }
+    } else if (base->lsc_update) {
+        LOGE("@%s, Error: LSC table size is 0", __func__);
+    }
+
+    return true;
+}
+
+bool IPCIntelAiq::unflattenSaResultsV2(sa_run_v2_params_results* res) {
+    LOGIPC("@%s, res:%p", __func__, res);
+    CheckError(!res, false, "@%s, res is nullptr", __func__);
+
+    ia_aiq_sa_results_v1* base = &res->base;
+
+    LOGIPC("sa_results_data:height:%d, width:%d", base->height, base->width);
+
+    for (int i = 0; i < LSC_MAX_BAYER_ORDER_NUM; i++) {
+        for (int j = 0; j < LSC_MAX_BAYER_ORDER_NUM; j++) {
+            lsc_grid_content* lgc = &res->lsc_grid[i][j];
+            base->lsc_grid[i][j] = lgc->content;
+        }
+    }
+
+    return true;
+}
+
+// statistics
+bool IPCIntelAiq::clientFlattenStatSetV4(uintptr_t aiq,
+                                         const ia_aiq_statistics_input_params_v4& inParams,
+                                         set_statistics_set_v4_params* params) {
+    LOGIPC("@%s, aiq:0x%, params:%p", __func__, aiq, params);
+    CheckError(reinterpret_cast<ia_aiq*>(aiq) == nullptr, false, "@%s, aiq is nullptr", __func__);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    params->ia_aiq = aiq;
+
+    set_statistics_params_data* input = &params->input;
+    input->base = inParams;
+    ia_aiq_statistics_input_params_v4* base = &input->base;
+
+    if (base->frame_ae_parameters) {
+        flattenAeResults(*base->frame_ae_parameters, &input->frame_ae_parameters);
+    }
+
+    if (base->frame_af_parameters) {
+        input->frame_af_parameters = *base->frame_af_parameters;
+    }
+
+    if (base->rgbs_grids) {
+        CheckError((base->num_rgbs_grids > MAX_NUMBER_OF_GRIDS), false,
+                   "@%s, num_rgbs_grids:%d > MAX_NUMBER_OF_GRIDS:%d", __func__,
+                   base->num_rgbs_grids, MAX_NUMBER_OF_GRIDS);
+
+        for (int i = 0; i < MAX_NUMBER_OF_GRIDS; i++) {
+            ia_aiq_rgbs_grid_data* rgbs_grids = &input->rgbs_grids[i];
+            rgbs_grids->base = *base->rgbs_grids[i];
+
+            CheckError(rgbs_grids->base.grid_width * rgbs_grids->base.grid_height > MAX_NUM_BLOCKS,
+                       false, "@%s, grid_width:%d * grid_height:%d is too big", __func__,
+                       rgbs_grids->base.grid_width, rgbs_grids->base.grid_height);
+
+            MEMCPY_S(rgbs_grids->blocks_ptr, sizeof(rgbs_grids->blocks_ptr),
+                     rgbs_grids->base.blocks_ptr,
+                     rgbs_grids->base.grid_width * rgbs_grids->base.grid_height *
+                         sizeof(*rgbs_grids->base.blocks_ptr));
+        }
+    }
+
+    if (base->hdr_rgbs_grid) {
+        ia_aiq_hdr_rgbs_grid_data* hdr_rgbs_grid = &input->hdr_rgbs_grid;
+        hdr_rgbs_grid->base = *base->hdr_rgbs_grid;
+        MEMCPY_S(hdr_rgbs_grid->blocks_ptr, sizeof(hdr_rgbs_grid->blocks_ptr),
+                 hdr_rgbs_grid->base.blocks_ptr,
+                 hdr_rgbs_grid->base.grid_width * hdr_rgbs_grid->base.grid_height *
+                     sizeof(*hdr_rgbs_grid->base.blocks_ptr));
+    }
+
+    if (base->af_grids) {
+        CheckError((base->num_af_grids > MAX_NUMBER_OF_AF_GRIDS), false,
+                   "@%s, num_af_grids:%d > MAX_NUMBER_OF_AF_GRIDS:%d", __func__, base->num_af_grids,
+                   MAX_NUMBER_OF_AF_GRIDS);
+
+        for (int i = 0; i < MAX_NUMBER_OF_AF_GRIDS; i++) {
+            ia_aiq_af_grid_data* af_grids = &input->af_grids[i];
+            af_grids->base = *base->af_grids[i];
+            MEMCPY_S(af_grids->filter_response_1, sizeof(af_grids->filter_response_1),
+                     af_grids->base.filter_response_1,
+                     af_grids->base.grid_width * af_grids->base.grid_height *
+                         sizeof(*af_grids->base.filter_response_1));
+            MEMCPY_S(af_grids->filter_response_2, sizeof(af_grids->filter_response_2),
+                     af_grids->base.filter_response_2,
+                     af_grids->base.grid_width * af_grids->base.grid_height *
+                         sizeof(*af_grids->base.filter_response_2));
+        }
+    }
+
+    if (base->frame_pa_parameters) {
+        flattenPaResultsV1(*base->frame_pa_parameters, &input->frame_pa_parameters);
+    }
+
+    if (base->faces) {
+        input->faces.base = *base->faces;
+        for (int i = 0; i < base->faces->num_faces; i++) {
+            input->faces.faces[i] = *(base->faces->faces + i);
+        }
+    }
+
+    if (base->awb_results) {
+        input->awb_results = *base->awb_results;
+    }
+
+    if (base->frame_sa_parameters) {
+        flattenSaResultsV2(*base->frame_sa_parameters, &input->frame_sa_parameters);
+    }
+
+    if (base->depth_grids) {
+        CheckError(base->num_depth_grids > MAX_NUMBER_OF_DEPTH_GRIDS, false,
+                   "@%s, num_depth_grids:%d > MAX_NUMBER_OF_DEPTH_GRIDS:%d", __func__,
+                   base->num_depth_grids, MAX_NUMBER_OF_DEPTH_GRIDS);
+
+        for (int i = 0; i < MAX_NUMBER_OF_DEPTH_GRIDS; i++) {
+            ia_aiq_depth_grid_data* depth_grids = &input->depth_grids[i];
+            depth_grids->base = *base->depth_grids[i];
+            MEMCPY_S(depth_grids->grid_rect, sizeof(depth_grids->grid_rect),
+                     depth_grids->base.grid_rect,
+                     depth_grids->base.grid_height * depth_grids->base.grid_width *
+                         sizeof(*depth_grids->base.grid_rect));
+            MEMCPY_S(depth_grids->depth_data, sizeof(depth_grids->depth_data),
+                     depth_grids->base.depth_data,
+                     depth_grids->base.grid_height * depth_grids->base.grid_width *
+                         sizeof(*depth_grids->base.depth_data));
+            MEMCPY_S(depth_grids->confidence, sizeof(depth_grids->confidence),
+                     depth_grids->base.confidence,
+                     depth_grids->base.grid_height * depth_grids->base.grid_width *
+                         sizeof(*depth_grids->base.confidence));
+        }
+    }
+
+    if (base->ir_grid) {
+        input->ir_grid = *base->ir_grid;
+        MEMCPY_S(input->ir_grid_data, MAX_IR_WEIGHT_GRID_DATA_SIZE * sizeof(unsigned short),
+                 base->ir_grid->data,
+                 base->ir_grid->width * base->ir_grid->height * sizeof(unsigned short));
+    }
+
+    return true;
+}
+
+bool IPCIntelAiq::serverUnflattenStatSetV4(set_statistics_set_v4_params* inParams,
+                                           ia_aiq_statistics_input_params_v4** params) {
+    LOGIPC("@%s, inParams:%p, params:%p", __func__, inParams, params);
+    CheckError(!inParams, false, "@%s, inParams is nullptr", __func__);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    set_statistics_params_data* input = &inParams->input;
+    ia_aiq_statistics_input_params_v4* base = &input->base;
+
+    if (base->frame_ae_parameters) {
+        unflattenAeResults(&input->frame_ae_parameters);
+        base->frame_ae_parameters = &input->frame_ae_parameters.base;
+    }
+
+    if (base->frame_af_parameters) {
+        base->frame_af_parameters = &input->frame_af_parameters;
+    }
+
+    if (base->rgbs_grids) {
+        CheckError(base->num_rgbs_grids > MAX_NUMBER_OF_GRIDS, false,
+                   "@%s, num_rgbs_grids:%d > MAX_NUMBER_OF_GRIDS:%d", __func__,
+                   base->num_rgbs_grids, MAX_NUMBER_OF_GRIDS);
+
+        for (int i = 0; i < base->num_rgbs_grids; i++) {
+            ia_aiq_rgbs_grid_data* rgbs_grids = &input->rgbs_grids[i];
+            rgbs_grids->base.blocks_ptr = rgbs_grids->blocks_ptr;
+
+            input->rgbs_grids_array[i] = &rgbs_grids->base;
+        }
+        base->rgbs_grids = (input->rgbs_grids_array);
+    }
+
+    if (base->hdr_rgbs_grid) {
+        input->hdr_rgbs_grid.base.blocks_ptr = input->hdr_rgbs_grid.blocks_ptr;
+        base->hdr_rgbs_grid = &input->hdr_rgbs_grid.base;
+    }
+
+    if (base->af_grids) {
+        CheckError((base->num_af_grids > MAX_NUMBER_OF_AF_GRIDS), false,
+                   "@%s, num_af_grids:%d > MAX_NUMBER_OF_AF_GRIDS:%d", __func__, base->num_af_grids,
+                   MAX_NUMBER_OF_AF_GRIDS);
+
+        for (int i = 0; i < base->num_af_grids; i++) {
+            ia_aiq_af_grid_data* af_grids = &input->af_grids[i];
+            af_grids->base.filter_response_1 = af_grids->filter_response_1;
+            af_grids->base.filter_response_2 = af_grids->filter_response_2;
+
+            input->af_grids_array[i] = &af_grids->base;
+        }
+        base->af_grids = input->af_grids_array;
+    }
+
+    if (base->frame_pa_parameters) {
+        unflattenPaResultsV1(&input->frame_pa_parameters);
+        base->frame_pa_parameters = &input->frame_pa_parameters.base;
+    }
+
+    if (base->faces) {
+        input->faces.base.faces = input->faces.faces;
+        base->faces = &input->faces.base;
+    }
+
+    if (base->awb_results) {
+        base->awb_results = &input->awb_results;
+    }
+
+    if (base->frame_sa_parameters) {
+        unflattenSaResultsV2(&input->frame_sa_parameters);
+        base->frame_sa_parameters = &input->frame_sa_parameters.base;
+    }
+
+    if (base->depth_grids) {
+        CheckError((base->num_depth_grids > MAX_NUMBER_OF_DEPTH_GRIDS), false,
+                   "@%s, num_depth_grids:%d > MAX_NUMBER_OF_DEPTH_GRIDS:%d", __func__,
+                   base->num_depth_grids, MAX_NUMBER_OF_DEPTH_GRIDS);
+
+        for (int i = 0; i < base->num_depth_grids; i++) {
+            ia_aiq_depth_grid_data* depth_grids = &input->depth_grids[i];
+            depth_grids->base.grid_rect = depth_grids->grid_rect;
+            depth_grids->base.depth_data = depth_grids->depth_data;
+            depth_grids->base.confidence = depth_grids->confidence;
+
+            input->depth_grids_array[i] = &depth_grids->base;
+        }
+        base->depth_grids = input->depth_grids_array;
+    }
+
+    if (base->ir_grid) {
+        input->ir_grid.data = input->ir_grid_data;
+        base->ir_grid = &input->ir_grid;
+    }
+
+    *params = base;
+
+    return true;
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelAiq.h b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelAiq.h
new file mode 100644
index 000000000000..4b911b7b4b00
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelAiq.h
@@ -0,0 +1,351 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_aiq.h>
+
+#include "FaceBase.h"
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+struct aiq_init_params {
+    unsigned int aiqb_size;
+    unsigned int nvm_size;
+    unsigned int aiqd_size;
+    unsigned int stats_max_width;
+    unsigned int stats_max_height;
+    unsigned int max_num_stats_in;
+    uintptr_t ia_mkn;
+    uintptr_t cmcRemoteHandle;
+    uintptr_t results;
+};
+
+struct aiq_deinit_params {
+    uintptr_t aiq_handle;
+};
+
+struct af_run_params {
+    uintptr_t aiq_handle;
+
+    ia_aiq_af_input_params base;
+    ia_rectangle focus_rect;
+    ia_aiq_manual_focus_parameters manual_focus_parameters;
+
+    ia_aiq_af_results results;
+};
+
+#define MAX_NUM_GAMMA_LUTS 2048
+#define MAX_NUM_TOME_MAP_LUTS 2048
+struct gbce_results_params {
+    ia_aiq_gbce_results base;
+
+    float r_gamma_lut[MAX_NUM_GAMMA_LUTS];
+    float b_gamma_lut[MAX_NUM_GAMMA_LUTS];
+    float g_gamma_lut[MAX_NUM_GAMMA_LUTS];
+    float tone_map_lut[MAX_NUM_TOME_MAP_LUTS];
+};
+struct gbce_run_params {
+    uintptr_t aiq_handle;
+
+    ia_aiq_gbce_input_params base;
+
+    gbce_results_params res;
+};
+
+#define MAX_NUM_EXPOSURES 3
+#define MAX_NUM_FLASHES 1
+#define MAX_NUM_OF_EXPOSURE_PLANS 4
+#define MAX_SIZE_WEIGHT_GRID (128 * 128)
+struct ae_run_params_results {
+    ia_aiq_ae_results base;
+
+    ia_aiq_ae_exposure_result exposures[MAX_NUM_EXPOSURES];
+    ia_aiq_hist_weight_grid weight_grid;
+    ia_aiq_flash_parameters flashes[MAX_NUM_FLASHES];
+    ia_aiq_aperture_control aperture_control;
+
+    // the below is in ia_aiq_ae_exposure_result exposures[MAX_NUM_EXPOSURES];
+    ia_aiq_exposure_parameters exposure[MAX_NUM_EXPOSURES];
+    ia_aiq_exposure_sensor_parameters sensor_exposure[MAX_NUM_EXPOSURES];
+    unsigned int exposure_plan_ids[MAX_NUM_EXPOSURES][MAX_NUM_OF_EXPOSURE_PLANS];
+
+    // the below is in ia_aiq_hist_weight_grid weight_grid;
+    unsigned char weights[MAX_SIZE_WEIGHT_GRID];
+};
+
+struct ae_run_params {
+    uintptr_t aiq_handle;
+
+    ia_aiq_ae_input_params base;
+    ia_aiq_exposure_sensor_descriptor sensor_descriptor;
+    ia_rectangle exposure_window;
+    ia_coordinate exposure_coordinate;
+    long manual_exposure_time_us;
+    float manual_analog_gain;
+    short manual_iso;
+    ia_aiq_ae_features aec_features;
+    ia_aiq_ae_manual_limits manual_limits;
+
+    ae_run_params_results res;
+};
+
+struct awb_run_params {
+    uintptr_t aiq_handle;
+
+    ia_aiq_awb_input_params base;
+    ia_aiq_awb_manual_cct_range manual_cct_range;
+    ia_coordinate manual_white_coordinate;
+
+    ia_aiq_awb_results results;
+};
+
+#define MAX_NUM_LUTS 128
+#define MAX_SECTOR_COUNT 128
+#define MAX_IR_WIDTH 128
+#define MAX_IR_HEIGHT 128
+#define MAX_NUM_IR_BLOCKS (MAX_IR_WIDTH * MAX_IR_HEIGHT)
+#define MAX_NUM_IR_MODES 5
+struct pa_run_params_results_v1 {
+    ia_aiq_pa_results_v1 base;
+
+    ia_aiq_advanced_ccm_t preferred_acm;
+    ia_aiq_ir_weight_t ir_weight;
+    ia_aiq_rgbir_t rgbir;
+
+    // for ia_aiq_color_channels_lut linearization
+    float gr[MAX_NUM_LUTS];
+    float r[MAX_NUM_LUTS];
+    float b[MAX_NUM_LUTS];
+    float gb[MAX_NUM_LUTS];
+
+    // for ia_aiq_advanced_ccm_t *preferred_acm
+    unsigned int hue_of_sectors[MAX_SECTOR_COUNT];
+    float advanced_color_conversion_matrices[MAX_SECTOR_COUNT][3][3];
+
+    // for ia_aiq_ir_weight_t *ir_weight
+    uint16_t ir_weight_grid_R[MAX_NUM_IR_BLOCKS];
+    uint16_t ir_weight_grid_G[MAX_NUM_IR_BLOCKS];
+    uint16_t ir_weight_grid_B[MAX_NUM_IR_BLOCKS];
+
+    // for ia_aiq_rgbir_t *rgbir
+    ia_aiq_rgbir_model_t models[MAX_NUM_IR_MODES];
+};
+
+struct ia_atbx_face_state_data {
+    ia_atbx_face_state base;
+
+    ia_atbx_face faces[MAX_FACES_DETECTABLE];
+};
+
+struct pa_run_v1_params {
+    uintptr_t aiq_handle;
+
+    ia_aiq_pa_input_params base;
+    ia_aiq_awb_results awb_results;
+    ia_aiq_exposure_parameters exposure_params;
+    ia_aiq_color_channels color_gains;
+
+    pa_run_params_results_v1 res;
+};
+
+#define LSC_MAX_BAYER_ORDER_NUM 4
+#define LSC_TABLE_MAX_WIDTH 100
+#define LSC_TABLE_MAX_HEIGHT 100
+#define LSC_TABLE_MAX_SIZE (LSC_TABLE_MAX_WIDTH * LSC_TABLE_MAX_HEIGHT)
+struct lsc_grid_content {
+    uint16_t content[LSC_TABLE_MAX_SIZE];
+};
+struct sa_run_v2_params_results {
+    ia_aiq_sa_results_v1 base;
+
+    lsc_grid_content lsc_grid[LSC_MAX_BAYER_ORDER_NUM][LSC_MAX_BAYER_ORDER_NUM];
+};
+
+struct sa_run_v2_params {
+    uintptr_t aiq_handle;
+
+    ia_aiq_sa_input_params_v1 base;
+    ia_aiq_frame_params sensor_frame_params;
+    ia_aiq_awb_results awb_results;
+
+    sa_run_v2_params_results res;
+};
+
+#define MAX_IA_BINARY_DATA_PARAMS_SIZE 500000
+struct ia_binary_data_params {
+    uintptr_t aiq_handle;
+    uint8_t data[MAX_IA_BINARY_DATA_PARAMS_SIZE];
+    unsigned int size;
+};
+
+#define MAX_IA_AIQ_VERSION_PARAMS_DATA_SIZE 100
+struct ia_aiq_version_params {
+    uintptr_t aiq_handle;
+    char data[MAX_IA_AIQ_VERSION_PARAMS_DATA_SIZE];
+    unsigned int size;
+};
+
+#define MAX_WIDTH 96
+#define MAX_HEIGHT 72
+#define MAX_NUM_BLOCKS (MAX_WIDTH * MAX_HEIGHT)
+struct ia_aiq_rgbs_grid_data {
+    ia_aiq_rgbs_grid base;
+
+    rgbs_grid_block blocks_ptr[MAX_NUM_BLOCKS];
+};
+
+struct ia_aiq_hdr_rgbs_grid_data {
+    ia_aiq_hdr_rgbs_grid base;
+
+    hdr_rgbs_grid_block blocks_ptr[MAX_NUM_BLOCKS];
+};
+
+#define MAX_AF_GRID_WIDTH 96
+#define MAX_AF_GRID_HEIGHT 72
+#define MAX_AF_GRID_SIZE (MAX_AF_GRID_HEIGHT * MAX_AF_GRID_WIDTH)
+struct ia_aiq_af_grid_data {
+    ia_aiq_af_grid base;
+
+    int filter_response_1[MAX_AF_GRID_SIZE];
+    int filter_response_2[MAX_AF_GRID_SIZE];
+};
+
+#define MAX_DEPTH_GRID_WIDHT 128
+#define MAX_DEPTH_GRID_HEIGHT 128
+#define MAX_DEPTH_GRID_SIZE (MAX_DEPTH_GRID_WIDHT * MAX_DEPTH_GRID_HEIGHT)
+struct ia_aiq_depth_grid_data {
+    ia_aiq_depth_grid base;
+
+    ia_rectangle grid_rect[MAX_DEPTH_GRID_SIZE];
+    int depth_data[MAX_DEPTH_GRID_SIZE];
+    unsigned char confidence[MAX_DEPTH_GRID_SIZE];
+};
+
+#define MAX_NUMBER_OF_GRIDS 1
+#define MAX_NUMBER_OF_AF_GRIDS 1
+#define MAX_NUMBER_OF_HISTROGRAMS 1
+#define MAX_NUMBER_OF_DEPTH_GRIDS 1
+#define MAX_IR_WEIGHT_GRID_DATA_SIZE 480
+struct set_statistics_params_data {
+    ia_aiq_statistics_input_params_v4 base;
+
+    ae_run_params_results frame_ae_parameters;
+
+    ia_aiq_af_results frame_af_parameters;
+
+    const ia_aiq_rgbs_grid* rgbs_grids_array[MAX_NUMBER_OF_GRIDS];
+    ia_aiq_rgbs_grid_data rgbs_grids[MAX_NUMBER_OF_GRIDS];
+
+    ia_aiq_hdr_rgbs_grid_data hdr_rgbs_grid;
+
+    const ia_aiq_af_grid* af_grids_array[MAX_NUMBER_OF_AF_GRIDS];
+    ia_aiq_af_grid_data af_grids[MAX_NUMBER_OF_AF_GRIDS];
+
+    pa_run_params_results_v1 frame_pa_parameters;
+
+    ia_atbx_face_state_data faces;
+
+    ia_aiq_awb_results awb_results;
+
+    sa_run_v2_params_results frame_sa_parameters;
+
+    const ia_aiq_depth_grid* depth_grids_array[MAX_NUMBER_OF_DEPTH_GRIDS];
+    ia_aiq_depth_grid_data depth_grids[MAX_NUMBER_OF_DEPTH_GRIDS];
+
+    ia_aiq_grid ir_grid;
+    unsigned short ir_grid_data[MAX_IR_WEIGHT_GRID_DATA_SIZE];
+};
+
+struct set_statistics_set_v4_params {
+    uintptr_t ia_aiq;
+    set_statistics_params_data input;
+};
+
+class IPCIntelAiq {
+ public:
+    IPCIntelAiq();
+    virtual ~IPCIntelAiq();
+
+    // for init
+    bool clientFlattenInit(const ia_binary_data* aiqbData, const ia_binary_data* nvmData,
+                           const ia_binary_data* aiqdData, unsigned int statsMaxWidth,
+                           unsigned int statsMaxHeight, unsigned int maxNumStatsIn, uintptr_t cmc,
+                           uintptr_t mkn, uint8_t* pData, unsigned int size);
+    bool serverUnflattenInit(const void* pData, int dataSize, ia_binary_data* aiqbData,
+                             ia_binary_data* nvmData, ia_binary_data* aiqdData);
+
+    // for ae
+    bool clientFlattenAe(uintptr_t aiq, const ia_aiq_ae_input_params& inParams,
+                         ae_run_params* params);
+    bool clientUnflattenAe(ae_run_params* params, ia_aiq_ae_results** results);
+    bool serverUnflattenAe(ae_run_params* inParams, ia_aiq_ae_input_params** params);
+    bool serverFlattenAe(const ia_aiq_ae_results& aeResults, ae_run_params* params);
+
+    bool flattenAeResults(const ia_aiq_ae_results& aeResults, ae_run_params_results* res);
+    bool unflattenAeResults(ae_run_params_results* res);
+
+    // for af
+    bool clientFlattenAf(uintptr_t aiq, const ia_aiq_af_input_params& inParams,
+                         af_run_params* params);
+    bool clientUnflattenAf(const af_run_params& params, ia_aiq_af_results** results);
+    bool serverUnflattenAf(af_run_params* inParams, ia_aiq_af_input_params** params);
+    bool serverFlattenAf(const ia_aiq_af_results& afResults, af_run_params* params);
+
+    // for awb
+    bool clientFlattenAwb(uintptr_t aiq, const ia_aiq_awb_input_params& inParams,
+                          awb_run_params* params);
+    bool clientUnflattenAwb(const awb_run_params& inParams, ia_aiq_awb_results** results);
+    bool serverUnflattenAwb(awb_run_params* inParams, ia_aiq_awb_input_params** params);
+    bool serverFlattenAwb(const ia_aiq_awb_results& awbResults, awb_run_params* params);
+
+    // for gbce
+    bool clientFlattenGbce(uintptr_t aiq, const ia_aiq_gbce_input_params& inParams,
+                           gbce_run_params* params);
+    bool clientUnflattenGbce(gbce_run_params* params, ia_aiq_gbce_results** results);
+    bool serverFlattenGbce(const ia_aiq_gbce_results& gbceResults, gbce_run_params* params);
+
+    bool flattenGbceResults(const ia_aiq_gbce_results& gbceResults, gbce_results_params* res);
+    bool unflattenGbceResults(gbce_results_params* res);
+
+    // for pa
+    bool clientFlattenPaV1(uintptr_t aiq, const ia_aiq_pa_input_params& inParams,
+                           pa_run_v1_params* params);
+    bool clientUnflattenPaV1(pa_run_v1_params* params, ia_aiq_pa_results_v1** results);
+    bool serverUnflattenPaV1(pa_run_v1_params* inParams, ia_aiq_pa_input_params** params);
+    bool serverFlattenPaV1(const ia_aiq_pa_results_v1& paResults, pa_run_v1_params* params);
+
+    bool flattenPaResultsV1(const ia_aiq_pa_results_v1& paResults, pa_run_params_results_v1* res);
+    bool unflattenPaResultsV1(pa_run_params_results_v1* res);
+
+    // for sa
+    bool clientFlattenSaV2(uintptr_t aiq, const ia_aiq_sa_input_params_v1& inParams,
+                           sa_run_v2_params* params);
+    bool clientUnflattenSaV2(sa_run_v2_params* params, ia_aiq_sa_results_v1** results);
+    bool serverUnflattenSaV2(const sa_run_v2_params& inParams, ia_aiq_sa_input_params_v1** params);
+    bool serverFlattenSaV2(const ia_aiq_sa_results_v1& saResults, sa_run_v2_params* params);
+
+    bool flattenSaResultsV2(const ia_aiq_sa_results_v1& saResults, sa_run_v2_params_results* res);
+    bool unflattenSaResultsV2(sa_run_v2_params_results* res);
+
+    // for statistics
+    bool clientFlattenStatSetV4(uintptr_t aiq, const ia_aiq_statistics_input_params_v4& inParams,
+                                set_statistics_set_v4_params* params);
+    bool serverUnflattenStatSetV4(set_statistics_set_v4_params* inParams,
+                                  ia_aiq_statistics_input_params_v4** params);
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelCmc.cpp b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelCmc.cpp
new file mode 100644
index 000000000000..ed0995a18a2b
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelCmc.cpp
@@ -0,0 +1,258 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IPC_INTEL_CMC"
+
+#include "modules/sandboxing/IPCIntelCmc.h"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IPCIntelCmc::IPCIntelCmc() {
+    LOGIPC("@%s", __func__);
+}
+
+IPCIntelCmc::~IPCIntelCmc() {
+    LOGIPC("@%s", __func__);
+}
+
+bool IPCIntelCmc::clientFlattenInit(const ia_binary_data& aiqb, cmc_init_params* params) {
+    LOGIPC("@%s, aiqb: data:%p, size:%d, params:%p", __func__, aiqb.data, aiqb.size, params);
+
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+    CheckError(!aiqb.data, false, "@%s, aiqb.data is nullptr", __func__);
+    CheckError(aiqb.size == 0, false, "@%s, aiqb.size is 0", __func__);
+    CheckError(aiqb.size > sizeof(params->input.data), false, "@%s, aiqb:%d is too big", __func__,
+               aiqb.size);
+
+    ia_binary_data_mod* input = &params->input;
+    MEMCPY_S(input->data, sizeof(input->data), aiqb.data, aiqb.size);
+    input->size = aiqb.size;
+
+    return true;
+}
+
+bool IPCIntelCmc::clientUnflattenInit(const cmc_init_params& params, ia_cmc_t** cmc,
+                                      uintptr_t* cmcRemoteHandle) {
+    LOGIPC("@%s, cmc:%p", __func__, cmc);
+    CheckError(!cmc, false, "@%s, cmc is nullptr", __func__);
+
+    int ret = unflattenCmcData(const_cast<ia_cmc_data*>(&params.results));
+    CheckError(ret == false, false, "%s, unflattenCmcData fails", __func__);
+
+    *cmc = const_cast<ia_cmc_t*>(&params.results.base);
+    *cmcRemoteHandle = params.results.cmcRemoteHandle;
+
+    return true;
+}
+
+bool IPCIntelCmc::serverUnflattenInit(const cmc_init_params& params, ia_binary_data* aiqb) {
+    LOGIPC("@%s, aiqb:%p", __func__, aiqb);
+    CheckError(aiqb == nullptr, false, "@%s, aiqb is nullptr", __func__);
+
+    ia_binary_data_mod* input = const_cast<ia_binary_data_mod*>(&params.input);
+    aiqb->data = input->data;
+    aiqb->size = input->size;
+
+    return true;
+}
+
+bool IPCIntelCmc::serverFlattenInit(const ia_cmc_t& cmc, cmc_init_params* params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    int ret = flattenCmcData(&cmc, &params->results);
+    CheckError(ret == false, false, "%s flattenCmcData fails", __func__);
+
+    return true;
+}
+
+bool IPCIntelCmc::flattenCmcData(const ia_cmc_t* cmc, ia_cmc_data* results) {
+    CheckError(!cmc || !results, false, "%s, cmc or results is nullptr", __func__);
+
+    results->base = *cmc;
+    results->cmcRemoteHandle = reinterpret_cast<uintptr_t>(cmc);
+
+    ia_cmc_t* base = &results->base;
+    if (base->cmc_general_data) {
+        results->cmc_general_data = *base->cmc_general_data;
+    }
+
+    cmc_parsed_black_level_t* baseBL = &base->cmc_parsed_black_level;
+    cmc_parsed_black_level_data* retBL = &results->cmc_parsed_black_level;
+    if (baseBL->cmc_black_level) {
+        retBL->cmc_black_level = *baseBL->cmc_black_level;
+    }
+    if (baseBL->cmc_black_level_luts) {
+        retBL->cmc_black_level_luts = *baseBL->cmc_black_level_luts;
+    }
+
+    if (base->cmc_saturation_level) {
+        results->cmc_saturation_level = *base->cmc_saturation_level;
+    }
+
+    if (base->cmc_sensitivity) {
+        results->cmc_sensitivity = *base->cmc_sensitivity;
+    }
+
+    cmc_parsed_lens_shading_t* baseLS = &base->cmc_parsed_lens_shading;
+    cmc_parsed_lens_shading_data* retLS = &results->cmc_parsed_lens_shading;
+    if (baseLS->cmc_lens_shading) {
+        retLS->cmc_lens_shading = *baseLS->cmc_lens_shading;
+    }
+    if (baseLS->cmc_lsc_grids) {
+        retLS->cmc_lsc_grids = *baseLS->cmc_lsc_grids;
+    }
+    if (baseLS->lsc_grids) {
+        retLS->lsc_grids = *baseLS->lsc_grids;
+    }
+    if (baseLS->cmc_lsc_rg_bg_ratios) {
+        retLS->cmc_lsc_rg_bg_ratios = *baseLS->cmc_lsc_rg_bg_ratios;
+    }
+
+    cmc_parsed_optics_t* baseOptics = &base->cmc_parsed_optics;
+    cmc_parsed_optics_data* retOptics = &results->cmc_parsed_optics;
+    if (baseOptics->cmc_optomechanics) {
+        retOptics->cmc_optomechanics = *baseOptics->cmc_optomechanics;
+    }
+    if (baseOptics->lut_apertures) {
+        retOptics->lut_apertures = *baseOptics->lut_apertures;
+    }
+
+    cmc_parsed_color_matrices_t* baseCM = &base->cmc_parsed_color_matrices;
+    cmc_parsed_color_matrices_data* retCM = &results->cmc_parsed_color_matrices;
+    if (baseCM->cmc_color_matrices) {
+        retCM->cmc_color_matrices = *baseCM->cmc_color_matrices;
+    }
+    if (baseCM->cmc_color_matrix) {
+        // fix asan issue:base->cmc_parsed_color_matrices.cmc_color_matrix is not 4 aligned
+        // use memcpy instead of *
+        MEMCPY_S(&retCM->cmc_color_matrix, sizeof(retCM->cmc_color_matrix),
+                 baseCM->cmc_color_matrix, sizeof(cmc_color_matrix_t));
+    }
+    if (baseCM->ccm_estimate_method) {
+        retCM->ccm_estimate_method = *baseCM->ccm_estimate_method;
+    }
+
+    cmc_parsed_analog_gain_conversion_t* baseAGC = &base->cmc_parsed_analog_gain_conversion;
+    cmc_parsed_analog_gain_conversion_data* retAGC = &results->cmc_parsed_analog_gain_conversion;
+    if (baseAGC->cmc_analog_gain_conversion) {
+        retAGC->cmc_analog_gain_conversion = *baseAGC->cmc_analog_gain_conversion;
+
+        CheckError(baseAGC->cmc_analog_gain_conversion->num_segments > MAX_NUM_SEGMENTS, false,
+                   "@%s, num_segments:%d is too big", __func__,
+                   baseAGC->cmc_analog_gain_conversion->num_segments);
+        CheckError(baseAGC->cmc_analog_gain_conversion->num_pairs > MAX_NUM_ANALOG_PAIRS, false,
+                   "@%s, num_pairs:%d is too big", __func__,
+                   baseAGC->cmc_analog_gain_conversion->num_pairs);
+
+        if (baseAGC->cmc_analog_gain_segments) {
+            MEMCPY_S(retAGC->cmc_analog_gain_segments, sizeof(retAGC->cmc_analog_gain_segments),
+                     baseAGC->cmc_analog_gain_segments,
+                     (sizeof(*baseAGC->cmc_analog_gain_segments) *
+                      baseAGC->cmc_analog_gain_conversion->num_segments));
+        }
+        if (baseAGC->cmc_analog_gain_pairs) {
+            MEMCPY_S(retAGC->cmc_analog_gain_pairs, sizeof(retAGC->cmc_analog_gain_pairs),
+                     baseAGC->cmc_analog_gain_pairs,
+                     (sizeof(*baseAGC->cmc_analog_gain_pairs) *
+                      baseAGC->cmc_analog_gain_conversion->num_pairs));
+        }
+    }
+
+    return true;
+}
+
+bool IPCIntelCmc::unflattenCmcData(ia_cmc_data* results) {
+    LOGIPC("@%s", __func__);
+    CheckError(!results, false, "%s, results is nullptr", __func__);
+
+    ia_cmc_t* base = &results->base;
+
+    if (base->cmc_general_data) {
+        base->cmc_general_data = &results->cmc_general_data;
+    }
+
+    cmc_parsed_black_level_t* baseBL = &base->cmc_parsed_black_level;
+    cmc_parsed_black_level_data* retBL = &results->cmc_parsed_black_level;
+    if (baseBL->cmc_black_level) {
+        baseBL->cmc_black_level = &retBL->cmc_black_level;
+    }
+    if (baseBL->cmc_black_level_luts) {
+        baseBL->cmc_black_level_luts = &retBL->cmc_black_level_luts;
+    }
+
+    if (base->cmc_saturation_level) {
+        base->cmc_saturation_level = &results->cmc_saturation_level;
+    }
+
+    if (base->cmc_sensitivity) {
+        base->cmc_sensitivity = &results->cmc_sensitivity;
+    }
+
+    cmc_parsed_lens_shading_t* baseLS = &base->cmc_parsed_lens_shading;
+    cmc_parsed_lens_shading_data* retLS = &results->cmc_parsed_lens_shading;
+    if (baseLS->cmc_lens_shading) {
+        baseLS->cmc_lens_shading = &retLS->cmc_lens_shading;
+    }
+    if (baseLS->cmc_lsc_grids) {
+        baseLS->cmc_lsc_grids = &retLS->cmc_lsc_grids;
+    }
+    if (baseLS->lsc_grids) {
+        baseLS->lsc_grids = &retLS->lsc_grids;
+    }
+    if (baseLS->cmc_lsc_rg_bg_ratios) {
+        baseLS->cmc_lsc_rg_bg_ratios = &retLS->cmc_lsc_rg_bg_ratios;
+    }
+
+    cmc_parsed_optics_t* baseOptics = &base->cmc_parsed_optics;
+    cmc_parsed_optics_data* retOptics = &results->cmc_parsed_optics;
+    if (baseOptics->cmc_optomechanics) {
+        baseOptics->cmc_optomechanics = &retOptics->cmc_optomechanics;
+    }
+    if (baseOptics->lut_apertures) {
+        baseOptics->lut_apertures = &retOptics->lut_apertures;
+    }
+
+    cmc_parsed_color_matrices_t* baseCM = &base->cmc_parsed_color_matrices;
+    cmc_parsed_color_matrices_data* retCM = &results->cmc_parsed_color_matrices;
+    if (baseCM->cmc_color_matrices) {
+        baseCM->cmc_color_matrices = &retCM->cmc_color_matrices;
+    }
+    if (baseCM->cmc_color_matrix) {
+        baseCM->cmc_color_matrix = &retCM->cmc_color_matrix;
+    }
+    if (baseCM->ccm_estimate_method) {
+        baseCM->ccm_estimate_method = &retCM->ccm_estimate_method;
+    }
+
+    cmc_parsed_analog_gain_conversion_t* baseAGC = &base->cmc_parsed_analog_gain_conversion;
+    cmc_parsed_analog_gain_conversion_data* retAGC = &results->cmc_parsed_analog_gain_conversion;
+    if (baseAGC->cmc_analog_gain_conversion) {
+        baseAGC->cmc_analog_gain_conversion = &retAGC->cmc_analog_gain_conversion;
+
+        if (baseAGC->cmc_analog_gain_segments) {
+            baseAGC->cmc_analog_gain_segments = retAGC->cmc_analog_gain_segments;
+        }
+        if (baseAGC->cmc_analog_gain_pairs) {
+            baseAGC->cmc_analog_gain_pairs = retAGC->cmc_analog_gain_pairs;
+        }
+    }
+
+    return true;
+}
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelCmc.h b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelCmc.h
new file mode 100644
index 000000000000..4c296172f095
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelCmc.h
@@ -0,0 +1,95 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_cmc_types.h>
+#include <ia_types.h>
+
+#include "modules/sandboxing/IPCCommon.h"
+
+namespace icamera {
+typedef struct {
+    cmc_black_level_t cmc_black_level;
+    cmc_black_level_lut_t cmc_black_level_luts;
+} cmc_parsed_black_level_data;
+
+typedef struct {
+    cmc_lens_shading_t cmc_lens_shading;
+    cmc_lsc_grid_t cmc_lsc_grids;
+    uint16_t lsc_grids;
+    chromaticity_t cmc_lsc_rg_bg_ratios;
+} cmc_parsed_lens_shading_data;
+
+typedef struct {
+    cmc_optomechanics_t cmc_optomechanics;
+    uint16_t lut_apertures;
+} cmc_parsed_optics_data;
+
+typedef struct {
+    cmc_color_matrices_t cmc_color_matrices;
+    cmc_color_matrix_t cmc_color_matrix;
+    uint16_t ccm_estimate_method;
+} cmc_parsed_color_matrices_data;
+
+#define MAX_NUM_SEGMENTS 512
+#define MAX_NUM_ANALOG_PAIRS 1024
+typedef struct {
+    cmc_analog_gain_conversion_t cmc_analog_gain_conversion;
+    cmc_analog_gain_segment_t cmc_analog_gain_segments[MAX_NUM_SEGMENTS];
+    cmc_analog_gain_pair_t cmc_analog_gain_pairs[MAX_NUM_ANALOG_PAIRS];
+} cmc_parsed_analog_gain_conversion_data;
+
+struct ia_cmc_data {
+    ia_cmc_t base;
+
+    cmc_general_data_t cmc_general_data;
+    cmc_parsed_black_level_data cmc_parsed_black_level;
+    cmc_saturation_level_t cmc_saturation_level;
+    cmc_sensitivity_t cmc_sensitivity;
+    cmc_parsed_lens_shading_data cmc_parsed_lens_shading;
+    cmc_parsed_optics_data cmc_parsed_optics;
+    cmc_parsed_color_matrices_data cmc_parsed_color_matrices;
+    cmc_parsed_analog_gain_conversion_data cmc_parsed_analog_gain_conversion;
+
+    uintptr_t cmcRemoteHandle;  // it stores the remote cmc pointer.
+};
+
+struct cmc_init_params {
+    ia_binary_data_mod input;
+    ia_cmc_data results;
+};
+
+struct cmc_deinit_params {
+    uintptr_t cmc_handle;
+};
+
+class IPCIntelCmc {
+ public:
+    IPCIntelCmc();
+    virtual ~IPCIntelCmc();
+
+    // for init
+    bool clientFlattenInit(const ia_binary_data& aiqb, cmc_init_params* params);
+    bool clientUnflattenInit(const cmc_init_params& params, ia_cmc_t** cmc,
+                             uintptr_t* cmcRemoteHandle);
+    bool serverFlattenInit(const ia_cmc_t& cmc, cmc_init_params* params);
+    bool serverUnflattenInit(const cmc_init_params& pData, ia_binary_data* aiqb);
+
+    bool flattenCmcData(const ia_cmc_t* cmc, ia_cmc_data* results);
+    bool unflattenCmcData(ia_cmc_data* results);
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelDvs.cpp b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelDvs.cpp
new file mode 100644
index 000000000000..8ac131e8e42d
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelDvs.cpp
@@ -0,0 +1,704 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IPC_INTEL_DVS"
+
+#include "modules/sandboxing/IPCIntelDvs.h"
+
+#include <ia_types.h>
+
+#include "CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IPCIntelDvs::IPCIntelDvs() {
+    LOGIPC("@%s", __func__);
+}
+
+IPCIntelDvs::~IPCIntelDvs() {
+    LOGIPC("@%s", __func__);
+}
+
+bool IPCIntelDvs::clientFlattenInit(void* pData, unsigned int size,
+                                    const ia_binary_data* dvsDataPtr, const ia_cmc_t* cmc) {
+    LOGIPC("@%s, pData:%p, size:%d, dvsDataPtr:%p, cmc:%p", __func__, pData, size, dvsDataPtr, cmc);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(!dvsDataPtr, false, "@%s, dvsDataPtr is nullptr", __func__);
+    CheckError(!cmc, false, "@%s, cmc is nullptr", __func__);
+    CheckError(size < sizeof(DvsInitParams), false, "@%s, buffer is small", __func__);
+    CheckError(dvsDataPtr->size > MAX_DVS_DATA_SIZE, false, "@%s, data:%d is too small", __func__,
+               dvsDataPtr->size);
+
+    DvsInitParams* params = static_cast<DvsInitParams*>(pData);
+    params->base.size = dvsDataPtr->size;
+    params->base.data = params->data;
+    MEMCPY_S(params->data, MAX_DVS_DATA_SIZE, dvsDataPtr->data, dvsDataPtr->size);
+
+    params->cmcHandle = reinterpret_cast<uintptr_t>(cmc);
+    return true;
+}
+
+bool IPCIntelDvs::serverUnflattenInit(void* pData, int size, ia_binary_data** dvsDataPtr,
+                                      ia_cmc_t** cmc) {
+    LOGIPC("@%s, pData:%p, size:%d, dvsDataPtr:%p, cmc:%p", __func__, pData, size, dvsDataPtr, cmc);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsInitParams), false, "@%s, buffer is small", __func__);
+    CheckError(!dvsDataPtr, false, "@%s, dvsDataPtr is nullptr", __func__);
+    CheckError(!cmc, false, "@%s, cmc is nullptr", __func__);
+
+    DvsInitParams* params = reinterpret_cast<DvsInitParams*>(pData);
+    params->base.data = params->data;
+
+    *dvsDataPtr = &params->base;
+    *cmc = reinterpret_cast<ia_cmc_t*>(params->cmcHandle);
+    return true;
+}
+
+bool IPCIntelDvs::serverFlattenInit(void* pData, unsigned int size, ia_dvs_state* dvs) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p", __func__, pData, size, dvs);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsInitParams), false, "@%s, buffer is small", __func__);
+
+    DvsInitParams* params = static_cast<DvsInitParams*>(pData);
+    params->dvsHandle = reinterpret_cast<uintptr_t>(dvs);
+    return true;
+}
+
+bool IPCIntelDvs::clientUnflattenInit(const void* pData, unsigned int size, ia_dvs_state** dvs) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p", __func__, pData, size, dvs);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsInitParams), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+
+    const DvsInitParams* params = static_cast<const DvsInitParams*>(pData);
+    *dvs = reinterpret_cast<ia_dvs_state*>(params->dvsHandle);
+
+    return true;
+}
+
+bool IPCIntelDvs::clientFlattenDeinit(void* pData, unsigned int size, ia_dvs_state* dvs) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p", __func__, pData, size, dvs);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsDeinitParams), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+
+    DvsDeinitParams* params = static_cast<DvsDeinitParams*>(pData);
+    params->dvsHandle = reinterpret_cast<uintptr_t>(dvs);
+
+    return true;
+}
+
+bool IPCIntelDvs::serverUnflattenDeinit(const void* pData, unsigned int size, ia_dvs_state** dvs) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p", __func__, pData, size, dvs);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsDeinitParams), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+
+    const DvsDeinitParams* params = static_cast<const DvsDeinitParams*>(pData);
+    *dvs = reinterpret_cast<ia_dvs_state*>(params->dvsHandle);
+
+    return true;
+}
+
+bool IPCIntelDvs::clientFlattenConfig(void* pData, unsigned int size, ia_dvs_state* dvs,
+                                      const ia_dvs_configuration* config, float zoomRatio) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p, config:%p, zoomRatio:%f", __func__, pData, size, dvs,
+           config, zoomRatio);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsConfigParams), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+    CheckError(!config, false, "@%s, config is nullptr", __func__);
+
+    DvsConfigParams* params = static_cast<DvsConfigParams*>(pData);
+    params->dvsHandle = reinterpret_cast<uintptr_t>(dvs);
+    MEMCPY_S(&params->configData, sizeof(ia_dvs_configuration), config,
+             sizeof(ia_dvs_configuration));
+    params->zoomRatio = zoomRatio;
+
+    return true;
+}
+
+bool IPCIntelDvs::serverUnflattenConfig(const void* pData, unsigned int size, ia_dvs_state** dvs,
+                                        ia_dvs_configuration** config, float* zoomRatio) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p, config:%p, zoomRatio:%f", __func__, pData, size, dvs,
+           config, zoomRatio);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsConfigParams), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+    CheckError(!config, false, "@%s, config is nullptr", __func__);
+    CheckError(!zoomRatio, false, "@%s, zoomRatio is nullptr", __func__);
+
+    DvsConfigParams* params =
+        const_cast<DvsConfigParams*>(static_cast<const DvsConfigParams*>(pData));
+    *dvs = reinterpret_cast<ia_dvs_state*>(params->dvsHandle);
+    *config = &params->configData;
+    *zoomRatio = params->zoomRatio;
+
+    return true;
+}
+
+bool IPCIntelDvs::clientFlattenNoneBlanckRation(void* pData, unsigned int size, ia_dvs_state* dvs,
+                                                float nonBlankingRatio) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p, nonBlankingRatio:%f", __func__, pData, size, dvs,
+           nonBlankingRatio);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsNoneBlankRatioParams), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+
+    DvsNoneBlankRatioParams* params = static_cast<DvsNoneBlankRatioParams*>(pData);
+    params->dvsHandle = reinterpret_cast<uintptr_t>(dvs);
+    params->nonBlankingRatio = nonBlankingRatio;
+
+    return true;
+}
+
+bool IPCIntelDvs::serverUnflattenNoneBlanckRation(const void* pData, unsigned int size,
+                                                  ia_dvs_state** dvs, float* nonBlankingRatio) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p, nonBlankingRatio:%f", __func__, pData, size, dvs,
+           nonBlankingRatio);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsNoneBlankRatioParams), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+    CheckError(!nonBlankingRatio, false, "@%s, nonBlankingRatio is nullptr", __func__);
+
+    const DvsNoneBlankRatioParams* params = static_cast<const DvsNoneBlankRatioParams*>(pData);
+    *dvs = reinterpret_cast<ia_dvs_state*>(params->dvsHandle);
+    *nonBlankingRatio = params->nonBlankingRatio;
+
+    return true;
+}
+
+bool IPCIntelDvs::clientFlattenDigitalZoomMode(void* pData, unsigned int size, ia_dvs_state* dvs,
+                                               ia_dvs_zoom_mode zoomMode) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p, zoomMode:%d", __func__, pData, size, dvs, zoomMode);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsDigitalZoomMode), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+
+    DvsDigitalZoomMode* params = static_cast<DvsDigitalZoomMode*>(pData);
+    params->dvsHandle = reinterpret_cast<uintptr_t>(dvs);
+    params->zoomMode = zoomMode;
+
+    return true;
+}
+
+bool IPCIntelDvs::serverUnflattenDigitalZoomMode(const void* pData, unsigned int size,
+                                                 ia_dvs_state** dvs, ia_dvs_zoom_mode* zoomMode) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p, zoomMode:%d", __func__, pData, size, dvs, zoomMode);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsDigitalZoomMode), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+    CheckError(!zoomMode, false, "@%s, zoomMode is nullptr", __func__);
+
+    const DvsDigitalZoomMode* params = static_cast<const DvsDigitalZoomMode*>(pData);
+    *dvs = reinterpret_cast<ia_dvs_state*>(params->dvsHandle);
+    *zoomMode = params->zoomMode;
+
+    return true;
+}
+
+bool IPCIntelDvs::clientFlattenDigitalZoomRegion(void* pData, unsigned int size, ia_dvs_state* dvs,
+                                                 ia_rectangle* zoomRegion) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p, zoomRegion:%p", __func__, pData, size, dvs, zoomRegion);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsDigitalZoomRegion), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+    CheckError(!zoomRegion, false, "@%s, zoomRegion is nullptr", __func__);
+
+    DvsDigitalZoomRegion* params = static_cast<DvsDigitalZoomRegion*>(pData);
+    params->dvsHandle = reinterpret_cast<uintptr_t>(dvs);
+    params->zoomRegion = *zoomRegion;
+
+    return true;
+}
+
+bool IPCIntelDvs::serverUnflattenDigitalZoomRegion(const void* pData, unsigned int size,
+                                                   ia_dvs_state** dvs, ia_rectangle** zoomRegion) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p, zoomRegion:%p", __func__, pData, size, dvs, zoomRegion);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsDigitalZoomRegion), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+    CheckError(!zoomRegion, false, "@%s, zoomRegion is nullptr", __func__);
+
+    DvsDigitalZoomRegion* params =
+        const_cast<DvsDigitalZoomRegion*>(static_cast<const DvsDigitalZoomRegion*>(pData));
+    *dvs = reinterpret_cast<ia_dvs_state*>(params->dvsHandle);
+    *zoomRegion = &params->zoomRegion;
+    return true;
+}
+
+bool IPCIntelDvs::clientFlattenDigitalZoomCoordinate(void* pData, unsigned int size,
+                                                     ia_dvs_state* dvs,
+                                                     ia_coordinate* zoomCoordinate) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p, zoomCoordinate:%p", __func__, pData, size, dvs,
+           zoomCoordinate);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsDigitalZoomCoordinate), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+    CheckError(!zoomCoordinate, false, "@%s, zoomCoordinate is nullptr", __func__);
+
+    DvsDigitalZoomCoordinate* params = static_cast<DvsDigitalZoomCoordinate*>(pData);
+    params->dvsHandle = reinterpret_cast<uintptr_t>(dvs);
+    params->zoomCoordinate = *zoomCoordinate;
+
+    return true;
+}
+
+bool IPCIntelDvs::serverUnflattenDigitalZoomCoordinate(const void* pData, unsigned int size,
+                                                       ia_dvs_state** dvs,
+                                                       ia_coordinate** zoomCoordinate) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p, zoomCoordinate:%p", __func__, pData, size, dvs,
+           zoomCoordinate);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsDigitalZoomCoordinate), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+    CheckError(!zoomCoordinate, false, "@%s, zoomCoordinate is nullptr", __func__);
+
+    DvsDigitalZoomCoordinate* params =
+        const_cast<DvsDigitalZoomCoordinate*>(static_cast<const DvsDigitalZoomCoordinate*>(pData));
+    *dvs = reinterpret_cast<ia_dvs_state*>(params->dvsHandle);
+    *zoomCoordinate = &params->zoomCoordinate;
+
+    return true;
+}
+
+bool IPCIntelDvs::clientFlattenDigitalZoomMagnitude(void* pData, unsigned int size,
+                                                    ia_dvs_state* dvs, float zoomRatio) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p, zoomRatio:%f", __func__, pData, size, dvs, zoomRatio);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsDigitalZoomMagnitude), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+
+    DvsDigitalZoomMagnitude* params = static_cast<DvsDigitalZoomMagnitude*>(pData);
+    params->dvsHandle = reinterpret_cast<uintptr_t>(dvs);
+    params->zoomRatio = zoomRatio;
+
+    return true;
+}
+
+bool IPCIntelDvs::serverUnflattenDigitalZoomMagnitude(const void* pData, unsigned int size,
+                                                      ia_dvs_state** dvs, float* zoomRatio) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p, zoomRatio:%f", __func__, pData, size, dvs, zoomRatio);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsDigitalZoomMagnitude), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+    CheckError(!zoomRatio, false, "@%s, zoomRatio is nullptr", __func__);
+
+    const DvsDigitalZoomMagnitude* params = static_cast<const DvsDigitalZoomMagnitude*>(pData);
+    *dvs = reinterpret_cast<ia_dvs_state*>(params->dvsHandle);
+    *zoomRatio = params->zoomRatio;
+
+    return true;
+}
+
+bool IPCIntelDvs::clientFlattenFreeMorphTable(void* pData, unsigned int size, ia_dvs_state* dvs,
+                                              ia_dvs_morph_table* morphTable) {
+    LOGIPC("@%s, pData:%p, size:%d, morphTable:%p", __func__, pData, size, morphTable);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsFreeMorphParams), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+    CheckError(!morphTable, false, "@%s, morphTable is nullptr", __func__);
+
+    DvsFreeMorphParams* params = static_cast<DvsFreeMorphParams*>(pData);
+    params->dvsHandle = reinterpret_cast<uintptr_t>(dvs);
+    params->morphHandle = reinterpret_cast<uintptr_t>(morphTable);
+
+    return true;
+}
+
+bool IPCIntelDvs::serverUnflattenFreeMorphTable(const void* pData, unsigned int size,
+                                                ia_dvs_state** dvs,
+                                                ia_dvs_morph_table** morphTable) {
+    LOGIPC("@%s, pData:%p, size:%d, morphTable:%p", __func__, pData, size, morphTable);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsFreeMorphParams), false, "@%s, buffer is small", __func__);
+    CheckError(!morphTable, false, "@%s, morphTable is nullptr", __func__);
+
+    const DvsFreeMorphParams* params = static_cast<const DvsFreeMorphParams*>(pData);
+    *dvs = reinterpret_cast<ia_dvs_state*>(params->dvsHandle);
+    *morphTable = reinterpret_cast<ia_dvs_morph_table*>(params->morphHandle);
+
+    return true;
+}
+
+bool IPCIntelDvs::clientFlattenAllocateMorphTable(void* pData, unsigned int size,
+                                                  ia_dvs_state* dvs) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p", __func__, pData, size, dvs);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsMorphParams), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+
+    DvsMorphParams* params = static_cast<DvsMorphParams*>(pData);
+    params->dvsHandle = reinterpret_cast<uintptr_t>(dvs);
+
+    return true;
+}
+
+bool IPCIntelDvs::serverUnflattenAllocateMorphTalbe(const void* pData, unsigned int size,
+                                                    ia_dvs_state** dvs) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p", __func__, pData, size, dvs);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsMorphParams), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+
+    const DvsMorphParams* params = static_cast<const DvsMorphParams*>(pData);
+    *dvs = reinterpret_cast<ia_dvs_state*>(params->dvsHandle);
+
+    return true;
+}
+
+bool IPCIntelDvs::serverFlattenAllocateMorphTalbe(void* pData, unsigned int size,
+                                                  ia_dvs_morph_table* morphTable) {
+    LOGIPC("@%s, pData:%p, size:%d, morphRemote:%p", __func__, pData, size, morphTable);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsMorphParams), false, "@%s, buffer is small", __func__);
+    CheckError(!morphTable, false, "@%s, morphTable is nullptr", __func__);
+
+    DvsMorphParams* params = static_cast<DvsMorphParams*>(pData);
+    params->morphHandle = reinterpret_cast<uintptr_t>(morphTable);
+
+    return true;
+}
+
+bool IPCIntelDvs::clientUnflattenAllocateMorphTalbe(const void* pData, unsigned int size,
+                                                    ia_dvs_morph_table** morphTable) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsMorphParams), false, "@%s, buffer is small", __func__);
+    CheckError(!morphTable, false, "@%s, morphLocal is nullptr", __func__);
+
+    const DvsMorphParams* params = static_cast<const DvsMorphParams*>(pData);
+    *morphTable = reinterpret_cast<ia_dvs_morph_table*>(params->morphHandle);
+    return true;
+}
+
+bool IPCIntelDvs::clientFlattenGetMorphTable(void* pData, unsigned int size, ia_dvs_state* dvs,
+                                             ia_dvs_morph_table* morphTable) {
+    LOGIPC("@%s, pData:%p, size:%d, morphTable:%p", __func__, pData, size, morphTable);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsMorphParams), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+    CheckError(!morphTable, false, "@%s, morphTable is nullptr", __func__);
+
+    DvsMorphParams* params = static_cast<DvsMorphParams*>(pData);
+    params->dvsHandle = reinterpret_cast<uintptr_t>(dvs);
+    params->morphHandle = reinterpret_cast<uintptr_t>(morphTable);
+
+    return true;
+}
+
+bool IPCIntelDvs::serverUnflattenGetMorphTalbe(const void* pData, unsigned int size,
+                                               ia_dvs_state** dvs,
+                                               ia_dvs_morph_table** morphTable) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p, morphTable:%p", __func__, pData, size, dvs, morphTable);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsMorphParams), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+    CheckError(!morphTable, false, "@%s, morphTable is nullptr", __func__);
+
+    const DvsMorphParams* params = static_cast<const DvsMorphParams*>(pData);
+    *dvs = reinterpret_cast<ia_dvs_state*>(params->dvsHandle);
+    *morphTable = reinterpret_cast<ia_dvs_morph_table*>(params->morphHandle);
+
+    return true;
+}
+
+bool IPCIntelDvs::flattenMorphTable(const ia_dvs_morph_table* morphTable, DvsMorphParams* params) {
+    LOGIPC("@%s", __func__);
+    CheckError(!morphTable, false, "@%s, morphTable is nullptr", __func__);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+    CheckError(!morphTable->xcoords_y || !morphTable->ycoords_y, false, "@%s, y coords is nullptr",
+               __func__);
+    CheckError(!morphTable->xcoords_uv || !morphTable->ycoords_uv, false,
+               "@%s, uv coords is nullptr", __func__);
+    CheckError(!morphTable->xcoords_uv_float || !morphTable->ycoords_uv_float, false,
+               "@%s, uv coords float is nullptr", __func__);
+
+    unsigned int SizeY = morphTable->width_y * morphTable->height_y * sizeof(int32_t);
+    unsigned int SizeUV = morphTable->width_uv * morphTable->height_uv * sizeof(int32_t);
+    unsigned int SizeYLocal = MAX_DVS_COORDS_Y_SIZE * sizeof(int32_t);
+    unsigned int SizeUVLocal = MAX_DVS_COORDS_UV_SIZE * sizeof(int32_t);
+    CheckError(SizeY > SizeYLocal, false, "@%s, coords_y data is small", __func__);
+    CheckError(SizeUV > SizeUVLocal, false, "@%s, coords_uv data is small", __func__);
+    MEMCPY_S(params->xcoordsY, SizeYLocal, morphTable->xcoords_y, SizeY);
+    MEMCPY_S(params->ycoordsY, SizeYLocal, morphTable->ycoords_y, SizeY);
+    MEMCPY_S(params->xcoordsUV, SizeUVLocal, morphTable->xcoords_uv, SizeUV);
+    MEMCPY_S(params->ycoordsUV, SizeUVLocal, morphTable->ycoords_uv, SizeUV);
+
+    SizeUV = morphTable->width_uv * morphTable->height_uv * sizeof(float);
+    SizeUVLocal = MAX_DVS_COORDS_UV_SIZE * sizeof(float);
+    CheckError(SizeUV > SizeUVLocal, false, "@%s, coords_uv data of float type is small", __func__);
+    MEMCPY_S(params->xcoordsUVFloat, SizeUVLocal, morphTable->xcoords_uv_float, SizeUV);
+    MEMCPY_S(params->ycoordsUVFloat, SizeUVLocal, morphTable->ycoords_uv_float, SizeUV);
+
+    return true;
+}
+
+bool IPCIntelDvs::serverFlattenGetMorphTalbe(void* pData, unsigned int size,
+                                             ia_dvs_morph_table* morphTable) {
+    LOGIPC("@%s, pData:%p, size:%d, morphTable:%p", __func__, pData, size, morphTable);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsMorphParams), false, "@%s, buffer is small", __func__);
+    CheckError(!morphTable, false, "@%s, morphTable is nullptr", __func__);
+
+    DvsMorphParams* params = static_cast<DvsMorphParams*>(pData);
+    params->morphTable = *morphTable;
+
+    return flattenMorphTable(morphTable, params);
+}
+
+bool IPCIntelDvs::unflattenMorphTalbe(DvsMorphParams* params) {
+    LOGIPC("@%s", __func__);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    params->morphTable.xcoords_y = params->xcoordsY;
+    params->morphTable.ycoords_y = params->ycoordsY;
+    params->morphTable.xcoords_uv = params->xcoordsUV;
+    params->morphTable.ycoords_uv = params->ycoordsUV;
+    params->morphTable.xcoords_uv_float = params->xcoordsUVFloat;
+    params->morphTable.ycoords_uv_float = params->ycoordsUVFloat;
+
+    return true;
+}
+
+bool IPCIntelDvs::clientUnflattenGetMorphTalbe(void* pData, unsigned int size,
+                                               ia_dvs_morph_table** morphTable) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsMorphParams), false, "@%s, buffer is small", __func__);
+    CheckError(!morphTable, false, "@%s, morphLocal is nullptr", __func__);
+
+    DvsMorphParams* params = static_cast<DvsMorphParams*>(pData);
+    bool ret = unflattenMorphTalbe(params);
+    CheckError(!ret, false, "@%s, unflattenMorphTalbe fails", __func__);
+    *morphTable = &params->morphTable;
+
+    return true;
+}
+
+bool IPCIntelDvs::clientFlattenSetStatistics(void* pData, unsigned int size, ia_dvs_state* dvs,
+                                             const ia_dvs_statistics* statistics,
+                                             const ia_aiq_ae_results* aeResults,
+                                             const ia_aiq_af_results* afResults,
+                                             const ia_aiq_sensor_events* sensorEvents,
+                                             uint64_t frameReadoutStart, uint64_t frameReadoutEnd) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsStatistcs), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+
+    DvsStatistcs* params = static_cast<DvsStatistcs*>(pData);
+    CLEAR(*params);
+    params->dvsHandle = reinterpret_cast<uintptr_t>(dvs);
+
+    if (statistics) {
+        params->statisticsFlag = true;
+        CheckError(MV_ENTRIE_COUNT < statistics->vector_count, false, "statistics buffer is small");
+        params->statistics.vector_count = statistics->vector_count;
+        MEMCPY_S(params->motion_vectors, MV_ENTRIE_COUNT * sizeof(ia_dvs_motion_vector),
+                 statistics->motion_vectors,
+                 statistics->vector_count * sizeof(ia_dvs_motion_vector));
+    }
+
+    if (aeResults) {
+        params->aeResultsFlag = true;
+        bool ret = mIpcAiq.flattenAeResults(*aeResults, &params->aeResultsBase);
+        CheckError(!ret, false, "failed to copy ae result");
+    }
+
+    if (afResults) {
+        params->afResultsFlag = true;
+        params->afResultsBase = *afResults;
+    }
+
+    if (sensorEvents) {
+        params->sensorEventsFlag = true;
+        size_t sensorDataLen = sizeof(ia_aiq_sensor_data);
+        size_t sensorEventLen = sensorDataLen * MAX_MOTION_SENSOR_COUNT_DVS;
+        CheckError(MAX_MOTION_SENSOR_COUNT_DVS < sensorEvents->num_accelerometer_events, false,
+                   "accelerometer_events buffer is small");
+        CheckError(MAX_MOTION_SENSOR_COUNT_DVS < sensorEvents->num_gravity_events, false,
+                   "gravity_events buffer is small");
+        CheckError(MAX_MOTION_SENSOR_COUNT_DVS < sensorEvents->num_gyroscope_events, false,
+                   "gyroscope_events buffer is small");
+        CheckError(MAX_MOTION_SENSOR_COUNT_DVS < sensorEvents->num_ambient_light_events, false,
+                   "ambient_light_events buffer is small");
+        CheckError(MAX_MOTION_SENSOR_COUNT_DVS < sensorEvents->num_dmd_events, false,
+                   "dmd_events buffer is small");
+        MEMCPY_S(params->sensorEvents.accelerometer_events, sensorEventLen,
+                 sensorEvents->accelerometer_events,
+                 sensorEvents->num_accelerometer_events * sensorDataLen);
+        MEMCPY_S(params->sensorEvents.gravity_events, sensorEventLen, sensorEvents->gravity_events,
+                 sensorEvents->num_gravity_events * sensorDataLen);
+        MEMCPY_S(params->sensorEvents.gyroscope_events, sensorEventLen,
+                 sensorEvents->gyroscope_events,
+                 sensorEvents->num_gyroscope_events * sensorDataLen);
+        MEMCPY_S(params->sensorEvents.ambient_light_events,
+                 sizeof(params->sensorEvents.ambient_light_events),
+                 sensorEvents->ambient_light_events,
+                 sensorEvents->num_ambient_light_events * sizeof(ia_aiq_ambient_light_events));
+        MEMCPY_S(params->sensorEvents.dmd_events, sizeof(params->sensorEvents.dmd_events),
+                 sensorEvents->dmd_events,
+                 sensorEvents->num_dmd_events * sizeof(ia_aiq_dmd_sensor_events));
+        params->sensorEventsBase = *sensorEvents;
+    }
+    params->frameReadoutStart = frameReadoutStart;
+    params->frameReadoutEnd = frameReadoutEnd;
+
+    return true;
+}
+
+bool IPCIntelDvs::serverUnflattenSetStatistics(
+    void* pData, unsigned int size, ia_dvs_state** dvs, ia_dvs_statistics** statistics,
+    ia_aiq_ae_results** aeResults, ia_aiq_af_results** afResults,
+    ia_aiq_sensor_events** sensorEvents, uint64_t* frameReadoutStart, uint64_t* frameReadoutEnd) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+
+    DvsStatistcs* params = static_cast<DvsStatistcs*>(pData);
+    *dvs = reinterpret_cast<ia_dvs_state*>(params->dvsHandle);
+
+    *statistics = nullptr;
+    if (params->statisticsFlag) {
+        params->statistics.motion_vectors = params->motion_vectors;
+        *statistics = &params->statistics;
+    }
+
+    *aeResults = nullptr;
+    if (params->aeResultsFlag) {
+        bool ret = mIpcAiq.unflattenAeResults(&params->aeResultsBase);
+        CheckError(!ret, false, "@%s, unflattenAeResults fails", __func__);
+        *aeResults = &params->aeResultsBase.base;
+    }
+
+    *afResults = nullptr;
+    if (params->afResultsFlag) {
+        *afResults = &params->afResultsBase;
+    }
+
+    *sensorEvents = nullptr;
+    if (params->sensorEventsFlag) {
+        if (params->sensorEventsBase.num_accelerometer_events != 0) {
+            params->sensorEventsBase.accelerometer_events =
+                params->sensorEvents.accelerometer_events;
+        }
+        if (params->sensorEventsBase.num_gravity_events != 0) {
+            params->sensorEventsBase.gravity_events = params->sensorEvents.gravity_events;
+        }
+        if (params->sensorEventsBase.num_gyroscope_events != 0) {
+            params->sensorEventsBase.gyroscope_events = params->sensorEvents.gyroscope_events;
+        }
+        if (params->sensorEventsBase.num_ambient_light_events != 0) {
+            params->sensorEventsBase.ambient_light_events =
+                params->sensorEvents.ambient_light_events;
+        }
+        if (params->sensorEventsBase.num_dmd_events != 0) {
+            params->sensorEventsBase.dmd_events = params->sensorEvents.dmd_events;
+        }
+        *sensorEvents = &params->sensorEventsBase;
+    }
+
+    *frameReadoutStart = params->frameReadoutStart;
+    *frameReadoutEnd = params->frameReadoutEnd;
+
+    return true;
+}
+
+bool IPCIntelDvs::clientFlattenExecute(void* pData, unsigned int size, ia_dvs_state* dvs,
+                                       uint16_t focusPosition) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p, focusPosition:%d", __func__, pData, size, dvs,
+           focusPosition);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsExecute), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+
+    DvsExecute* params = static_cast<DvsExecute*>(pData);
+    params->dvsHandle = reinterpret_cast<uintptr_t>(dvs);
+    params->focusPosition = focusPosition;
+
+    return true;
+}
+
+bool IPCIntelDvs::serverUnflattenExecute(const void* pData, unsigned int size, ia_dvs_state** dvs,
+                                         uint16_t* focusPosition) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p, focusPosition:%p", __func__, pData, size, dvs,
+           focusPosition);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsExecute), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+    CheckError(!focusPosition, false, "@%s, focusPosition is nullptr", __func__);
+
+    const DvsExecute* params = static_cast<const DvsExecute*>(pData);
+    *dvs = reinterpret_cast<ia_dvs_state*>(params->dvsHandle);
+    *focusPosition = params->focusPosition;
+
+    return true;
+}
+
+bool IPCIntelDvs::clientFlattenImageTransformation(void* pData, unsigned int size,
+                                                   ia_dvs_state* dvs) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p", __func__, pData, size, dvs);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsImageTransformation), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+
+    DvsImageTransformation* params = static_cast<DvsImageTransformation*>(pData);
+    params->dvsHandle = reinterpret_cast<uintptr_t>(dvs);
+
+    return true;
+}
+
+bool IPCIntelDvs::serverUnflattenImageTransformation(void* pData, unsigned int size,
+                                                     ia_dvs_state** dvs) {
+    LOGIPC("@%s, pData:%p, size:%d, dvs:%p", __func__, pData, size, dvs);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsImageTransformation), false, "@%s, buffer is small", __func__);
+    CheckError(!dvs, false, "@%s, dvs is nullptr", __func__);
+
+    DvsImageTransformation* params = static_cast<DvsImageTransformation*>(pData);
+    *dvs = reinterpret_cast<ia_dvs_state*>(params->dvsHandle);
+
+    return true;
+}
+
+bool IPCIntelDvs::serverFlattenImageTransformation(
+    void* pData, unsigned int size, ia_dvs_image_transformation* imageTransformation) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsImageTransformation), false, "@%s, buffer is small", __func__);
+    CheckError(!imageTransformation, false, "@%s, imageTransformation is nullptr", __func__);
+
+    DvsImageTransformation* params = static_cast<DvsImageTransformation*>(pData);
+    params->imageTransformation = *imageTransformation;
+
+    return true;
+}
+
+bool IPCIntelDvs::clientUnflattenImageTransformation(
+    const void* pData, unsigned int size, ia_dvs_image_transformation** imageTransformation) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(DvsImageTransformation), false, "@%s, buffer is small", __func__);
+    CheckError(!imageTransformation, false, "@%s, imageTransformation is nullptr", __func__);
+
+    DvsImageTransformation* params =
+        const_cast<DvsImageTransformation*>(static_cast<const DvsImageTransformation*>(pData));
+    *imageTransformation = &params->imageTransformation;
+
+    return true;
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelDvs.h b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelDvs.h
new file mode 100644
index 000000000000..6ce6ae6ba519
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelDvs.h
@@ -0,0 +1,239 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_dvs.h>
+#include <ia_dvs_types.h>
+#include <ia_isp_bxt.h>
+
+#include "DvsResult.h"
+#include "modules/sandboxing/IPCCommon.h"
+#include "modules/sandboxing/IPCIntelAiq.h"
+
+namespace icamera {
+#define MAX_DVS_DATA_SIZE MAX_IA_BINARY_DATA_SIZE
+
+struct DvsInitParams {
+    ia_binary_data base;
+    unsigned char data[MAX_DVS_DATA_SIZE];
+    uintptr_t cmcHandle;
+
+    // the below is returned
+    uintptr_t dvsHandle;
+};
+
+struct DvsDeinitParams {
+    uintptr_t dvsHandle;
+};
+
+struct DvsConfigParams {
+    uintptr_t dvsHandle;
+    ia_dvs_configuration configData;
+    float zoomRatio;
+};
+
+struct DvsNoneBlankRatioParams {
+    uintptr_t dvsHandle;
+    float nonBlankingRatio;
+};
+
+struct DvsDigitalZoomMode {
+    uintptr_t dvsHandle;
+    ia_dvs_zoom_mode zoomMode;
+};
+
+struct DvsDigitalZoomRegion {
+    uintptr_t dvsHandle;
+    ia_rectangle zoomRegion;
+};
+
+struct DvsDigitalZoomCoordinate {
+    uintptr_t dvsHandle;
+    ia_coordinate zoomCoordinate;
+};
+
+struct DvsDigitalZoomMagnitude {
+    uintptr_t dvsHandle;
+    float zoomRatio;
+};
+
+struct DvsFreeMorphParams {
+    uintptr_t dvsHandle;
+    uintptr_t morphHandle;
+};
+
+struct DvsMorphParams {
+    uintptr_t dvsHandle;
+    uintptr_t morphHandle;  // save remote morph table pointer
+
+    ia_dvs_morph_table morphTable;  // used by client
+    uint32_t xcoordsY[MAX_DVS_COORDS_Y_SIZE];
+    uint32_t ycoordsY[MAX_DVS_COORDS_Y_SIZE];
+    uint32_t xcoordsUV[MAX_DVS_COORDS_UV_SIZE];
+    uint32_t ycoordsUV[MAX_DVS_COORDS_UV_SIZE];
+    float xcoordsUVFloat[MAX_DVS_COORDS_UV_SIZE];
+    float ycoordsUVFloat[MAX_DVS_COORDS_UV_SIZE];
+};
+
+struct DvsExecute {
+    uintptr_t dvsHandle;
+    uint16_t focusPosition;
+};
+
+struct DvsImageTransformation {
+    uintptr_t dvsHandle;
+    ia_dvs_image_transformation imageTransformation;
+};
+
+#define MAX_MOTION_SENSOR_COUNT_DVS 100
+struct DvsSensorEvents {
+    ia_aiq_sensor_data accelerometer_events[MAX_MOTION_SENSOR_COUNT_DVS];
+    ia_aiq_sensor_data gravity_events[MAX_MOTION_SENSOR_COUNT_DVS];
+    ia_aiq_sensor_data gyroscope_events[MAX_MOTION_SENSOR_COUNT_DVS];
+    ia_aiq_ambient_light_events ambient_light_events[MAX_MOTION_SENSOR_COUNT_DVS];
+    ia_aiq_dmd_sensor_events dmd_events[MAX_MOTION_SENSOR_COUNT_DVS];
+};
+
+#define MV_ENTRIE_COUNT 5000
+struct DvsStatistcs {
+    uintptr_t dvsHandle;
+
+    bool statisticsFlag;
+    ia_dvs_statistics statistics;
+    ia_dvs_motion_vector motion_vectors[MV_ENTRIE_COUNT];
+
+    bool aeResultsFlag;
+    ae_run_params_results aeResultsBase;
+
+    bool afResultsFlag;
+    ia_aiq_af_results afResultsBase;
+
+    bool sensorEventsFlag;
+    ia_aiq_sensor_events sensorEventsBase;
+    DvsSensorEvents sensorEvents;
+
+    uint64_t frameReadoutStart;
+    uint64_t frameReadoutEnd;
+};
+
+class IPCIntelDvs {
+ public:
+    IPCIntelDvs();
+    ~IPCIntelDvs();
+
+    // for DvsInitParams
+    bool clientFlattenInit(void* pData, unsigned int size, const ia_binary_data* dvsDataPtr,
+                           const ia_cmc_t* cmc);
+    bool serverUnflattenInit(void* pData, int size, ia_binary_data** dvsDataPtr, ia_cmc_t** cmc);
+    bool serverFlattenInit(void* pData, unsigned int size, ia_dvs_state* dvs);
+    bool clientUnflattenInit(const void* pData, unsigned int size, ia_dvs_state** dvs);
+
+    // for DvsDeinitParams
+    bool clientFlattenDeinit(void* pData, unsigned int size, ia_dvs_state* dvs);
+    bool serverUnflattenDeinit(const void* pData, unsigned int size, ia_dvs_state** dvs);
+
+    // for DvsConfigParams
+    bool clientFlattenConfig(void* pData, unsigned int size, ia_dvs_state* dvs,
+                             const ia_dvs_configuration* config, float zoomRatio);
+    bool serverUnflattenConfig(const void* pData, unsigned int size, ia_dvs_state** dvs,
+                               ia_dvs_configuration** config, float* zoomRatio);
+
+    // for DvsNoneBlankRatioParams
+    bool clientFlattenNoneBlanckRation(void* pData, unsigned int size, ia_dvs_state* dvs,
+                                       float nonBlankingRatio);
+    bool serverUnflattenNoneBlanckRation(const void* pData, unsigned int size, ia_dvs_state** dvs,
+                                         float* nonBlankingRatio);
+
+    // for DvsDigitalZoomMode
+    bool clientFlattenDigitalZoomMode(void* pData, unsigned int size, ia_dvs_state* dvs,
+                                      ia_dvs_zoom_mode zoomMode);
+    bool serverUnflattenDigitalZoomMode(const void* pData, unsigned int size, ia_dvs_state** dvs,
+                                        ia_dvs_zoom_mode* zoomMode);
+
+    // for DvsDigitalZoomRegion
+    bool clientFlattenDigitalZoomRegion(void* pData, unsigned int size, ia_dvs_state* dvs,
+                                        ia_rectangle* zoomRegion);
+    bool serverUnflattenDigitalZoomRegion(const void* pData, unsigned int size, ia_dvs_state** dvs,
+                                          ia_rectangle** zoomRegion);
+
+    // for DvsDigitalZoomCoordinate
+    bool clientFlattenDigitalZoomCoordinate(void* pData, unsigned int size, ia_dvs_state* dvs,
+                                            ia_coordinate* zoomCoordinate);
+    bool serverUnflattenDigitalZoomCoordinate(const void* pData, unsigned int size,
+                                              ia_dvs_state** dvs, ia_coordinate** zoomCoordinate);
+
+    // for DvsDigitalZoomMagnitude
+    bool clientFlattenDigitalZoomMagnitude(void* pData, unsigned int size, ia_dvs_state* dvs,
+                                           float zoomRatio);
+    bool serverUnflattenDigitalZoomMagnitude(const void* pData, unsigned int size,
+                                             ia_dvs_state** dvs, float* zoomRatio);
+
+    // for DvsFreeMorphParams
+    bool clientFlattenFreeMorphTable(void* pData, unsigned int size, ia_dvs_state* dvs,
+                                     ia_dvs_morph_table* morphTable);
+    bool serverUnflattenFreeMorphTable(const void* pData, unsigned int size, ia_dvs_state** dvs,
+                                       ia_dvs_morph_table** morphTable);
+
+    // for DvsMorphParams
+    bool clientFlattenAllocateMorphTable(void* pData, unsigned int size, ia_dvs_state* dvs);
+    bool serverUnflattenAllocateMorphTalbe(const void* pData, unsigned int size,
+                                           ia_dvs_state** dvs);
+    bool serverFlattenAllocateMorphTalbe(void* pData, unsigned int size,
+                                         ia_dvs_morph_table* morphTable);
+    bool clientUnflattenAllocateMorphTalbe(const void* pData, unsigned int size,
+                                           ia_dvs_morph_table** morphTable);
+    bool clientFlattenGetMorphTable(void* pData, unsigned int size, ia_dvs_state* dvs,
+                                    ia_dvs_morph_table* morphTable);
+    bool serverUnflattenGetMorphTalbe(const void* pData, unsigned int size, ia_dvs_state** dvs,
+                                      ia_dvs_morph_table** morphTable);
+    bool serverFlattenGetMorphTalbe(void* pData, unsigned int size, ia_dvs_morph_table* morphTable);
+    bool clientUnflattenGetMorphTalbe(void* pData, unsigned int size,
+                                      ia_dvs_morph_table** morphTable);
+    bool unflattenMorphTalbe(DvsMorphParams* params);
+    bool flattenMorphTable(const ia_dvs_morph_table* morphTable, DvsMorphParams* params);
+
+    // for DvsStatistcs
+    bool clientFlattenSetStatistics(void* pData, unsigned int size, ia_dvs_state* dvs,
+                                    const ia_dvs_statistics* statistics,
+                                    const ia_aiq_ae_results* aeResults,
+                                    const ia_aiq_af_results* afResults,
+                                    const ia_aiq_sensor_events* sensorEvents,
+                                    uint64_t frameReadoutStart, uint64_t frameReadoutEnd);
+    bool serverUnflattenSetStatistics(void* pData, unsigned int size, ia_dvs_state** dvs,
+                                      ia_dvs_statistics** statistics, ia_aiq_ae_results** aeResults,
+                                      ia_aiq_af_results** afResults,
+                                      ia_aiq_sensor_events** sensorEvents,
+                                      uint64_t* frameReadoutStart, uint64_t* frameReadoutEnd);
+
+    // for DvsExecute
+    bool clientFlattenExecute(void* pData, unsigned int size, ia_dvs_state* dvs,
+                              uint16_t focusPosition);
+    bool serverUnflattenExecute(const void* pData, unsigned int size, ia_dvs_state** dvs,
+                                uint16_t* focusPosition);
+
+    // for DvsImageTransformation
+    bool clientFlattenImageTransformation(void* pData, unsigned int size, ia_dvs_state* dvs);
+    bool serverUnflattenImageTransformation(void* pData, unsigned int size, ia_dvs_state** dvs);
+    bool serverFlattenImageTransformation(void* pData, unsigned int size,
+                                          ia_dvs_image_transformation* imageTransformation);
+    bool clientUnflattenImageTransformation(const void* pData, unsigned int size,
+                                            ia_dvs_image_transformation** imageTransformation);
+
+ private:
+    IPCIntelAiq mIpcAiq;
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelFD.cpp b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelFD.cpp
new file mode 100644
index 000000000000..825d07269d40
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelFD.cpp
@@ -0,0 +1,63 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IPC_FACE_DETECTION"
+
+#include "modules/sandboxing/IPCIntelFD.h"
+
+#include "CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IPCIntelFD::IPCIntelFD() {
+    LOG1("@%s", __func__);
+}
+
+IPCIntelFD::~IPCIntelFD() {
+    LOG1("@%s", __func__);
+}
+
+bool IPCIntelFD::clientFlattenInit(unsigned int max_face_num, FaceDetectionInitParams* params) {
+    LOG1("@%s, params:%p", __func__, params);
+    CheckError(params == nullptr, false, "@%s, params is nullptr", __func__);
+
+    params->max_face_num = max_face_num;
+
+    return true;
+}
+
+bool IPCIntelFD::serverUnflattenRun(const FaceDetectionRunParams& inParams, void* imageData,
+                                    pvl_image* image) {
+    LOG1("@%s, image:%p", __func__, image);
+    CheckError(image == nullptr, false, "@%s, iamge is nullptr", __func__);
+
+    image->size = inParams.size;
+    image->width = inParams.width;
+    image->height = inParams.height;
+    image->format = inParams.format;
+    image->stride = inParams.stride;
+    image->rotation = inParams.rotation;
+
+    if (imageData) {
+        image->data = const_cast<uint8_t*>(static_cast<uint8_t*>(imageData));
+    } else {
+        image->data = const_cast<uint8_t*>(inParams.data);
+    }
+
+    return true;
+}
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelFD.h b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelFD.h
new file mode 100644
index 000000000000..ebfabfa07620
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelFD.h
@@ -0,0 +1,32 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "FaceBase.h"
+#include "modules/sandboxing/IPCCommon.h"
+
+namespace icamera {
+class IPCIntelFD {
+ public:
+    IPCIntelFD();
+    virtual ~IPCIntelFD();
+
+    bool clientFlattenInit(unsigned int max_face_num, FaceDetectionInitParams* params);
+    bool serverUnflattenRun(const FaceDetectionRunParams& inParams, void* imageData,
+                            pvl_image* image);
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelLard.cpp b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelLard.cpp
new file mode 100644
index 000000000000..f3b3e1c40bd4
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelLard.cpp
@@ -0,0 +1,298 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IPC_INTEL_LARD"
+
+#include "modules/sandboxing/IPCIntelLard.h"
+
+#include <ia_types.h>
+
+#include "CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IPCIntelLard::IPCIntelLard() {
+    LOGIPC("@%s", __func__);
+}
+
+IPCIntelLard::~IPCIntelLard() {
+    LOGIPC("@%s", __func__);
+}
+
+bool IPCIntelLard::clientFlattenInit(void* pData, unsigned int size,
+                                     const ia_binary_data* lard_data_ptr) {
+    LOGIPC("@%s, pData:%p, size:%d, lard_data_ptr:%p", __func__, pData, size, lard_data_ptr);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(lard_init_params), false, "@%s, buffer is small", __func__);
+    CheckError(!lard_data_ptr, false, "@%s, lard_data_ptr is nullptr", __func__);
+
+    lard_init_params* params = static_cast<lard_init_params*>(pData);
+    params->base = *lard_data_ptr;
+
+    CheckError(params->base.size > MAX_LARD_DATA_SIZE, false, "@%s, data:%d is too small", __func__,
+               params->base.size);
+    MEMCPY_S(params->data, MAX_LARD_DATA_SIZE, params->base.data, params->base.size);
+
+    return true;
+}
+
+bool IPCIntelLard::serverUnflattenInit(void* pData, int size, ia_binary_data* lard_data_ptr) {
+    LOGIPC("@%s, pData:%p, size:%d, binaryData:%p", __func__, pData, size, lard_data_ptr);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(lard_init_params), false, "@%s, buffer is small", __func__);
+    CheckError(!lard_data_ptr, false, "@%s, binaryData is nullptr", __func__);
+
+    lard_init_params* params = static_cast<lard_init_params*>(pData);
+    params->base.data = static_cast<void*>(params->data);
+    *lard_data_ptr = params->base;
+
+    return true;
+}
+
+bool IPCIntelLard::serverFlattenInit(void* pData, int size, ia_lard* lard) {
+    LOGIPC("@%s, pData:%p, size:%d, lard:%p", __func__, pData, size, lard);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(lard_init_params), false, "@%s, buffer is small", __func__);
+    CheckError(!lard, false, "@%s, lard is nullptr", __func__);
+
+    lard_init_params* params = static_cast<lard_init_params*>(pData);
+
+    params->results = reinterpret_cast<uintptr_t>(lard);
+
+    return true;
+}
+
+bool IPCIntelLard::clientUnflattenInit(void* pData, unsigned int size, ia_lard** lard) {
+    LOGIPC("@%s, pData:%p, size:%d, lard:%p", __func__, pData, size, lard);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(lard_init_params), false, "@%s, buffer is small", __func__);
+    CheckError(!lard, false, "@%s, lard is nullptr", __func__);
+
+    lard_init_params* params = static_cast<lard_init_params*>(pData);
+    *lard = reinterpret_cast<ia_lard*>(params->results);
+
+    return true;
+}
+
+bool IPCIntelLard::clientFlattenGetTagList(void* pData, unsigned int size, ia_lard* lard,
+                                           unsigned int mode_tag) {
+    LOGIPC("@%s, pData:%p, size:%d, lard:%p, mode_tag:%d", __func__, pData, size, lard, mode_tag);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(lard_get_tag_list_params), false, "@%s, buffer is small", __func__);
+    CheckError(!lard, false, "@%s, lard is nullptr", __func__);
+
+    lard_get_tag_list_params* params = static_cast<lard_get_tag_list_params*>(pData);
+    params->lard = reinterpret_cast<uintptr_t>(lard);
+    params->mode_tag = mode_tag;
+
+    return true;
+}
+
+bool IPCIntelLard::serverUnflattenGetTagList(void* pData, unsigned int size, ia_lard** lard,
+                                             unsigned int* mode_tag) {
+    LOGIPC("@%s, pData:%p, size:%d, lard:%p, mode_tag:%p", __func__, pData, size, lard, mode_tag);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(lard_get_tag_list_params), false, "@%s, buffer is small", __func__);
+    CheckError(!lard, false, "@%s, lard is nullptr", __func__);
+    CheckError(!mode_tag, false, "@%s, mode_tag is nullptr", __func__);
+
+    lard_get_tag_list_params* params = static_cast<lard_get_tag_list_params*>(pData);
+    *lard = reinterpret_cast<ia_lard*>(params->lard);
+    *mode_tag = params->mode_tag;
+
+    return true;
+}
+
+bool IPCIntelLard::serverFlattenGetTagList(void* pData, unsigned int size, unsigned int num_tags,
+                                           const unsigned int* tags) {
+    LOGIPC("@%s, pData:%p, size:%d, num_tags:%d, tags:%p", __func__, pData, size, num_tags, tags);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(lard_get_tag_list_params), false, "@%s, buffer is small", __func__);
+    CheckError(!tags, false, "@%s, tags is nullptr", __func__);
+
+    lard_get_tag_list_params* params = static_cast<lard_get_tag_list_params*>(pData);
+    int tagsSize = num_tags * sizeof(unsigned int);
+
+    CheckError(tagsSize > MAX_LARD_DATA_SIZE, false, "@%s, tags:%d is small", __func__, tagsSize);
+    switch (params->mode_tag) {
+        case LCMC_TAG:
+            params->num_cmc_tags = num_tags;
+            MEMCPY_S(params->cmc_tags_list, tagsSize, tags, tagsSize);
+            break;
+        case LAIQ_TAG:
+            params->num_aiq_tags = num_tags;
+            MEMCPY_S(params->aiq_tags_list, tagsSize, tags, tagsSize);
+            break;
+        case LISP_TAG:
+            params->num_isp_tags = num_tags;
+            MEMCPY_S(params->isp_tags_list, tagsSize, tags, tagsSize);
+            break;
+        case LTHR_TAG:
+            params->num_others_tags = num_tags;
+            MEMCPY_S(params->others_tags_list, tagsSize, tags, tagsSize);
+            break;
+        default:
+            return false;
+    }
+
+    return true;
+}
+
+bool IPCIntelLard::clientUnflattenGetTagList(void* pData, unsigned int size, unsigned int mode_tag,
+                                             unsigned int* num_tags, const unsigned int** tags) {
+    LOGIPC("@%s, pData:%p, size:%d, mode_tag:%d, num_tags:%p, tags:%p", __func__, pData, size,
+           mode_tag, num_tags, tags);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(lard_get_tag_list_params), false, "@%s, buffer is small", __func__);
+    CheckError(!num_tags, false, "@%s, num_tags is nullptr", __func__);
+    CheckError(!tags, false, "@%s, tags is nullptr", __func__);
+
+    lard_get_tag_list_params* params = static_cast<lard_get_tag_list_params*>(pData);
+
+    switch (mode_tag) {
+        case LCMC_TAG:
+            *num_tags = params->num_cmc_tags;
+            *tags = params->cmc_tags_list;
+            break;
+        case LAIQ_TAG:
+            *num_tags = params->num_aiq_tags;
+            *tags = params->aiq_tags_list;
+            break;
+        case LISP_TAG:
+            *num_tags = params->num_isp_tags;
+            *tags = params->isp_tags_list;
+            break;
+        case LTHR_TAG:
+            *num_tags = params->num_others_tags;
+            *tags = params->others_tags_list;
+            break;
+        default:
+            return false;
+    }
+
+    return true;
+}
+
+bool IPCIntelLard::clientFlattenRun(void* pData, unsigned int size, ia_lard* lard,
+                                    ia_lard_input_params* inputParams) {
+    LOGIPC("@%s, pData:%p, size:%d, lard:%p, inputParams:%p", __func__, pData, size, lard,
+           inputParams);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(lard_run_params), false, "@%s, buffer is small", __func__);
+    CheckError(!lard, false, "@%s, lard is nullptr", __func__);
+    CheckError(!inputParams, false, "@%s, inputParams is nullptr", __func__);
+
+    lard_run_params* params = static_cast<lard_run_params*>(pData);
+    params->lard = reinterpret_cast<uintptr_t>(lard);
+    params->inputParams = *inputParams;
+
+    return true;
+}
+
+bool IPCIntelLard::serverUnflattenRun(void* pData, unsigned int size, ia_lard** lard,
+                                      ia_lard_input_params** inputParams) {
+    LOGIPC("@%s, pData:%p, size:%d, lard:%p, inputParams:%p", __func__, pData, size, lard,
+           inputParams);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(lard_run_params), false, "@%s, buffer is small", __func__);
+    CheckError(!lard, false, "@%s, lard is nullptr", __func__);
+    CheckError(!inputParams, false, "@%s, inputParams is nullptr", __func__);
+
+    lard_run_params* params = static_cast<lard_run_params*>(pData);
+    *lard = reinterpret_cast<ia_lard*>(params->lard);
+    *inputParams = &params->inputParams;
+
+    return true;
+}
+
+bool IPCIntelLard::serverFlattenRun(void* pData, unsigned int size, ia_lard_results* result) {
+    LOGIPC("@%s, pData:%p, size:%d, result:%p", __func__, pData, size, result);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(lard_run_params), false, "@%s, buffer is small", __func__);
+    CheckError(!result, false, "@%s, result is nullptr", __func__);
+
+    lard_run_params* params = static_cast<lard_run_params*>(pData);
+
+    lard_run_params_results* results = &params->results;
+    ia_lard_results* base = &results->base;
+    *base = *result;
+
+    CheckError(base->aiqb_cmc_data.size > MAX_LARD_DATA_SIZE, false, "@%s, cmc:%d is too big",
+               __func__, base->aiqb_cmc_data.size);
+    CheckError(base->aiqb_aiq_data.size > MAX_LARD_DATA_SIZE, false, "@%s, aiq:%d is too big",
+               __func__, base->aiqb_aiq_data.size);
+    CheckError(base->aiqb_isp_data.size > MAX_LARD_DATA_SIZE, false, "@%s, isp:%d is too big",
+               __func__, base->aiqb_isp_data.size);
+    CheckError(base->aiqb_other_data.size > MAX_LARD_DATA_SIZE, false, "@%s, other:%d is too big",
+               __func__, base->aiqb_other_data.size);
+
+    MEMCPY_S(results->cmc_tags_list, MAX_LARD_DATA_SIZE, base->aiqb_cmc_data.data,
+             base->aiqb_cmc_data.size);
+    MEMCPY_S(results->aiq_tags_list, MAX_LARD_DATA_SIZE, base->aiqb_aiq_data.data,
+             base->aiqb_aiq_data.size);
+    MEMCPY_S(results->isp_tags_list, MAX_LARD_DATA_SIZE, base->aiqb_isp_data.data,
+             base->aiqb_isp_data.size);
+    MEMCPY_S(results->others_tags_list, MAX_LARD_DATA_SIZE, base->aiqb_other_data.data,
+             base->aiqb_other_data.size);
+
+    return true;
+}
+
+bool IPCIntelLard::clientUnflattenRun(void* pData, unsigned int size, ia_lard_results** result) {
+    LOGIPC("@%s, pData:%p, size:%d, result:%p", __func__, pData, size, result);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(lard_run_params), false, "@%s, buffer is small", __func__);
+    CheckError(!result, false, "@%s, result is nullptr", __func__);
+
+    lard_run_params* params = static_cast<lard_run_params*>(pData);
+    lard_run_params_results* results = &params->results;
+    ia_lard_results* base = &results->base;
+
+    base->aiqb_cmc_data.data = results->cmc_tags_list;
+    base->aiqb_aiq_data.data = results->aiq_tags_list;
+    base->aiqb_isp_data.data = results->isp_tags_list;
+    base->aiqb_other_data.data = results->others_tags_list;
+
+    *result = base;
+
+    return true;
+}
+
+bool IPCIntelLard::clientFlattenDeinit(void* pData, unsigned int size, ia_lard* lard) {
+    LOGIPC("@%s, pData:%p, size:%d, lard:%p", __func__, pData, size, lard);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(lard_deinit_params), false, "@%s, buffer is small", __func__);
+    CheckError(!lard, false, "@%s, lard is nullptr", __func__);
+
+    lard_deinit_params* params = static_cast<lard_deinit_params*>(pData);
+    params->lard = reinterpret_cast<uintptr_t>(lard);
+
+    return true;
+}
+
+bool IPCIntelLard::serverUnflattenDeinit(void* pData, unsigned int size, ia_lard** lard) {
+    LOGIPC("@%s, pData:%p, size:%d, lard:%p", __func__, pData, size, lard);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(lard_deinit_params), false, "@%s, buffer is small", __func__);
+    CheckError(!lard, false, "@%s, lard is nullptr", __func__);
+
+    lard_deinit_params* params = static_cast<lard_deinit_params*>(pData);
+    *lard = reinterpret_cast<ia_lard*>(params->lard);
+
+    return true;
+}
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelLard.h b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelLard.h
new file mode 100644
index 000000000000..eb6a94696906
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelLard.h
@@ -0,0 +1,104 @@
+/*
+ * Copyright (C) 2019 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_lard.h>
+
+#include "modules/sandboxing/IPCCommon.h"
+
+namespace icamera {
+
+#define MAX_LARD_DATA_SIZE MAX_IA_BINARY_DATA_SIZE
+
+struct lard_init_params {
+    ia_binary_data base;
+    unsigned char data[MAX_LARD_DATA_SIZE];
+
+    // the below is returned
+    uintptr_t results;
+};
+
+struct lard_get_tag_list_params {
+    uintptr_t lard;
+    unsigned int mode_tag;
+
+    // the below is returned
+    unsigned int num_cmc_tags;
+    unsigned int num_aiq_tags;
+    unsigned int num_isp_tags;
+    unsigned int num_others_tags;
+    unsigned int cmc_tags_list[MAX_LARD_DATA_SIZE];
+    unsigned int aiq_tags_list[MAX_LARD_DATA_SIZE];
+    unsigned int isp_tags_list[MAX_LARD_DATA_SIZE];
+    unsigned int others_tags_list[MAX_LARD_DATA_SIZE];
+};
+
+struct lard_run_params_results {
+    ia_lard_results base;
+
+    unsigned int cmc_tags_list[MAX_LARD_DATA_SIZE];
+    unsigned int aiq_tags_list[MAX_LARD_DATA_SIZE];
+    unsigned int isp_tags_list[MAX_LARD_DATA_SIZE];
+    unsigned int others_tags_list[MAX_LARD_DATA_SIZE];
+};
+
+struct lard_run_params {
+    uintptr_t lard;
+    ia_lard_input_params inputParams;
+
+    // the below is returned
+    lard_run_params_results results;
+};
+
+struct lard_deinit_params {
+    uintptr_t lard;
+};
+
+class IPCIntelLard {
+ public:
+    IPCIntelLard();
+    virtual ~IPCIntelLard();
+
+    // for ia_lard_init
+    bool clientFlattenInit(void* pData, unsigned int size, const ia_binary_data* lard_data_ptr);
+    bool serverUnflattenInit(void* pData, int size, ia_binary_data* lard_data_ptr);
+    bool serverFlattenInit(void* pData, int size, ia_lard* lard);
+    bool clientUnflattenInit(void* pData, unsigned int size, ia_lard** lard);
+
+    // for ia_lard_get_tag_list
+    bool clientFlattenGetTagList(void* pData, unsigned int size, ia_lard* lard,
+                                 unsigned int mode_tag);
+    bool serverUnflattenGetTagList(void* pData, unsigned int size, ia_lard** lard,
+                                   unsigned int* mode_tag);
+    bool serverFlattenGetTagList(void* pData, unsigned int size, unsigned int num_tags,
+                                 const unsigned int* tags);
+    bool clientUnflattenGetTagList(void* pData, unsigned int size, unsigned int mode_tag,
+                                   unsigned int* num_tags, const unsigned int** tags);
+
+    // ia_lard_run
+    bool clientFlattenRun(void* pData, unsigned int size, ia_lard* lard,
+                          ia_lard_input_params* inputParams);
+    bool serverUnflattenRun(void* pData, unsigned int size, ia_lard** lard,
+                            ia_lard_input_params** inputParams);
+    bool serverFlattenRun(void* pData, unsigned int size, ia_lard_results* result);
+    bool clientUnflattenRun(void* pData, unsigned int size, ia_lard_results** result);
+
+    // ia_lard_deinit
+    bool clientFlattenDeinit(void* pData, unsigned int size, ia_lard* lard);
+    bool serverUnflattenDeinit(void* pData, unsigned int size, ia_lard** lard);
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelLtm.cpp b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelLtm.cpp
new file mode 100644
index 000000000000..6918c2b4e7d0
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelLtm.cpp
@@ -0,0 +1,331 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IPC_INTEL_LTM"
+
+#include "modules/sandboxing/IPCIntelLtm.h"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IPCIntelLtm::IPCIntelLtm() {
+    LOGIPC("@%s", __func__);
+}
+
+IPCIntelLtm::~IPCIntelLtm() {
+    LOGIPC("@%s", __func__);
+}
+
+bool IPCIntelLtm::clientFlattenInit(const ia_binary_data& inData, uintptr_t mkn_hanlde,
+                                    LtmInitParams* params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    CheckError(sizeof(params->data) < inData.size, false, "@%s, buffer is small", __func__);
+    params->binary_data.data = params->data;
+    MEMCPY_S(params->data, sizeof(params->data), inData.data, inData.size);
+    params->binary_data.size = inData.size;
+    params->mkn_handle = mkn_hanlde;
+
+    return true;
+}
+
+bool IPCIntelLtm::serverUnflattenInit(LtmInitParams* params, ia_binary_data* inData,
+                                      uintptr_t* mkn_hanlde) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+    CheckError(!inData, false, "@%s, inData is nullptr", __func__);
+    CheckError(!mkn_hanlde, false, "@%s, mkn_hanlde is nullptr", __func__);
+
+    params->binary_data.data = params->data;
+    *inData = params->binary_data;
+    *mkn_hanlde = params->mkn_handle;
+
+    return true;
+}
+
+bool IPCIntelLtm::serverFlattenInit(LtmInitParams* params, ia_ltm* ltm_handle) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+    CheckError(!ltm_handle, false, "@%s, ltm_handle is nullptr", __func__);
+
+    params->results = reinterpret_cast<uintptr_t>(ltm_handle);
+
+    return true;
+}
+
+bool IPCIntelLtm::clientUnflattenInit(LtmInitParams* params, ia_ltm** ltm_handle) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+    CheckError(!ltm_handle, false, "@%s, ltm_handle is nullptr", __func__);
+
+    *ltm_handle = reinterpret_cast<ia_ltm*>(params->results);
+    return true;
+}
+
+int IPCIntelLtm::deepCopyAeResults(const ia_aiq_ae_results& src, LtmInputAeResult* params) {
+    LOGIPC("@%s", __func__);
+
+    params->aeResultsBase = src;
+
+    if (src.exposures) {
+        CheckError(src.num_exposures > MAX_NUM_EXPOSURES_LTM, BAD_VALUE,
+                   "exposure buffer is small");
+        for (unsigned int i = 0; i < src.num_exposures; i++) {
+            params->exposures[i].exposure_index = src.exposures[i].exposure_index;
+            params->exposures[i].distance_from_convergence =
+                src.exposures[i].distance_from_convergence;
+            params->exposures[i].converged = src.exposures[i].converged;
+            if (src.exposures[i].exposure) {
+                params->exposure[i] = *src.exposures[i].exposure;
+            }
+            if (src.exposures[i].sensor_exposure) {
+                params->sensor_exposure[i] = *src.exposures[i].sensor_exposure;
+            }
+            params->exposures[i].num_exposure_plan = src.exposures[i].num_exposure_plan;
+            if (src.exposures[i].exposure_plan_ids) {
+                CheckError(src.exposures[i].num_exposure_plan > MAX_NUM_OF_EXPOSURE_PLANS_LTM,
+                           BAD_VALUE, "exposures[%d].exposure_plan_ids buffer is small", i);
+                MEMCPY_S(params->exposure_plan_ids[i],
+                         MAX_NUM_OF_EXPOSURE_PLANS_LTM * sizeof(unsigned int),
+                         src.exposures[i].exposure_plan_ids,
+                         src.exposures[i].num_exposure_plan * sizeof(unsigned int));
+            }
+        }
+    }
+
+    if (src.weight_grid) {
+        params->weightGrid = *src.weight_grid;
+        unsigned int gridElements = src.weight_grid->width * src.weight_grid->height;
+        CheckError(MAX_SIZE_WEIGHT_GRID_LTM < gridElements, false, "weight_grid buffer is small");
+        MEMCPY_S(params->weights, MAX_SIZE_WEIGHT_GRID_LTM * sizeof(char), src.weight_grid->weights,
+                 gridElements * sizeof(char));
+    }
+
+    if (src.flashes) {
+        CheckError(src.num_flashes > MAX_NUM_FLASHES_LTM, BAD_VALUE, "flash buffer is small");
+        MEMCPY_S(params->flashes, MAX_NUM_FLASHES_LTM * sizeof(ia_aiq_flash_parameters),
+                 src.flashes, src.num_flashes * sizeof(ia_aiq_flash_parameters));
+    }
+
+    if (src.aperture_control) {
+        params->apertureControl = *src.aperture_control;
+    }
+    return OK;
+}
+
+int IPCIntelLtm::deepCopyRgbsGridData(const ia_aiq_rgbs_grid& src, ia_aiq_rgbs_grid* dst) {
+    LOGIPC("%s", __func__);
+
+    CheckError(!src.blocks_ptr || src.grid_width == 0 || src.grid_height == 0, BAD_VALUE,
+               "Failed to deep copy Rgbs grid data - invalid source");
+    CheckError(!dst || !dst->blocks_ptr, BAD_VALUE,
+               "Failed to deep copy Rgbs grid data - invalid destination");
+
+    size_t gridSize = src.grid_width * src.grid_height;
+    size_t gridSizeLocal = MAX_WIDTH_STATISTICS * MAX_HEIGHT_STATISTICS;
+    CheckError(gridSizeLocal < gridSize, false, "rgbs_grid buffer is small");
+    dst->grid_width = src.grid_width;
+    dst->grid_height = src.grid_height;
+    dst->shading_correction = src.shading_correction;
+    MEMCPY_S(dst->blocks_ptr, gridSizeLocal * sizeof(rgbs_grid_block), src.blocks_ptr,
+             gridSize * sizeof(rgbs_grid_block));
+
+    LOGIPC("%s, grid size=[%dx%d]", __func__, dst->grid_width, dst->grid_height);
+    return OK;
+}
+
+int IPCIntelLtm::deepCopyHdrRgbsGridData(const ia_aiq_hdr_rgbs_grid& src,
+                                         ia_aiq_hdr_rgbs_grid* dst) {
+    LOGIPC("%s", __func__);
+
+    CheckError(!src.blocks_ptr || src.grid_width == 0 || src.grid_height == 0, BAD_VALUE,
+               "Failed to deep copy HDR Rgbs grid data - invalid source");
+    CheckError(!dst || !dst->blocks_ptr, BAD_VALUE,
+               "Failed to deep copy HDR Rgbs grid data - invalid destination");
+
+    size_t gridSize = src.grid_width * src.grid_height;
+    size_t gridSizeLocal = BXT_RGBS_GRID_MAX_WIDTH * BXT_RGBS_GRID_MAX_HEIGHT;
+    CheckError(gridSizeLocal < gridSize, false, "hdr_rgbs_grid buffer is small");
+
+    dst->grid_width = src.grid_width;
+    dst->grid_height = src.grid_height;
+    dst->grid_data_bit_depth = src.grid_data_bit_depth;
+    dst->shading_correction = src.shading_correction;
+    MEMCPY_S(dst->blocks_ptr, gridSizeLocal * sizeof(hdr_rgbs_grid_block), src.blocks_ptr,
+             gridSize * sizeof(hdr_rgbs_grid_block));
+
+    LOGIPC("%s, grid size=[%dx%d]", __func__, dst->grid_width, dst->grid_height);
+    return OK;
+}
+
+bool IPCIntelLtm::clientFlattenRun(uintptr_t ltm, const ia_ltm_input_params& ltmParams,
+                                   int imageDataSize, void* paramsAddr) {
+    LOGIPC("@%s, paramsAddr:%p", __func__, paramsAddr);
+    CheckError(!paramsAddr, false, "@%s, paramsAddr is nullptr", __func__);
+
+    LtmRunParams* params = static_cast<LtmRunParams*>(paramsAddr);
+    params->ltm_handle = ltm;
+    params->inputParamsBase = ltmParams;
+    LtmRunInputParams* inputParams = &params->inputParams;
+
+    int ret = UNKNOWN_ERROR;
+    if (ltmParams.ae_results) {
+        ret = deepCopyAeResults(*ltmParams.ae_results, &inputParams->aeResult);
+        CheckError(ret != OK, false, "@%s, Failed to deepCopyAeResult", __func__);
+    }
+
+    if (ltmParams.yv_grid) {
+        MEMCPY_S(&inputParams->yvGrid, sizeof(ia_isp_bxt_hdr_yv_grid_t), ltmParams.yv_grid,
+                 sizeof(ia_isp_bxt_hdr_yv_grid_t));
+    }
+
+    if (ltmParams.rgbs_grid_ptr) {
+        inputParams->rgbsGrid.rgbsGridbase.blocks_ptr = inputParams->rgbsGrid.blocks;
+        ret = deepCopyRgbsGridData(*ltmParams.rgbs_grid_ptr, &inputParams->rgbsGrid.rgbsGridbase);
+        CheckError(ret != OK, false, "@%s, Failed to deepCopyRgbsGridData", __func__);
+    }
+
+    if (ltmParams.hdr_rgbs_grid_ptr) {
+        inputParams->hdrRgbsGrid.hdrRgbsGridBase.blocks_ptr = inputParams->hdrRgbsGrid.blocks;
+        ret = deepCopyHdrRgbsGridData(*ltmParams.hdr_rgbs_grid_ptr,
+                                      &inputParams->hdrRgbsGrid.hdrRgbsGridBase);
+        CheckError(ret != OK, false, "@%s, Failed to deepCopyHdrRgbsGridData", __func__);
+    }
+
+    if (ltmParams.input_image_ptr) {
+        inputParams->inputImage = *ltmParams.input_image_ptr;
+        if (ltmParams.input_image_ptr->image_data) {
+            CheckError(imageDataSize < ltmParams.input_image_ptr->image_data->size, false,
+                       "image_data buffer is small");
+            CheckError(ltmParams.input_image_ptr->image_data->size != 0 &&
+                           !ltmParams.input_image_ptr->image_data->data,
+                       false, "image_data error");
+            inputParams->imageData = *ltmParams.input_image_ptr->image_data;
+            void* imageData =
+                static_cast<void*>(static_cast<char*>(paramsAddr) + sizeof(LtmRunParams));
+            MEMCPY_S(imageData, imageDataSize, ltmParams.input_image_ptr->image_data->data,
+                     ltmParams.input_image_ptr->image_data->size);
+        }
+    }
+
+    if (ltmParams.gtm_input_params_ptr) {
+        MEMCPY_S(&inputParams->gtmInputParams, sizeof(ia_ltm_gtm_input_params),
+                 ltmParams.gtm_input_params_ptr, sizeof(ia_ltm_gtm_input_params));
+    }
+
+    return true;
+}
+
+bool IPCIntelLtm::serverUnflattenRun(void* pData, ia_ltm** ltm, ia_ltm_input_params** inputParams) {
+    LOGIPC("@%s, pData:%p", __func__, pData);
+    CheckError(!pData, false, "@%s, params is nullptr", __func__);
+    CheckError(!ltm, false, "@%s, ltm is nullptr", __func__);
+    CheckError(!inputParams, false, "@%s, inputParams is nullptr", __func__);
+
+    LtmRunParams* params = static_cast<LtmRunParams*>(pData);
+    LtmRunInputParams* paramsTmp = &params->inputParams;
+    if (params->inputParamsBase.ae_results) {
+        LtmInputAeResult* aeResult = &paramsTmp->aeResult;
+        params->inputParamsBase.ae_results = &aeResult->aeResultsBase;
+        if (aeResult->aeResultsBase.exposures) {
+            aeResult->aeResultsBase.exposures = aeResult->exposures;
+            for (unsigned int i = 0; i < aeResult->aeResultsBase.num_exposures; i++) {
+                if (aeResult->exposures[i].exposure) {
+                    aeResult->exposures[i].exposure = &aeResult->exposure[i];
+                }
+                if (aeResult->exposures[i].sensor_exposure) {
+                    aeResult->exposures[i].sensor_exposure = &aeResult->sensor_exposure[i];
+                }
+                if (aeResult->exposures[i].exposure_plan_ids) {
+                    aeResult->exposures[i].exposure_plan_ids = aeResult->exposure_plan_ids[i];
+                }
+            }
+        }
+
+        if (aeResult->aeResultsBase.weight_grid) {
+            aeResult->aeResultsBase.weight_grid = &aeResult->weightGrid;
+            aeResult->weightGrid.weights = aeResult->weights;
+        }
+
+        if (aeResult->aeResultsBase.flashes) {
+            aeResult->aeResultsBase.flashes = aeResult->flashes;
+        }
+        if (aeResult->aeResultsBase.aperture_control) {
+            aeResult->aeResultsBase.aperture_control = &aeResult->apertureControl;
+        }
+    }
+
+    if (params->inputParamsBase.yv_grid) {
+        params->inputParamsBase.yv_grid = &paramsTmp->yvGrid;
+    }
+
+    if (params->inputParamsBase.rgbs_grid_ptr) {
+        paramsTmp->rgbsGrid.rgbsGridbase.blocks_ptr = paramsTmp->rgbsGrid.blocks;
+        params->inputParamsBase.rgbs_grid_ptr = &paramsTmp->rgbsGrid.rgbsGridbase;
+    }
+
+    if (params->inputParamsBase.hdr_rgbs_grid_ptr) {
+        paramsTmp->hdrRgbsGrid.hdrRgbsGridBase.blocks_ptr = paramsTmp->hdrRgbsGrid.blocks;
+        params->inputParamsBase.hdr_rgbs_grid_ptr = &paramsTmp->hdrRgbsGrid.hdrRgbsGridBase;
+    }
+
+    if (params->inputParamsBase.input_image_ptr) {
+        params->inputParamsBase.input_image_ptr = &paramsTmp->inputImage;
+        if (paramsTmp->inputImage.image_data) {
+            paramsTmp->inputImage.image_data = &paramsTmp->imageData;
+            if (paramsTmp->imageData.size != 0) {
+                const_cast<ia_binary_data*>(paramsTmp->inputImage.image_data)->data =
+                    static_cast<void*>(static_cast<char*>(pData) + sizeof(LtmRunParams));
+            }
+        }
+    }
+
+    if (params->inputParamsBase.gtm_input_params_ptr) {
+        params->inputParamsBase.gtm_input_params_ptr = &paramsTmp->gtmInputParams;
+    }
+
+    *ltm = reinterpret_cast<ia_ltm*>(params->ltm_handle);
+    *inputParams = &params->inputParamsBase;
+
+    return true;
+}
+
+bool IPCIntelLtm::serverFlattenRun(const ia_ltm_results& ltmResults,
+                                   const ia_ltm_drc_params& drcResults, ia_ltm* ltm_handle,
+                                   LtmRunParams* params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    MEMCPY_S(&params->ltmResult, sizeof(ia_ltm_results), &ltmResults, sizeof(ia_ltm_results));
+    MEMCPY_S(&params->drcResult, sizeof(ia_ltm_drc_params), &drcResults, sizeof(ia_ltm_results));
+
+    return true;
+}
+
+bool IPCIntelLtm::clientUnflattenRun(LtmRunParams* params, ia_ltm_results** ltmResults,
+                                     ia_ltm_drc_params** drcResults) {
+    LOGIPC("@%s", __func__);
+    CheckError(!params, false, "@%s, params is nullptr", __func__);
+
+    *ltmResults = &params->ltmResult;
+    *drcResults = &params->drcResult;
+    return true;
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelLtm.h b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelLtm.h
new file mode 100644
index 000000000000..9d5d3c590fc4
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelLtm.h
@@ -0,0 +1,117 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_ltm.h>
+
+#include "IPCCommon.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+struct LtmInitParams {
+    ia_binary_data binary_data;
+    unsigned char data[MAX_IA_BINARY_DATA_SIZE];
+    uintptr_t mkn_handle;
+    uintptr_t results;
+};
+
+struct LtmDeinitParams {
+    uintptr_t ltm_handle;
+};
+
+#define MAX_NUM_EXPOSURES_LTM 10
+#define MAX_NUM_FLASHES_LTM 2
+#define MAX_NUM_OF_EXPOSURE_PLANS_LTM 4
+#define MAX_SIZE_WEIGHT_GRID_LTM 2048
+
+struct LtmInputAeResult {
+    ia_aiq_ae_results aeResultsBase;
+
+    ia_aiq_ae_exposure_result exposures[MAX_NUM_EXPOSURES_LTM];
+    ia_aiq_hist_weight_grid weightGrid;
+    ia_aiq_flash_parameters flashes[MAX_NUM_FLASHES_LTM];
+    ia_aiq_aperture_control apertureControl;
+
+    // the below is in ia_aiq_ae_exposure_result exposures[MAX_NUM_EXPOSURES];
+    ia_aiq_exposure_parameters exposure[MAX_NUM_EXPOSURES_LTM];
+    ia_aiq_exposure_sensor_parameters sensor_exposure[MAX_NUM_EXPOSURES_LTM];
+    unsigned int exposure_plan_ids[MAX_NUM_EXPOSURES_LTM][MAX_NUM_OF_EXPOSURE_PLANS_LTM];
+
+    // the below is in ia_aiq_hist_weight_grid weight_grid;
+    unsigned char weights[MAX_SIZE_WEIGHT_GRID_LTM];
+};
+
+#define MAX_WIDTH_STATISTICS 24
+#define MAX_HEIGHT_STATISTICS 16
+struct LtmInputRgbSGrid {
+    ia_aiq_rgbs_grid rgbsGridbase;
+    rgbs_grid_block blocks[MAX_WIDTH_STATISTICS * MAX_HEIGHT_STATISTICS];
+};
+
+#define BXT_RGBS_GRID_MAX_WIDTH 96
+#define BXT_RGBS_GRID_MAX_HEIGHT 72
+struct LtmInputHdrRgbsGrid {
+    ia_aiq_hdr_rgbs_grid hdrRgbsGridBase;
+    hdr_rgbs_grid_block blocks[BXT_RGBS_GRID_MAX_WIDTH * BXT_RGBS_GRID_MAX_HEIGHT];
+};
+
+struct LtmRunInputParams {
+    LtmInputAeResult aeResult;
+    ia_isp_bxt_hdr_yv_grid_t yvGrid;
+    LtmInputRgbSGrid rgbsGrid;
+    LtmInputHdrRgbsGrid hdrRgbsGrid;
+    ia_ltm_input_image inputImage;
+    ia_binary_data imageData;
+    ia_ltm_gtm_input_params gtmInputParams;
+};
+
+struct LtmRunParams {
+    uintptr_t ltm_handle;
+    ia_ltm_input_params inputParamsBase;
+    LtmRunInputParams inputParams;
+
+    ia_ltm_results ltmResult;
+    ia_ltm_drc_params drcResult;
+};
+
+class IPCIntelLtm {
+ public:
+    IPCIntelLtm();
+    virtual ~IPCIntelLtm();
+
+    // for init
+    bool clientFlattenInit(const ia_binary_data& inData, uintptr_t mkn_hanlde,
+                           LtmInitParams* params);
+    bool serverUnflattenInit(LtmInitParams* params, ia_binary_data* inData, uintptr_t* mkn_hanlde);
+    bool serverFlattenInit(LtmInitParams* params, ia_ltm* ltm_handle);
+    bool clientUnflattenInit(LtmInitParams* params, ia_ltm** ltm_handle);
+
+    // for run
+    bool clientFlattenRun(uintptr_t ltm, const ia_ltm_input_params& ltmParams, int imageDataSize,
+                          void* paramsAddr);
+    bool serverUnflattenRun(void* pData, ia_ltm** ltm, ia_ltm_input_params** inputParams);
+    bool serverFlattenRun(const ia_ltm_results& ltmResults, const ia_ltm_drc_params& drcResults,
+                          ia_ltm* ltm_handle, LtmRunParams* params);
+    bool clientUnflattenRun(LtmRunParams* params, ia_ltm_results** ltmResults,
+                            ia_ltm_drc_params** drcResults);
+
+ private:
+    int deepCopyAeResults(const ia_aiq_ae_results& src, LtmInputAeResult* params);
+    int deepCopyRgbsGridData(const ia_aiq_rgbs_grid& src, ia_aiq_rgbs_grid* dst);
+    int deepCopyHdrRgbsGridData(const ia_aiq_hdr_rgbs_grid& src, ia_aiq_hdr_rgbs_grid* dst);
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelMkn.cpp b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelMkn.cpp
new file mode 100644
index 000000000000..f247fdec94bc
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelMkn.cpp
@@ -0,0 +1,79 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IPC_INTEL_MKN"
+
+#include "modules/sandboxing/IPCIntelMkn.h"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Errors.h"
+
+namespace icamera {
+IPCIntelMkn::IPCIntelMkn() {
+    LOGIPC("@%s", __func__);
+}
+
+IPCIntelMkn::~IPCIntelMkn() {
+    LOGIPC("@%s", __func__);
+}
+
+bool IPCIntelMkn::clientFlattenInit(ia_mkn_config_bits mkn_config_bits, size_t mkn_section_1_size,
+                                    size_t mkn_section_2_size, MknInitParams* params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(params == nullptr, false, "@%s, params is nullptr", __func__);
+
+    params->mkn_config_bits = mkn_config_bits;
+    params->mkn_section_1_size = mkn_section_1_size;
+    params->mkn_section_2_size = mkn_section_2_size;
+
+    return true;
+}
+
+bool IPCIntelMkn::clientFlattenPrepare(uintptr_t mkn, ia_mkn_trg data_target,
+                                       MknPrepareParams* params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(params == nullptr, false, "@%s, params is nullptr", __func__);
+
+    params->mkn_handle = mkn;
+    params->data_target = data_target;
+
+    return true;
+}
+
+bool IPCIntelMkn::clientUnflattenPrepare(MknPrepareParams* params, ia_binary_data* mknData) {
+    LOGIPC("@%s, mknData:%p", __func__, mknData);
+    CheckError(params == nullptr, false, "@%s, params is nullptr", __func__);
+    CheckError(mknData == nullptr, false, "@%s, mknData is nullptr", __func__);
+
+    params->results.data = static_cast<void*>(params->data);
+    *mknData = params->results;
+
+    LOGIPC("@%s, mknData.size:%d", __func__, mknData->size);
+
+    return true;
+}
+
+bool IPCIntelMkn::serverFlattenPrepare(const ia_binary_data& inData, MknPrepareParams* params) {
+    LOGIPC("@%s, params:%p", __func__, params);
+    CheckError(params == nullptr, false, "@%s, params is nullptr", __func__);
+    CheckError(sizeof(params->data) < inData.size, false, "@%s, buffer is small", __func__);
+
+    params->results.size = inData.size;
+    MEMCPY_S(params->data, sizeof(params->data), inData.data, inData.size);
+
+    return true;
+}
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelMkn.h b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelMkn.h
new file mode 100644
index 000000000000..0e714a34afe7
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelMkn.h
@@ -0,0 +1,62 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_mkn_encoder.h>
+
+#include "IPCCommon.h"
+#include "iutils/Utils.h"
+
+struct MknInitParams {
+    ia_mkn_config_bits mkn_config_bits;
+    size_t mkn_section_1_size;
+    size_t mkn_section_2_size;
+    uintptr_t results;
+};
+
+struct MknPrepareParams {
+    uintptr_t mkn_handle;
+    ia_mkn_trg data_target;
+    ia_binary_data results;
+    char data[MAKERNOTE_SECTION1_SIZE + MAKERNOTE_SECTION2_SIZE];
+};
+
+struct MknDeinitParams {
+    uintptr_t mkn_handle;
+};
+
+struct MknEnableParams {
+    uintptr_t mkn_handle;
+    bool enable_data_collection;
+};
+
+namespace icamera {
+class IPCIntelMkn {
+ public:
+    IPCIntelMkn();
+    virtual ~IPCIntelMkn();
+
+    // for init
+    bool clientFlattenInit(ia_mkn_config_bits mkn_config_bits, size_t mkn_section_1_size,
+                           size_t mkn_section_2_size, MknInitParams* params);
+
+    // for prepare
+    bool clientFlattenPrepare(uintptr_t mkn, ia_mkn_trg data_target, MknPrepareParams* params);
+    bool clientUnflattenPrepare(MknPrepareParams* params, ia_binary_data* mknData);
+    bool serverFlattenPrepare(const ia_binary_data& inData, MknPrepareParams* results);
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelPGParam.cpp b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelPGParam.cpp
new file mode 100644
index 000000000000..68ebaf957f9a
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelPGParam.cpp
@@ -0,0 +1,543 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IPCIntelPGParam"
+
+#include "modules/sandboxing/IPCIntelPGParam.h"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+bool IPCIntelPGParam::clientFlattenInit(void* pData, int dataSize, int pgId, uintptr_t client,
+                                        ia_p2p_platform_t platform,
+                                        const PgConfiguration& pgConfig) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_init_params), false, "@%s, dataSize is small", __func__);
+
+    pg_param_init_params* params = static_cast<pg_param_init_params*>(pData);
+    CheckError(pgConfig.pgManifestSize > sizeof(params->pgManifestData), false,
+               "@%s, manifest buffer is small", __func__);
+
+    params->pgId = pgId;
+    params->client = client;
+    params->platform = platform;
+
+    params->pgConfig.fragmentCount = pgConfig.fragmentCount;
+    params->pgConfig.inputMainFrame = pgConfig.inputMainFrame;
+    params->pgConfig.outputMainFrame = pgConfig.outputMainFrame;
+
+    params->pgConfig.pgManifestSize = pgConfig.pgManifestSize;
+    MEMCPY_S(params->pgManifestData, sizeof(params->pgManifestData), pgConfig.pgManifest,
+             pgConfig.pgManifestSize);
+
+    params->disableDataTermialsCount = pgConfig.disableDataTermials.size();
+    CheckError(params->disableDataTermialsCount > IPU_MAX_TERMINAL_COUNT, false,
+               "@%s, disableDataTermials is big", __func__);
+    memset(params->disableDataTermialsData, -1, sizeof(params->disableDataTermialsData));
+    for (int i = 0; i < params->disableDataTermialsCount; i++) {
+        params->disableDataTermialsData[i] = pgConfig.disableDataTermials[i];
+    }
+
+    return true;
+}
+
+bool IPCIntelPGParam::serverUnflattenInit(void* pData, int dataSize, int* pgId, uintptr_t* client,
+                                          ia_p2p_platform_t* platform, PgConfiguration* pgConfig) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_init_params), false, "@%s, buffer is small", __func__);
+    CheckError(!pgId || !client, false, "@%s, nullptr client", __func__);
+    CheckError(!platform || !pgConfig, false, "@%s, nullptr config", __func__);
+
+    pg_param_init_params* params = static_cast<pg_param_init_params*>(pData);
+    *pgId = params->pgId;
+    *client = params->client;
+    *platform = params->platform;
+
+    pgConfig->fragmentCount = params->pgConfig.fragmentCount;
+    pgConfig->inputMainFrame = params->pgConfig.inputMainFrame;
+    pgConfig->outputMainFrame = params->pgConfig.outputMainFrame;
+    pgConfig->pgManifestSize = params->pgConfig.pgManifestSize;
+    void* dataPtr = static_cast<void*>(params->pgManifestData);
+    pgConfig->pgManifest = static_cast<ia_css_program_group_manifest_t*>(dataPtr);
+
+    pgConfig->disableDataTermials.clear();
+    CheckError(params->disableDataTermialsCount > IPU_MAX_TERMINAL_COUNT, false,
+               "@%s, disableDataTermials is big", __func__);
+    for (int i = 0; i < params->disableDataTermialsCount; i++) {
+        pgConfig->disableDataTermials.push_back(params->disableDataTermialsData[i]);
+    }
+
+    return true;
+}
+
+bool IPCIntelPGParam::clientFlattenPrepare(void* pData, int dataSize, uintptr_t client,
+                                           unsigned int ipuParamSize, int32_t ipuParamHandle,
+                                           const ia_css_rbm_t* rbm) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_prepare_params), false, "@%s, buffer is small", __func__);
+
+    pg_param_prepare_params* params = static_cast<pg_param_prepare_params*>(pData);
+    params->client = client;
+    params->ipuParamSize = ipuParamSize;
+    params->ipuParamHandle = ipuParamHandle;
+
+    if (rbm) {
+        params->rbm = &params->rbmData;
+        MEMCPY_S(&params->rbmData, sizeof(params->rbmData), rbm, sizeof(*rbm));
+    } else {
+        params->rbm = nullptr;
+    }
+
+    return true;
+}
+
+bool IPCIntelPGParam::serverUnflattenPrepare(void* pData, int dataSize, uintptr_t* client,
+                                             void* palDataAddr, ia_binary_data* ipuParameters,
+                                             ia_css_rbm_t** rbm, ia_css_kernel_bitmap_t** bitmap,
+                                             uint32_t** maxStatsSize) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_prepare_params), false, "@%s, buffer is small", __func__);
+    CheckError(!client, false, "@%s, nullptr client", __func__);
+    CheckError(!ipuParameters || !rbm || !bitmap, false, "@%s, nullptr outputs", __func__);
+    CheckError(!palDataAddr, false, "%s, palDataAddr is nullptr", __func__);
+
+    pg_param_prepare_params* params = static_cast<pg_param_prepare_params*>(pData);
+    *client = params->client;
+    ipuParameters->size = params->ipuParamSize;
+    ipuParameters->data = palDataAddr;
+    *rbm = params->rbm ? &params->rbmData : nullptr;
+    *bitmap = &params->bitmapData;
+    if (maxStatsSize) *maxStatsSize = &params->maxStatsSize;
+    return true;
+}
+
+bool IPCIntelPGParam::clientUnflattenPrepare(const void* pData, int dataSize,
+                                             ia_css_kernel_bitmap_t* bitmap,
+                                             uint32_t* maxStatsSize) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_prepare_params), false, "@%s, buffer is small", __func__);
+    CheckError(!bitmap, false, "@%s, bitmap is nullptr", __func__);
+
+    const pg_param_prepare_params* params = static_cast<const pg_param_prepare_params*>(pData);
+    MEMCPY_S(bitmap, sizeof(*bitmap), &params->bitmapData, sizeof(params->bitmapData));
+
+    if (maxStatsSize) *maxStatsSize = params->maxStatsSize;
+    return true;
+}
+
+int IPCIntelPGParam::getTotalPGBufferSize(int pgSize) {
+    int size = sizeof(pg_param_allocate_pg_params);
+    size += PAGE_ALIGN(pgSize);
+    return size;
+}
+
+bool IPCIntelPGParam::assignPGBuffer(void* pData, int dataSize, int pgSize, void** pgBuffer) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < pgSize, false, "@%s, buffer is small", __func__);
+    CheckError(!pgBuffer, false, "@%s, payloads is nullptr", __func__);
+
+    uintptr_t pgAddr = reinterpret_cast<uintptr_t>(pData);
+    CheckError(pgAddr & ((getpagesize() - 1)), false, "@%s, pg addr is not aligned", __func__);
+    *pgBuffer = pData;
+    return true;
+}
+
+bool IPCIntelPGParam::clientFlattenAllocatePGBuffer(void* pData, int dataSize, uintptr_t client,
+                                                    int pgSize) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    uintptr_t paramAddr =
+        reinterpret_cast<uintptr_t>(pData) + dataSize - sizeof(pg_param_allocate_pg_params);
+    CheckError(paramAddr < reinterpret_cast<uintptr_t>(pData) + pgSize, false,
+               "@%s, dataSize is small", __func__);
+
+    pg_param_allocate_pg_params* params = reinterpret_cast<pg_param_allocate_pg_params*>(paramAddr);
+    params->client = client;
+    params->pgSize = pgSize;
+
+    return true;
+}
+
+bool IPCIntelPGParam::serverUnflattenAllocatePGBuffer(const void* pData, int dataSize,
+                                                      uintptr_t* client, int* pgSize) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_allocate_pg_params), false, "@%s, buffer is small",
+               __func__);
+    CheckError(!client || !pgSize, false, "@%s, nullptr input", __func__);
+
+    uintptr_t paramAddr =
+        reinterpret_cast<uintptr_t>(pData) + dataSize - sizeof(pg_param_allocate_pg_params);
+    pg_param_allocate_pg_params* params = reinterpret_cast<pg_param_allocate_pg_params*>(paramAddr);
+    CheckError(paramAddr < reinterpret_cast<uintptr_t>(pData) + params->pgSize, false,
+               "@%s, dataSize is small", __func__);
+    *client = params->client;
+    *pgSize = params->pgSize;
+
+    return true;
+}
+
+bool IPCIntelPGParam::clientFlattenGetFragDescs(void* pData, int dataSize, uintptr_t client,
+                                                int descCount) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_get_fragment_desc_params), false,
+               "@%s, dataSize is small", __func__);
+
+    pg_param_get_fragment_desc_params* params =
+        static_cast<pg_param_get_fragment_desc_params*>(pData);
+    CheckError(descCount > sizeof(params->descsData), false, "@%s, descCount is big", __func__);
+
+    params->client = client;
+    params->descCount = descCount;
+    return true;
+}
+
+bool IPCIntelPGParam::serverUnflattenGetFragDescs(void* pData, int dataSize, uintptr_t* client,
+                                                  int* descCount, ia_p2p_fragment_desc** descs) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_get_fragment_desc_params), false,
+               "@%s, dataSize is small", __func__);
+    CheckError(!client, false, "@%s, nullptr client", __func__);
+    CheckError(!descCount || !descs, false, "@%s, nullptr outputs", __func__);
+
+    pg_param_get_fragment_desc_params* params =
+        static_cast<pg_param_get_fragment_desc_params*>(pData);
+    *client = params->client;
+    *descCount = params->descCount;
+    *descs = params->descsData;
+    return true;
+}
+
+bool IPCIntelPGParam::serverFlattenGetFragDescs(void* pData, int dataSize, int count) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_get_fragment_desc_params), false,
+               "@%s, dataSize is small", __func__);
+
+    pg_param_get_fragment_desc_params* params =
+        static_cast<pg_param_get_fragment_desc_params*>(pData);
+    params->returnCount = count;
+    return true;
+}
+
+bool IPCIntelPGParam::clientUnflattenGetFragDescs(const void* pData, int dataSize, int* count,
+                                                  ia_p2p_fragment_desc* descs) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_get_fragment_desc_params), false,
+               "@%s, dataSize is small", __func__);
+    CheckError(!count || !descs, false, "@%s, nullptr outputs", __func__);
+
+    const pg_param_get_fragment_desc_params* params =
+        static_cast<const pg_param_get_fragment_desc_params*>(pData);
+    *count = params->returnCount;
+    MEMCPY_S(descs, sizeof(ia_p2p_fragment_desc) * params->descCount, params->descsData,
+             sizeof(params->descsData));
+    return true;
+}
+
+bool IPCIntelPGParam::clientFlattenPrepareProgram(void* pData, int dataSize, uintptr_t client) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_prepare_program_params), false, "@%s, dataSize is small",
+               __func__);
+
+    pg_param_prepare_program_params* params = static_cast<pg_param_prepare_program_params*>(pData);
+    params->client = client;
+    return true;
+}
+
+bool IPCIntelPGParam::serverUnflattenPrepareProgram(const void* pData, int dataSize,
+                                                    uintptr_t* client) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_prepare_program_params), false, "@%s, dataSize is small",
+               __func__);
+    CheckError(!client, false, "@%s, nullptr client", __func__);
+
+    const pg_param_prepare_program_params* params =
+        static_cast<const pg_param_prepare_program_params*>(pData);
+    *client = params->client;
+    return true;
+}
+
+bool IPCIntelPGParam::serverFlattenPrepareProgram(void* pData, int dataSize, int payloadCount,
+                                                  const ia_binary_data* payloads) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_prepare_program_params), false, "@%s, dataSize is small",
+               __func__);
+    CheckError(payloadCount > IPU_MAX_TERMINAL_COUNT, false, "@%s, payloadCount is big", __func__);
+    CheckError(!payloads, false, "@%s, payloads is nullptr", __func__);
+
+    pg_param_prepare_program_params* params = static_cast<pg_param_prepare_program_params*>(pData);
+    params->payloadCount = payloadCount;
+    MEMCPY_S(params->payloads, sizeof(ia_binary_data) * payloadCount, payloads,
+             sizeof(ia_binary_data) * payloadCount);
+    return true;
+}
+
+bool IPCIntelPGParam::clientUnflattenPrepareProgram(const void* pData, int dataSize,
+                                                    int* payloadCount, ia_binary_data* payloads) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_prepare_program_params), false, "@%s, dataSize is small",
+               __func__);
+    CheckError(!payloads, false, "@%s, payloads is nullptr", __func__);
+
+    const pg_param_prepare_program_params* params =
+        static_cast<const pg_param_prepare_program_params*>(pData);
+
+    CheckError(*payloadCount < params->payloadCount, false, "@%s, payloadCount is small", __func__);
+    *payloadCount = params->payloadCount;
+    for (int i = 0; i < params->payloadCount; i++) {
+        payloads[i].size = params->payloads[i].size;
+        payloads[i].data = nullptr;
+    }
+    return true;
+}
+
+int IPCIntelPGParam::getTotalPayloadSize(int payloadCount, const ia_binary_data* payloads) {
+    CheckError(!payloads, 0, "@%s, payloads is nullptr", __func__);
+    int size = sizeof(pg_param_allocate_payloads_params);
+    for (int i = 0; i < payloadCount; i++) {
+        if (payloads[i].size) {
+            size += PAGE_ALIGN(payloads[i].size);
+        }
+    }
+    return size;
+}
+
+bool IPCIntelPGParam::assignPayloads(void* pData, int dataSize, int payloadCount,
+                                     ia_binary_data* payloads) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(!payloads, false, "@%s, payloads is nullptr", __func__);
+
+    uintptr_t payloadAddr = reinterpret_cast<uintptr_t>(pData);
+    CheckError(payloadAddr & ((getpagesize() - 1)), false, "@%s, payload addr is not aligned",
+               __func__);
+    for (int i = 0; i < payloadCount; i++) {
+        if (payloads[i].size > 0) {
+            CheckError(payloadAddr > reinterpret_cast<uintptr_t>(pData) + dataSize, false,
+                       "@%s, buffer is small", __func__);
+            payloads[i].data = reinterpret_cast<void*>(payloadAddr);
+            payloadAddr += PAGE_ALIGN(payloads[i].size);
+        }
+    }
+    return true;
+}
+
+bool IPCIntelPGParam::clientFlattenAllocatePayloads(void* pData, int dataSize, uintptr_t client,
+                                                    int payloadCount,
+                                                    const ia_binary_data* payloads) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_allocate_payloads_params), false,
+               "@%s, dataSize is small", __func__);
+    CheckError(!payloads, false, "@%s, payloads is nullptr", __func__);
+
+    uintptr_t paramAddr =
+        reinterpret_cast<uintptr_t>(pData) + dataSize - sizeof(pg_param_allocate_payloads_params);
+    pg_param_allocate_payloads_params* params =
+        reinterpret_cast<pg_param_allocate_payloads_params*>(paramAddr);
+    params->client = client;
+    params->payloadCount = payloadCount;
+    for (int i = 0; i < payloadCount; i++) {
+        params->payloads[i].size = payloads[i].size;
+        params->payloads[i].data = payloads[i].data;
+        CheckError(reinterpret_cast<uintptr_t>(payloads[i].data) > paramAddr, false,
+                   "@%s, buffer is small", __func__);
+    }
+
+    return true;
+}
+
+bool IPCIntelPGParam::serverUnflattenAllocatePayloads(void* pData, int dataSize, uintptr_t* client,
+                                                      int* payloadCount,
+                                                      ia_binary_data** payloads) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_allocate_payloads_params), false,
+               "@%s, dataSize is small", __func__);
+    CheckError(!client, false, "@%s, nullptr client", __func__);
+    CheckError(!payloads, false, "@%s, payloads is nullptr", __func__);
+    CheckError(!payloadCount || !payloads, false, "@%s, nullptr outputs", __func__);
+
+    uintptr_t paramAddr =
+        reinterpret_cast<uintptr_t>(pData) + dataSize - sizeof(pg_param_allocate_payloads_params);
+    pg_param_allocate_payloads_params* params =
+        reinterpret_cast<pg_param_allocate_payloads_params*>(paramAddr);
+    *client = params->client;
+    *payloadCount = params->payloadCount;
+    *payloads = params->payloads;  // Copy directly and ignore client pointers
+
+    return true;
+}
+
+bool IPCIntelPGParam::getPayloadOffsets(const void* pData, int dataSize, int payloadCount,
+                                        const ia_binary_data* payloads, int32_t* payloadOffsets) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(!payloads, false, "@%s, nullptr payloads", __func__);
+    CheckError(!payloadOffsets, false, "@%s, nullptr payloadOffsets", __func__);
+
+    uintptr_t startAddr = reinterpret_cast<uintptr_t>(pData);
+    for (int i = 0; i < payloadCount; i++) {
+        if (payloads[i].size <= 0) continue;
+
+        int32_t offset = reinterpret_cast<uintptr_t>(payloads[i].data) - startAddr;
+        CheckError((offset > (dataSize - payloads[i].size) || offset < 0), false,
+                   "@%s, error offset %d", __func__, offset);
+        payloadOffsets[i] = offset;
+    }
+    return 0;
+}
+
+bool IPCIntelPGParam::getPayloadData(void* pData, int dataSize, int payloadCount,
+                                     const int32_t* payloadOffsets, ia_binary_data* payloads) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(!payloads, false, "@%s, nullptr payloads", __func__);
+    CheckError(!payloadOffsets, false, "@%s, nullptr payloadOffsets", __func__);
+
+    unsigned char* startAddr = reinterpret_cast<unsigned char*>(pData);
+    for (int i = 0; i < payloadCount; i++) {
+        if (payloads[i].size <= 0) continue;
+
+        CheckError(payloadOffsets[i] > dataSize - payloads[i].size, false, "@%s, error offset %d",
+                   __func__, payloadOffsets[i]);
+        payloads[i].data = startAddr + payloadOffsets[i];
+    }
+    return true;
+}
+
+bool IPCIntelPGParam::clientFlattenEncode(void* pData, int dataSize, uintptr_t client,
+                                          unsigned int ipuParamSize, int32_t ipuParamHandle,
+                                          int32_t payloadCount, const ia_binary_data* payloads,
+                                          void* payloadMemory, int payloadMemorySize) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_encode_params), false, "@%s, buffer is small", __func__);
+    CheckError(!payloads, false, "@%s, nullptr payloads", __func__);
+    CheckError(payloadCount > IPU_MAX_TERMINAL_COUNT, false, "@%s, wrong payloadCount", __func__);
+    CheckError(!payloadMemory, false, "@%s, wrong payloadMemory", __func__);
+
+    pg_param_encode_params* params = static_cast<pg_param_encode_params*>(pData);
+    params->client = client;
+    params->ipuParamSize = ipuParamSize;
+    params->ipuParamHandle = ipuParamHandle;
+    params->payloadCount = payloadCount;
+    MEMCPY_S(params->payloads, sizeof(params->payloads), payloads,
+             sizeof(ia_binary_data) * payloadCount);
+    getPayloadOffsets(payloadMemory, payloadMemorySize, payloadCount, payloads,
+                      params->payloadOffsets);
+    return true;
+}
+
+bool IPCIntelPGParam::serverUnflattenEncode(void* pData, int dataSize, uintptr_t* client,
+                                            void* palDataAddr, ia_binary_data* ipuParameters,
+                                            int32_t* payloadCount, ia_binary_data** payloads,
+                                            int32_t** payloadOffsets) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_encode_params), false, "@%s, small dataSize", __func__);
+    CheckError(!client, false, "@%s, nullptr client", __func__);
+    CheckError(!ipuParameters, false, "@%s, nullptr output", __func__);
+    CheckError(!palDataAddr, false, "%s, palDataAddr is nullptr", __func__);
+    CheckError(!payloadCount, false, "%s, payloadCount is nullptr", __func__);
+    CheckError(!payloads, false, "%s, payloads is nullptr", __func__);
+    CheckError(!payloadOffsets, false, "%s, payloadOffsets is nullptr", __func__);
+
+    pg_param_encode_params* params = static_cast<pg_param_encode_params*>(pData);
+    *client = params->client;
+    ipuParameters->size = params->ipuParamSize;
+    ipuParameters->data = palDataAddr;
+    *payloadCount = params->payloadCount;
+    *payloads = params->payloads;
+    *payloadOffsets = params->payloadOffsets;
+    return true;
+}
+
+bool IPCIntelPGParam::clientFlattenDecode(void* pData, int dataSize, uintptr_t client,
+                                          int32_t payloadCount, const ia_binary_data* payloads,
+                                          void* payloadMemory, int payloadMemorySize,
+                                          int32_t statsHandle) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_decode_params), false, "@%s, size is small", __func__);
+    CheckError(!payloads, false, "@%s, nullptr payloads", __func__);
+    CheckError(payloadCount > IPU_MAX_TERMINAL_COUNT, false, "@%s, wrong payloadCount", __func__);
+    CheckError(!payloadMemory, false, "@%s, wrong payloadMemory", __func__);
+
+    pg_param_decode_params* params = static_cast<pg_param_decode_params*>(pData);
+    params->client = client;
+    params->payloadCount = payloadCount;
+    MEMCPY_S(params->payloads, sizeof(params->payloads), payloads,
+             sizeof(ia_binary_data) * payloadCount);
+    getPayloadOffsets(payloadMemory, payloadMemorySize, payloadCount, payloads,
+                      params->payloadOffsets);
+    params->clientStatsHandle = statsHandle;
+    return true;
+}
+
+bool IPCIntelPGParam::serverUnflattenDecode(void* pData, int dataSize, uintptr_t* client,
+                                            int32_t* payloadCount, ia_binary_data** payloads,
+                                            int32_t** payloadOffsets) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_decode_params), false, "@%s, size is small", __func__);
+    CheckError(!client, false, "@%s, nullptr client", __func__);
+    CheckError(!payloadCount, false, "%s, payloadCount is nullptr", __func__);
+    CheckError(!payloads, false, "%s, payloads is nullptr", __func__);
+    CheckError(!payloadOffsets, false, "%s, payloadOffsets is nullptr", __func__);
+
+    pg_param_decode_params* params = static_cast<pg_param_decode_params*>(pData);
+    *client = params->client;
+    *payloadCount = params->payloadCount;
+    *payloads = params->payloads;
+    *payloadOffsets = params->payloadOffsets;
+    return true;
+}
+
+bool IPCIntelPGParam::serverFlattenDecode(void* pData, int dataSize,
+                                          const ia_binary_data& statistics) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_decode_params), false, "@%s, size is small", __func__);
+
+    pg_param_decode_params* params = static_cast<pg_param_decode_params*>(pData);
+    params->clientStatsSize = statistics.size;
+    return true;
+}
+
+bool IPCIntelPGParam::clientUnflattenDecode(void* pData, int dataSize, ia_binary_data* statistics) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_decode_params), false, "@%s, size is small", __func__);
+    CheckError(!statistics, false, "@%s, statistics is nullptr", __func__);
+
+    pg_param_decode_params* params = static_cast<pg_param_decode_params*>(pData);
+    statistics->size = params->clientStatsSize;
+    return true;
+}
+
+bool IPCIntelPGParam::clientFlattenDeinit(void* pData, int dataSize, uintptr_t client) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_deinit_params), false, "@%s, buffer is small", __func__);
+
+    pg_param_deinit_params* params = static_cast<pg_param_deinit_params*>(pData);
+    params->client = client;
+
+    return true;
+}
+
+bool IPCIntelPGParam::serverUnflattenDeinit(const void* pData, int dataSize, uintptr_t* client) {
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pg_param_deinit_params), false, "@%s, size is small", __func__);
+    CheckError(!client, false, "@%s, nullptr client", __func__);
+
+    const pg_param_deinit_params* params = static_cast<const pg_param_deinit_params*>(pData);
+    *client = params->client;
+
+    return true;
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelPGParam.h b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelPGParam.h
new file mode 100644
index 000000000000..f69cd29d9df1
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIntelPGParam.h
@@ -0,0 +1,180 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+
+#include "modules/sandboxing/IPCCommon.h"
+#include "src/core/psysprocessor/PGUtils.h"
+
+namespace icamera {
+
+#define MAX_PROCESS_GROUP_SIZE 8192
+#define MAX_PAL_SIZE 0x800000  // 8M
+#define MAX_STATISTICS_SIZE MAX_IA_BINARY_DATA_SIZE
+
+struct pg_param_init_params {
+    int pgId;
+    uintptr_t client;
+    ia_p2p_platform_t platform;
+    PgConfiguration pgConfig;
+    unsigned char pgManifestData[MAX_PROCESS_GROUP_SIZE];
+    int disableDataTermialsData[IPU_MAX_TERMINAL_COUNT];
+    int disableDataTermialsCount;
+};
+
+struct pg_param_prepare_params {
+    uintptr_t client;
+    uint32_t ipuParamSize;
+    int32_t ipuParamHandle;
+    ia_css_rbm_t* rbm;
+    ia_css_rbm_t rbmData;
+
+    // Output
+    ia_css_kernel_bitmap_t bitmapData;
+    uint32_t maxStatsSize;
+};
+
+// Shared memory: pg + struct
+// as pg memory addr should be page size aligned
+struct pg_param_allocate_pg_params {
+    uintptr_t client;
+    int pgSize;
+};
+
+struct pg_param_get_fragment_desc_params {
+    uintptr_t client;
+    int descCount;
+
+    // Output
+    int returnCount;
+    ia_p2p_fragment_desc descsData[IPU_MAX_TERMINAL_COUNT * IA_P2P_MAX_FRAGMENTS];
+};
+
+struct pg_param_prepare_program_params {
+    uintptr_t client;
+
+    // Output
+    int payloadCount;
+    ia_binary_data payloads[IPU_MAX_TERMINAL_COUNT];  // save size of payloads
+};
+
+// Shared memory: payloads + struct
+// as payload memory addr should be page size aligned
+struct pg_param_allocate_payloads_params {
+    uintptr_t client;
+    int payloadCount;
+    ia_binary_data payloads[IPU_MAX_TERMINAL_COUNT];  // save size of payloads
+};
+
+struct pg_param_encode_params {
+    uintptr_t client;
+    uint32_t ipuParamSize;
+    int32_t ipuParamHandle;
+    int32_t payloadCount;
+    ia_binary_data payloads[IPU_MAX_TERMINAL_COUNT];
+    int32_t payloadOffsets[IPU_MAX_TERMINAL_COUNT];
+};
+
+struct pg_param_decode_params {
+    uintptr_t client;
+    int32_t payloadCount;
+    ia_binary_data payloads[IPU_MAX_TERMINAL_COUNT];
+    int32_t payloadOffsets[IPU_MAX_TERMINAL_COUNT];
+    uint32_t clientStatsSize;
+    int32_t clientStatsHandle;
+
+    // Output
+    uint32_t statsSize;
+};
+
+struct pg_param_deinit_params {
+    uintptr_t client;
+};
+
+class IPCIntelPGParam {
+ public:
+    IPCIntelPGParam() {}
+    virtual ~IPCIntelPGParam() {}
+
+    bool clientFlattenInit(void* pData, int dataSize, int pgId, uintptr_t client,
+                           ia_p2p_platform_t platform, const PgConfiguration& pgConfig);
+    bool serverUnflattenInit(void* pData, int dataSize, int* pgId, uintptr_t* client,
+                             ia_p2p_platform_t* platform, PgConfiguration* pgConfig);
+
+    bool clientFlattenPrepare(void* pData, int dataSize, uintptr_t client,
+                              unsigned int ipuParamSize, int32_t ipuParamHandle,
+                              const ia_css_rbm_t* rbm);
+    bool serverUnflattenPrepare(void* pData, int dataSize, uintptr_t* client, void* palDataAddr,
+                                ia_binary_data* ipuParameters, ia_css_rbm_t** rbm,
+                                ia_css_kernel_bitmap_t** bitmap, uint32_t** maxStatsSize);
+    bool clientUnflattenPrepare(const void* pData, int dataSize, ia_css_kernel_bitmap_t* bitmap,
+                                uint32_t* maxStatsSize = nullptr);
+
+    int getTotalPGBufferSize(int pgSize);
+    bool assignPGBuffer(void* pData, int dataSize, int pgSize, void** pgBuffer);
+    bool clientFlattenAllocatePGBuffer(void* pData, int dataSize, uintptr_t client, int pgSize);
+    bool serverUnflattenAllocatePGBuffer(const void* pData, int dataSize, uintptr_t* client,
+                                         int* pgSize);
+
+    bool clientFlattenGetFragDescs(void* pData, int dataSize, uintptr_t client, int descCount);
+    bool serverUnflattenGetFragDescs(void* pData, int dataSize, uintptr_t* client, int* descCount,
+                                     ia_p2p_fragment_desc** descs);
+    bool serverFlattenGetFragDescs(void* pData, int dataSize, int count);
+    bool clientUnflattenGetFragDescs(const void* pData, int dataSize, int* count,
+                                     ia_p2p_fragment_desc* descs);
+
+    bool clientFlattenPrepareProgram(void* pData, int dataSize, uintptr_t client);
+    bool serverUnflattenPrepareProgram(const void* pData, int dataSize, uintptr_t* client);
+    bool serverFlattenPrepareProgram(void* pData, int dataSize, int payloadCount,
+                                     const ia_binary_data* payloads);
+    bool clientUnflattenPrepareProgram(const void* pData, int dataSize, int* payloadCount,
+                                       ia_binary_data* payloads);
+
+    int getTotalPayloadSize(int payloadCount, const ia_binary_data* payloads);
+    bool assignPayloads(void* pData, int dataSize, int payloadCount, ia_binary_data* payloads);
+    bool clientFlattenAllocatePayloads(void* pData, int dataSize, uintptr_t client,
+                                       int payloadCount, const ia_binary_data* payloads);
+    bool serverUnflattenAllocatePayloads(void* pData, int dataSize, uintptr_t* client,
+                                         int* payloadCount, ia_binary_data** payloads);
+
+    bool getPayloadOffsets(const void* pData, int dataSize, int payloadCount,
+                           const ia_binary_data* payloads, int32_t* payloadOffsets);
+    bool getPayloadData(void* pData, int dataSize, int payloadCount, const int32_t* payloadOffsets,
+                        ia_binary_data* payloads);
+
+    bool clientFlattenEncode(void* pData, int dataSize, uintptr_t client, unsigned int ipuParamSize,
+                             int32_t ipuParamHandle, int32_t payloadCount,
+                             const ia_binary_data* payloads, void* payloadMemory,
+                             int payloadMemorySize);
+    bool serverUnflattenEncode(void* pData, int dataSize, uintptr_t* client, void* palDataAddr,
+                               ia_binary_data* ipuParameters, int32_t* payloadCount,
+                               ia_binary_data** payloads, int32_t** payloadOffsets);
+
+    bool clientFlattenDecode(void* pData, int dataSize, uintptr_t client, int32_t payloadCount,
+                             const ia_binary_data* payloads, void* payloadMemory,
+                             int payloadMemorySize, int32_t statsHandle);
+    bool serverUnflattenDecode(void* pData, int dataSize, uintptr_t* client, int32_t* payloadCount,
+                               ia_binary_data** payloads, int32_t** payloadOffsets);
+    bool serverFlattenDecode(void* pData, int dataSize, const ia_binary_data& statistics);
+    bool clientUnflattenDecode(void* pData, int dataSize, ia_binary_data* statistics);
+
+    bool clientFlattenDeinit(void* pData, int dataSize, uintptr_t client);
+    bool serverUnflattenDeinit(const void* pData, int dataSize, uintptr_t* client);
+};
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIspParamAdaptor.cpp b/camera/hal/intel/ipu6/modules/sandboxing/IPCIspParamAdaptor.cpp
new file mode 100644
index 000000000000..29c487739fb7
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIspParamAdaptor.cpp
@@ -0,0 +1,554 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IPC_ISP_PARAM_ADAPTOR"
+
+#include "modules/sandboxing/IPCIspParamAdaptor.h"
+
+#include <memory>
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IPCIspParamAdaptor::IPCIspParamAdaptor() {
+    LOGIPC("@%s", __func__);
+}
+
+IPCIspParamAdaptor::~IPCIspParamAdaptor() {
+    LOGIPC("@%s", __func__);
+}
+
+bool IPCIspParamAdaptor::clientFlattenInit(void* pData, uint32_t size,
+                                           const ia_binary_data* ispData, const ia_cmc_t* iaCmc,
+                                           uint32_t maxStatsWidth, uint32_t maxStatsHeight,
+                                           uint32_t maxNumStatsIn, ia_mkn* iaMkn) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData || !ispData || !iaCmc, false, "@%s, pData, ispData or iaCmc is nullptr",
+               __func__);
+    CheckError(size < sizeof(IspBxtInitParam), false, "@%s, buffer is small", __func__);
+    CheckError(ispData->size > MAX_IA_BINARY_DATA_SIZE, false,
+               "%s, the buffer of isp data is too small", __func__);
+
+    IspBxtInitParam* params = static_cast<IspBxtInitParam*>(pData);
+    params->iaIsp = *ispData;
+    if (params->iaIsp.data) {
+        MEMCPY_S(params->ispAiqbData, sizeof(params->ispAiqbData), ispData->data, ispData->size);
+    }
+
+    params->iaCmcHandle = reinterpret_cast<uintptr_t>(iaCmc);
+    params->maxStatsWidth = maxStatsWidth;
+    params->maxStatsHeight = maxStatsHeight;
+    params->maxStatsIn = maxNumStatsIn;
+    params->iaMkn = nullptr;  // Not used in current
+
+    return true;
+}
+
+bool IPCIspParamAdaptor::serverUnflattenInit(IspBxtInitParam* pData, uint32_t size,
+                                             ia_binary_data** Isp, ia_cmc_t** Cmc) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(IspBxtInitParam), false, "@%s, buffer is small", __func__);
+    CheckError(!Isp || !Cmc, false, "@%s, Isp or Cmc is nullptr", __func__);
+
+    if (pData->iaIsp.data) {
+        CheckError(pData->iaIsp.size > MAX_IA_BINARY_DATA_SIZE, false,
+                   "%s, the buffer of isp data is too small", __func__);
+        pData->iaIsp.data = pData->ispAiqbData;
+    }
+    *Isp = &pData->iaIsp;
+    *Cmc = reinterpret_cast<ia_cmc_t*>(pData->iaCmcHandle);
+
+    return true;
+}
+
+bool IPCIspParamAdaptor::clientFlattenDeInit(void* pData, uint32_t size,
+                                             const ia_isp_bxt* ispHandle) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(!ispHandle, false, "@%s, ispHandle is nullptr", __func__);
+    CheckError(size < sizeof(IspBxtDeInitParam), false, "@%s, buffer is small", __func__);
+
+    IspBxtDeInitParam* params = static_cast<IspBxtDeInitParam*>(pData);
+    params->ispRemoteHandle = reinterpret_cast<uintptr_t>(ispHandle);
+
+    return true;
+}
+
+bool IPCIspParamAdaptor::serverUnflattenDeInit(const void* pData, uint32_t size,
+                                               ia_isp_bxt** ispHandle) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(!ispHandle, false, "@%s, ispHandle is nullptr", __func__);
+    CheckError(size < sizeof(IspBxtDeInitParam), false, "@%s, buffer is small", __func__);
+
+    const IspBxtDeInitParam* params = static_cast<const IspBxtDeInitParam*>(pData);
+    *ispHandle = reinterpret_cast<ia_isp_bxt*>(params->ispRemoteHandle);
+
+    return true;
+}
+
+bool IPCIspParamAdaptor::clientFlattenGetPalSize(void* pData, uint32_t size,
+                                                 const ia_isp_bxt_program_group* programGroup) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(!programGroup, false, "@%s, programGroup is nullptr", __func__);
+    CheckError(size < sizeof(PalDataSizeParam), false, "@%s, buffer is small", __func__);
+
+    PalDataSizeParam* params = static_cast<PalDataSizeParam*>(pData);
+
+    bool ret = flattenProgramGroup(programGroup, &params->programGroup);
+    CheckError(ret == false, false, "%s, flattenProgramGroup fails", __func__);
+
+    return true;
+}
+
+bool IPCIspParamAdaptor::serverUnflattenGetPalSize(void* pData, uint32_t size,
+                                                   ia_isp_bxt_program_group** programGroup) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(!programGroup, false, "@%s, programGroup is nullptr", __func__);
+    CheckError(size < sizeof(PalDataSizeParam), false, "@%s, buffer is small", __func__);
+
+    PalDataSizeParam* params = static_cast<PalDataSizeParam*>(pData);
+
+    bool ret = unflattenProgramGroup(&params->programGroup);
+    CheckError(ret == false, false, "%s, unflattenProgramGroup fails", __func__);
+
+    *programGroup = &params->programGroup.group;
+
+    return true;
+}
+
+bool IPCIspParamAdaptor::clientFlattenConvertStats(void* pData, uint32_t size,
+                                                   const ia_isp_bxt* ispHandle,
+                                                   const ConvertInputParam* inputParams,
+                                                   int32_t statsHandle) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(!ispHandle || !inputParams, false, "@%s, ispHandle or inputParams is nullptr",
+               __func__);
+    CheckError(size < sizeof(ConvertStatsParam), false, "@%s, buffer is small", __func__);
+
+    ConvertStatsParam* params = static_cast<ConvertStatsParam*>(pData);
+    params->ispRemoteHandle = reinterpret_cast<uintptr_t>(ispHandle);
+    params->multiExpo = inputParams->multiExpo;
+
+    params->statsBuffer = *inputParams->statsBuffer;
+    params->statsHandle = statsHandle;
+
+    bool ret = mIpcAiq.flattenAeResults(*inputParams->aeResults, &params->aeResults);
+    CheckError(ret == false, false, "%s, flattenAeResults fails", __func__);
+
+    params->dvsReso = *inputParams->dvsReso;
+
+    return true;
+}
+
+bool IPCIspParamAdaptor::serverUnflattenConvertStats(void* pData, uint32_t size,
+                                                     ia_isp_bxt** ispHandle,
+                                                     ConvertInputParam* inputParams,
+                                                     ConvertResult* result, void* statsAddr) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(!ispHandle || !inputParams || !result, false,
+               "@%s, ispHandle, inputParams or result is nullptr", __func__);
+    CheckError(size < sizeof(ConvertStatsParam), false, "@%s, buffer is small", __func__);
+    CheckError(!statsAddr, false, "@%s, statsAddr is nullptr", __func__);
+
+    ConvertStatsParam* params = static_cast<ConvertStatsParam*>(pData);
+    *ispHandle = reinterpret_cast<ia_isp_bxt*>(params->ispRemoteHandle);
+    inputParams->multiExpo = params->multiExpo;
+
+    inputParams->statsBuffer = &params->statsBuffer;
+    if (params->statsBuffer.size > 0) {
+        params->statsBuffer.data = statsAddr;
+    }
+
+    inputParams->dvsReso = &params->dvsReso;
+    ae_run_params_results* res = &params->aeResults;
+
+    bool ret = mIpcAiq.unflattenAeResults(res);
+    CheckError(ret == false, false, "%s, unflattenAeResults fails", __func__);
+    inputParams->aeResults = &res->base;
+
+    CLEAR(params->queryResults);
+    result->queryResults = &params->queryResults;
+
+    return true;
+}
+
+bool IPCIspParamAdaptor::serverFlattenConvertStats(void* pData, uint32_t size,
+                                                   const ConvertResult& result) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(ConvertStatsParam), false, "@%s, buffer is small", __func__);
+
+    ConvertStatsParam* params = static_cast<ConvertStatsParam*>(pData);
+    // the queryResults uses SHM
+
+    // flatten rgbs grid
+    CLEAR(params->rgbsGridArray);
+    for (int i = 0; i < MAX_NUM_EXPOSURES; ++i) {
+        ia_aiq_rgbs_grid* rgbs = result.rgbsGrid[i];
+        if (rgbs) {
+            params->rgbsGrid[i].base = *rgbs;
+            size_t rgbsSize = rgbs->grid_width * rgbs->grid_height * sizeof(rgbs_grid_block);
+            size_t memSize = sizeof(params->rgbsGrid[i].blocks_ptr);
+            CheckError(memSize < rgbsSize, false, "%s, memory for rgbs is too small", __func__);
+            MEMCPY_S(params->rgbsGrid[i].blocks_ptr, memSize, rgbs->blocks_ptr, rgbsSize);
+            params->rgbsGridArray[i] = &params->rgbsGrid[i].base;
+        }
+    }
+
+    // flatten af grid
+    params->afGridPtr = nullptr;
+    if (result.afGrid) {
+        params->afGrid.base = *result.afGrid;
+        size_t afSize = result.afGrid->grid_width * result.afGrid->grid_height * sizeof(int);
+        size_t memSize = sizeof(params->afGrid.filter_response_1);
+        CheckError(memSize < afSize, false, "%s, memory for af grid is too small", __func__);
+        MEMCPY_S(params->afGrid.filter_response_1, memSize, result.afGrid->filter_response_1,
+                 afSize);
+        MEMCPY_S(params->afGrid.filter_response_2, memSize, result.afGrid->filter_response_2,
+                 afSize);
+        params->afGridPtr = &params->afGrid.base;
+    }
+
+    // flatten dvs grid
+    params->dvsStatsPtr = nullptr;
+    if (result.dvsStats) {
+        params->dvsStats.base = *result.dvsStats;
+        CheckError(MV_ENTRIE_COUNT < result.dvsStats->vector_count, false,
+                   "%s, memory for dvs statistics is too small", __func__);
+        MEMCPY_S(params->dvsStats.motion_vectors, MV_ENTRIE_COUNT * sizeof(ia_dvs_motion_vector),
+                 result.dvsStats->motion_vectors,
+                 result.dvsStats->vector_count * sizeof(ia_dvs_motion_vector));
+        params->dvsStatsPtr = &params->dvsStats.base;
+    }
+
+    return true;
+}
+
+bool IPCIspParamAdaptor::clientUnflattenConvertStats(void* pData, uint32_t size,
+                                                     ConvertResult* result) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData || !result, false, "@%s, pData or result is nullptr", __func__);
+    CheckError(size < sizeof(ConvertStatsParam), false, "@%s, buffer is small", __func__);
+
+    ConvertStatsParam* params = static_cast<ConvertStatsParam*>(pData);
+    *result->queryResults = params->queryResults;
+
+    for (int i = 0; i < MAX_NUM_EXPOSURES; ++i) {
+        if (params->rgbsGridArray[i]) {
+            params->rgbsGrid[i].base.blocks_ptr = params->rgbsGrid[i].blocks_ptr;
+            result->rgbsGrid[i] = &params->rgbsGrid[i].base;
+        }
+    }
+
+    if (params->afGridPtr) {
+        params->afGrid.base.filter_response_1 = params->afGrid.filter_response_1;
+        params->afGrid.base.filter_response_2 = params->afGrid.filter_response_2;
+        result->afGrid = &params->afGrid.base;
+    }
+
+    if (params->dvsStatsPtr) {
+        params->dvsStats.base.motion_vectors = params->dvsStats.motion_vectors;
+        result->dvsStats = &params->dvsStats.base;
+    }
+
+    return true;
+}
+
+bool IPCIspParamAdaptor::clientFlattenRunPal(void* pData, uint32_t size,
+                                             const ia_isp_bxt* ispHandle,
+                                             const ia_isp_bxt_input_params_v2* inputParams,
+                                             const ia_binary_data* outputData,
+                                             const int32_t palDataHandle) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(!ispHandle || !inputParams || !outputData, false,
+               "@%s, ispHandle, inputParams or outputData is nullptr", __func__);
+    CheckError(size < sizeof(RunPalParam), false, "@%s, buffer is small", __func__);
+
+    RunPalParam* params = static_cast<RunPalParam*>(pData);
+    params->ispRemoteHandle = reinterpret_cast<uintptr_t>(ispHandle);
+    params->inputParamsBase = *inputParams;
+    ia_isp_bxt_input_params_v2* base = &params->inputParamsBase;
+
+    if (base->sensor_frame_params) {
+        params->frameParam = *inputParams->sensor_frame_params;
+    }
+
+    if (base->awb_results) {
+        params->awbResult = *inputParams->awb_results;
+    }
+
+    bool ret = true;
+    if (base->gbce_results) {
+        ret = mIpcAiq.flattenGbceResults(*inputParams->gbce_results, &params->gbceResult);
+        CheckError(ret == false, false, "%s, flattenGbceResults fails", __func__);
+    }
+
+    if (base->ae_results) {
+        ret = mIpcAiq.flattenAeResults(*inputParams->ae_results, &params->aeResults);
+        CheckError(ret == false, false, "%s, flattenAeResults fails", __func__);
+    }
+
+    if (base->pa_results) {
+        ret = mIpcAiq.flattenPaResultsV1(*inputParams->pa_results, &params->paResult);
+        CheckError(ret == false, false, "%s, flattenPaResultsV1 fails", __func__);
+    }
+
+    if (base->sa_results) {
+        mIpcAiq.flattenSaResultsV2(*inputParams->sa_results, &params->saResult);
+        CheckError(ret == false, false, "%s, flattenSaResultsV2 fails", __func__);
+    }
+
+    if (base->weight_grid) {
+        params->weightGrid = *inputParams->weight_grid;
+        size_t weightSize = (inputParams->weight_grid->width * inputParams->weight_grid->height *
+                             sizeof(unsigned char));
+        CheckError(weightSize > MAX_SIZE_WEIGHT_GRID * sizeof(unsigned char), false,
+                   "%s, the buffer for weight grid is too small", __func__);
+        MEMCPY_S(params->weights, MAX_SIZE_WEIGHT_GRID * sizeof(unsigned char),
+                 inputParams->weight_grid->weights, weightSize);
+        params->weightGrid.weights = const_cast<unsigned char*>(params->weights);
+    }
+
+    if (base->program_group) {
+        CLEAR(params->programGroup);
+        ret = flattenProgramGroup(inputParams->program_group, &params->programGroup);
+        CheckError(ret == false, false, "%s, flattenProgramGroup fails", __func__);
+    }
+
+    if (base->dvs_morph_table) {
+        ret = mIpcDvs.flattenMorphTable(inputParams->dvs_morph_table, &params->dvsResult);
+        CheckError(ret == false, false, "%s, flattenMorphTable fails", __func__);
+    }
+
+    if (base->custom_controls) {
+        params->customControl = *inputParams->custom_controls;
+
+        size_t customCtlSize = inputParams->custom_controls->count * sizeof(float);
+        size_t maxCtlSize = MAX_CUSTOM_CONTROLS_SIZE * sizeof(float);
+        CheckError(customCtlSize > maxCtlSize, false,
+                   "%s, the buffer for custom controls is too small", __func__);
+        if (inputParams->custom_controls->parameters) {
+            MEMCPY_S(params->customCtlParams, maxCtlSize, inputParams->custom_controls->parameters,
+                     customCtlSize);
+            params->customControl.parameters = const_cast<float*>(params->customCtlParams);
+        }
+    }
+
+    if (base->pal_override) {
+        params->palOverride = *inputParams->pal_override;
+        if (inputParams->pal_override->size > 0) {
+            MEMCPY_S(params->overrideData, sizeof(params->overrideData),
+                     inputParams->pal_override->data, inputParams->pal_override->size);
+            params->palOverride.data = static_cast<void*>(params->overrideData);
+        }
+    }
+
+    if (base->ltm_results) {
+        params->ltmResult = *inputParams->ltm_results;
+    }
+
+    if (base->ltm_drc_params) {
+        params->drcResult = *inputParams->ltm_drc_params;
+    }
+
+    if (base->gdc_transformation) {
+        params->gdcTransform = *inputParams->gdc_transformation;
+    }
+
+    if (base->view_params) {
+        MEMCPY_S(&params->viewConfig, sizeof(params->viewConfig), inputParams->view_params,
+                 sizeof(ia_view_config_t));
+    }
+
+    if (base->bcomp_results) {
+        params->bcompResult = *inputParams->bcomp_results;
+    }
+
+    if (base->gdc_mbr_limits) {
+        params->gdcLimit = *inputParams->gdc_mbr_limits;
+    }
+
+    if (outputData) {
+        params->palOutput = *outputData;
+        params->palDataHandle = palDataHandle;
+    }
+
+    return true;
+}
+
+bool IPCIspParamAdaptor::serverUnflattenRunPal(void* pData, uint32_t size, ia_isp_bxt** ispHandle,
+                                               ia_isp_bxt_input_params_v2** paramsRes,
+                                               ia_binary_data** palOutput) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, false, "@%s, pData is nullptr", __func__);
+    CheckError(!ispHandle || !paramsRes || !palOutput, false,
+               "@%s, ispHandle paramsRes or palOutput is nullptr", __func__);
+    CheckError(size < sizeof(RunPalParam), false, "@%s, buffer is small", __func__);
+
+    RunPalParam* params = static_cast<RunPalParam*>(pData);
+    ia_isp_bxt_input_params_v2* base = &params->inputParamsBase;
+
+    if (base->sensor_frame_params) {
+        base->sensor_frame_params = &params->frameParam;
+    }
+
+    if (base->awb_results) {
+        base->awb_results = &params->awbResult;
+    }
+
+    bool ret = true;
+    if (base->gbce_results) {
+        ret = mIpcAiq.unflattenGbceResults(&params->gbceResult);
+        CheckError(ret == false, false, "%s, unflattenGbceResults fails", __func__);
+        base->gbce_results = &params->gbceResult.base;
+    }
+
+    if (base->ae_results) {
+        ret = mIpcAiq.unflattenAeResults(&params->aeResults);
+        CheckError(ret == false, false, "%s, unflattenAeResults fails", __func__);
+        base->ae_results = &params->aeResults.base;
+    }
+
+    if (base->pa_results) {
+        ret = mIpcAiq.unflattenPaResultsV1(&params->paResult);
+        CheckError(ret == false, false, "%s, unflattenPaResultsV1 fails", __func__);
+        base->pa_results = &params->paResult.base;
+    }
+
+    if (base->sa_results) {
+        ret = mIpcAiq.unflattenSaResultsV2(&params->saResult);
+        CheckError(ret == false, false, "%s, unflattenSaResultsV2 fails", __func__);
+        base->sa_results = &params->saResult.base;
+    }
+
+    if (base->weight_grid) {
+        if (params->weightGrid.weights) {
+            params->weightGrid.weights = const_cast<unsigned char*>(params->weights);
+        }
+        base->weight_grid = &params->weightGrid;
+    }
+
+    if (base->program_group) {
+        ret = unflattenProgramGroup(&params->programGroup);
+        CheckError(ret == false, false, "%s, unflattenProgramGroup fails", __func__);
+        base->program_group = &params->programGroup.group;
+    }
+
+    if (base->dvs_morph_table) {
+        ret = mIpcDvs.unflattenMorphTalbe(&params->dvsResult);
+        CheckError(ret == false, false, "%s, unflattenMorphTalbe fails", __func__);
+        base->dvs_morph_table = &params->dvsResult.morphTable;
+    }
+
+    if (base->custom_controls) {
+        if (params->customControl.parameters) {
+            params->customControl.parameters = const_cast<float*>(params->customCtlParams);
+        }
+        base->custom_controls = &params->customControl;
+    }
+
+    if (base->pal_override) {
+        if (params->palOverride.data) {
+            params->palOverride.data = static_cast<void*>(params->overrideData);
+        }
+        base->pal_override = &params->palOverride;
+    }
+
+    if (base->ltm_results) {
+        base->ltm_results = &params->ltmResult;
+    }
+
+    if (base->ltm_drc_params) {
+        base->ltm_drc_params = &params->drcResult;
+    }
+
+    if (base->gdc_transformation) {
+        base->gdc_transformation = &params->gdcTransform;
+    }
+
+    if (base->view_params) {
+        base->view_params = reinterpret_cast<ia_isp_bxt_view_params_t*>(&params->viewConfig);
+    }
+
+    if (base->bcomp_results) {
+        base->bcomp_results = &params->bcompResult;
+    }
+
+    if (base->gdc_mbr_limits) {
+        base->gdc_mbr_limits = &params->gdcLimit;
+    }
+
+    *ispHandle = reinterpret_cast<ia_isp_bxt*>(params->ispRemoteHandle);
+    *paramsRes = &params->inputParamsBase;
+    *palOutput = &params->palOutput;
+
+    return true;
+}
+
+bool IPCIspParamAdaptor::flattenProgramGroup(const ia_isp_bxt_program_group* src,
+                                             GraphKernelArray* res) {
+    CheckError(!src || !res, false, "@%s, src or dst is nullptr", __func__);
+    CheckError(src->kernel_count > MAX_STREAM_KERNEL_COUNT, false,
+               "%s the buffer of kernel array is tool small", __func__);
+
+    res->group = *src;
+    for (unsigned int j = 0; j < res->group.kernel_count; ++j) {
+        res->runKernels[j] = src->run_kernels[j];
+        if (res->runKernels[j].resolution_info) {
+            res->resoInfo[j] = *src->run_kernels[j].resolution_info;
+        }
+        if (res->runKernels[j].resolution_history) {
+            res->resoHistory[j] = *src->run_kernels[j].resolution_history;
+        }
+    }
+    if (res->group.pipe) {
+        res->pipeInfo = *src->pipe;
+    }
+
+    return true;
+}
+
+bool IPCIspParamAdaptor::unflattenProgramGroup(GraphKernelArray* result) {
+    CheckError(!result, false, "@%s, result is nullptr", __func__);
+    CheckError(result->group.kernel_count > MAX_STREAM_KERNEL_COUNT, false,
+               "%s, the buffer of kernel array is too small", __func__);
+
+    result->group.run_kernels = result->runKernels;
+    for (unsigned j = 0; j < result->group.kernel_count; ++j) {
+        if (result->runKernels[j].resolution_info) {
+            result->runKernels[j].resolution_info = &result->resoInfo[j];
+        }
+
+        if (result->runKernels[j].resolution_history) {
+            result->runKernels[j].resolution_history = &result->resoHistory[j];
+        }
+    }
+    if (result->group.pipe) {
+        result->group.pipe = &result->pipeInfo;
+    }
+
+    return true;
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/IPCIspParamAdaptor.h b/camera/hal/intel/ipu6/modules/sandboxing/IPCIspParamAdaptor.h
new file mode 100644
index 000000000000..25b3279963d2
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/IPCIspParamAdaptor.h
@@ -0,0 +1,170 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "modules/algowrapper/StatsTypes.h"
+#include "modules/sandboxing/IPCCommon.h"
+#include "modules/sandboxing/IPCGraphConfig.h"
+#include "modules/sandboxing/IPCIntelAiq.h"
+#include "modules/sandboxing/IPCIntelDvs.h"
+
+namespace icamera {
+
+struct IspBxtInitParam {
+    ia_binary_data iaIsp;
+    int8_t ispAiqbData[MAX_IA_BINARY_DATA_SIZE];
+    uintptr_t iaCmcHandle;
+    uint32_t maxStatsWidth;
+    uint32_t maxStatsHeight;
+    uint32_t maxStatsIn;
+    ia_mkn* iaMkn;  // hal always passes nullptr to ia_bxt_isp
+
+    uintptr_t ispRemoteHandle;
+};
+
+struct IspBxtDeInitParam {
+    uintptr_t ispRemoteHandle;
+};
+
+struct PalDataSizeParam {
+    GraphKernelArray programGroup;
+    int palDataSize;
+};
+
+struct DvsStatsParam {
+    ia_dvs_statistics base;
+    ia_dvs_motion_vector motion_vectors[MV_ENTRIE_COUNT];
+};
+
+struct ConvertStatsParam {
+    uintptr_t ispRemoteHandle;
+
+    // Input params
+    bool multiExpo;
+    ia_binary_data statsBuffer;
+    int32_t statsHandle;
+    camera_resolution_t dvsReso;
+    ae_run_params_results aeResults;
+
+    // Output result
+    ia_isp_bxt_statistics_query_results_t queryResults;
+
+    ia_aiq_rgbs_grid* rgbsGridArray[MAX_NUM_EXPOSURES];
+    ia_aiq_rgbs_grid_data rgbsGrid[MAX_NUM_EXPOSURES];
+
+    ia_aiq_af_grid* afGridPtr;
+    ia_aiq_af_grid_data afGrid;
+
+    ia_dvs_statistics* dvsStatsPtr;
+    DvsStatsParam dvsStats;
+};
+
+#define MAX_CUSTOM_CONTROLS_SIZE 1024
+struct RunPalParam {
+    ia_isp_bxt_input_params_v2 inputParamsBase;
+
+    // sensor frame parameters
+    ia_aiq_frame_params frameParam;
+
+    // for 3a result
+    ia_aiq_awb_results awbResult;
+    gbce_results_params gbceResult;
+
+    ae_run_params_results aeResults;
+    pa_run_params_results_v1 paResult;
+    sa_run_v2_params_results saResult;
+
+    ia_aiq_hist_weight_grid weightGrid;
+    unsigned char weights[MAX_SIZE_WEIGHT_GRID];
+
+    GraphKernelArray programGroup;
+    DvsMorphParams dvsResult;
+
+    // for custom control
+    ia_isp_custom_controls customControl;
+    float customCtlParams[MAX_CUSTOM_CONTROLS_SIZE];
+
+    // for pal override
+    ia_binary_data palOverride;
+    int8_t overrideData[MAX_IA_BINARY_DATA_SIZE];
+
+    // for ltm result
+    ia_ltm_results ltmResult;
+    ia_ltm_drc_params drcResult;
+
+    ia_dvs_image_transformation gdcTransform;
+
+    ia_view_config_t viewConfig;
+    ia_bcomp_results bcompResult;
+    ia_isp_bxt_gdc_limits gdcLimit;
+
+    ia_binary_data palOutput;
+    int32_t palDataHandle;
+
+    uintptr_t ispRemoteHandle;
+};
+
+class IPCIspParamAdaptor {
+ public:
+    IPCIspParamAdaptor();
+    virtual ~IPCIspParamAdaptor();
+
+    // init
+    bool clientFlattenInit(void* pData, uint32_t size, const ia_binary_data* ispData,
+                           const ia_cmc_t* iaCmc, uint32_t maxStatsWidth, uint32_t maxStatsHeight,
+                           uint32_t maxNumStatsIn, ia_mkn* iaMkn);
+    bool serverUnflattenInit(IspBxtInitParam* pData, uint32_t size, ia_binary_data** Isp,
+                             ia_cmc_t** Cmc);
+
+    // deinit
+    bool clientFlattenDeInit(void* pData, uint32_t size, const ia_isp_bxt* ispHandle);
+    bool serverUnflattenDeInit(const void* pData, uint32_t size, ia_isp_bxt** ispHandle);
+
+    // get pal data size
+    bool clientFlattenGetPalSize(void* pData, uint32_t size,
+                                 const ia_isp_bxt_program_group* programGroup);
+    bool serverUnflattenGetPalSize(void* pData, uint32_t size,
+                                   ia_isp_bxt_program_group** programGroup);
+
+    // convert stats
+    bool clientFlattenConvertStats(void* pData, uint32_t size, const ia_isp_bxt* ispHandle,
+                                   const ConvertInputParam* inputParams, int32_t statsHandle);
+    bool serverUnflattenConvertStats(void* pData, uint32_t size, ia_isp_bxt** ispHandle,
+                                     ConvertInputParam* inputParams, ConvertResult* result,
+                                     void* statsAddr);
+    bool serverFlattenConvertStats(void* pData, uint32_t size, const ConvertResult& result);
+    bool clientUnflattenConvertStats(void* pData, uint32_t size, ConvertResult* result);
+
+    // run pal
+    bool clientFlattenRunPal(void* pData, uint32_t size, const ia_isp_bxt* ispHandle,
+                             const ia_isp_bxt_input_params_v2* inputParams,
+                             const ia_binary_data* outputData, const int32_t palDataHandle);
+    bool serverUnflattenRunPal(void* pData, uint32_t size, ia_isp_bxt** ispHandle,
+                               ia_isp_bxt_input_params_v2** paramsRes, ia_binary_data** palOutput);
+
+ private:
+    bool flattenProgramGroup(const ia_isp_bxt_program_group* src, GraphKernelArray* res);
+    bool unflattenProgramGroup(GraphKernelArray* result);
+
+ private:
+    IPCIntelAiq mIpcAiq;
+    IPCIntelDvs mIpcDvs;
+
+    // Disable copy constructor and assignment operator
+    DISALLOW_COPY_AND_ASSIGN(IPCIspParamAdaptor);
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/GraphConfigImpl.cpp b/camera/hal/intel/ipu6/modules/sandboxing/client/GraphConfigImpl.cpp
new file mode 100644
index 000000000000..0081ea574a8c
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/GraphConfigImpl.cpp
@@ -0,0 +1,186 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "ClientGraphConfigImpl"
+
+#include "modules/sandboxing/client/GraphConfigImpl.h"
+
+#include "iutils/CameraLog.h"
+
+using std::map;
+using std::string;
+using std::vector;
+
+namespace icamera {
+
+GraphConfigImpl::GraphConfigImpl() : mCameraId(-1), mInitialized(false) {
+    uintptr_t personal = reinterpret_cast<uintptr_t>(this);
+    std::string parseName = "/graphParse" + std::to_string(personal) + "Shm";
+
+    mMems = {{parseName.c_str(), sizeof(GraphParseParams), &mMemParse, false}};
+
+    bool success = mCommon.allocateAllShmMems(&mMems);
+    if (!success) {
+        mCommon.releaseAllShmMems(mMems);
+        return;
+    }
+
+    LOGIPC("@%s, done, cameraId: %d", __func__, mCameraId);
+    mInitialized = true;
+}
+
+GraphConfigImpl::GraphConfigImpl(int32_t camId, ConfigMode mode, GraphSettingType type)
+        : mCameraId(camId),
+          mConfigMode(mode),
+          mType(type),
+          mInitialized(false) {
+    uintptr_t personal = reinterpret_cast<uintptr_t>(this);
+    std::string configStreamsName = "/graphConfigStreams" + std::to_string(personal) + "Shm";
+    std::string getGraphDataName = "/graphGetData" + std::to_string(personal) + "Shm";
+    std::string getPgIdName = "/graphGetPgId" + std::to_string(personal) + "Shm";
+    std::string getConnection = "/graphGetConnection" + std::to_string(personal) + "Shm";
+
+    mMems = {{configStreamsName.c_str(), sizeof(GraphConfigStreamParams), &mMemConfig, false},
+             {getGraphDataName.c_str(), sizeof(GraphGetDataParams), &mMemGetData, false},
+             {getPgIdName.c_str(), sizeof(GraphGetPgIdParams), &mMemGetPgId, false},
+             {getConnection.c_str(), sizeof(GraphGetConnectionParams), &mMemGetConnection, false}};
+
+    bool success = mCommon.allocateAllShmMems(&mMems);
+    if (!success) {
+        mCommon.releaseAllShmMems(mMems);
+        return;
+    }
+
+    mInitialized = true;
+    LOGIPC("@%s, done, cameraId: %d, configMode: %d, type", __func__, mCameraId, mConfigMode,
+           mType);
+}
+
+GraphConfigImpl::~GraphConfigImpl() {
+    mInitialized = false;
+    mCommon.releaseAllShmMems(mMems);
+    mMems.clear();
+}
+
+void GraphConfigImpl::addCustomKeyMap() {
+    LOGIPC("@%s", __func__);
+    CheckError(!mInitialized, VOID_VALUE, "@%s, mInitialized is false", __func__);
+
+    bool ret = mCommon.requestSync(IPC_GRAPH_ADD_KEY);
+    CheckError(!ret, VOID_VALUE, "@%s, requestSync fails", __func__);
+}
+
+status_t GraphConfigImpl::parse(int cameraId, const char* graphDescFile, const char* settingsFile) {
+    LOGIPC("@%s", __func__);
+    CheckError(!mInitialized, UNKNOWN_ERROR, "@%s, mInitialized is false", __func__);
+
+    bool ret = mIpc.clientFlattenParse(mMemParse.mAddr, mMemParse.mSize, cameraId, graphDescFile,
+                                       settingsFile);
+    CheckError(!ret, UNKNOWN_ERROR, "@%s, clientFlattenParse fails", __func__);
+
+    status_t rt = mCommon.requestSync(IPC_GRAPH_PARSE, mMemParse.mHandle);
+    CheckError(!rt, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    return OK;
+}
+
+void GraphConfigImpl::releaseGraphNodes() {
+    LOGIPC("@%s", __func__);
+    CheckError(!mInitialized, VOID_VALUE, "@%s, mInitialized is false", __func__);
+
+    bool ret = mCommon.requestSync(IPC_GRAPH_RELEASE_NODES);
+    CheckError(!ret, VOID_VALUE, "@%s, requestSync fails", __func__);
+}
+
+status_t GraphConfigImpl::configStreams(const vector<HalStream*>& activeStreams) {
+    LOGIPC("@%s", __func__);
+    CheckError(!mInitialized, UNKNOWN_ERROR, "@%s, mInitialized is false", __func__);
+
+    GraphBaseInfo info = {mCameraId, mConfigMode};
+    bool ret = mIpc.clientFlattenConfigStreams(mMemConfig.mAddr, mMemConfig.mSize, info, mType,
+                                               activeStreams);
+    CheckError(!ret, UNKNOWN_ERROR, "@%s, clientFlattenConfigStreams fails", __func__);
+
+    ret = mCommon.requestSync(IPC_GRAPH_CONFIG_STREAMS, mMemConfig.mHandle);
+    CheckError(!ret, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    return OK;
+}
+
+status_t GraphConfigImpl::getGraphConfigData(IGraphType::GraphConfigData* data) {
+    LOGIPC("@%s", __func__);
+    CheckError(!mInitialized, UNKNOWN_ERROR, "@%s, mInitialized is false", __func__);
+
+    GraphBaseInfo info = {mCameraId, mConfigMode};
+    bool ret = mIpc.clientFlattenGetGraphData(mMemGetData.mAddr, mMemGetData.mSize, info);
+    CheckError(!ret, UNKNOWN_ERROR, "@%s, clientFlattenGetGraphData fails", __func__);
+
+    ret = mCommon.requestSync(IPC_GRAPH_GET_CONFIG_DATA, mMemGetData.mHandle);
+    CheckError(!ret, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    ret = mIpc.clientUnflattenGetGraphData(mMemGetData.mAddr, mMemGetData.mSize, data);
+    CheckError(!ret, UNKNOWN_ERROR, "@%s, clientUnflattenGetGraphData fails", __func__);
+
+    return OK;
+}
+
+status_t GraphConfigImpl::pipelineGetConnections(
+    const std::vector<std::string>& pgList, std::vector<IGraphType::ScalerInfo>* scalerInfo,
+    std::vector<IGraphType::PipelineConnection>* confVector) {
+    LOGIPC("@%s", __func__);
+    CheckError(!mInitialized, UNKNOWN_ERROR, "@%s, mInitialized is false", __func__);
+
+    GraphBaseInfo info = {mCameraId, mConfigMode};
+    bool ret = mIpc.clientFlattenGetConnection(mMemGetConnection.mAddr, mMemGetConnection.mSize,
+                                               info, pgList);
+    CheckError(!ret, UNKNOWN_ERROR, "@%s, clientFlattenGetConnection fails", __func__);
+
+    ret = mCommon.requestSync(IPC_GRAPH_GET_CONNECTION, mMemGetConnection.mHandle);
+    CheckError(!ret, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    ret = mIpc.clientUnFlattenGetConnection(mMemGetConnection.mAddr, mMemGetConnection.mSize,
+                                            scalerInfo, confVector);
+    CheckError(!ret, UNKNOWN_ERROR, "@%s, clientUnFlattenGetConnection fails", __func__);
+
+    return OK;
+}
+
+status_t GraphConfigImpl::getPgIdForKernel(const uint32_t streamId, const int32_t kernelId,
+                                           int32_t* pgId) {
+    LOGIPC("@%s", __func__);
+    CheckError(!mInitialized, UNKNOWN_ERROR, "@%s, mInitialized is false", __func__);
+
+    GraphBaseInfo info = {mCameraId, mConfigMode};
+    bool ret =
+        mIpc.clientFlattenGetPgId(mMemGetPgId.mAddr, mMemGetPgId.mSize, info, streamId, kernelId);
+    CheckError(!ret, UNKNOWN_ERROR, "@%s, clientFlattenGetPgId fails", __func__);
+
+    ret = mCommon.requestSync(IPC_GRAPH_GET_PG_ID, mMemGetPgId.mHandle);
+    CheckError(!ret, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    ret = mIpc.clientUnFlattenGetPgId(mMemGetPgId.mAddr, mMemGetPgId.mSize, pgId);
+    CheckError(!ret, UNKNOWN_ERROR, "@%s, clientUnFlattenGetPgId fails", __func__);
+
+    return OK;
+}
+
+status_t GraphConfigImpl::getProgramGroup(std::string pgName,
+                                          ia_isp_bxt_program_group* programGroup) {
+    // TODO: Add this API support in the future.
+    LOGIPC("@%s", __func__);
+    return OK;
+}
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/GraphConfigImpl.h b/camera/hal/intel/ipu6/modules/sandboxing/client/GraphConfigImpl.h
new file mode 100644
index 000000000000..77d928a3b256
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/GraphConfigImpl.h
@@ -0,0 +1,75 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+#include <string>
+#include <utility>
+#include <vector>
+
+#include "IntelAlgoCommon.h"
+#include "iutils/Errors.h"
+#include "iutils/Thread.h"
+#include "iutils/Utils.h"
+#include "modules/sandboxing/IPCGraphConfig.h"
+#include "src/platformdata/CameraTypes.h"
+#include "src/platformdata/gc/HalStream.h"
+#include "src/platformdata/gc/IGraphConfig.h"
+
+namespace icamera {
+
+class GraphConfigImpl {
+ public:
+    GraphConfigImpl();
+    GraphConfigImpl(int32_t camId, ConfigMode mode, GraphSettingType type);
+    virtual ~GraphConfigImpl();
+
+    void addCustomKeyMap();
+    status_t parse(int cameraId, const char* graphDescFile, const char* settingsFile);
+    void releaseGraphNodes();
+
+    status_t configStreams(const std::vector<HalStream*>& activeStreams);
+    status_t getGraphConfigData(IGraphType::GraphConfigData* data);
+
+    int getProgramGroup(std::string pgName, ia_isp_bxt_program_group* programGroupForPG);
+    status_t getPgIdForKernel(const uint32_t streamId, const int32_t kernelId, int32_t* pgId);
+
+    status_t pipelineGetConnections(const std::vector<std::string>& pgList,
+                                    std::vector<IGraphType::ScalerInfo>* scalerInfo,
+                                    std::vector<IGraphType::PipelineConnection>* confVector);
+
+ private:
+    IPCGraphConfig mIpc;
+    IntelAlgoCommon mCommon;
+
+    ShmMemInfo mMemParse;
+    ShmMemInfo mMemConfig;
+    ShmMemInfo mMemGetData;
+    ShmMemInfo mMemGetPgId;
+    ShmMemInfo mMemGetConnection;
+
+    std::vector<ShmMem> mMems;
+
+    int mCameraId;
+    ConfigMode mConfigMode;
+    GraphSettingType mType;
+    bool mInitialized;
+
+    // Disable copy constructor and assignment operator
+    DISALLOW_COPY_AND_ASSIGN(GraphConfigImpl);
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAiq.cpp b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAiq.cpp
new file mode 100644
index 000000000000..9ab424756a89
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAiq.cpp
@@ -0,0 +1,319 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelAiq"
+
+#include "modules/sandboxing/client/IntelAiq.h"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelAiq::IntelAiq() {
+    LOG1("@%s", __func__);
+
+    mAiq = reinterpret_cast<uintptr_t>(nullptr);
+
+    uintptr_t personal = reinterpret_cast<uintptr_t>(this);
+    std::string deinitName = "/aiqDeinitShm" + std::to_string(personal) + "Shm";
+    std::string aeName = "/aiqAeShm" + std::to_string(personal) + "Shm";
+    std::string afName = "/aiqAfShm" + std::to_string(personal) + "Shm";
+    std::string awbName = "/aiqAwbShm" + std::to_string(personal) + "Shm";
+    std::string gbceName = "/aiqGbceShm" + std::to_string(personal) + "Shm";
+    std::string aiqdName = "/aiqAiqdShm" + std::to_string(personal) + "Shm";
+    std::string paName = "/aiqPaShm" + std::to_string(personal) + "Shm";
+    std::string saName = "/aiqSaShm" + std::to_string(personal) + "Shm";
+    std::string statName = "/aiqStatShm" + std::to_string(personal) + "Shm";
+    std::string versionName = "/aiqVersionShm" + std::to_string(personal) + "Shm";
+
+    mMems = {{deinitName.c_str(), sizeof(aiq_deinit_params), &mMemDeinit, false},
+             {aeName.c_str(), sizeof(ae_run_params), &mMemAe, false},
+             {afName.c_str(), sizeof(af_run_params), &mMemAf, false},
+             {awbName.c_str(), sizeof(awb_run_params), &mMemAwb, false},
+             {gbceName.c_str(), sizeof(gbce_run_params), &mMemGbce, false},
+             {aiqdName.c_str(), sizeof(ia_binary_data_params), &mMemAiqd, false},
+             {paName.c_str(), sizeof(pa_run_v1_params), &mMemPa, false},
+             {saName.c_str(), sizeof(sa_run_v2_params), &mMemSa, false},
+             {statName.c_str(), sizeof(set_statistics_set_v4_params), &mMemStat, false},
+             {versionName.c_str(), sizeof(ia_aiq_version_params), &mMemVersion, false}};
+
+    bool success = mCommon.allocateAllShmMems(&mMems);
+    if (!success) {
+        mCommon.releaseAllShmMems(mMems);
+        return;
+    }
+
+    LOG1("@%s, done", __func__);
+    mInitialized = true;
+}
+
+IntelAiq::~IntelAiq() {
+    LOG1("@%s", __func__);
+    mCommon.releaseAllShmMems(mMems);
+}
+
+ia_aiq* IntelAiq::init(const ia_binary_data* aiqbData, const ia_binary_data* nvmData,
+                       const ia_binary_data* aiqdData, unsigned int statsMaxWidth,
+                       unsigned int statsMaxHeight, unsigned int maxNumStatsIn, ia_cmc_t* cmc,
+                       ia_mkn* mkn) {
+    LOG1("@%s, aiqbData:%p, nvmData:%p, aiqdData:%p", __func__, aiqbData, nvmData, aiqdData);
+    CheckError(!mInitialized, nullptr, "@%s, mInitialized is false", __func__);
+
+    unsigned int aiqbSize = aiqbData ? aiqbData->size : 0;
+    unsigned int nvmSize = nvmData ? nvmData->size : 0;
+    unsigned int aiqdSize = aiqdData ? aiqdData->size : 0;
+    unsigned int size = sizeof(aiq_init_params) + aiqbSize + nvmSize + aiqdSize;
+    LOG2("@%s, aiqbSize:%d, nvmSize:%d, aiqdSize:%d", __func__, aiqbSize, nvmSize, aiqdSize);
+
+    uintptr_t personal = reinterpret_cast<uintptr_t>(this);
+    std::string initName = "/aiqInitShm" + std::to_string(personal) + "Shm";
+    ShmMemInfo shm;
+    shm.mName = initName.c_str();
+    shm.mSize = size;
+    bool ret = mCommon.allocShmMem(shm.mName, shm.mSize, &shm);
+    CheckError(!ret, nullptr, "@%s, allocShmMem fails", __func__);
+
+    ret = mIpc.clientFlattenInit(aiqbData, nvmData, aiqdData, statsMaxWidth, statsMaxHeight,
+                                 maxNumStatsIn, reinterpret_cast<uintptr_t>(cmc),
+                                 reinterpret_cast<uintptr_t>(mkn), static_cast<uint8_t*>(shm.mAddr),
+                                 size);
+    if (!ret) {
+        mCommon.freeShmMem(shm);
+        LOGE("@%s, clientFlattenInit fails", __func__);
+        return nullptr;
+    }
+
+    ret = mCommon.requestSync(IPC_AIQ_INIT, shm.mHandle);
+    if (!ret) {
+        mCommon.freeShmMem(shm);
+        LOGE("@%s, requestSync fails", __func__);
+        return nullptr;
+    }
+
+    aiq_init_params* params = static_cast<aiq_init_params*>(shm.mAddr);
+    mAiq = params->results;
+    LOG2("@%s, success, aiq:%p\n", __func__, reinterpret_cast<ia_aiq*>(mAiq));
+
+    mCommon.freeShmMem(shm);
+
+    return reinterpret_cast<ia_aiq*>(mAiq);
+}
+
+ia_err IntelAiq::aeRun(const ia_aiq_ae_input_params* inputParams, ia_aiq_ae_results** results) {
+    LOG1("@%s, inputParams:%p, results:%p", __func__, inputParams, results);
+    CheckError(!mInitialized, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(reinterpret_cast<ia_aiq*>(mAiq) == nullptr, ia_err_general, "@%s, mAiq is nullptr",
+               __func__);
+    CheckError(!inputParams, ia_err_argument, "@%s, inputParams is nullptr", __func__);
+    CheckError(!results, ia_err_argument, "@%s, results is nullptr", __func__);
+
+    ae_run_params* params = static_cast<ae_run_params*>(mMemAe.mAddr);
+
+    bool ret = mIpc.clientFlattenAe(mAiq, *inputParams, params);
+    CheckError(!ret, ia_err_general, "@%s, clientFlattenAe fails", __func__);
+
+    ret = mCommon.requestSync(IPC_AIQ_AE_RUN, mMemAe.mHandle);
+    CheckError(!ret, ia_err_general, "@%s, requestSync fails", __func__);
+
+    ret = mIpc.clientUnflattenAe(params, results);
+    CheckError(!ret, ia_err_general, "@%s, clientUnflattenAe fails", __func__);
+
+    return ia_err_none;
+}
+
+ia_err IntelAiq::afRun(const ia_aiq_af_input_params* inputParams, ia_aiq_af_results** results) {
+    LOG1("@%s, inputParams:%p, results:%p", __func__, inputParams, results);
+    CheckError(!mInitialized, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(reinterpret_cast<ia_aiq*>(mAiq) == nullptr, ia_err_general, "@%s, mAiq is nullptr",
+               __func__);
+    CheckError(!inputParams, ia_err_argument, "@%s, inputParams is nullptr", __func__);
+    CheckError(!results, ia_err_argument, "@%s, results is nullptr", __func__);
+
+    af_run_params* params = static_cast<af_run_params*>(mMemAf.mAddr);
+
+    bool ret = mIpc.clientFlattenAf(mAiq, *inputParams, params);
+    CheckError(!ret, ia_err_general, "@%s, clientFlattenAf fails", __func__);
+
+    ret = mCommon.requestSync(IPC_AIQ_AF_RUN, mMemAf.mHandle);
+    CheckError(!ret, ia_err_general, "@%s, requestSync fails", __func__);
+
+    ret = mIpc.clientUnflattenAf(*params, results);
+    CheckError(!ret, ia_err_general, "@%s, clientUnflattenAf fails", __func__);
+
+    return ia_err_none;
+}
+
+ia_err IntelAiq::awbRun(const ia_aiq_awb_input_params* inputParams, ia_aiq_awb_results** results) {
+    LOG1("@%s, inputParams:%p, results:%p", __func__, inputParams, results);
+    CheckError(!mInitialized, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(reinterpret_cast<ia_aiq*>(mAiq) == nullptr, ia_err_general, "@%s, mAiq is nullptr",
+               __func__);
+    CheckError(!inputParams, ia_err_argument, "@%s, inputParams is nullptr", __func__);
+    CheckError(!results, ia_err_argument, "@%s, results is nullptr", __func__);
+
+    awb_run_params* params = static_cast<awb_run_params*>(mMemAwb.mAddr);
+
+    bool ret = mIpc.clientFlattenAwb(mAiq, *inputParams, params);
+    CheckError(!ret, ia_err_general, "@%s, clientFlattenAwb fails", __func__);
+
+    ret = mCommon.requestSync(IPC_AIQ_AWB_RUN, mMemAwb.mHandle);
+    CheckError(!ret, ia_err_general, "@%s, requestSync fails", __func__);
+
+    ret = mIpc.clientUnflattenAwb(*params, results);
+    CheckError(!ret, ia_err_general, "@%s, clientUnflattenAwb fails", __func__);
+
+    return ia_err_none;
+}
+
+ia_err IntelAiq::gbceRun(const ia_aiq_gbce_input_params* inputParams,
+                         ia_aiq_gbce_results** results) {
+    LOG1("@%s, inputParams:%p, results:%p", __func__, inputParams, results);
+    CheckError(!mInitialized, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(reinterpret_cast<ia_aiq*>(mAiq) == nullptr, ia_err_general, "@%s, mAiq is nullptr",
+               __func__);
+    CheckError(!inputParams, ia_err_argument, "@%s, inputParams is nullptr", __func__);
+    CheckError(!results, ia_err_argument, "@%s, results is nullptr", __func__);
+
+    gbce_run_params* params = static_cast<gbce_run_params*>(mMemGbce.mAddr);
+
+    bool ret = mIpc.clientFlattenGbce(mAiq, *inputParams, params);
+    CheckError(!ret, ia_err_general, "@%s, clientFlattenGbce fails", __func__);
+
+    ret = mCommon.requestSync(IPC_AIQ_GBCE_RUN, mMemGbce.mHandle);
+    CheckError(!ret, ia_err_general, "@%s, requestSync fails", __func__);
+
+    ret = mIpc.clientUnflattenGbce(params, results);
+    CheckError(!ret, ia_err_general, "@%s, clientUnflattenGbce fails", __func__);
+
+    return ia_err_none;
+}
+
+ia_err IntelAiq::paRunV1(const ia_aiq_pa_input_params* inputParams,
+                         ia_aiq_pa_results_v1** results) {
+    LOG1("@%s, inputParams:%p, results:%p", __func__, inputParams, results);
+    CheckError(!mInitialized, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(reinterpret_cast<ia_aiq*>(mAiq) == nullptr, ia_err_general, "@%s, mAiq is nullptr",
+               __func__);
+    CheckError(!inputParams, ia_err_argument, "@%s, inputParams is nullptr", __func__);
+    CheckError(!results, ia_err_argument, "@%s, results is nullptr", __func__);
+
+    pa_run_v1_params* params = static_cast<pa_run_v1_params*>(mMemPa.mAddr);
+
+    bool ret = mIpc.clientFlattenPaV1(mAiq, *inputParams, params);
+    CheckError(!ret, ia_err_general, "@%s, clientFlattenPaV1 fails", __func__);
+
+    ret = mCommon.requestSync(IPC_AIQ_PA_RUN_V1, mMemPa.mHandle);
+    CheckError(!ret, ia_err_general, "@%s, requestSync fails", __func__);
+
+    ret = mIpc.clientUnflattenPaV1(params, results);
+    CheckError(!ret, ia_err_general, "@%s, clientUnflattenPaV1 fails", __func__);
+
+    return ia_err_none;
+}
+
+ia_err IntelAiq::saRunV2(const ia_aiq_sa_input_params_v1* inputParams,
+                         ia_aiq_sa_results_v1** results) {
+    LOG1("@%s, inputParams:%p, results:%p", __func__, inputParams, results);
+    CheckError(!mInitialized, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(reinterpret_cast<ia_aiq*>(mAiq) == nullptr, ia_err_general, "@%s, mAiq is nullptr",
+               __func__);
+    CheckError(!inputParams, ia_err_argument, "@%s, inputParams is nullptr", __func__);
+    CheckError(!results, ia_err_argument, "@%s, results is nullptr", __func__);
+
+    sa_run_v2_params* params = static_cast<sa_run_v2_params*>(mMemSa.mAddr);
+
+    bool ret = mIpc.clientFlattenSaV2(mAiq, *inputParams, params);
+    CheckError(!ret, ia_err_general, "@%s, clientFlattenSaV2 fails", __func__);
+
+    ret = mCommon.requestSync(IPC_AIQ_SA_RUN_V2, mMemSa.mHandle);
+    CheckError(!ret, ia_err_general, "@%s, requestSync fails", __func__);
+
+    ret = mIpc.clientUnflattenSaV2(params, results);
+    CheckError(!ret, ia_err_general, "@%s, clientUnflattenSaV2 fails", __func__);
+
+    return ia_err_none;
+}
+
+ia_err IntelAiq::statisticsSetV4(const ia_aiq_statistics_input_params_v4* inputParams) {
+    LOG1("@%s, inputParams:%p", __func__, inputParams);
+    CheckError(!mInitialized, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(reinterpret_cast<ia_aiq*>(mAiq) == nullptr, ia_err_general, "@%s, mAiq is nullptr",
+               __func__);
+    CheckError(!inputParams, ia_err_argument, "@%s, inputParams is nullptr", __func__);
+
+    set_statistics_set_v4_params* params =
+        static_cast<set_statistics_set_v4_params*>(mMemStat.mAddr);
+
+    bool ret = mIpc.clientFlattenStatSetV4(mAiq, *inputParams, params);
+    CheckError(!ret, ia_err_general, "@%s, clientFlattenStatSetV4 fails", __func__);
+
+    ret = mCommon.requestSync(IPC_AIQ_STATISTICS_SET_V4, mMemStat.mHandle);
+    CheckError(!ret, ia_err_general, "@%s, requestSync fails", __func__);
+
+    return ia_err_none;
+}
+
+ia_err IntelAiq::getAiqdData(ia_binary_data* outData) {
+    LOG1("@%s, outData:%p", __func__, outData);
+    CheckError(!mInitialized, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(reinterpret_cast<ia_aiq*>(mAiq) == nullptr, ia_err_general, "@%s, mAiq is nullptr",
+               __func__);
+    CheckError(!outData, ia_err_argument, "@%s, outData is nullptr", __func__);
+
+    ia_binary_data_params* params = static_cast<ia_binary_data_params*>(mMemAiqd.mAddr);
+
+    params->aiq_handle = mAiq;
+
+    int ret = mCommon.requestSync(IPC_AIQ_GET_AIQD_DATA, mMemAiqd.mHandle);
+    CheckError(!ret, ia_err_general, "@%s, requestSync fails", __func__);
+
+    outData->data = params->data;
+    outData->size = params->size;
+
+    return ia_err_none;
+}
+
+void IntelAiq::deinit() {
+    LOG1("@%s", __func__);
+    CheckError(!mInitialized, VOID_VALUE, "@%s, mInitialized is false", __func__);
+    CheckError(reinterpret_cast<ia_aiq*>(mAiq) == nullptr, VOID_VALUE, "@%s, mAiq is nullptr",
+               __func__);
+
+    aiq_deinit_params* params = static_cast<aiq_deinit_params*>(mMemDeinit.mAddr);
+    params->aiq_handle = mAiq;
+
+    int ret = mCommon.requestSync(IPC_AIQ_DEINIT, mMemDeinit.mHandle);
+    CheckError(!ret, VOID_VALUE, "@%s, requestSync fails", __func__);
+
+    mAiq = reinterpret_cast<uintptr_t>(nullptr);
+}
+
+void IntelAiq::getVersion(std::string* version) {
+    LOG1("@%s", __func__);
+    CheckError(!mInitialized, VOID_VALUE, "@%s, mInitialized is false", __func__);
+    CheckError(reinterpret_cast<ia_aiq*>(mAiq) == nullptr, VOID_VALUE, "@%s, mAiq is nullptr",
+               __func__);
+
+    ia_aiq_version_params* params = static_cast<ia_aiq_version_params*>(mMemVersion.mAddr);
+    params->aiq_handle = mAiq;
+
+    int ret = mCommon.requestSync(IPC_AIQ_GET_VERSION, mMemVersion.mHandle);
+    CheckError(!ret, VOID_VALUE, "@%s, requestSync fails", __func__);
+
+    *version = params->data;
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAiq.h b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAiq.h
new file mode 100644
index 000000000000..109881543c00
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAiq.h
@@ -0,0 +1,70 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_aiq.h>
+#include <ia_types.h>
+
+#include <string>
+#include <vector>
+
+#include "modules/sandboxing/IPCIntelAiq.h"
+#include "modules/sandboxing/client/IntelAlgoCommon.h"
+
+namespace icamera {
+class IntelAiq {
+ public:
+    IntelAiq();
+    virtual ~IntelAiq();
+
+    ia_aiq* init(const ia_binary_data* aiqbData, const ia_binary_data* nvmData,
+                 const ia_binary_data* aiqdData, unsigned int statsMaxWidth,
+                 unsigned int statsMaxHeight, unsigned int maxNumStatsIn, ia_cmc_t* cmc,
+                 ia_mkn* mkn);
+    ia_err aeRun(const ia_aiq_ae_input_params* inputParams, ia_aiq_ae_results** results);
+    ia_err afRun(const ia_aiq_af_input_params* inputParams, ia_aiq_af_results** results);
+    ia_err awbRun(const ia_aiq_awb_input_params* inputParams, ia_aiq_awb_results** results);
+    ia_err gbceRun(const ia_aiq_gbce_input_params* inputParams, ia_aiq_gbce_results** results);
+    ia_err paRunV1(const ia_aiq_pa_input_params* inputParams, ia_aiq_pa_results_v1** results);
+    ia_err saRunV2(const ia_aiq_sa_input_params_v1* inputParams, ia_aiq_sa_results_v1** results);
+    ia_err statisticsSetV4(const ia_aiq_statistics_input_params_v4* inputParams);
+    ia_err getAiqdData(ia_binary_data* outData);
+    void deinit();
+    void getVersion(std::string* version);
+
+ private:
+    IPCIntelAiq mIpc;
+    IntelAlgoCommon mCommon;
+
+    uintptr_t mAiq;
+
+    bool mInitialized;
+
+    ShmMemInfo mMemDeinit;
+    ShmMemInfo mMemAe;
+    ShmMemInfo mMemAf;
+    ShmMemInfo mMemAwb;
+    ShmMemInfo mMemGbce;
+    ShmMemInfo mMemAiqd;
+    ShmMemInfo mMemPa;
+    ShmMemInfo mMemSa;
+    ShmMemInfo mMemStat;
+    ShmMemInfo mMemVersion;
+
+    std::vector<ShmMem> mMems;
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAlgoClient.cpp b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAlgoClient.cpp
new file mode 100644
index 000000000000..ae2de6332fa1
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAlgoClient.cpp
@@ -0,0 +1,432 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelAlgoClient"
+
+#include "modules/sandboxing/client/IntelAlgoClient.h"
+
+#include <fcntl.h>
+#include <sys/mman.h>
+#include <sys/stat.h>
+#include <sys/types.h>
+
+#include <memory>
+#include <string>
+#include <vector>
+
+#include "Parameters.h"
+#include "PlatformData.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+IntelAlgoClient* IntelAlgoClient::sInstance = nullptr;
+Mutex IntelAlgoClient::sLock;
+
+IntelAlgoClient* IntelAlgoClient::getInstance() {
+    AutoMutex lock(sLock);
+
+    if (!sInstance) {
+        sInstance = new IntelAlgoClient;
+    }
+
+    return sInstance;
+}
+
+void IntelAlgoClient::releaseInstance() {
+    AutoMutex lock(sLock);
+
+    if (sInstance) {
+        delete sInstance;
+        sInstance = nullptr;
+    }
+}
+
+IntelAlgoClient::IntelAlgoClient()
+        : mErrCb(nullptr),
+          mGpuBridge(nullptr),
+          mIPCStatus(true),
+          mMojoManager(nullptr),
+          mInitialized(false) {
+    LOGIPC("@%s", __func__);
+}
+
+IntelAlgoClient::~IntelAlgoClient() {
+    LOGIPC("@%s", __func__);
+}
+
+int IntelAlgoClient::initialize() {
+    LOGIPC("@%s, mMojoManager: %p", __func__, mMojoManager);
+    CheckError(!mMojoManager, UNKNOWN_ERROR, "@%s, mMojoManager is nullptr", __func__);
+
+    mCallback = base::Bind(&IntelAlgoClient::callbackHandler, base::Unretained(this));
+    IntelAlgoClient::return_callback = returnCallback;
+
+    mNotifyCallback = base::Bind(&IntelAlgoClient::notifyHandler, base::Unretained(this));
+    IntelAlgoClient::notify = notifyCallback;
+
+    mBridge = cros::CameraAlgorithmBridge::CreateInstance(cros::CameraAlgorithmBackend::kVendorCpu,
+                                                          mMojoManager);
+    CheckError(!mBridge, UNKNOWN_ERROR, "@%s, mBridge is nullptr", __func__);
+    CheckError(mBridge->Initialize(this) != 0, UNKNOWN_ERROR, "@%s, mBridge init fails", __func__);
+
+    if (PlatformData::isUsingGpuAlgo()) {
+        LOGIPC("@%s GPU algo enabled", __func__);
+        mGpuBridge = cros::CameraAlgorithmBridge::CreateInstance(
+            cros::CameraAlgorithmBackend::kGoogleGpu, mMojoManager);
+        CheckError(!mGpuBridge, UNKNOWN_ERROR, "@%s, mGpuBridge is nullptr", __func__);
+        CheckError(mGpuBridge->Initialize(this) != 0, UNKNOWN_ERROR, "@%s, mGpuBridge init fails",
+                   __func__);
+    }
+
+    for (int i = 0; i < IPC_GROUP_NUM; i++) {
+        if (static_cast<IPC_GROUP>(i) != IPC_GROUP_GPU) {
+            mRunner[i] =
+                std::unique_ptr<Runner>(new Runner(static_cast<IPC_GROUP>(i), mBridge.get()));
+        } else if (mGpuBridge) {
+            mRunner[i] = std::unique_ptr<Runner>(new Runner(IPC_GROUP_GPU, mGpuBridge.get()));
+        }
+    }
+    mInitialized = true;
+
+    return OK;
+}
+
+bool IntelAlgoClient::isIPCFine() {
+    std::lock_guard<std::mutex> l(mIPCStatusMutex);
+    LOGIPC("@%s, mIPCStatus:%d", __func__, mIPCStatus);
+
+    return mIPCStatus;
+}
+
+void IntelAlgoClient::registerErrorCallback(const camera_callback_ops_t* errCb) {
+    LOGIPC("@%s, errCb:%p", __func__, errCb);
+
+    std::lock_guard<std::mutex> l(mIPCStatusMutex);
+    mErrCb = errCb;
+
+    if (!mIPCStatus && mErrCb) {
+        camera_msg_data_t data = {CAMERA_IPC_ERROR, {0}};
+        mErrCb->notify(mErrCb, data);
+    }
+}
+
+int IntelAlgoClient::allocateShmMem(const std::string& name, int size, int* fd, void** addr) {
+    LOGIPC("@%s, name:%s, size:%d", __func__, name.c_str(), size);
+
+    *fd = -1;
+    *addr = nullptr;
+    int shmFd = -1;
+    void* shmAddr = nullptr;
+
+    shmFd = shm_open(name.c_str(), O_CREAT | O_RDWR, S_IRUSR | S_IWUSR);
+    CheckError((shmFd == -1), UNKNOWN_ERROR, "@%s, call shm_open fail", __func__);
+
+    do {
+        int ret = fcntl(shmFd, F_GETFD);
+        if (ret == -1) {
+            LOGE("@%s, call fcntl fail, error %s", __func__, strerror(errno));
+            break;
+        }
+
+        ret = ftruncate(shmFd, size);
+        if (ret == -1) {
+            LOGE("@%s, call ftruncate fail, error %s", __func__, strerror(errno));
+            break;
+        }
+
+        struct stat sb;
+        ret = fstat(shmFd, &sb);
+        if (ret == -1) {
+            LOGE("@%s, call fstat fail, error %s", __func__, strerror(errno));
+            break;
+        }
+
+        shmAddr = mmap(0, sb.st_size, PROT_WRITE, MAP_SHARED, shmFd, 0);
+        if (!shmAddr) {
+            LOGE("@%s, call mmap fail, error %s", __func__, strerror(errno));
+            break;
+        }
+
+        *fd = shmFd;
+        *addr = shmAddr;
+
+        return OK;
+    } while (0);
+
+    close(shmFd);
+    return UNKNOWN_ERROR;
+}
+
+void IntelAlgoClient::releaseShmMem(const std::string& name, int size, int fd, void* addr) {
+    LOGIPC("@%s, name:%s, size:%d, fd:%d, addr:%p", __func__, name.c_str(), size, fd, addr);
+
+    munmap(addr, size);
+    close(fd);
+    shm_unlink(name.c_str());
+}
+
+int IntelAlgoClient::requestSync(IPC_CMD cmd, int32_t bufferHandle) {
+    LOGIPC("@%s, cmd:%d:%s, bufferHandle:%d, mInitialized:%d", __func__, cmd,
+           IntelAlgoIpcCmdToString(cmd), bufferHandle, mInitialized);
+    CheckError(!mInitialized, UNKNOWN_ERROR, "@%s, mInitialized is false", __func__);
+    CheckError(!isIPCFine(), UNKNOWN_ERROR, "@%s, IPC error happens", __func__);
+
+    IPC_GROUP group = IntelAlgoIpcCmdToGroup(cmd);
+
+    return mRunner[group]->requestSync(cmd, bufferHandle);
+}
+
+int IntelAlgoClient::requestSync(IPC_CMD cmd) {
+    return requestSync(cmd, -1);
+}
+
+int32_t IntelAlgoClient::registerBuffer(int bufferFd, void* addr, ShmMemUsage usage) {
+    LOGIPC("@%s, bufferFd:%d, mInitialized:%d", __func__, bufferFd, mInitialized);
+    CheckError(!mInitialized, -1, "@%s, mInitialized is false", __func__);
+    CheckError(!isIPCFine(), -1, "@%s, IPC error happens", __func__);
+
+    int32_t handle = -1;
+    if (usage == CPU_ALGO_SHM) {
+        handle = mBridge->RegisterBuffer(bufferFd);
+    } else if (mGpuBridge) {
+        handle = mGpuBridge->RegisterBuffer(bufferFd);
+    }
+    if (handle >= 0) {
+        std::lock_guard<std::mutex> l(mShmMapMutex);
+        mShmMap[addr] = handle;
+    }
+    return handle;
+}
+
+void IntelAlgoClient::deregisterBuffer(int32_t bufferHandle, ShmMemUsage usage) {
+    LOGIPC("@%s, bufferHandle:%d, mInitialized:%d", __func__, bufferHandle, mInitialized);
+    CheckError(!mInitialized, VOID_VALUE, "@%s, mInitialized is false", __func__);
+    CheckError(!isIPCFine(), VOID_VALUE, "@%s, IPC error happens", __func__);
+
+    {
+        std::lock_guard<std::mutex> l(mShmMapMutex);
+        for (auto& item : mShmMap) {
+            if (item.second == bufferHandle) {
+                mShmMap.erase(item.first);
+                break;
+            }
+        }
+    }
+    std::vector<int32_t> handles({bufferHandle});
+    if (usage == CPU_ALGO_SHM) {
+        mBridge->DeregisterBuffers(handles);
+    } else if (mGpuBridge) {
+        mGpuBridge->DeregisterBuffers(handles);
+    }
+}
+
+int32_t IntelAlgoClient::registerGbmBuffer(int bufferFd) {
+    LOGIPC("@%s, bufferFd:%d, mInitialized:%d", __func__, bufferFd, mInitialized);
+    CheckError(!mInitialized, -1, "@%s, mInitialized is false", __func__);
+    CheckError(!isIPCFine(), -1, "@%s, IPC error happens", __func__);
+
+    return mBridge->RegisterBuffer(bufferFd);
+}
+
+void IntelAlgoClient::deregisterGbmBuffer(int32_t bufferHandle) {
+    LOGIPC("@%s, bufferHandle:%d, mInitialized:%d", __func__, bufferHandle, mInitialized);
+    CheckError(!mInitialized, VOID_VALUE, "@%s, mInitialized is false", __func__);
+    CheckError(!isIPCFine(), VOID_VALUE, "@%s, IPC error happens", __func__);
+
+    std::vector<int32_t> handles({bufferHandle});
+    mBridge->DeregisterBuffers(handles);
+}
+
+int32_t IntelAlgoClient::getBufferHandle(void* addr) {
+    CheckError(!mInitialized, -1, "@%s, mInitialized is false", __func__);
+    if (!addr) return -1;
+
+    std::lock_guard<std::mutex> l(mShmMapMutex);
+    CheckError(mShmMap.find(addr) == mShmMap.end(), -1, "%s, Invalid client addr", __func__);
+    return mShmMap[addr];
+}
+
+void IntelAlgoClient::callbackHandler(uint32_t req_id, uint32_t status, int32_t buffer_handle) {
+    LOGIPC("@%s, req_id:%d, status:%d, buffer_handle:%d", __func__, req_id, status, buffer_handle);
+
+    IPC_GROUP group = IntelAlgoIpcCmdToGroup(static_cast<IPC_CMD>(req_id));
+    mRunner[group]->callbackHandler(status, buffer_handle);
+}
+
+void IntelAlgoClient::notifyHandler(uint32_t msg) {
+    LOGIPC("@%s, msg:%d", __func__, msg);
+
+    if (msg != CAMERA_ALGORITHM_MSG_IPC_ERROR) {
+        LOGE("@%s, receive msg:%d, not CAMERA_ALGORITHM_MSG_IPC_ERROR", __func__, msg);
+        return;
+    }
+
+    std::lock_guard<std::mutex> l(mIPCStatusMutex);
+    mIPCStatus = false;
+
+    if (mErrCb) {
+        camera_msg_data_t data = {CAMERA_IPC_ERROR, {0}};
+        mErrCb->notify(mErrCb, data);
+    } else {
+        LOGE("@%s, mErrCb is nullptr, no device error is sent out", __func__);
+    }
+    LOGE("@%s, receive CAMERA_ALGORITHM_MSG_IPC_ERROR", __func__);
+}
+
+void IntelAlgoClient::returnCallback(const camera_algorithm_callback_ops_t* callback_ops,
+                                     uint32_t req_id, uint32_t status, int32_t buffer_handle) {
+    LOGIPC("@%s", __func__);
+    CheckError(!callback_ops, VOID_VALUE, "@%s, callback_ops is nullptr", __func__);
+
+    auto s = const_cast<IntelAlgoClient*>(static_cast<const IntelAlgoClient*>(callback_ops));
+    s->callbackHandler(req_id, status, buffer_handle);
+}
+
+void IntelAlgoClient::notifyCallback(const struct camera_algorithm_callback_ops* callback_ops,
+                                     camera_algorithm_error_msg_code_t msg) {
+    LOGIPC("@%s", __func__);
+    CheckError(!callback_ops, VOID_VALUE, "@%s, callback_ops is nullptr", __func__);
+
+    auto s = const_cast<IntelAlgoClient*>(static_cast<const IntelAlgoClient*>(callback_ops));
+    s->notifyHandler((uint32_t)msg);
+}
+
+IntelAlgoClient::Runner::Runner(IPC_GROUP group, cros::CameraAlgorithmBridge* bridge)
+        : mGroup(group),
+          mBridge(bridge),
+          mIsCallbacked(false),
+          mCbResult(true),
+          mInitialized(false) {
+    LOGIPC("@%s, group:%d", __func__, mGroup);
+
+    pthread_condattr_t attr;
+    int ret = pthread_condattr_init(&attr);
+    if (ret != 0) {
+        LOGE("@%s, call pthread_condattr_init fails, ret:%d", __func__, ret);
+        pthread_condattr_destroy(&attr);
+        return;
+    }
+
+    ret = pthread_condattr_setclock(&attr, CLOCK_MONOTONIC);
+    if (ret != 0) {
+        LOGE("@%s, call pthread_condattr_setclock fails, ret:%d", __func__, ret);
+        pthread_condattr_destroy(&attr);
+        return;
+    }
+
+    ret = pthread_cond_init(&mCbCond, &attr);
+    if (ret != 0) {
+        LOGE("@%s, call pthread_cond_init fails, ret:%d", __func__, ret);
+        pthread_condattr_destroy(&attr);
+        return;
+    }
+
+    pthread_condattr_destroy(&attr);
+
+    ret = pthread_mutex_init(&mCbLock, nullptr);
+    CheckError(ret != 0, VOID_VALUE, "@%s, call pthread_mutex_init fails, ret:%d", __func__, ret);
+
+    mInitialized = true;
+}
+
+IntelAlgoClient::Runner::~Runner() {
+    LOGIPC("@%s, group:%d", __func__, mGroup);
+
+    int ret = pthread_cond_destroy(&mCbCond);
+    if (ret != 0) {
+        LOGE("@%s, call pthread_cond_destroy fails, ret:%d", __func__, ret);
+    }
+
+    ret = pthread_mutex_destroy(&mCbLock);
+    if (ret != 0) {
+        LOGE("@%s, call pthread_mutex_destroy fails, ret:%d", __func__, ret);
+    }
+}
+
+int IntelAlgoClient::Runner::requestSync(IPC_CMD cmd, int32_t bufferHandle) {
+    LOGIPC("@%s, cmd:%d:%s, group:%d, bufferHandle:%d, mInitialized:%d", __func__, cmd,
+           IntelAlgoIpcCmdToString(cmd), mGroup, bufferHandle, mInitialized);
+    CheckError(!mInitialized, UNKNOWN_ERROR, "@%s, mInitialized is false", __func__);
+
+    std::lock_guard<std::mutex> lck(mMutex);
+
+    std::vector<uint8_t> reqHeader(IPC_REQUEST_HEADER_USED_NUM);
+    reqHeader[0] = IPC_MATCHING_KEY;
+
+    // cmd is for request id, no duplicate command will be issued at any given time.
+    mBridge->Request(cmd, reqHeader, bufferHandle);
+    int ret = waitCallback();
+    CheckError((ret != OK), UNKNOWN_ERROR, "@%s, call waitCallback fail", __func__);
+
+    LOGIPC("@%s, cmd:%d:%s, group:%d, mCbResult:%d, done!", __func__, cmd,
+           IntelAlgoIpcCmdToString(cmd), mGroup, mCbResult);
+
+    // check callback result
+    CheckError((mCbResult != true), UNKNOWN_ERROR, "@%s, callback fail", __func__);
+
+    return OK;
+}
+
+void IntelAlgoClient::Runner::callbackHandler(uint32_t status, int32_t buffer_handle) {
+    LOGIPC("@%s, group:%d, status:%d, buffer_handle:%d", __func__, mGroup, status, buffer_handle);
+    if (status != 0) {
+        LOGE("@%s, group:%d, status:%d, buffer_handle:%d", __func__, mGroup, status, buffer_handle);
+    }
+    mCbResult = status != 0 ? false : true;
+
+    pthread_mutex_lock(&mCbLock);
+    mIsCallbacked = true;
+    int ret = pthread_cond_signal(&mCbCond);
+    pthread_mutex_unlock(&mCbLock);
+
+    CheckError(ret != 0, VOID_VALUE, "@%s, group:%d, call pthread_cond_signal fails, ret:%d",
+               __func__, mGroup, ret);
+}
+
+int IntelAlgoClient::Runner::waitCallback() {
+    LOGIPC("@%s, group:%d", __func__, mGroup);
+
+    nsecs_t startTime = CameraUtils::systemTime();
+
+    pthread_mutex_lock(&mCbLock);
+    if (!mIsCallbacked) {
+        int ret = 0;
+        struct timespec ts = {0, 0};
+        clock_gettime(CLOCK_MONOTONIC, &ts);
+        ts.tv_sec += 5;  // 5s timeout
+
+        while (!mIsCallbacked && !ret) {
+            ret = pthread_cond_timedwait(&mCbCond, &mCbLock, &ts);
+        }
+        if (ret != 0) {
+            LOGE("@%s, group:%d, call pthread_cond_timedwait fail, ret:%d, it takes %" PRId64 "ms",
+                 __func__, mGroup, ret, (CameraUtils::systemTime() - startTime) / 1000000);
+            pthread_mutex_unlock(&mCbLock);
+            return UNKNOWN_ERROR;
+        }
+    }
+    mIsCallbacked = false;
+    pthread_mutex_unlock(&mCbLock);
+
+    LOGIPC("@%s: group:%d, it takes %" PRId64 "ms", __func__, mGroup,
+           (CameraUtils::systemTime() - startTime) / 1000000);
+
+    return OK;
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAlgoClient.h b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAlgoClient.h
new file mode 100644
index 000000000000..e076573acc8d
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAlgoClient.h
@@ -0,0 +1,134 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <pthread.h>
+
+#include <memory>
+#include <mutex>
+#include <string>
+#include <unordered_map>
+
+#include "CameraLog.h"
+#include "Parameters.h"
+#include "base/bind.h"
+#include "base/callback.h"
+#include "cros-camera/camera_algorithm_bridge.h"
+#include "iutils/Thread.h"
+#include "modules/sandboxing/IPCCommon.h"
+
+namespace icamera {
+
+typedef enum ShmMemUsage { CPU_ALGO_SHM, GPU_ALGO_SHM } ShmMemUsage;
+
+class IntelAlgoClient : public camera_algorithm_callback_ops_t {
+ public:
+    static IntelAlgoClient* getInstance();
+    static void releaseInstance();
+
+    IntelAlgoClient();
+    virtual ~IntelAlgoClient();
+
+    void setMojoManager(cros::CameraMojoChannelManager* manager) { mMojoManager = manager; }
+
+    // Connect to the algo processes(cpu and gpu)
+    // It must be called after all preparation are ready in camera service
+    int initialize();
+
+    bool isIPCFine();
+
+    // when IPC error happens, device error
+    // will be sent out via the camera_callback_ops_t which belongs to CameraHal.
+    // before the CameraHal be terminated, set nullptr in the function.
+    void registerErrorCallback(const camera_callback_ops_t* errCb);
+
+    int allocateShmMem(const std::string& name, int size, int* fd, void** addr);
+    void releaseShmMem(const std::string& name, int size, int fd, void* addr);
+
+    int requestSync(IPC_CMD cmd, int32_t bufferHandle);
+    int requestSync(IPC_CMD cmd);
+
+    int32_t registerBuffer(int bufferFd, void* addr, ShmMemUsage usage = CPU_ALGO_SHM);
+    void deregisterBuffer(int32_t bufferHandle, ShmMemUsage usage = CPU_ALGO_SHM);
+    int32_t registerGbmBuffer(int bufferFd);
+    void deregisterGbmBuffer(int32_t bufferHandle);
+    int32_t getBufferHandle(void* addr);
+
+ private:
+    int waitCallback();
+
+    void callbackHandler(uint32_t req_id, uint32_t status, int32_t buffer_handle);
+    void notifyHandler(uint32_t msg);
+
+    // when the request is done, the callback will be received.
+    static void returnCallback(const camera_algorithm_callback_ops_t* callback_ops, uint32_t req_id,
+                               uint32_t status, int32_t buffer_handle);
+    // when IPC error happens in the bridge, notifyCallback will be called.
+    static void notifyCallback(const struct camera_algorithm_callback_ops* callback_ops,
+                               camera_algorithm_error_msg_code_t msg);
+
+ private:
+    /*
+     * Get access to the IntelAlgoClient singleton.
+     */
+    static IntelAlgoClient* sInstance;
+    static Mutex sLock;  // Guard for singleton creation.
+
+    const camera_callback_ops_t* mErrCb;
+
+    std::unique_ptr<cros::CameraAlgorithmBridge> mBridge;
+    std::unique_ptr<cros::CameraAlgorithmBridge> mGpuBridge;
+
+    base::Callback<void(uint32_t, uint32_t, int32_t)> mCallback;
+    base::Callback<void(uint32_t)> mNotifyCallback;
+    bool mIPCStatus;             // true: no error happens, false: error happens
+    std::mutex mIPCStatusMutex;  // the mutex for mIPCStatus
+
+    std::unordered_map<void*, int32_t> mShmMap;  // <addr in client, server handle in server>
+    std::mutex mShmMapMutex;                     // the mutex for mShmMap
+
+    cros::CameraMojoChannelManager* mMojoManager;
+    bool mInitialized;
+
+ private:
+    class Runner {
+     public:
+        Runner(IPC_GROUP group, cros::CameraAlgorithmBridge* bridge);
+        virtual ~Runner();
+        int requestSync(IPC_CMD cmd, int32_t bufferHandle);
+        void callbackHandler(uint32_t status, int32_t buffer_handle);
+
+     private:
+        int waitCallback();
+
+     private:
+        IPC_GROUP mGroup;
+        cros::CameraAlgorithmBridge* mBridge;
+        pthread_mutex_t mCbLock;
+        pthread_cond_t mCbCond;
+        bool mIsCallbacked;
+        bool mCbResult;  // true: success, false: fail
+
+        bool mInitialized;
+
+        std::mutex mMutex;  // the mutex for the public method
+    };
+
+    std::unique_ptr<Runner> mRunner[IPC_GROUP_NUM];
+};
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAlgoCommon.cpp b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAlgoCommon.cpp
new file mode 100644
index 000000000000..6211a3d84e8e
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAlgoCommon.cpp
@@ -0,0 +1,131 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelAlgoCommon"
+
+#include "modules/sandboxing/client/IntelAlgoCommon.h"
+
+#include <string>
+#include <vector>
+
+#include "CameraLog.h"
+#include "PlatformData.h"
+
+namespace icamera {
+IntelAlgoCommon::IntelAlgoCommon() {
+    LOGIPC("@%s", __func__);
+
+    mClient = IntelAlgoClient::getInstance();
+    LOGIPC("@%s, mClient:%p", __func__, mClient);
+}
+
+IntelAlgoCommon::~IntelAlgoCommon() {
+    LOGIPC("@%s", __func__);
+}
+
+bool IntelAlgoCommon::allocShmMem(const std::string& name, int size, ShmMemInfo* shm,
+                                  ShmMemUsage usage) {
+    LOGIPC("@%s", __func__);
+    CheckError(mClient == nullptr, false, "@%s, mClient is nullptr", __func__);
+
+    shm->mName = name;
+    shm->mSize = size;
+    int ret = mClient->allocateShmMem(shm->mName, shm->mSize, &shm->mFd, &shm->mAddr);
+    CheckError((ret != OK), false, "@%s, call allocateShmMem fail", __func__);
+
+    shm->mHandle = mClient->registerBuffer(shm->mFd, shm->mAddr, usage);
+    if (shm->mHandle < 0) {
+        LOGE("@%s, call mBridge->RegisterBuffer fail", __func__);
+        mClient->releaseShmMem(shm->mName, shm->mSize, shm->mFd, shm->mAddr);
+        return false;
+    }
+
+    return true;
+}
+
+int32_t IntelAlgoCommon::registerGbmBuffer(int bufferFd) {
+    LOGIPC("@%s, bufferFd:%d", __func__, bufferFd);
+    CheckError(mClient == nullptr, -1, "@%s, mClient is nullptr", __func__);
+
+    return mClient->registerGbmBuffer(bufferFd);
+}
+
+void IntelAlgoCommon::deregisterGbmBuffer(int32_t bufferHandle) {
+    LOGIPC("@%s, bufferHandle:%d", __func__, bufferHandle);
+    CheckError(mClient == nullptr, VOID_VALUE, "@%s, mClient is nullptr", __func__);
+
+    mClient->deregisterGbmBuffer(bufferHandle);
+}
+
+bool IntelAlgoCommon::requestSync(IPC_CMD cmd, int32_t handle) {
+    LOGIPC("@%s", __func__);
+    CheckError(mClient == nullptr, false, "@%s, mClient is nullptr", __func__);
+
+    return mClient->requestSync(cmd, handle) == OK ? true : false;
+}
+
+bool IntelAlgoCommon::requestSync(IPC_CMD cmd) {
+    LOGIPC("@%s", __func__);
+    CheckError(mClient == nullptr, false, "@%s, mClient is nullptr", __func__);
+
+    return mClient->requestSync(cmd) == OK ? true : false;
+}
+
+void IntelAlgoCommon::freeShmMem(const ShmMemInfo& shm, ShmMemUsage usage) {
+    LOGIPC("@%s, mHandle:%d, mFd:%d, mName:%s, mSize:%d, mAddr:%p", __func__, shm.mHandle, shm.mFd,
+           shm.mName.c_str(), shm.mSize, shm.mAddr);
+    CheckError(mClient == nullptr, VOID_VALUE, "@%s, mClient is nullptr", __func__);
+    if (shm.mHandle < 0 || shm.mFd < 0) {
+        LOGE("@%s, mHandle:%d, mFd:%d, one of them < 0", __func__, shm.mHandle, shm.mFd);
+        return;
+    }
+
+    mClient->deregisterBuffer(shm.mHandle, usage);
+    mClient->releaseShmMem(shm.mName, shm.mSize, shm.mFd, shm.mAddr);
+}
+
+bool IntelAlgoCommon::allocateAllShmMems(std::vector<ShmMem>* mems) {
+    LOGIPC("@%s", __func__);
+
+    for (auto& it : *mems) {
+        ShmMemInfo* mem = it.mem;
+        mem->mName = it.name;
+        mem->mSize = it.size;
+        bool ret = allocShmMem(mem->mName, mem->mSize, mem);
+        CheckError(!ret, false, "@%s, allocShmMem fails, name:%s, size:%d", __func__,
+                   mem->mName.c_str(), mem->mSize);
+        it.allocated = true;
+    }
+
+    return true;
+}
+
+void IntelAlgoCommon::releaseAllShmMems(const std::vector<ShmMem>& mems) {
+    LOGIPC("@%s", __func__);
+
+    for (auto& it : mems) {
+        if (it.allocated) {
+            freeShmMem(*it.mem);
+        }
+    }
+}
+
+int32_t IntelAlgoCommon::getShmMemHandle(void* addr) {
+    CheckError(mClient == nullptr, -1, "@%s, mClient is nullptr", __func__);
+    return mClient->getBufferHandle(addr);
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAlgoCommon.h b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAlgoCommon.h
new file mode 100644
index 000000000000..5634528dc929
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelAlgoCommon.h
@@ -0,0 +1,63 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <string>
+#include <vector>
+
+#include "IntelAlgoClient.h"
+
+namespace icamera {
+typedef struct ShmMemInfo {
+    std::string mName;
+    int mSize;
+    int mFd;
+    void* mAddr;
+    int32_t mHandle;
+    ShmMemInfo() : mName(""), mSize(0), mFd(-1), mAddr(nullptr), mHandle(-1) {}
+} ShmMemInfo;
+
+typedef struct ShmMem {
+    std::string name;
+    int size;
+    ShmMemInfo* mem;
+    bool allocated;
+} ShmMem;
+
+class IntelAlgoCommon {
+ public:
+    IntelAlgoCommon();
+    virtual ~IntelAlgoCommon();
+
+    bool allocShmMem(const std::string& name, int size, ShmMemInfo* shm,
+                     ShmMemUsage usage = CPU_ALGO_SHM);
+    int32_t registerGbmBuffer(int bufferFd);
+    void deregisterGbmBuffer(int32_t bufferHandle);
+    bool requestSync(IPC_CMD cmd, int32_t handle);
+    bool requestSync(IPC_CMD cmd);
+    void freeShmMem(const ShmMemInfo& shm, ShmMemUsage usage = CPU_ALGO_SHM);
+
+    bool allocateAllShmMems(std::vector<ShmMem>* mems);
+    void releaseAllShmMems(const std::vector<ShmMem>& mems);
+
+    int32_t getShmMemHandle(void* addr);
+
+ private:
+    IntelAlgoClient* mClient;
+};
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelCmc.cpp b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelCmc.cpp
new file mode 100644
index 000000000000..94406539310a
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelCmc.cpp
@@ -0,0 +1,107 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelCmc"
+
+#include "modules/sandboxing/client/IntelCmc.h"
+
+#include <string>
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelCmc::IntelCmc() : mInitialized(false) {
+    LOGIPC("@%s", __func__);
+
+    mHandle = nullptr;
+    mCmcRemoteHandle = reinterpret_cast<uintptr_t>(nullptr);
+
+    uintptr_t personal = reinterpret_cast<uintptr_t>(this);
+    std::string initName = "/cmcInit" + std::to_string(personal) + "Shm";
+    std::string deinitName = "/cmcDeinit" + std::to_string(personal) + "Shm";
+
+    mMems = {{initName.c_str(), sizeof(cmc_init_params), &mMemInit, false},
+             {deinitName.c_str(), sizeof(cmc_deinit_params), &mMemDeinit, false}};
+
+    bool success = mCommon.allocateAllShmMems(&mMems);
+    if (!success) {
+        mCommon.releaseAllShmMems(mMems);
+        return;
+    }
+
+    LOGIPC("@%s, done", __func__);
+    mInitialized = true;
+}
+
+IntelCmc::~IntelCmc() {
+    LOGIPC("@%s", __func__);
+    mCommon.releaseAllShmMems(mMems);
+}
+
+bool IntelCmc::init(const ia_binary_data* aiqbData, const ia_binary_data* nvmData) {
+    LOGIPC("@%s, aiqbData:%p, nvmData:%p", __func__, aiqbData, nvmData);
+    CheckError(nvmData, false, "@%s, nvmData should be nullptr", __func__);
+
+    CheckError(mInitialized == false, false, "@%s, mInitialized is false", __func__);
+    CheckError(!aiqbData, false, "@%s, aiqbData is nullptr", __func__);
+    CheckError(!aiqbData->data, false, "@%s, aiqbData->data is nullptr", __func__);
+    CheckError(aiqbData->size == 0, false, "@%s, aiqbData->size is 0", __func__);
+
+    cmc_init_params* params = static_cast<cmc_init_params*>(mMemInit.mAddr);
+
+    bool ret = mIpc.clientFlattenInit(*aiqbData, params);
+    CheckError(ret == false, false, "@%s, clientFlattenInit fails", __func__);
+
+    ret = mCommon.requestSync(IPC_CMC_INIT, mMemInit.mHandle);
+    CheckError(ret == false, false, "@%s, requestSync fails", __func__);
+
+    ret = mIpc.clientUnflattenInit(*params, &mHandle, &mCmcRemoteHandle);
+    CheckError(ret == false, false, "@%s, clientUnflattenInit fails", __func__);
+
+    return true;
+}
+
+ia_cmc_t* IntelCmc::getCmc() const {
+    LOGIPC("@%s, mHandle:%p", __func__, mHandle);
+
+    return mHandle;
+}
+
+uintptr_t IntelCmc::getCmcHandle() const {
+    LOGIPC("@%s", __func__);
+
+    return mCmcRemoteHandle;
+}
+
+void IntelCmc::deinit() {
+    LOGIPC("@%s, mCmc:%p", __func__, mHandle);
+
+    CheckError(mInitialized == false, VOID_VALUE, "@%s, mInitialized is false", __func__);
+    CheckError(!mHandle, VOID_VALUE, "@%s, mHandle is nullptr", __func__);
+    CheckError(reinterpret_cast<ia_cmc_t*>(mCmcRemoteHandle) == nullptr, VOID_VALUE,
+               "@%s, mCmcRemoteHandle is nullptr", __func__);
+
+    cmc_deinit_params* params = static_cast<cmc_deinit_params*>(mMemDeinit.mAddr);
+    params->cmc_handle = mCmcRemoteHandle;
+
+    bool ret = mCommon.requestSync(IPC_CMC_DEINIT, mMemDeinit.mHandle);
+    CheckError(ret == false, VOID_VALUE, "@%s, requestSync fails", __func__);
+    mHandle = nullptr;
+    mCmcRemoteHandle = reinterpret_cast<uintptr_t>(nullptr);
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelCmc.h b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelCmc.h
new file mode 100644
index 000000000000..be5544bfbacb
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelCmc.h
@@ -0,0 +1,58 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+#include <ia_cmc_parser.h>
+
+#include <vector>
+
+#include "IntelAlgoCommon.h"
+#include "modules/sandboxing/IPCIntelCmc.h"
+
+namespace icamera {
+class IntelCmc {
+ public:
+    IntelCmc();
+    virtual ~IntelCmc();
+
+    // the nvmData must be nullptr currently
+    bool init(const ia_binary_data* aiqbData, const ia_binary_data* nvmData);
+
+    ia_cmc_t* getCmc() const;
+    uintptr_t getCmcHandle() const;
+
+    void deinit();
+
+ private:
+    IPCIntelCmc mIpc;
+    IntelAlgoCommon mCommon;
+
+    bool mInitialized;
+
+    ShmMemInfo mMemInit;
+    ShmMemInfo mMemDeinit;
+
+    std::vector<ShmMem> mMems;
+
+    ia_cmc_t* mHandle;  // it points to the SHM
+
+    // the pointer is in another process.
+    // because the ia_aiq_init() needs the cmc pointer,
+    // so keep the pinter in the IntelCmc and pass it aiq when init aiq.
+    // at the same time, it will be used when deinit the cmc.
+    uintptr_t mCmcRemoteHandle;
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelDvs.cpp b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelDvs.cpp
new file mode 100644
index 000000000000..ce5bd9a37e8d
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelDvs.cpp
@@ -0,0 +1,336 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "ClientIntelDvs"
+
+#include "modules/sandboxing/client/IntelDvs.h"
+
+#include <string>
+
+#include "CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelDvs::IntelDvs() : mInitialized(false) {
+    LOGIPC("@%s", __func__);
+
+    uintptr_t personal = reinterpret_cast<uintptr_t>(this);
+    std::string initName = "/dvsInit" + std::to_string(personal) + SHM_NAME;
+    std::string deinitName = "/dvsDeinit" + std::to_string(personal) + SHM_NAME;
+    std::string configName = "/dvsConfig" + std::to_string(personal) + SHM_NAME;
+    std::string setNonBlankRatioName = "/dvsSetNonBlankRatio" + std::to_string(personal) + SHM_NAME;
+    std::string setDigitalZoomModeName =
+        "/dvsSetDigitalZoomModeName" + std::to_string(personal) + SHM_NAME;
+    std::string setDigitalZoomRegionName =
+        "/dvsSetDigitalZoomRegionName" + std::to_string(personal) + SHM_NAME;
+    std::string setDigitalZoomCoordinateName =
+        "/dvsSetDigitalZoomCoordinateName" + std::to_string(personal) + SHM_NAME;
+    std::string setDigitalZoomMagnitudeName =
+        "/dvsSetDigitalZoomMagnitudeName" + std::to_string(personal) + SHM_NAME;
+    std::string freeMorphTableName = "/dvsFreeMorphTableName" + std::to_string(personal) + SHM_NAME;
+    std::string allocateMorphTalbeName =
+        "/dvsAllocateMorphTalbeName" + std::to_string(personal) + SHM_NAME;
+    std::string getMorphTableName = "/dvsGetMorphTableName" + std::to_string(personal) + SHM_NAME;
+    std::string setStatisticsName = "/dvsSetStatisticsName" + std::to_string(personal) + SHM_NAME;
+    std::string executeName = "/dvsExecuteName" + std::to_string(personal) + SHM_NAME;
+    std::string getImageTransformationName =
+        "/dvsGetImageTransformationName" + std::to_string(personal) + SHM_NAME;
+
+    mMems = {
+        {initName.c_str(), sizeof(DvsInitParams), &mMemInit, false},
+        {deinitName.c_str(), sizeof(DvsDeinitParams), &mMemDeinit, false},
+        {configName.c_str(), sizeof(DvsConfigParams), &mMemConfig, false},
+        {setNonBlankRatioName.c_str(), sizeof(DvsNoneBlankRatioParams), &mMemNonBlankRatio, false},
+        {setDigitalZoomModeName.c_str(), sizeof(DvsDigitalZoomMode), &mMemZoomMode, false},
+        {setDigitalZoomRegionName.c_str(), sizeof(DvsDigitalZoomRegion), &mMemZoomRegion, false},
+        {setDigitalZoomCoordinateName.c_str(), sizeof(DvsDigitalZoomCoordinate),
+         &mMemZoomCoordinate, false},
+        {setDigitalZoomMagnitudeName.c_str(), sizeof(DvsDigitalZoomMagnitude), &mMemZoomMagnitude,
+         false},
+        {freeMorphTableName.c_str(), sizeof(DvsFreeMorphParams), &mMemFreeMorph, false},
+        {allocateMorphTalbeName.c_str(), sizeof(DvsMorphParams), &mMemAllocateMorph, false},
+        {getMorphTableName.c_str(), sizeof(DvsMorphParams), &mMemGetMorphTable, false},
+        {setStatisticsName.c_str(), sizeof(DvsStatistcs), &mMemStatistics, false},
+        {executeName.c_str(), sizeof(DvsExecute), &mMemExecute, false},
+        {getImageTransformationName.c_str(), sizeof(DvsImageTransformation), &mMemImageTransfor,
+         false},
+    };
+
+    bool success = mCommon.allocateAllShmMems(&mMems);
+    if (!success) {
+        mCommon.releaseAllShmMems(mMems);
+        return;
+    }
+
+    LOGIPC("@%s, done", __func__);
+    mInitialized = true;
+}
+
+IntelDvs::~IntelDvs() {
+    LOGIPC("@%s", __func__);
+    mCommon.releaseAllShmMems(mMems);
+}
+
+ia_err IntelDvs::init(const ia_binary_data& aiqTuningBinary, const ia_cmc_t* cmc,
+                      ia_dvs_state** dvsHandle) {
+    LOGIPC("@%s", __func__);
+    CheckError(mInitialized == false, ia_err_none, "@%s, mInitialized is false", __func__);
+    CheckError(aiqTuningBinary.data == nullptr, ia_err_none, "@%s, aiqTuningBinary.data is nullptr",
+               __func__);
+    CheckError(aiqTuningBinary.size == 0, ia_err_none, "@%s, aiqTuningBinary.size is 0", __func__);
+    CheckError(dvsHandle == nullptr, ia_err_none, "@%s, dvsHandle is nullptr", __func__);
+
+    bool ret = mIpc.clientFlattenInit(mMemInit.mAddr, mMemInit.mSize, &aiqTuningBinary, cmc);
+    CheckError(ret == false, ia_err_none, "@%s, clientFlattenInit fails", __func__);
+
+    ret = mCommon.requestSync(IPC_DVS_INIT, mMemInit.mHandle);
+    CheckError(ret == false, ia_err_none, "@%s, requestSync fails", __func__);
+
+    ret = mIpc.clientUnflattenInit(mMemInit.mAddr, mMemInit.mSize, dvsHandle);
+    CheckError(ret == false, ia_err_none, "@%s, clientUnflattenInit fails", __func__);
+    return ia_err_none;
+}
+
+void IntelDvs::deinit(ia_dvs_state* dvsHandle) {
+    LOGIPC("@%s, dvsHandle:%p", __func__, dvsHandle);
+    CheckError(mInitialized == false, VOID_VALUE, "@%s, mInitialized is false", __func__);
+    CheckError(dvsHandle == nullptr, VOID_VALUE, "@%s, dvsHandle is nullptr", __func__);
+
+    bool ret = mIpc.clientFlattenDeinit(mMemDeinit.mAddr, mMemDeinit.mSize, dvsHandle);
+    CheckError(ret == false, VOID_VALUE, "@%s, clientFlattenDeinit fails", __func__);
+
+    ret = mCommon.requestSync(IPC_DVS_DEINIT, mMemDeinit.mHandle);
+    CheckError(ret == false, VOID_VALUE, "@%s, requestSync fails", __func__);
+}
+
+ia_err IntelDvs::config(ia_dvs_state* dvsHandle, ia_dvs_configuration* config, float zoomRatio) {
+    LOGIPC("@%s, dvsHandle:%p, config:%p, zoomRatio:%f", __func__, dvsHandle, config, zoomRatio);
+    CheckError(mInitialized == false, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(dvsHandle == nullptr, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+    CheckError(config == nullptr, ia_err_general, "@%s, config is nullptr", __func__);
+
+    bool ret =
+        mIpc.clientFlattenConfig(mMemConfig.mAddr, mMemConfig.mSize, dvsHandle, config, zoomRatio);
+    CheckError(ret == false, ia_err_general, "@%s, clientFlattenConfig fails", __func__);
+
+    ret = mCommon.requestSync(IPC_DVS_CONFIG, mMemConfig.mHandle);
+    CheckError(ret == false, ia_err_general, "@%s, requestSync fails", __func__);
+
+    return ia_err_none;
+}
+
+ia_err IntelDvs::setNonBlankRatio(ia_dvs_state* dvsHandle, float nonBlankingRatio) {
+    LOGIPC("@%s, dvsHandle:%p, nonBlankingRatio:%f", __func__, dvsHandle, nonBlankingRatio);
+    CheckError(mInitialized == false, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(dvsHandle == nullptr, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+
+    bool ret = mIpc.clientFlattenNoneBlanckRation(mMemNonBlankRatio.mAddr, mMemNonBlankRatio.mSize,
+                                                  dvsHandle, nonBlankingRatio);
+    CheckError(ret == false, ia_err_general, "@%s, clientFlattenNoneBlanckRation fails", __func__);
+
+    ret = mCommon.requestSync(IPC_DVS_SET_NONE_BLANK_RATION, mMemNonBlankRatio.mHandle);
+    CheckError(ret == false, ia_err_general, "@%s, requestSync fails", __func__);
+
+    return ia_err_none;
+}
+
+ia_err IntelDvs::setDigitalZoomMode(ia_dvs_state* dvsHandle, ia_dvs_zoom_mode zoomMode) {
+    LOGIPC("@%s, dvsHandle:%p, zoomMode:%f", __func__, dvsHandle, zoomMode);
+    CheckError(mInitialized == false, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(dvsHandle == nullptr, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+
+    bool ret = mIpc.clientFlattenDigitalZoomMode(mMemZoomMode.mAddr, mMemZoomMode.mSize, dvsHandle,
+                                                 zoomMode);
+    CheckError(ret == false, ia_err_general, "@%s, clientFlattenDigitalZoomMode fails", __func__);
+
+    ret = mCommon.requestSync(IPC_DVS_SET_DIGITAL_ZOOM_MODE, mMemZoomMode.mHandle);
+    CheckError(ret == false, ia_err_general, "@%s, requestSync fails", __func__);
+
+    return ia_err_none;
+}
+
+ia_err IntelDvs::setDigitalZoomRegion(ia_dvs_state* dvsHandle, ia_rectangle* zoomRegion) {
+    LOGIPC("@%s, dvsHandle:%p, zoomRegion:%p", __func__, dvsHandle, zoomRegion);
+    CheckError(mInitialized == false, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(dvsHandle == nullptr, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+    CheckError(zoomRegion == nullptr, ia_err_general, "@%s, zoomRegion is nullptr", __func__);
+
+    bool ret = mIpc.clientFlattenDigitalZoomRegion(mMemZoomRegion.mAddr, mMemZoomRegion.mSize,
+                                                   dvsHandle, zoomRegion);
+    CheckError(ret == false, ia_err_general, "@%s, clientFlattenDigitalZoomRegion fails", __func__);
+
+    ret = mCommon.requestSync(IPC_DVS_SET_DIGITAL_ZOOM_REGION, mMemZoomRegion.mHandle);
+    CheckError(ret == false, ia_err_general, "@%s, requestSync fails", __func__);
+
+    return ia_err_none;
+}
+
+ia_err IntelDvs::setDigitalZoomCoordinate(ia_dvs_state* dvsHandle, ia_coordinate* zoomCoordinate) {
+    LOGIPC("@%s, dvsHandle:%p, zoomCoordinate:%p", __func__, dvsHandle, zoomCoordinate);
+    CheckError(mInitialized == false, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(dvsHandle == nullptr, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+    CheckError(zoomCoordinate == nullptr, ia_err_general, "@%s, zoomCoordinate is nullptr",
+               __func__);
+
+    bool ret = mIpc.clientFlattenDigitalZoomCoordinate(
+        mMemZoomCoordinate.mAddr, mMemZoomCoordinate.mSize, dvsHandle, zoomCoordinate);
+    CheckError(ret == false, ia_err_general, "@%s, clientFlattenDigitalZoomCoordinate fails",
+               __func__);
+
+    ret = mCommon.requestSync(IPC_DVS_SET_DIGITAL_ZOOM_COORDINATE, mMemZoomCoordinate.mHandle);
+    CheckError(ret == false, ia_err_general, "@%s, requestSync fails", __func__);
+
+    return ia_err_none;
+}
+
+ia_err IntelDvs::setDigitalZoomMagnitude(ia_dvs_state* dvsHandle, float zoomRatio) {
+    LOGIPC("@%s, dvsHandle:%p, zoomRatio:%f", __func__, dvsHandle, zoomRatio);
+    CheckError(mInitialized == false, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(dvsHandle == nullptr, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+
+    bool ret = mIpc.clientFlattenDigitalZoomMagnitude(
+        mMemZoomMagnitude.mAddr, mMemZoomMagnitude.mSize, dvsHandle, zoomRatio);
+    CheckError(ret == false, ia_err_general, "@%s, clientFlattenDigitalZoomMagnitude fails",
+               __func__);
+
+    ret = mCommon.requestSync(IPC_DVS_SET_DIGITAL_ZOOM_MAGNITUDE, mMemZoomMagnitude.mHandle);
+    CheckError(ret == false, ia_err_general, "@%s, requestSync fails", __func__);
+
+    return ia_err_none;
+}
+
+void IntelDvs::freeMorphTable(ia_dvs_state* dvsHandle, ia_dvs_morph_table* morphTable) {
+    LOGIPC("@%s, dvsHandle%p, morphTable:%p", __func__, dvsHandle, morphTable);
+    CheckError(mInitialized == false, VOID_VALUE, "@%s, mInitialized is false", __func__);
+    CheckError(dvsHandle == nullptr, VOID_VALUE, "@%s, dvsHandle is nullptr", __func__);
+    CheckError(morphTable == nullptr, VOID_VALUE, "@%s, morphTable is nullptr", __func__);
+
+    bool ret = mIpc.clientFlattenFreeMorphTable(mMemFreeMorph.mAddr, mMemFreeMorph.mSize, dvsHandle,
+                                                morphTable);
+    CheckError(ret == false, VOID_VALUE, "@%s, clientFlattenFreeMorphTable fails", __func__);
+
+    ret = mCommon.requestSync(IPC_DVS_FREE_MORPH_TABLE, mMemFreeMorph.mHandle);
+    CheckError(ret == false, VOID_VALUE, "@%s, requestSync fails", __func__);
+}
+
+ia_dvs_morph_table* IntelDvs::allocateMorphTalbe(ia_dvs_state* dvsHandle) {
+    LOGIPC("@%s, dvsHandle:%p", __func__, dvsHandle);
+    CheckError(mInitialized == false, nullptr, "@%s, mInitialized is false", __func__);
+    CheckError(dvsHandle == nullptr, nullptr, "@%s, dvsHandle is nullptr", __func__);
+
+    bool ret = mIpc.clientFlattenAllocateMorphTable(mMemAllocateMorph.mAddr,
+                                                    mMemAllocateMorph.mSize, dvsHandle);
+    CheckError(ret == false, nullptr, "@%s, clientFlattenAllocateMorphTable fails", __func__);
+
+    ret = mCommon.requestSync(IPC_DVS_ALLOCATE_MORPH_TABLE, mMemAllocateMorph.mHandle);
+    CheckError(ret == false, nullptr, "@%s, requestSync fails", __func__);
+
+    ia_dvs_morph_table* morphTable = nullptr;
+    ret = mIpc.clientUnflattenAllocateMorphTalbe(mMemAllocateMorph.mAddr, mMemAllocateMorph.mSize,
+                                                 &morphTable);
+    CheckError(ret == false, nullptr, "@%s, clientUnflattenAllocateMorphTalbe fails", __func__);
+
+    return morphTable;
+}
+
+int IntelDvs::getMorphTable(ia_dvs_state* dvsHandle, ia_dvs_morph_table* morphTable,
+                            DvsResult* result) {
+    LOGIPC("@%s, dvsHandle:%p, morphTable:%p", __func__, dvsHandle, morphTable);
+    CheckError(mInitialized == false, UNKNOWN_ERROR, "@%s, mInitialized is false", __func__);
+    CheckError(dvsHandle == nullptr, UNKNOWN_ERROR, "@%s, dvsHandle is nullptr", __func__);
+    CheckError(morphTable == nullptr, UNKNOWN_ERROR, "@%s, morphTable is nullptr", __func__);
+    CheckError(result == nullptr, UNKNOWN_ERROR, "@%s, result is nullptr", __func__);
+
+    bool ret = mIpc.clientFlattenGetMorphTable(mMemGetMorphTable.mAddr, mMemGetMorphTable.mSize,
+                                               dvsHandle, morphTable);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientFlattenGetMorphTable fails", __func__);
+
+    ret = mCommon.requestSync(IPC_DVS_GET_MORPH_TABLE, mMemGetMorphTable.mHandle);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    ia_dvs_morph_table* morphTableTmp = nullptr;
+    ret = mIpc.clientUnflattenGetMorphTalbe(mMemGetMorphTable.mAddr, mMemGetMorphTable.mSize,
+                                            &morphTableTmp);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientUnflattenGetMorphTalbe fails", __func__);
+
+    int err = DvsResult::deepCopyDvsResults(*morphTableTmp, &result->mMorphTable);
+    CheckError(err != OK, UNKNOWN_ERROR, "@%s, deepCopyDvsResults fails", __func__);
+
+    return OK;
+}
+
+ia_err IntelDvs::setStatistics(ia_dvs_state* dvsHandle, const ia_dvs_statistics* statistics,
+                               const ia_aiq_ae_results* aeResults,
+                               const ia_aiq_af_results* afResults,
+                               const ia_aiq_sensor_events* sensorEvents, uint64_t frameReadoutStart,
+                               uint64_t frameReadoutEnd) {
+    LOGIPC("@%s, dvsHandle:%p", __func__, dvsHandle);
+    CheckError(mInitialized == false, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(dvsHandle == nullptr, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+
+    bool ret = mIpc.clientFlattenSetStatistics(mMemStatistics.mAddr, mMemStatistics.mSize,
+                                               dvsHandle, statistics, aeResults, afResults,
+                                               sensorEvents, frameReadoutStart, frameReadoutEnd);
+    CheckError(ret == false, ia_err_general, "@%s, clientFlattenSetStatistics fails", __func__);
+
+    ret = mCommon.requestSync(IPC_DVS_SET_STATISTICS, mMemStatistics.mHandle);
+    CheckError(ret == false, ia_err_general, "@%s, requestSync fails", __func__);
+
+    return ia_err_none;
+}
+
+ia_err IntelDvs::execute(ia_dvs_state* dvsHandle, uint16_t focusPosition) {
+    LOGIPC("@%s, dvsHandle:%p", __func__, dvsHandle);
+    CheckError(mInitialized == false, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(dvsHandle == nullptr, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+
+    bool ret =
+        mIpc.clientFlattenExecute(mMemExecute.mAddr, mMemExecute.mSize, dvsHandle, focusPosition);
+    CheckError(ret == false, ia_err_general, "@%s, clientFlattenGetMorphTable fails", __func__);
+
+    ret = mCommon.requestSync(IPC_DVS_EXECUTE, mMemExecute.mHandle);
+    CheckError(ret == false, ia_err_general, "@%s, requestSync fails", __func__);
+
+    return ia_err_none;
+}
+
+ia_err IntelDvs::getImageTransformation(ia_dvs_state* dvsHandle,
+                                        ia_dvs_image_transformation* imageTransformation) {
+    LOGIPC("@%s, dvsHandle:%p", __func__, dvsHandle);
+    CheckError(mInitialized == false, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(dvsHandle == nullptr, ia_err_general, "@%s, dvsHandle is nullptr", __func__);
+    CheckError(imageTransformation == nullptr, ia_err_general,
+               "@%s, imageTransformation is nullptr", __func__);
+
+    bool ret = mIpc.clientFlattenImageTransformation(mMemImageTransfor.mAddr,
+                                                     mMemImageTransfor.mSize, dvsHandle);
+    CheckError(ret == false, ia_err_general, "@%s, clientFlattenImageTransformation fails",
+               __func__);
+
+    ret = mCommon.requestSync(IPC_DVS_GET_IMAGE_TRANSFORMATION, mMemImageTransfor.mHandle);
+    CheckError(ret == false, ia_err_general, "@%s, requestSync fails", __func__);
+
+    ia_dvs_image_transformation* info = nullptr;
+    ret = mIpc.clientUnflattenImageTransformation(mMemImageTransfor.mAddr, mMemImageTransfor.mSize,
+                                                  &info);
+    CheckError(ret == false, ia_err_general, "@%s, clientUnflattenImageTransformation fails",
+               __func__);
+
+    MEMCPY_S(imageTransformation, sizeof(ia_dvs_image_transformation), info,
+             sizeof(ia_dvs_image_transformation));
+    return ia_err_none;
+}
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelDvs.h b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelDvs.h
new file mode 100644
index 000000000000..0b4354f9280f
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelDvs.h
@@ -0,0 +1,78 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_dvs.h>
+#include <ia_dvs_types.h>
+#include <ia_isp_bxt.h>
+
+#include <vector>
+
+#include "DvsResult.h"
+#include "IntelAlgoCommon.h"
+#include "modules/sandboxing/IPCIntelDvs.h"
+
+namespace icamera {
+class IntelDvs {
+ public:
+    IntelDvs();
+    virtual ~IntelDvs();
+
+    ia_err init(const ia_binary_data& aiqTuningBinary, const ia_cmc_t* cmc,
+                ia_dvs_state** dvsHandle);
+    void deinit(ia_dvs_state* dvsHandle);
+    ia_err config(ia_dvs_state* dvsHandle, ia_dvs_configuration* config, float zoomRatio);
+    ia_err setNonBlankRatio(ia_dvs_state* dvsHandle, float nonBlankingRatio);
+    ia_err setDigitalZoomMode(ia_dvs_state* dvsHandle, ia_dvs_zoom_mode zoomMode);
+    ia_err setDigitalZoomRegion(ia_dvs_state* dvsHandle, ia_rectangle* zoomRegion);
+    ia_err setDigitalZoomCoordinate(ia_dvs_state* dvsHandle, ia_coordinate* zoomCoordinate);
+    ia_err setDigitalZoomMagnitude(ia_dvs_state* dvsHandle, float zoomRatio);
+    void freeMorphTable(ia_dvs_state* dvsHandle, ia_dvs_morph_table* morphTable);
+    ia_dvs_morph_table* allocateMorphTalbe(ia_dvs_state* dvsHandle);
+    int getMorphTable(ia_dvs_state* dvsHandle, ia_dvs_morph_table* morphTable, DvsResult* result);
+    ia_err setStatistics(ia_dvs_state* dvsHandle, const ia_dvs_statistics* statistics,
+                         const ia_aiq_ae_results* aeResults, const ia_aiq_af_results* afResults,
+                         const ia_aiq_sensor_events* sensorEvents, uint64_t frameReadoutStart,
+                         uint64_t frameReadoutEnd);
+    ia_err execute(ia_dvs_state* dvsHandle, uint16_t focusPosition);
+    ia_err getImageTransformation(ia_dvs_state* dvsHandle,
+                                  ia_dvs_image_transformation* imageTransformation);
+
+ private:
+    IPCIntelDvs mIpc;
+    IntelAlgoCommon mCommon;
+
+    bool mInitialized;
+
+    ShmMemInfo mMemInit;
+    ShmMemInfo mMemDeinit;
+    ShmMemInfo mMemConfig;
+    ShmMemInfo mMemNonBlankRatio;
+    ShmMemInfo mMemZoomMode;
+    ShmMemInfo mMemZoomRegion;
+    ShmMemInfo mMemZoomCoordinate;
+    ShmMemInfo mMemZoomMagnitude;
+    ShmMemInfo mMemFreeMorph;
+    ShmMemInfo mMemAllocateMorph;
+    ShmMemInfo mMemGetMorphTable;
+    ShmMemInfo mMemStatistics;
+    ShmMemInfo mMemExecute;
+    ShmMemInfo mMemImageTransfor;
+
+    std::vector<ShmMem> mMems;
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelFaceDetection.cpp b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelFaceDetection.cpp
new file mode 100644
index 000000000000..194be112bf6e
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelFaceDetection.cpp
@@ -0,0 +1,121 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "ClientIntelFaceDetection"
+
+#include "modules/sandboxing/client/IntelFaceDetection.h"
+
+#include <algorithm>
+
+#include "FaceBase.h"
+
+namespace icamera {
+IntelFaceDetection::IntelFaceDetection() : mInitialized(false) {
+    LOG1("@%s", __func__);
+
+    uintptr_t personal = reinterpret_cast<uintptr_t>(this);
+    mMems = {{("/faceDetectionInit" + std::to_string(personal) + "Shm"),
+              sizeof(FaceDetectionInitParams), &mMemInit, false}};
+
+    for (int i = 0; i < MAX_STORE_FACE_DATA_BUF_NUM; i++) {
+        mMems.push_back(
+            {("/faceDetectionRun" + std::to_string(i) + std::to_string(personal) + "Shm"),
+             sizeof(FaceDetectionRunParams), &mMemRunBufs[i], false});
+    }
+
+    bool success = mCommon.allocateAllShmMems(&mMems);
+    if (!success) {
+        LOGE("@%s,Failed to call allocateAllShmMems", __func__);
+        mCommon.releaseAllShmMems(mMems);
+        return;
+    }
+
+    LOG1("@%s, done", __func__);
+    mInitialized = true;
+}
+
+IntelFaceDetection::~IntelFaceDetection() {
+    LOG1("@%s", __func__);
+    mCommon.releaseAllShmMems(mMems);
+}
+
+status_t IntelFaceDetection::init(FaceDetectionInitParams* initParams, int dataSize) {
+    CheckError(!mInitialized, UNKNOWN_ERROR, "@%s, mInitialized is false", __func__);
+
+    LOG1("@%s, initParams:%p, dataSize:%d", __func__, initParams, dataSize);
+    CheckError(initParams == nullptr, UNKNOWN_ERROR, "@%s, initParams is nullptr", __func__);
+    CheckError(dataSize < static_cast<int>(sizeof(FaceDetectionInitParams)), UNKNOWN_ERROR,
+               "@%s, buffer is small", __func__);
+
+    unsigned int maxFacesNum =
+        std::min(initParams->max_face_num, static_cast<unsigned int>(MAX_FACES_DETECTABLE));
+    LOG2("@%s, maxFacesNum:%d", __func__, maxFacesNum);
+
+    FaceDetectionInitParams* params = static_cast<FaceDetectionInitParams*>(mMemInit.mAddr);
+
+    bool ret = mIpc.clientFlattenInit(maxFacesNum, params);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientFlattenInit fails", __func__);
+
+    ret = mCommon.requestSync(IPC_FD_INIT, mMemInit.mHandle);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    return OK;
+}
+
+status_t IntelFaceDetection::deinit() {
+    LOG1("@%s", __func__);
+    CheckError(!mInitialized, UNKNOWN_ERROR, "@%s, mInitialized is false", __func__);
+
+    bool ret = mCommon.requestSync(IPC_FD_DEINIT);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    return OK;
+}
+
+FaceDetectionRunParams* IntelFaceDetection::prepareRunBuffer(unsigned int index) {
+    LOG1("@%s", __func__);
+    CheckError(index >= MAX_STORE_FACE_DATA_BUF_NUM, nullptr, "@%s, index is error %d", __func__,
+               index);
+    CheckError(!mInitialized, nullptr, "@%s, mInitialized is false", __func__);
+
+    return static_cast<FaceDetectionRunParams*>(mMemRunBufs[index].mAddr);
+}
+
+status_t IntelFaceDetection::run(FaceDetectionRunParams* runParams, int dataSize, int dmafd) {
+    CheckError(!mInitialized, UNKNOWN_ERROR, "@%s, mInitialized is false", __func__);
+    CheckError(!runParams, UNKNOWN_ERROR, "@%s,runParams is nullptr", __func__);
+    CheckError(dataSize < static_cast<int>(sizeof(FaceDetectionRunParams)), UNKNOWN_ERROR,
+               "@%s, buffer is small", __func__);
+
+    if (dmafd >= 0) {
+        runParams->bufferHandle = mCommon.registerGbmBuffer(dmafd);
+        CheckError((runParams->bufferHandle < 0), false, "@%s, call mCommon.registerGbmBuffer",
+                   __func__);
+    }
+
+    int32_t runBufHandle = mCommon.getShmMemHandle(static_cast<void*>(runParams));
+    CheckError(runBufHandle < 0, UNKNOWN_ERROR, "@%s, getShmMemHandle fails", __func__, runParams);
+
+    bool ret = mCommon.requestSync(IPC_FD_RUN, runBufHandle);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    if (dmafd >= 0) {
+        mCommon.deregisterGbmBuffer(runParams->bufferHandle);
+    }
+
+    return OK;
+}
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelFaceDetection.h b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelFaceDetection.h
new file mode 100644
index 000000000000..4f6b567e0cdb
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelFaceDetection.h
@@ -0,0 +1,48 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <vector>
+
+#include "FaceBase.h"
+#include "IntelAlgoCommon.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+#include "modules/sandboxing/IPCIntelFD.h"
+
+namespace icamera {
+class IntelFaceDetection {
+ public:
+    IntelFaceDetection();
+    virtual ~IntelFaceDetection();
+
+    status_t init(FaceDetectionInitParams* initData, int dataSize);
+    status_t deinit();
+    status_t run(FaceDetectionRunParams* runParams, int dataSize, int dmafd = -1);
+    FaceDetectionRunParams* prepareRunBuffer(unsigned int index);
+
+ private:
+    IPCIntelFD mIpc;
+    IntelAlgoCommon mCommon;
+
+    bool mInitialized;
+
+    ShmMemInfo mMemInit;
+    ShmMemInfo mMemRunBufs[MAX_STORE_FACE_DATA_BUF_NUM];
+    std::vector<ShmMem> mMems;
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelIspParamAdaptor.cpp b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelIspParamAdaptor.cpp
new file mode 100644
index 000000000000..8675432b67bf
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelIspParamAdaptor.cpp
@@ -0,0 +1,196 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelIspParamAdaptor"
+
+#include "modules/sandboxing/client/IntelIspParamAdaptor.h"
+
+#include <string>
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+IntelIspParamAdaptor::IntelIspParamAdaptor() : mInitialized(false) {
+    LOGIPC("@%s", __func__);
+
+    uintptr_t personal = reinterpret_cast<uintptr_t>(this);
+    std::string initName = "/adaptorInit" + std::to_string(personal) + SHM_NAME;
+    std::string deInitName = "/adaptorDeinit" + std::to_string(personal) + SHM_NAME;
+    std::string getSize = "/adaptorGetSize" + std::to_string(personal) + SHM_NAME;
+    std::string convertStats = "/adaptorConvertStats" + std::to_string(personal) + SHM_NAME;
+    std::string runPal = "/adaptorRunPal" + std::to_string(personal) + SHM_NAME;
+
+    mMems = {{initName.c_str(), sizeof(IspBxtInitParam), &mMemInit, false},
+             {deInitName.c_str(), sizeof(IspBxtDeInitParam), &mMemDeinit, false},
+             {getSize.c_str(), sizeof(PalDataSizeParam), &mMemGetSize, false},
+             {convertStats.c_str(), sizeof(ConvertStatsParam), &mMemStats, false},
+             {runPal.c_str(), sizeof(RunPalParam), &mMemRunPal, false}};
+
+    bool success = mCommon.allocateAllShmMems(&mMems);
+    if (!success) {
+        mCommon.releaseAllShmMems(mMems);
+        return;
+    }
+
+    LOGIPC("@%s, done", __func__);
+    mInitialized = true;
+}
+
+IntelIspParamAdaptor::~IntelIspParamAdaptor() {
+    LOGIPC("@%s", __func__);
+    mPalDataMems.clear();
+    mCommon.releaseAllShmMems(mMems);
+}
+
+ia_isp_bxt* IntelIspParamAdaptor::init(const ia_binary_data* ispData, const ia_cmc_t* iaCmc,
+                                       unsigned int maxStatsWidth, unsigned int maxStatsHeight,
+                                       unsigned int maxNumStatsIn, ia_mkn* iaMkn) {
+    CheckError(!ispData || !iaCmc, nullptr, "%s, No CPF or CMC data", __func__);
+    LOGIPC("%s, ispData size: %d, pointer: %p, max width: %u, max height: %u", __func__,
+           ispData->size, ispData->data, maxStatsWidth, maxStatsHeight);
+    CheckError(mInitialized == false, nullptr, "@%s, mInitialized is false", __func__);
+
+    bool ret = mIpc.clientFlattenInit(mMemInit.mAddr, mMemInit.mSize, ispData, iaCmc, maxStatsWidth,
+                                      maxStatsHeight, maxNumStatsIn, iaMkn);
+    CheckError(ret == false, nullptr, "@%s, clientFlattenInit fails", __func__);
+
+    ret = mCommon.requestSync(IPC_ISP_ADAPTOR_INIT, mMemInit.mHandle);
+    CheckError(ret == false, nullptr, "@%s, requestSync fails", __func__);
+
+    IspBxtInitParam* params = static_cast<IspBxtInitParam*>(mMemInit.mAddr);
+    return reinterpret_cast<ia_isp_bxt*>(params->ispRemoteHandle);
+}
+
+void IntelIspParamAdaptor::deInit(const ia_isp_bxt* ispBxtHandle) {
+    LOGIPC("@%s", __func__);
+    CheckError(!ispBxtHandle, VOID_VALUE, "%s, ispBxtHandle is nullptr", __func__);
+    CheckError(mInitialized == false, VOID_VALUE, "@%s, mInitialized is false", __func__);
+
+    bool ret = mIpc.clientFlattenDeInit(mMemDeinit.mAddr, mMemDeinit.mSize, ispBxtHandle);
+    CheckError(ret == false, VOID_VALUE, "@%s, clientFlattenDeInit fails", __func__);
+
+    ret = mCommon.requestSync(IPC_ISP_ADAPTOR_DEINIT, mMemDeinit.mHandle);
+    CheckError(ret == false, VOID_VALUE, "@%s, requestSync fails", __func__);
+}
+
+int IntelIspParamAdaptor::getPalDataSize(const ia_isp_bxt_program_group* programGroup) {
+    LOGIPC("@%s", __func__);
+    CheckError(mInitialized == false, -1, "@%s, mInitialized is false", __func__);
+    CheckError(!programGroup, -1, "%s, programGroup is nullptr", __func__);
+
+    bool ret = mIpc.clientFlattenGetPalSize(mMemGetSize.mAddr, mMemGetSize.mSize, programGroup);
+    CheckError(ret == false, -1, "@%s, clientFlattenGetPalSize fails", __func__);
+
+    ret = mCommon.requestSync(IPC_ISP_GET_PAL_SIZE, mMemGetSize.mHandle);
+    CheckError(ret == false, -1, "@%s, requestSync fails", __func__);
+
+    PalDataSizeParam* params = static_cast<PalDataSizeParam*>(mMemGetSize.mAddr);
+    LOGIPC("@%s, the pal data size is: %d", __func__, params->palDataSize);
+
+    return params->palDataSize;
+}
+
+status_t IntelIspParamAdaptor::queryAndConvertStats(const ia_isp_bxt* ispBxtHandle,
+                                                    const ConvertInputParam* inputParams,
+                                                    ConvertResult* result) {
+    LOGIPC("@%s", __func__);
+    CheckError(!ispBxtHandle, UNKNOWN_ERROR, "%s, ispBxtHandle is nullptr", __func__);
+    CheckError(!inputParams || !result, UNKNOWN_ERROR, "%s, inputParams or result nullptr",
+               __func__);
+    CheckError(!inputParams->dvsReso || !inputParams->aeResults, UNKNOWN_ERROR,
+               "%s, inputParams or result nullptr", __func__);
+    CheckError((!inputParams->statsBuffer || !inputParams->statsBuffer->data ||
+                inputParams->statsBuffer->size <= 0),
+               UNKNOWN_ERROR, "%s, Wrong statistics buffer", __func__);
+    CheckError(!result->queryResults, UNKNOWN_ERROR, "%s, queryResults is nullptr", __func__);
+    CheckError(mInitialized == false, UNKNOWN_ERROR, "@%s, mInitialized is false", __func__);
+
+    int32_t statsHandle = mCommon.getShmMemHandle(inputParams->statsBuffer->data);
+    bool ret = mIpc.clientFlattenConvertStats(mMemStats.mAddr, mMemStats.mSize, ispBxtHandle,
+                                              inputParams, statsHandle);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientFlattenConvertStats fails", __func__);
+
+    ret = mCommon.requestSync(IPC_ISP_CONVERT_STATS, mMemStats.mHandle);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    ret = mIpc.clientUnflattenConvertStats(mMemStats.mAddr, mMemStats.mSize, result);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientUnflattenConvertStats fails", __func__);
+
+    return OK;
+}
+
+void* IntelIspParamAdaptor::allocatePalBuffer(int streamId, int index, int palDataSize) {
+    CheckError(mInitialized == false, nullptr, "@%s, mInitialized is false", __func__);
+
+    uintptr_t personal = reinterpret_cast<uintptr_t>(this);
+    std::string palDataName = "/palData" + std::to_string(streamId) + std::to_string(index) +
+                              std::to_string(personal) + SHM_NAME;
+
+    ShmMemInfo memInfo;
+    CLEAR(memInfo);
+    bool success = mCommon.allocShmMem(palDataName, palDataSize, &memInfo);
+    CheckError(success == false, nullptr, "%s, failed to allocate share memory for pal", __func__);
+    LOGIPC("%s, the buffer handle: %d, address: %p", __func__, memInfo.mHandle, memInfo.mAddr);
+
+    mPalDataMems.push_back(memInfo);
+
+    return memInfo.mAddr;
+}
+
+void IntelIspParamAdaptor::freePalBuffer(void* addr) {
+    CheckError(mInitialized == false, VOID_VALUE, "@%s, mInitialized is false", __func__);
+
+    for (auto& mem : mPalDataMems) {
+        if (mem.mAddr == addr) {
+            mCommon.freeShmMem(mem);
+        }
+    }
+}
+
+status_t IntelIspParamAdaptor::runPal(ia_isp_bxt* ispBxtHandle,
+                                      const ia_isp_bxt_input_params_v2* inputParams,
+                                      ia_binary_data* outputData) {
+    LOGIPC("@%s", __func__);
+    CheckError((!ispBxtHandle || !inputParams), UNKNOWN_ERROR,
+               "%s, ispBxtHandle or inputParams is nullptr", __func__);
+    CheckError((!outputData || !outputData->data || outputData->size <= 0), UNKNOWN_ERROR,
+               "%s, Wrong pal data buffer", __func__);
+    CheckError(mInitialized == false, UNKNOWN_ERROR, "@%s, mInitialized is false", __func__);
+
+    int32_t palDataHandle = mCommon.getShmMemHandle(outputData->data);
+    CheckError(!palDataHandle, UNKNOWN_ERROR, "%s, the pal buffer(%p) doesn't exist in SHM list",
+               __func__, outputData->data);
+    LOGIPC("%s, pal buffer address: %p, buffer handle: %d", __func__, outputData->data,
+           palDataHandle);
+
+    bool ret = mIpc.clientFlattenRunPal(mMemRunPal.mAddr, mMemRunPal.mSize, ispBxtHandle,
+                                        inputParams, outputData, palDataHandle);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientFlattenRunPal fails", __func__);
+
+    ret = mCommon.requestSync(IPC_ISP_RUN_PAL, mMemRunPal.mHandle);
+    CheckError(ret == false, -1, "@%s, requestSync fails", __func__);
+
+    RunPalParam* params = static_cast<RunPalParam*>(mMemRunPal.mAddr);
+    outputData->size = params->palOutput.size;
+    LOGIPC("%s, the buffer handle is: %d size: %d ptr: %p after running pal", __func__,
+           params->palDataHandle, outputData->size, outputData->data);
+
+    return OK;
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelIspParamAdaptor.h b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelIspParamAdaptor.h
new file mode 100644
index 000000000000..164c256a3234
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelIspParamAdaptor.h
@@ -0,0 +1,59 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <vector>
+
+#include "IntelAlgoCommon.h"
+#include "iutils/Errors.h"
+#include "modules/sandboxing/IPCIspParamAdaptor.h"
+
+namespace icamera {
+
+class IntelIspParamAdaptor {
+ public:
+    IntelIspParamAdaptor();
+    virtual ~IntelIspParamAdaptor();
+
+    ia_isp_bxt* init(const ia_binary_data* ispData, const ia_cmc_t* iaCmc,
+                     unsigned int maxStatsWidth, unsigned int maxStatsHeight,
+                     unsigned int maxNumStatsIn, ia_mkn* iaMkn);
+    void deInit(const ia_isp_bxt* ispBxtHandle);
+    int getPalDataSize(const ia_isp_bxt_program_group* programGroup);
+    status_t runPal(ia_isp_bxt* ispBxtHandle, const ia_isp_bxt_input_params_v2* inputParams,
+                    ia_binary_data* outputData);
+    status_t queryAndConvertStats(const ia_isp_bxt* ispBxtHandle,
+                                  const ConvertInputParam* inputParams, ConvertResult* result);
+    void* allocatePalBuffer(int streamId, int index, int palDataSize);
+    void freePalBuffer(void* addr);
+
+ private:
+    IPCIspParamAdaptor mIpc;
+    IntelAlgoCommon mCommon;
+
+    bool mInitialized;
+    ShmMemInfo mMemInit;
+    ShmMemInfo mMemDeinit;
+    ShmMemInfo mMemGetSize;
+    ShmMemInfo mMemStats;
+    ShmMemInfo mMemRunPal;
+    std::vector<ShmMemInfo> mPalDataMems;
+
+    std::vector<ShmMem> mMems;
+};
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelLard.cpp b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelLard.cpp
new file mode 100644
index 000000000000..3b12340b4bf0
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelLard.cpp
@@ -0,0 +1,137 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelLard"
+
+#include "modules/sandboxing/client/IntelLard.h"
+
+#include <string>
+
+#include "CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelLard::IntelLard() : mInitialized(false) {
+    LOGIPC("@%s", __func__);
+
+    uintptr_t personal = reinterpret_cast<uintptr_t>(this);
+    std::string initName = "/lardInit" + std::to_string(personal) + SHM_NAME;
+    std::string getTagListName = "/lardGetTagList" + std::to_string(personal) + SHM_NAME;
+    std::string runName = "/lardRun" + std::to_string(personal) + SHM_NAME;
+    std::string deinitName = "/lardDeinit" + std::to_string(personal) + SHM_NAME;
+
+    mMems = {{initName.c_str(), sizeof(lard_init_params), &mMemInit, false},
+             {getTagListName.c_str(), sizeof(lard_get_tag_list_params), &mMemGetTagList, false},
+             {runName.c_str(), sizeof(lard_run_params), &mMemRun, false},
+             {deinitName.c_str(), sizeof(lard_deinit_params), &mMemDeinit, false}};
+
+    bool success = mCommon.allocateAllShmMems(&mMems);
+    if (!success) {
+        mCommon.releaseAllShmMems(mMems);
+        return;
+    }
+
+    LOGIPC("@%s, done", __func__);
+    mInitialized = true;
+}
+
+IntelLard::~IntelLard() {
+    LOGIPC("@%s", __func__);
+    mCommon.releaseAllShmMems(mMems);
+}
+
+ia_lard* IntelLard::init(const ia_binary_data* lard_data_ptr) {
+    LOGIPC("@%s, binaryData:%p", __func__, lard_data_ptr);
+
+    CheckError(mInitialized == false, nullptr, "@%s, mInitialized is false", __func__);
+    CheckError(!lard_data_ptr, nullptr, "@%s, lard_data_ptr is nullptr", __func__);
+    CheckError(!lard_data_ptr->data, nullptr, "@%s, lard_data_ptr->data is nullptr", __func__);
+    CheckError(lard_data_ptr->size == 0, nullptr, "@%s, lard_data_ptr->size is 0", __func__);
+
+    bool ret = mIpc.clientFlattenInit(mMemInit.mAddr, mMemInit.mSize, lard_data_ptr);
+    CheckError(ret == false, nullptr, "@%s, clientFlattenInit fails", __func__);
+
+    ret = mCommon.requestSync(IPC_LARD_INIT, mMemInit.mHandle);
+    CheckError(ret == false, nullptr, "@%s, requestSync fails", __func__);
+
+    ia_lard* lard = nullptr;
+    ret = mIpc.clientUnflattenInit(mMemInit.mAddr, mMemInit.mSize, &lard);
+    CheckError(ret == false, nullptr, "@%s, clientUnflattenInit fails", __func__);
+
+    return lard;
+}
+
+ia_err IntelLard::getTagList(ia_lard* ia_lard_ptr, unsigned int mode_tag, unsigned int* num_tags,
+                             const unsigned int** tags) {
+    LOGIPC("@%s, ia_lard_ptr:%p, mode_tag:%d", __func__, ia_lard_ptr, mode_tag);
+
+    CheckError(mInitialized == false, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(ia_lard_ptr == nullptr, ia_err_general, "@%s, ia_lard_ptr is nullptr", __func__);
+    CheckError(num_tags == nullptr, ia_err_general, "@%s, num_tags is nullptr", __func__);
+    CheckError(tags == nullptr, ia_err_general, "@%s, tags is nullptr", __func__);
+
+    bool ret = mIpc.clientFlattenGetTagList(mMemGetTagList.mAddr, mMemGetTagList.mSize, ia_lard_ptr,
+                                            mode_tag);
+    CheckError(ret == false, ia_err_general, "@%s, clientFlattenGetTagList fails", __func__);
+
+    ret = mCommon.requestSync(IPC_LARD_GET_TAG_LIST, mMemGetTagList.mHandle);
+    CheckError(ret == false, ia_err_general, "@%s, requestSync fails", __func__);
+
+    ret = mIpc.clientUnflattenGetTagList(mMemGetTagList.mAddr, mMemGetTagList.mSize, mode_tag,
+                                         num_tags, tags);
+    CheckError(ret == false, ia_err_general, "@%s, clientUnflattenGetTagList fails", __func__);
+
+    return ia_err_none;
+}
+
+ia_err IntelLard::run(ia_lard* ia_lard_ptr, ia_lard_input_params* lard_input_params_ptr,
+                      ia_lard_results** lard_results_ptr) {
+    LOGIPC("@%s, ia_lard_ptr:%p, lard_input_params_ptr:%p", __func__, ia_lard_ptr,
+           lard_input_params_ptr);
+
+    CheckError(mInitialized == false, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(ia_lard_ptr == nullptr, ia_err_general, "@%s, ia_lard_ptr is nullptr", __func__);
+    CheckError(lard_input_params_ptr == nullptr, ia_err_general,
+               "@%s, lard_input_params_ptr is nullptr", __func__);
+    CheckError(lard_results_ptr == nullptr, ia_err_general, "@%s, lard_results_ptr is nullptr",
+               __func__);
+
+    bool ret =
+        mIpc.clientFlattenRun(mMemRun.mAddr, mMemRun.mSize, ia_lard_ptr, lard_input_params_ptr);
+    CheckError(ret == false, ia_err_general, "@%s, clientFlattenRun fails", __func__);
+
+    ret = mCommon.requestSync(IPC_LARD_RUN, mMemRun.mHandle);
+    CheckError(ret == false, ia_err_general, "@%s, requestSync fails", __func__);
+
+    ret = mIpc.clientUnflattenRun(mMemRun.mAddr, mMemRun.mSize, lard_results_ptr);
+    CheckError(ret == false, ia_err_general, "@%s, clientUnflattenRun fails", __func__);
+
+    return ia_err_none;
+}
+
+void IntelLard::deinit(ia_lard* ia_lard_ptr) {
+    LOGIPC("@%s, ia_lard_ptr:%p", __func__, ia_lard_ptr);
+
+    CheckError(mInitialized == false, VOID_VALUE, "@%s, mInitialized is false", __func__);
+    CheckError(ia_lard_ptr == nullptr, VOID_VALUE, "@%s, ia_lard_ptr is nullptr", __func__);
+
+    bool ret = mIpc.clientFlattenDeinit(mMemDeinit.mAddr, mMemDeinit.mSize, ia_lard_ptr);
+    CheckError(ret == false, VOID_VALUE, "@%s, clientFlattenDeinit fails", __func__);
+
+    ret = mCommon.requestSync(IPC_LARD_DEINIT, mMemDeinit.mHandle);
+    CheckError(ret == false, VOID_VALUE, "@%s, requestSync fails", __func__);
+}
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelLard.h b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelLard.h
new file mode 100644
index 000000000000..3d2473ec4957
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelLard.h
@@ -0,0 +1,52 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_lard.h>
+
+#include <vector>
+
+#include "IntelAlgoCommon.h"
+#include "modules/sandboxing/IPCIntelLard.h"
+
+namespace icamera {
+class IntelLard {
+ public:
+    IntelLard();
+    virtual ~IntelLard();
+
+    ia_lard* init(const ia_binary_data* lard_data_ptr);
+    ia_err getTagList(ia_lard* ia_lard_ptr, unsigned int mode_tag, unsigned int* num_tags,
+                      const unsigned int** tags);
+    ia_err run(ia_lard* ia_lard_ptr, ia_lard_input_params* lard_input_params_ptr,
+               ia_lard_results** lard_results_ptr);
+    void deinit(ia_lard* ia_lard_ptr);
+
+ private:
+    IPCIntelLard mIpc;
+    IntelAlgoCommon mCommon;
+
+    bool mInitialized;
+
+    ShmMemInfo mMemInit;
+    ShmMemInfo mMemGetTagList;
+    ShmMemInfo mMemRun;
+    ShmMemInfo mMemDeinit;
+
+    std::vector<ShmMem> mMems;
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelLtm.cpp b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelLtm.cpp
new file mode 100644
index 000000000000..c3a3bf0001c0
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelLtm.cpp
@@ -0,0 +1,133 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "ClientIntelLtm"
+
+#include "modules/sandboxing/client/IntelLtm.h"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelLtm::IntelLtm() : mInitialized(false), mRunParamsSize(0) {
+    LOGIPC("@%s", __func__);
+
+    mCaller = reinterpret_cast<uintptr_t>(this);
+    mMems = {
+        {("/ltmInit" + std::to_string(mCaller) + "Shm"), sizeof(LtmInitParams), &mMemInit, false},
+        {("/ltmDeinit" + std::to_string(mCaller) + "Shm"), sizeof(LtmDeinitParams), &mMemDeinit,
+         false}};
+
+    bool success = mCommon.allocateAllShmMems(&mMems);
+    if (!success) {
+        mCommon.releaseAllShmMems(mMems);
+        return;
+    }
+
+    LOGIPC("@%s, done", __func__);
+    mInitialized = true;
+}
+
+IntelLtm::~IntelLtm() {
+    LOGIPC("@%s", __func__);
+    mCommon.releaseAllShmMems(mMems);
+    mCommon.freeShmMem(mMemRun);
+}
+
+ia_ltm* IntelLtm::init(const ia_binary_data* lard_data_ptr, ia_mkn* mkn) {
+    LOGIPC("@%s", __func__);
+    CheckError(!lard_data_ptr, nullptr, "@%s, lard_data_ptr is null", __func__);
+    CheckError(!mkn, nullptr, "@%s, mkn is null", __func__);
+    CheckError(mInitialized == false, nullptr, "@%s, mInitialized is false", __func__);
+
+    LtmInitParams* params = static_cast<LtmInitParams*>(mMemInit.mAddr);
+
+    bool ret = mIpc.clientFlattenInit(*lard_data_ptr, reinterpret_cast<uintptr_t>(mkn), params);
+    CheckError(ret == false, nullptr, "@%s, clientFlattenInit fails", __func__);
+
+    ret = mCommon.requestSync(IPC_LTM_INIT, mMemInit.mHandle);
+    CheckError(ret == false, nullptr, "@%s, requestSync fails", __func__);
+
+    ia_ltm* ltm = nullptr;
+    ret = mIpc.clientUnflattenInit(params, &ltm);
+    CheckError(ret == false, nullptr, "@%s, clientUnflattenInit fails", __func__);
+
+    return ltm;
+}
+
+void IntelLtm::deinit(ia_ltm* ltm) {
+    LOGIPC("@%s", __func__);
+    CheckError(!ltm, VOID_VALUE, "@%s, ltm is nullptr", __func__);
+    CheckError(mInitialized == false, VOID_VALUE, "@%s, mInitialized is false", __func__);
+
+    LtmDeinitParams* params = static_cast<LtmDeinitParams*>(mMemDeinit.mAddr);
+    params->ltm_handle = reinterpret_cast<uintptr_t>(ltm);
+
+    bool ret = mCommon.requestSync(IPC_LTM_DEINIT, mMemDeinit.mHandle);
+    CheckError(ret == false, VOID_VALUE, "@%s, requestSync fails", __func__);
+}
+
+int IntelLtm::allocaRunImage(int dataSize) {
+    mRunParamsSize = 0;
+    mCommon.freeShmMem(mMemRun);
+    mMemRun = {};
+
+    mMemRun.mName = "/ltmRun" + std::to_string(mCaller) + "Shm";
+    mMemRun.mSize = dataSize;
+    bool ret = mCommon.allocShmMem(mMemRun.mName, mMemRun.mSize, &mMemRun);
+    CheckError(!ret, mRunParamsSize, "@%s, allocShmMem fails", __func__);
+
+    mRunParamsSize = dataSize;
+    return mRunParamsSize;
+}
+
+ia_err IntelLtm::run(ia_ltm* ltm, const ia_ltm_input_params* inputParams,
+                     ia_ltm_results** ltmResults, ia_ltm_drc_params** drcResults) {
+    LOGIPC("@%s", __func__);
+
+    CheckError(mInitialized == false, ia_err_general, "@%s, mInitialized is false", __func__);
+    CheckError(!ltm, ia_err_general, "@%s, ltm is nullptr", __func__);
+    CheckError(!inputParams, ia_err_general, "@%s, inputParams is nullptr", __func__);
+    CheckError(!ltmResults, ia_err_general, "@%s, ltmResultsis is nullptr", __func__);
+    CheckError(!drcResults, ia_err_general, "@%s, drcResults is nullptr", __func__);
+
+    int runParamsSize = sizeof(LtmRunParams);
+    if (inputParams->input_image_ptr && inputParams->input_image_ptr->image_data) {
+        runParamsSize += inputParams->input_image_ptr->image_data->size;
+    }
+
+    if (mRunParamsSize < runParamsSize) {
+        LOGIPC("@%s, mRunParamsSize %d, runParamsSize %d", __func__, mRunParamsSize, runParamsSize);
+        int dataSize = allocaRunImage(runParamsSize);
+        CheckError(dataSize <= 0, ia_err_general, "@%s, Failed to allocaRunImage", __func__);
+    }
+
+    bool ret = mIpc.clientFlattenRun(reinterpret_cast<uintptr_t>(ltm), *inputParams, mMemRun.mSize,
+                                     mMemRun.mAddr);
+    CheckError(ret == false, ia_err_general, "@%s, clientFlattenPrepare fails", __func__);
+
+    ret = mCommon.requestSync(IPC_LTM_RUN, mMemRun.mHandle);
+    CheckError(ret == false, ia_err_general, "@%s, requestSync fails", __func__);
+
+    LtmRunParams* params = static_cast<LtmRunParams*>(mMemRun.mAddr);
+    ret = mIpc.clientUnflattenRun(params, ltmResults, drcResults);
+    CheckError(ret == false, ia_err_general, "@%s, clientUnflattenPrepare fails", __func__);
+
+    return ia_err_none;
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelLtm.h b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelLtm.h
new file mode 100644
index 000000000000..c394b201c703
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelLtm.h
@@ -0,0 +1,53 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_ltm.h>
+
+#include <vector>
+
+#include "IntelAlgoCommon.h"
+#include "modules/sandboxing/IPCIntelLtm.h"
+
+namespace icamera {
+class IntelLtm {
+ public:
+    IntelLtm();
+    virtual ~IntelLtm();
+
+    ia_ltm* init(const ia_binary_data* lard_data_ptr, ia_mkn* mkn);
+    void deinit(ia_ltm* ltm);
+    ia_err run(ia_ltm* ltm, const ia_ltm_input_params* inputParams, ia_ltm_results** ltmResults,
+               ia_ltm_drc_params** drcResults);
+
+ private:
+    int allocaRunImage(int dataSize);
+
+ private:
+    uintptr_t mCaller;
+    IPCIntelLtm mIpc;
+    IntelAlgoCommon mCommon;
+
+    bool mInitialized;
+    int mRunParamsSize;
+
+    ShmMemInfo mMemInit;
+    ShmMemInfo mMemRun;
+    ShmMemInfo mMemDeinit;
+    std::vector<ShmMem> mMems;
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelMkn.cpp b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelMkn.cpp
new file mode 100644
index 000000000000..4b792af9f718
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelMkn.cpp
@@ -0,0 +1,119 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "ClientIntelMkn"
+
+#include "modules/sandboxing/client/IntelMkn.h"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelMkn::IntelMkn() : mInitialized(false) {
+    LOGIPC("@%s", __func__);
+
+    uintptr_t personal = reinterpret_cast<uintptr_t>(this);
+    mMems = {
+        {("/mknInit" + std::to_string(personal) + "Shm"), sizeof(MknInitParams), &mMemInit, false},
+        {("/mknDeinit" + std::to_string(personal) + "Shm"), sizeof(MknDeinitParams), &mMemDeinit,
+         false},
+        {("/mknPrepare" + std::to_string(personal) + "Shm"), sizeof(MknPrepareParams), &mMemPrepare,
+         false},
+        {("/mknEnable" + std::to_string(personal) + "Shm"), sizeof(MknEnableParams), &mMemEnable,
+         false}};
+
+    bool success = mCommon.allocateAllShmMems(&mMems);
+    if (!success) {
+        mCommon.releaseAllShmMems(mMems);
+        return;
+    }
+
+    LOGIPC("@%s, done", __func__);
+    mInitialized = true;
+}
+
+IntelMkn::~IntelMkn() {
+    LOGIPC("@%s", __func__);
+    mCommon.releaseAllShmMems(mMems);
+}
+
+ia_mkn* IntelMkn::init(ia_mkn_config_bits mkn_config_bits, size_t mkn_section_1_size,
+                       size_t mkn_section_2_size) {
+    LOGIPC("@%s, mkn_config_bits:%d, mkn_section_1_size:%zu, mkn_section_2_size:%zu", __func__,
+           mkn_config_bits, mkn_section_1_size, mkn_section_2_size);
+    CheckError(mInitialized == false, nullptr, "@%s, mInitialized is false", __func__);
+
+    MknInitParams* params = static_cast<MknInitParams*>(mMemInit.mAddr);
+
+    bool ret =
+        mIpc.clientFlattenInit(mkn_config_bits, mkn_section_1_size, mkn_section_2_size, params);
+    CheckError(ret == false, nullptr, "@%s, clientFlattenInit fails", __func__);
+
+    ret = mCommon.requestSync(IPC_MKN_INIT, mMemInit.mHandle);
+    CheckError(ret == false, nullptr, "@%s, requestSync fails", __func__);
+
+    return reinterpret_cast<ia_mkn*>(params->results);
+}
+
+void IntelMkn::deinit(ia_mkn* pMkn) {
+    LOGIPC("@%s", __func__);
+    CheckError(pMkn == nullptr, VOID_VALUE, "@%s, mkn is nullptr", __func__);
+    CheckError(mInitialized == false, VOID_VALUE, "@%s, mInitialized is false", __func__);
+
+    MknDeinitParams* params = static_cast<MknDeinitParams*>(mMemDeinit.mAddr);
+    params->mkn_handle = reinterpret_cast<uintptr_t>(pMkn);
+
+    bool ret = mCommon.requestSync(IPC_MKN_DEINIT, mMemDeinit.mHandle);
+    CheckError(ret == false, VOID_VALUE, "@%s, requestSync fails", __func__);
+}
+
+int IntelMkn::prepare(ia_mkn* pMkn, ia_mkn_trg data_target, ia_binary_data* pBinaryData) {
+    LOGIPC("@%s", __func__);
+
+    *pBinaryData = {nullptr, 0};
+    CheckError(pMkn == nullptr, UNKNOWN_ERROR, "@%s, mkn is nullptr", __func__);
+    CheckError(mInitialized == false, UNKNOWN_ERROR, "@%s, mInitialized is false", __func__);
+
+    MknPrepareParams* params = static_cast<MknPrepareParams*>(mMemPrepare.mAddr);
+    bool ret = mIpc.clientFlattenPrepare(reinterpret_cast<uintptr_t>(pMkn), data_target, params);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientFlattenPrepare fails", __func__);
+
+    ret = mCommon.requestSync(IPC_MKN_PREPARE, mMemPrepare.mHandle);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    ret = mIpc.clientUnflattenPrepare(params, pBinaryData);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientUnflattenPrepare fails", __func__);
+
+    return OK;
+}
+
+int IntelMkn::enable(ia_mkn* pMkn, bool enable_data_collection) {
+    LOGIPC("@%s, enable_data_collection:%d", __func__, enable_data_collection);
+
+    CheckError(pMkn == nullptr, UNKNOWN_ERROR, "@%s, mkn is nullptr", __func__);
+    CheckError(mInitialized == false, UNKNOWN_ERROR, "@%s, mInitialized is false", __func__);
+
+    MknEnableParams* params = static_cast<MknEnableParams*>(mMemEnable.mAddr);
+    params->mkn_handle = reinterpret_cast<uintptr_t>(pMkn);
+    params->enable_data_collection = enable_data_collection;
+
+    bool ret = mCommon.requestSync(IPC_MKN_ENABLE, mMemEnable.mHandle);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    return OK;
+}
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelMkn.h b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelMkn.h
new file mode 100644
index 000000000000..caf4a883caa6
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelMkn.h
@@ -0,0 +1,54 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_mkn_encoder.h>
+#include <ia_mkn_types.h>
+#include <ia_types.h>
+
+#include <vector>
+
+#include "IntelAlgoCommon.h"
+#include "modules/sandboxing/IPCIntelMkn.h"
+
+namespace icamera {
+class IntelMkn {
+ public:
+    IntelMkn();
+    ~IntelMkn();
+
+    ia_mkn* init(ia_mkn_config_bits mkn_config_bits, size_t mkn_section_1_size,
+                 size_t mkn_section_2_size);
+    void deinit(ia_mkn* pMkn);
+
+    int prepare(ia_mkn* pMkn, ia_mkn_trg data_target, ia_binary_data* pBinaryData);
+    int enable(ia_mkn* pMkn, bool enable_data_collection);
+
+ private:
+    IPCIntelMkn mIpc;
+    IntelAlgoCommon mCommon;
+
+    bool mInitialized;
+
+    ShmMemInfo mMemInit;
+    ShmMemInfo mMemDeinit;
+    ShmMemInfo mMemPrepare;
+    ShmMemInfo mMemEnable;
+
+    std::vector<ShmMem> mMems;
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelPGParam.cpp b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelPGParam.cpp
new file mode 100644
index 000000000000..ac0fd0bdd9f9
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelPGParam.cpp
@@ -0,0 +1,294 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelPGParamC"
+
+#include "modules/sandboxing/client/IntelPGParam.h"
+
+#include <string>
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+IntelPGParam::IntelPGParam(int pgId)
+        : mInitialized(false),
+          mPgId(pgId),
+          mClient(reinterpret_cast<uintptr_t>(this)),
+          mPayloadCount(0),
+          mPGBuffer(nullptr) {
+    std::string initName = "/pgParamInit" + std::to_string(mClient) + SHM_NAME;
+    std::string prepareName = "/pgParamPrepare" + std::to_string(mClient) + SHM_NAME;
+    std::string getFragDescsName = "/pgParamGetFragDescs" + std::to_string(mClient) + SHM_NAME;
+    std::string prepareProgramName = "/pgParamPrepareProgram" + std::to_string(mClient) + SHM_NAME;
+    std::string encodeName = "/pgParamEncode" + std::to_string(mClient) + SHM_NAME;
+    std::string decodeName = "/pgParamDecode" + std::to_string(mClient) + SHM_NAME;
+    std::string deinitName = "/pgParamDeinit" + std::to_string(mClient) + SHM_NAME;
+
+    mMems = {
+        {initName.c_str(), sizeof(pg_param_init_params), &mMemInit, false},
+        {prepareName.c_str(), sizeof(pg_param_prepare_params), &mMemPrepare, false},
+        {getFragDescsName.c_str(), sizeof(pg_param_get_fragment_desc_params), &mMemGetFragDescs,
+         false},
+        {prepareProgramName.c_str(), sizeof(pg_param_prepare_program_params), &mMemPrepareProgram,
+         false},
+        {encodeName.c_str(), sizeof(pg_param_encode_params), &mMemEncode, false},
+        {decodeName.c_str(), sizeof(pg_param_decode_params), &mMemDecode, false},
+        {deinitName.c_str(), sizeof(pg_param_deinit_params), &mMemDeinit, false},
+    };
+
+    bool success = mCommon.allocateAllShmMems(&mMems);
+    if (!success) {
+        mCommon.releaseAllShmMems(mMems);
+        return;
+    }
+
+    // Allocate when use
+    mMemAllocatePayloads.mName = "/pgParamAllocPayloads" + std::to_string(mClient) + SHM_NAME;
+    mMemAllocatePayloads.mSize = 0;
+    mMemAllocatePGBuffer.mName = "/pgParamAllocPG" + std::to_string(mClient) + SHM_NAME;
+    mMemAllocatePGBuffer.mSize = 0;
+    mMemStatistics.mName = "/pgParamStats" + std::to_string(mClient) + SHM_NAME;
+    mMemStatistics.mSize = 0;
+    mMaxStatsSize = 0;
+
+    LOGIPC("@%s, done", __func__);
+    mInitialized = true;
+}
+
+IntelPGParam::~IntelPGParam() {
+    mCommon.releaseAllShmMems(mMems);
+    if (mMemAllocatePayloads.mSize) {
+        mCommon.freeShmMem(mMemAllocatePayloads);
+    }
+    if (mMemAllocatePGBuffer.mSize) {
+        mCommon.freeShmMem(mMemAllocatePGBuffer);
+    }
+    if (mMemStatistics.mSize) {
+        mCommon.freeShmMem(mMemStatistics);
+    }
+}
+
+int IntelPGParam::init(ia_p2p_platform_t platform, const PgConfiguration& pgConfig) {
+    CheckError(mInitialized == false, INVALID_OPERATION, "@%s, mInitialized is false", __func__);
+
+    bool ret =
+        mIpc.clientFlattenInit(mMemInit.mAddr, mMemInit.mSize, mPgId, mClient, platform, pgConfig);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientFlattenInit fails", __func__);
+
+    ret = mCommon.requestSync(IPC_PG_PARAM_INIT, mMemInit.mHandle);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    return OK;
+}
+
+int IntelPGParam::prepare(const ia_binary_data* ipuParameters, const ia_css_rbm_t* rbm,
+                          ia_css_kernel_bitmap_t* bitmap, uint32_t* maxStatsSize) {
+    CheckError(mInitialized == false, INVALID_OPERATION, "@%s, mInitialized is false", __func__);
+    CheckError(!ipuParameters, INVALID_OPERATION, "@%s, ipuParams error", __func__);
+
+    int32_t palHandle = mCommon.getShmMemHandle(ipuParameters->data);
+    bool ret = mIpc.clientFlattenPrepare(mMemPrepare.mAddr, mMemPrepare.mSize, mClient,
+                                         ipuParameters->size, palHandle, rbm);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientFlattenPrepare fails", __func__);
+
+    ret = mCommon.requestSync(IPC_PG_PARAM_PREPARE, mMemPrepare.mHandle);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    ret = mIpc.clientUnflattenPrepare(mMemPrepare.mAddr, mMemPrepare.mSize, bitmap, &mMaxStatsSize);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientUnflattenPrepare fails", __func__);
+
+    if (mMemStatistics.mAddr && mMemStatistics.mSize <= mMaxStatsSize) {
+        mCommon.freeShmMem(mMemStatistics);
+        mMemStatistics.mSize = 0;
+        mMemStatistics.mAddr = nullptr;
+    }
+    if (maxStatsSize) *maxStatsSize = mMaxStatsSize;
+    return OK;
+}
+
+int IntelPGParam::getFragmentDescriptors(int descCount, ia_p2p_fragment_desc* descs) {
+    CheckError(mInitialized == false, INVALID_OPERATION, "@%s, mInitialized is false", __func__);
+
+    bool ret = mIpc.clientFlattenGetFragDescs(mMemGetFragDescs.mAddr, mMemGetFragDescs.mSize,
+                                              mClient, descCount);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientFlattenGetFragDescs fails", __func__);
+
+    ret = mCommon.requestSync(IPC_PG_PARAM_GET_FRAG_DESCS, mMemGetFragDescs.mHandle);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    int count = 0;
+    ret = mIpc.clientUnflattenGetFragDescs(mMemGetFragDescs.mAddr, mMemGetFragDescs.mSize, &count,
+                                           descs);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientUnflattenGetFragDescs fails", __func__);
+    return count;
+}
+
+void* IntelPGParam::allocatePGBuffer(int pgSize) {
+    CheckError(mInitialized == false, nullptr, "@%s, mInitialized is false", __func__);
+
+    mPGBuffer = nullptr;
+    int size = mIpc.getTotalPGBufferSize(pgSize);
+    if (mMemAllocatePGBuffer.mAddr && mMemAllocatePGBuffer.mSize < size) {
+        mCommon.freeShmMem(mMemAllocatePGBuffer);
+        mMemAllocatePGBuffer.mSize = 0;
+        mMemAllocatePGBuffer.mAddr = nullptr;
+    }
+    if (!mMemAllocatePGBuffer.mAddr) {
+        mMemAllocatePGBuffer.mSize = size;
+        bool ret = mCommon.allocShmMem(mMemAllocatePGBuffer.mName, mMemAllocatePGBuffer.mSize,
+                                       &mMemAllocatePGBuffer);
+        CheckError(ret == false, nullptr, "@%s, allocShmMem fails", __func__);
+    }
+
+    void* pgBuffer = nullptr;
+    bool ret = mIpc.assignPGBuffer(mMemAllocatePGBuffer.mAddr, mMemAllocatePGBuffer.mSize, pgSize,
+                                   &pgBuffer);
+    CheckError(ret == false, nullptr, "@%s, assignPGBuffer fails", __func__);
+
+    ret = mIpc.clientFlattenAllocatePGBuffer(mMemAllocatePGBuffer.mAddr, mMemAllocatePGBuffer.mSize,
+                                             mClient, pgSize);
+    CheckError(ret == false, nullptr, "@%s, clientFlattenAllocatePGBuffer fails", __func__);
+
+    ret = mCommon.requestSync(IPC_PG_PARAM_ALLOCATE_PG, mMemAllocatePGBuffer.mHandle);
+    CheckError(ret == false, nullptr, "@%s, requestSync fails", __func__);
+
+    mPGBuffer = reinterpret_cast<ia_css_process_group_t*>(pgBuffer);
+    return mPGBuffer;
+}
+
+int IntelPGParam::setPGAndPrepareProgram(ia_css_process_group_t* pg) {
+    CheckError(mInitialized == false, INVALID_OPERATION, "@%s, mInitialized is false", __func__);
+    CheckError(mPGBuffer != pg, INVALID_OPERATION, "@%s, pg is not recognized", __func__);
+
+    bool ret = mIpc.clientFlattenPrepareProgram(mMemPrepareProgram.mAddr, mMemPrepareProgram.mSize,
+                                                mClient);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientFlattenPrepareProgram fails", __func__);
+
+    ret = mCommon.requestSync(IPC_PG_PARAM_PREPARE_PROGRAM, mMemPrepareProgram.mHandle);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    // Get size of payloads
+    mPayloadCount = ARRAY_SIZE(mPayloads);
+    ret = mIpc.clientUnflattenPrepareProgram(mMemPrepareProgram.mAddr, mMemPrepareProgram.mSize,
+                                             &mPayloadCount, mPayloads);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientUnlattenPrepareProgram fails", __func__);
+
+    return OK;
+}
+
+int IntelPGParam::allocatePayloads(int payloadCount, ia_binary_data* payloads) {
+    CheckError(mInitialized == false, INVALID_OPERATION, "@%s, mInitialized is false", __func__);
+    CheckError(payloadCount < mPayloadCount, UNKNOWN_ERROR, "@%s, payloadCount is small", __func__);
+    CheckError(!payloads, UNKNOWN_ERROR, "@%s, payloads is nullptr", __func__);
+    bool ret = false;
+
+    // Allocate memory
+    int size = mIpc.getTotalPayloadSize(mPayloadCount, mPayloads);
+    CheckError(size <= 0, UNKNOWN_ERROR, "@%s, payloads size error", __func__);
+    if (mMemAllocatePayloads.mAddr && mMemAllocatePayloads.mSize < size) {
+        mCommon.freeShmMem(mMemAllocatePayloads);
+        mMemAllocatePayloads.mSize = 0;
+        mMemAllocatePayloads.mAddr = nullptr;
+    }
+    if (!mMemAllocatePayloads.mAddr) {
+        mMemAllocatePayloads.mSize = size;
+        ret = mCommon.allocShmMem(mMemAllocatePayloads.mName, mMemAllocatePayloads.mSize,
+                                  &mMemAllocatePayloads);
+        CheckError(ret == false, UNKNOWN_ERROR, "@%s, allocShmMem fails", __func__);
+    }
+
+    // Split memory of payloads in client side
+    ret = mIpc.assignPayloads(mMemAllocatePayloads.mAddr, mMemAllocatePayloads.mSize, mPayloadCount,
+                              mPayloads);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, allocatePayloads fails", __func__);
+
+    ret = mIpc.clientFlattenAllocatePayloads(mMemAllocatePayloads.mAddr, mMemAllocatePayloads.mSize,
+                                             mClient, mPayloadCount, mPayloads);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientFlattenAllocatePayloads fails", __func__);
+
+    ret = mCommon.requestSync(IPC_PG_PARAM_ALLOCATE_PAYLOADS, mMemAllocatePayloads.mHandle);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    MEMCPY_S(payloads, sizeof(ia_binary_data) * mPayloadCount, mPayloads,
+             sizeof(ia_binary_data) * mPayloadCount);
+    return mPayloadCount;
+}
+
+int IntelPGParam::updatePALAndEncode(const ia_binary_data* ipuParameters, int payloadCount,
+                                     ia_binary_data* payloads) {
+    CheckError(mInitialized == false, INVALID_OPERATION, "@%s, mInitialized is false", __func__);
+    CheckError(!ipuParameters, INVALID_OPERATION, "@%s, ipuParams error", __func__);
+    // Check shared memory of payloads
+    CheckError(payloadCount != mPayloadCount, BAD_VALUE, "@%s, payloadCount error", __func__);
+
+    int32_t palHandle = mCommon.getShmMemHandle(ipuParameters->data);
+    bool ret = mIpc.clientFlattenEncode(mMemEncode.mAddr, mMemEncode.mSize, mClient,
+                                        ipuParameters->size, palHandle, payloadCount, payloads,
+                                        mMemAllocatePayloads.mAddr, mMemAllocatePayloads.mSize);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientFlattenEncode fails", __func__);
+
+    ret = mCommon.requestSync(IPC_PG_PARAM_ENCODE, mMemEncode.mHandle);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    return OK;
+}
+
+int IntelPGParam::decode(int payloadCount, ia_binary_data* payloads, ia_binary_data* statistics) {
+    CheckError(mInitialized == false, INVALID_OPERATION, "@%s, mInitialized is false", __func__);
+    // Check shared memory of payloads
+    CheckError(payloadCount != mPayloadCount, BAD_VALUE, "@%s, payloadCount error", __func__);
+
+    // Check share memory of statistics
+    CheckError(!statistics, BAD_VALUE, "@%s, statistics nullptr", __func__);
+    CheckError(!mMaxStatsSize, BAD_VALUE, "@%s, bad max stats size", __func__);
+    bool ret = true;
+    int32_t statsHandle = -1;
+    // Prepare shared stats memory
+    if (!mMemStatistics.mAddr) {
+        mMemStatistics.mSize = mMaxStatsSize;
+        ret = mCommon.allocShmMem(mMemStatistics.mName, mMemStatistics.mSize, &mMemStatistics);
+        CheckError(ret == false, UNKNOWN_ERROR, "@%s, alloc statsData fails", __func__);
+    }
+    statsHandle = mCommon.getShmMemHandle(mMemStatistics.mAddr);
+
+    ret = mIpc.clientFlattenDecode(mMemDecode.mAddr, mMemDecode.mSize, mClient, payloadCount,
+                                   payloads, mMemAllocatePayloads.mAddr, mMemAllocatePayloads.mSize,
+                                   statsHandle);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientFlattenDecode fails", __func__);
+
+    ret = mCommon.requestSync(IPC_PG_PARAM_DECODE, mMemDecode.mHandle);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, requestSync fails", __func__);
+
+    ret = mIpc.clientUnflattenDecode(mMemDecode.mAddr, mMemDecode.mSize, statistics);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, clientUnflattenDecode fails", __func__);
+    statistics->data = mMemStatistics.mAddr;
+
+    return OK;
+}
+
+void IntelPGParam::deinit() {
+    CheckError(mInitialized == false, VOID_VALUE, "@%s, mInitialized is false", __func__);
+
+    bool ret = mIpc.clientFlattenDeinit(mMemDeinit.mAddr, mMemDeinit.mSize, mClient);
+    CheckError(ret == false, VOID_VALUE, "@%s, clientFlattenDeinit fails", __func__);
+
+    ret = mCommon.requestSync(IPC_PG_PARAM_DEINIT, mMemDeinit.mHandle);
+    CheckError(ret == false, VOID_VALUE, "@%s, requestSync fails", __func__);
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelPGParam.h b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelPGParam.h
new file mode 100644
index 000000000000..c26a52553224
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelPGParam.h
@@ -0,0 +1,71 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+#include <vector>
+
+#include "IntelAlgoCommon.h"
+#include "modules/sandboxing/IPCIntelPGParam.h"
+
+namespace icamera {
+
+class IntelPGParam {
+ public:
+    explicit IntelPGParam(int pgId);
+    ~IntelPGParam();
+
+    int init(ia_p2p_platform_t platform, const PgConfiguration& Pgconfiguration);
+    int prepare(const ia_binary_data* ipuParameters, const ia_css_rbm_t* rbm,
+                ia_css_kernel_bitmap_t* bitmap, uint32_t* maxStatsSize = nullptr);
+    void* allocatePGBuffer(int pgSize);
+    int getFragmentDescriptors(int terminalIdx, ia_p2p_fragment_desc* desc);
+    int setPGAndPrepareProgram(ia_css_process_group_t* pg);
+    int allocatePayloads(int payloadCount, ia_binary_data* payloads);
+    int updatePALAndEncode(const ia_binary_data* ipuParams, int payloadCount,
+                           ia_binary_data* payloads);
+    int decode(int payloadCount, ia_binary_data* payload, ia_binary_data* statistics);
+    void deinit();
+
+ private:
+    IPCIntelPGParam mIpc;
+    IntelAlgoCommon mCommon;
+    bool mInitialized;
+
+    ShmMemInfo mMemInit;
+    ShmMemInfo mMemPrepare;
+    ShmMemInfo mMemGetFragDescs;
+    ShmMemInfo mMemAllocatePGBuffer;
+    ShmMemInfo mMemPrepareProgram;
+    ShmMemInfo mMemAllocatePayloads;
+    ShmMemInfo mMemEncode;
+    ShmMemInfo mMemDecode;
+    ShmMemInfo mMemDeinit;
+    ShmMemInfo mMemStatistics;
+    std::vector<ShmMem> mMems;
+
+    int mPgId;
+    uintptr_t mClient;
+
+    // Shared memory in client, to avoid memory copy
+    int mPayloadCount;
+    ia_binary_data mPayloads[IPU_MAX_TERMINAL_COUNT];  // save sizes
+    ia_css_process_group_t* mPGBuffer;
+    uint32_t mMaxStatsSize;
+};
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelTNR7US.cpp b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelTNR7US.cpp
new file mode 100644
index 000000000000..89f28d5c3b33
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelTNR7US.cpp
@@ -0,0 +1,161 @@
+/*
+ * Copyright (C) 2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelTNRClient"
+
+#include "modules/sandboxing/client/IntelTNR7US.h"
+
+#include <string>
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+IntelTNR7US::IntelTNR7US() : mRefBufferSize(0), mRefBufIndex(0), mTnrRunInfo(nullptr) {
+    LOG1("%s ", __func__);
+    CLEAR(mRefBufAddr);
+}
+
+IntelTNR7US::~IntelTNR7US() {
+    LOG1("%s ", __func__);
+    mCommon.freeShmMem(mTnrRunInfoMem, GPU_ALGO_SHM);
+    int ret = mCommon.requestSync(IPC_GPU_TNR_DEINIT);
+    CheckError(!ret, VOID_VALUE, "@%s, requestSync fails", __func__);
+}
+
+int IntelTNR7US::init(int width, int height) {
+    uintptr_t personal = reinterpret_cast<uintptr_t>(this);
+    std::string initName = "/TnrRun" + std::to_string(personal) + "Shm";
+    mTnrRunInfoMem.mName = initName.c_str();
+    mTnrRunInfoMem.mSize = sizeof(TnrRunInfo);
+    bool ret = mCommon.allocShmMem(mTnrRunInfoMem.mName, mTnrRunInfoMem.mSize, &mTnrRunInfoMem,
+                                   GPU_ALGO_SHM);
+    CheckError(!ret, UNKNOWN_ERROR, "@%s, allocShmMem fails", __func__);
+    mTnrRunInfo = static_cast<TnrRunInfo*>(mTnrRunInfoMem.mAddr);
+
+    initName = "/TnrRes" + std::to_string(personal) + "Shm";
+    ShmMemInfo resolutionMems;
+    resolutionMems.mName = initName.c_str();
+    resolutionMems.mSize = sizeof(TnrResolution);
+    ret = mCommon.allocShmMem(resolutionMems.mName, resolutionMems.mSize, &resolutionMems,
+                              GPU_ALGO_SHM);
+    if (!ret) {
+        LOGE("@%s, alloc resolution allocShmMem fails", __func__);
+        mCommon.freeShmMem(mTnrRunInfoMem, GPU_ALGO_SHM);
+        return UNKNOWN_ERROR;
+    }
+
+    TnrResolution* res = static_cast<TnrResolution*>(resolutionMems.mAddr);
+    *res = {width, height};
+
+    ret = mCommon.requestSync(IPC_GPU_TNR_INIT, resolutionMems.mHandle);
+    if (!ret) {
+        LOGE("@%s, IPC %d failed", __func__, IPC_GPU_TNR_INIT);
+        mCommon.freeShmMem(mTnrRunInfoMem, GPU_ALGO_SHM);
+    }
+    mCommon.freeShmMem(resolutionMems, GPU_ALGO_SHM);
+
+    return ret ? OK : UNKNOWN_ERROR;
+}
+
+int IntelTNR7US::runTnrFrame(const void* inBufAddr, void* outBufAddr, uint32_t inBufSize,
+                             uint32_t outBufSize, Tnr7Param* tnrParam) {
+    CheckError(!inBufAddr || !outBufAddr || !tnrParam, UNKNOWN_ERROR,
+               "@%s, invalid data buffer or parameter buffer", __func__);
+    int32_t inHandle = mCommon.getShmMemHandle(const_cast<void*>(inBufAddr));
+    CheckError(inHandle < 0, UNKNOWN_ERROR, "@%s, can't find inBuf handle", __func__);
+    int32_t refHandle = mCommon.getShmMemHandle(static_cast<void*>(mRefBufAddr[mRefBufIndex]));
+    CheckError(refHandle < 0, UNKNOWN_ERROR, "@%s, can't find refBuf handle", __func__);
+    CheckError(mParamMems.mAddr != tnrParam, UNKNOWN_ERROR, "@%s, invalid tnr parameter", __func__);
+
+    mTnrRunInfo->inHandle = inHandle;
+    mTnrRunInfo->outHandle = refHandle;
+    mTnrRunInfo->paramHandle = mParamMems.mHandle;
+
+    int32_t runHandle = mCommon.getShmMemHandle(static_cast<void*>(mTnrRunInfo));
+    bool ret = mCommon.requestSync(IPC_GPU_TNR_RUN_FRAME, runHandle);
+    CheckError(!ret, OK, "@%s, run tnr fails", __func__);
+    MEMCPY_S(outBufAddr, outBufSize, mRefBufAddr[mRefBufIndex], mRefBufferSize);
+    mRefBufIndex = (mRefBufIndex + 1) % TnrRefBufCount;
+
+    return OK;
+}
+
+void* IntelTNR7US::allocCamBuf(uint32_t bufSize, int id) {
+    LOG1("%s size %d", __func__, bufSize);
+    uintptr_t personal = reinterpret_cast<uintptr_t>(this);
+    std::string initName = "/TnrCam" + std::to_string(personal) + std::to_string(id) + "Shm";
+    ShmMemInfo shm;
+    shm.mName = initName.c_str();
+    shm.mSize = bufSize;
+    bool ret = mCommon.allocShmMem(shm.mName, shm.mSize, &shm, GPU_ALGO_SHM);
+    CheckError(!ret, nullptr, "@%s, allocShmMem fails", __func__);
+    ret = mCommon.requestSync(IPC_GPU_TNR_PREPARE_SURFACE, shm.mHandle);
+    if (!ret) {
+        mCommon.freeShmMem(shm, GPU_ALGO_SHM);
+        return nullptr;
+    }
+    mCamBufMems.push_back(shm);
+
+    return shm.mAddr;
+}
+
+int IntelTNR7US::allocRefBufs(uint32_t bufSize) {
+    mRefBufferSize = bufSize;
+    for (int i = 0; i < TnrRefBufCount; i++) {
+        mRefBufAddr[i] = allocCamBuf(bufSize, MAX_BUFFER_COUNT + i);
+        // will release all buffer in freeAllBufs
+        CheckError(!mRefBufAddr[i], UNKNOWN_ERROR, "@%s, alloc reference buffer fails", __func__);
+    }
+    return OK;
+}
+
+void IntelTNR7US::freeAllBufs() {
+    LOG1("%s ", __func__);
+    if (mParamMems.mAddr) {
+        mCommon.freeShmMem(mParamMems, GPU_ALGO_SHM);
+    }
+    for (auto& camBuf : mCamBufMems) {
+        if (camBuf.mAddr) {
+            mCommon.freeShmMem(camBuf, GPU_ALGO_SHM);
+        }
+    }
+}
+
+Tnr7Param* IntelTNR7US::allocTnr7ParamBuf() {
+    LOG1("%s ", __func__);
+    uintptr_t personal = reinterpret_cast<uintptr_t>(this);
+    std::string initName = "/TnrParam" + std::to_string(personal) + "Shm";
+
+    mParamMems.mName = initName.c_str();
+    mParamMems.mSize = sizeof(Tnr7Param);
+    bool ret = mCommon.allocShmMem(mParamMems.mName, mParamMems.mSize, &mParamMems, GPU_ALGO_SHM);
+    CheckError(!ret, nullptr, "@%s, allocShmMem fails", __func__);
+
+    return reinterpret_cast<Tnr7Param*>(mParamMems.mAddr);
+}
+
+int IntelTNR7US::asyncParamUpdate(int gain) {
+    mTnrRunInfo->gain = gain;
+
+    int32_t runHandle = mCommon.getShmMemHandle(static_cast<void*>(mTnrRunInfo));
+    bool ret = mCommon.requestSync(IPC_GPU_TNR_PARAM_UPDATE, runHandle);
+    CheckError(!ret, UNKNOWN_ERROR, "@%s, IPC_GPU_TNR_PARAM_UPDATE requestSync fails", __func__);
+    return OK;
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/client/IntelTNR7US.h b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelTNR7US.h
new file mode 100644
index 000000000000..0630c7e5b9a8
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/client/IntelTNR7US.h
@@ -0,0 +1,54 @@
+/*
+ * Copyright (C) 2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <vector>
+
+#include "BufferQueue.h"
+#include "CameraBuffer.h"
+#include "Parameters.h"
+#include "PlatformData.h"
+#include "TNRCommon.h"
+
+namespace icamera {
+
+class IntelTNR7US {
+ public:
+    IntelTNR7US();
+    ~IntelTNR7US();
+    int init(int width, int height);
+    int runTnrFrame(const void* inBufAddr, void* outBufAddr, uint32_t inBufSize,
+                    uint32_t outBufSize, Tnr7Param* tnrParam);
+    Tnr7Param* allocTnr7ParamBuf();
+    void* allocCamBuf(uint32_t bufSize, int id);
+    int allocRefBufs(uint32_t bufSize);
+    void freeAllBufs();
+    int asyncParamUpdate(int gain);
+
+ private:
+    static const int TnrRefBufCount = TNR7US_REFERENCE_BUFFER_COUNT;
+    IntelAlgoCommon mCommon;
+    int mRefBufferSize;
+    int mRefBufIndex;
+    void* mRefBufAddr[TnrRefBufCount];
+    TnrRunInfo* mTnrRunInfo;
+    std::vector<ShmMemInfo> mCamBufMems;
+    ShmMemInfo mParamMems;
+    ShmMemInfo mTnrRunInfoMem;
+    DISALLOW_COPY_AND_ASSIGN(IntelTNR7US);
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/GraphConfigServer.cpp b/camera/hal/intel/ipu6/modules/sandboxing/server/GraphConfigServer.cpp
new file mode 100644
index 000000000000..c0b7f25c7632
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/GraphConfigServer.cpp
@@ -0,0 +1,164 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "GraphConfigServer"
+
+#include "modules/sandboxing/server/GraphConfigServer.h"
+
+#include <string>
+#include <vector>
+
+#include "CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+GraphConfigServer::GraphConfigServer() {
+    LOGIPC("@%s", __func__);
+}
+
+GraphConfigServer::~GraphConfigServer() {
+    LOGIPC("@%s", __func__);
+}
+
+void GraphConfigServer::addCustomKeyMap() {
+    std::shared_ptr<GraphConfigImpl> graphConfigImpl = std::make_shared<GraphConfigImpl>();
+    graphConfigImpl->addCustomKeyMap();
+}
+
+status_t GraphConfigServer::parse(void* pData, size_t dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%zu", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    GraphParseParams* parseParam = nullptr;
+    bool ret = mIpc.serverUnflattenParse(pData, dataSize, &parseParam);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenParse fails", __func__);
+
+    std::shared_ptr<GraphConfigImpl> graphConfigImpl = std::make_shared<GraphConfigImpl>();
+    status_t rt = graphConfigImpl->parse(parseParam->cameraId, parseParam->GD, parseParam->gdSize,
+                                         parseParam->GS, parseParam->gsSize);
+    CheckError(rt != OK, UNKNOWN_ERROR, "@%s, Failed to parse the graph xml data", __func__);
+
+    return OK;
+}
+
+void GraphConfigServer::releaseGraphNodes() {
+    if (mGraphConfigMap.empty()) return;
+    mGraphConfigMap.begin()->second->releaseGraphNodes();
+}
+
+status_t GraphConfigServer::configStreams(void* pData, size_t dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%zu", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    GraphBaseInfo info;
+    GraphSettingType type;
+    std::vector<HalStream*> streams;
+    bool ret = mIpc.serverUnflattenConfigStreams(pData, dataSize, &info, &type, &streams);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenConfigStreams fails", __func__);
+
+    // release the old item
+    auto it = mGraphConfigMap.find(info);
+    if (it != mGraphConfigMap.end()) {
+        mGraphConfigMap.erase(it);
+    }
+    std::shared_ptr<GraphConfigImpl> graphConfigImpl =
+        std::make_shared<GraphConfigImpl>(info.cameraId, info.configMode, type);
+    status_t rt = graphConfigImpl->configStreams(streams);
+    CheckError(rt != OK, ret, "@%s, Failed to configStreams, cameraId: %d, configMode: %d",
+               __func__, info.cameraId, info.configMode);
+
+    mGraphConfigMap[info] = graphConfigImpl;
+
+    return OK;
+}
+
+status_t GraphConfigServer::getGraphConfigData(void* pData, size_t dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%zu", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    GraphBaseInfo info;
+    bool ret = mIpc.serverUnflattenGetGraphData(pData, dataSize, &info);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenGetGraphData fails", __func__);
+
+    LOGIPC("%s, cameraId: %d, configMode: %d", __func__, info.cameraId, info.configMode);
+
+    auto it = mGraphConfigMap.find(info);
+    CheckError(it == mGraphConfigMap.end(), UNKNOWN_ERROR,
+               "%s, Failed to find the graph config. cameraId: %d", __func__, info.cameraId);
+
+    IGraphType::GraphConfigData graphData;
+    status_t rt = it->second->getGraphConfigData(&graphData);
+    CheckError(rt != OK, UNKNOWN_ERROR, "%s, Failed to getGraphConfigData: cameraId: %d", __func__,
+               info.cameraId);
+
+    ret = mIpc.serverFlattenGetGraphData(pData, dataSize, graphData);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenGetGraphData fails", __func__);
+
+    return OK;
+}
+
+status_t GraphConfigServer::getPgIdForKernel(void* pData, size_t dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%zu", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    uint32_t streamId = -1;
+    int32_t kernelId = 0;
+    GraphBaseInfo info;
+    bool ret = mIpc.serverUnFlattenGetPgId(pData, dataSize, &info, &streamId, &kernelId);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnFlattenGetPgId fails", __func__);
+
+    LOGIPC("%s, cameraId: %d, configMode: %d", __func__, info.cameraId, info.configMode);
+
+    auto it = mGraphConfigMap.find(info);
+    CheckError(it == mGraphConfigMap.end(), UNKNOWN_ERROR,
+               "%s, Failed to find the graph config. cameraId: %d", __func__, info.cameraId);
+
+    int32_t pgId = -1;
+    it->second->getPgIdForKernel(streamId, kernelId, &pgId);
+
+    ret = mIpc.serverFlattenGetPgId(pData, dataSize, pgId);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenGetPgId fails", __func__);
+
+    return OK;
+}
+
+status_t GraphConfigServer::pipelineGetConnections(void* pData, size_t dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%zu", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    GraphBaseInfo info;
+    std::vector<std::string> pgList;
+    bool ret = mIpc.serverUnFlattenGetConnection(pData, dataSize, &info, &pgList);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnFlattenGetPgId fails", __func__);
+
+    LOGIPC("%s, cameraId: %d, configMode: %d", __func__, info.cameraId, info.configMode);
+
+    auto it = mGraphConfigMap.find(info);
+    CheckError(it == mGraphConfigMap.end(), UNKNOWN_ERROR,
+               "%s, Failed to find the graph config. cameraId: %d", __func__, info.cameraId);
+
+    std::vector<IGraphType::PipelineConnection> confVector;
+    std::vector<IGraphType::ScalerInfo> scalerInfo;
+    status_t rt = it->second->pipelineGetConnections(pgList, &scalerInfo, &confVector);
+    CheckError(rt != OK, UNKNOWN_ERROR, "%s, Failed to getConnection: cameraId: %d", __func__,
+               info.cameraId);
+
+    ret = mIpc.serverFlattenGetConnection(pData, dataSize, scalerInfo, confVector);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenGetPgId fails", __func__);
+
+    return OK;
+}
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/GraphConfigServer.h b/camera/hal/intel/ipu6/modules/sandboxing/server/GraphConfigServer.h
new file mode 100644
index 000000000000..6a0996b03b42
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/GraphConfigServer.h
@@ -0,0 +1,44 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <map>
+#include <memory>
+
+#include "iutils/Errors.h"
+#include "modules/algowrapper/graph/GraphConfigImpl.h"
+#include "modules/sandboxing/IPCGraphConfig.h"
+
+namespace icamera {
+class GraphConfigServer {
+ public:
+    GraphConfigServer();
+    virtual ~GraphConfigServer();
+
+    void addCustomKeyMap();
+    status_t parse(void* pData, size_t dataSize);
+    void releaseGraphNodes();
+    status_t configStreams(void* pData, size_t dataSize);
+    status_t getGraphConfigData(void* pData, size_t dataSize);
+    status_t getPgIdForKernel(void* pData, size_t dataSize);
+    status_t pipelineGetConnections(void* pData, size_t dataSize);
+
+ private:
+    std::map<GraphBaseInfo, std::shared_ptr<GraphConfigImpl> > mGraphConfigMap;
+    IPCGraphConfig mIpc;
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelAiqServer.cpp b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelAiqServer.cpp
new file mode 100644
index 000000000000..00484c2a5cc3
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelAiqServer.cpp
@@ -0,0 +1,327 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelAiqServer"
+
+#include "modules/sandboxing/server/IntelAiqServer.h"
+
+#include <ia_aiq.h>
+#include <ia_cmc_parser.h>
+
+#include <algorithm>
+#include <memory>
+#include <string>
+#include <utility>
+
+namespace icamera {
+IntelAiqServer::IntelAiqServer() {
+    LOGIPC("@%s", __func__);
+}
+
+IntelAiqServer::~IntelAiqServer() {
+    LOGIPC("@%s", __func__);
+}
+
+status_t IntelAiqServer::init(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(!pData, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(aiq_init_params), UNKNOWN_ERROR, "@%s, buffer is small", __func__);
+
+    aiq_init_params* params = static_cast<aiq_init_params*>(pData);
+
+    ia_binary_data aiqbData = {nullptr, 0};
+    ia_binary_data nvmData = {nullptr, 0};
+    ia_binary_data aiqdData = {nullptr, 0};
+    bool ret = mIpc.serverUnflattenInit(pData, dataSize, &aiqbData, &nvmData, &aiqdData);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenInit fails", __func__);
+
+    std::unique_ptr<IntelAiq> intelAiq = std::make_unique<IntelAiq>();
+
+    ia_aiq* aiq = intelAiq->init(&aiqbData, &nvmData, &aiqdData, params->stats_max_width,
+                                 params->stats_max_height, params->max_num_stats_in,
+                                 reinterpret_cast<ia_cmc_t*>(params->cmcRemoteHandle),
+                                 reinterpret_cast<ia_mkn*>(params->ia_mkn));
+    CheckError(!aiq, UNKNOWN_ERROR, "@%s, intelAiq.init fails", __func__);
+
+    mIntelAiqs[aiq] = std::move(intelAiq);
+
+    params->results = reinterpret_cast<uintptr_t>(aiq);
+
+    return OK;
+}
+
+status_t IntelAiqServer::aeRun(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(!pData, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(ae_run_params), UNKNOWN_ERROR, "@%s, buffer is small", __func__);
+
+    ae_run_params* params = static_cast<ae_run_params*>(pData);
+
+    ia_aiq_ae_input_params* aeParams = nullptr;
+    bool ret = mIpc.serverUnflattenAe(params, &aeParams);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenAe fails", __func__);
+
+    ia_aiq* aiq = reinterpret_cast<ia_aiq*>(params->aiq_handle);
+    if (mIntelAiqs.find(aiq) == mIntelAiqs.end()) {
+        LOGE("@%s, aiq:%p doesn't exist", __func__, aiq);
+        return UNKNOWN_ERROR;
+    }
+
+    ia_aiq_ae_results* aeResults = nullptr;
+    ia_err err = mIntelAiqs[aiq]->aeRun(aeParams, &aeResults);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, aeRun fails %d", __func__, err);
+
+    ret = mIpc.serverFlattenAe(*aeResults, params);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenAe fails", __func__);
+
+    return OK;
+}
+
+status_t IntelAiqServer::afRun(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(!pData, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(af_run_params), UNKNOWN_ERROR, "@%s, buffer is small", __func__);
+
+    af_run_params* params = static_cast<af_run_params*>(pData);
+
+    ia_aiq_af_input_params* afParams = nullptr;
+    bool ret = mIpc.serverUnflattenAf(params, &afParams);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflatten fails", __func__);
+
+    ia_aiq* aiq = reinterpret_cast<ia_aiq*>(params->aiq_handle);
+    if (mIntelAiqs.find(aiq) == mIntelAiqs.end()) {
+        LOGE("@%s, aiq:%p doesn't exist", __func__, aiq);
+        return UNKNOWN_ERROR;
+    }
+
+    ia_aiq_af_results* afResults = nullptr;
+    ia_err err = mIntelAiqs[aiq]->afRun(afParams, &afResults);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, afRun fails %d", __func__, err);
+
+    ret = mIpc.serverFlattenAf(*afResults, params);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlatten fails", __func__);
+
+    return OK;
+}
+
+status_t IntelAiqServer::awbRun(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(!pData, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(awb_run_params), UNKNOWN_ERROR, "@%s, buffer is small", __func__);
+
+    awb_run_params* params = static_cast<awb_run_params*>(pData);
+
+    ia_aiq_awb_input_params* awbParams = nullptr;
+    bool ret = mIpc.serverUnflattenAwb(params, &awbParams);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenAwb fails", __func__);
+
+    ia_aiq* aiq = reinterpret_cast<ia_aiq*>(params->aiq_handle);
+    if (mIntelAiqs.find(aiq) == mIntelAiqs.end()) {
+        LOGE("@%s, aiq:%p doesn't exist", __func__, aiq);
+        return UNKNOWN_ERROR;
+    }
+
+    ia_aiq_awb_results* awbResults = nullptr;
+    ia_err err = mIntelAiqs[aiq]->awbRun(awbParams, &awbResults);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, awbRun fails %d", __func__, err);
+
+    ret = mIpc.serverFlattenAwb(*awbResults, params);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenAwb fails", __func__);
+
+    return OK;
+}
+
+status_t IntelAiqServer::gbceRun(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(!pData, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(gbce_run_params), UNKNOWN_ERROR, "@%s, buffer is small", __func__);
+
+    gbce_run_params* params = static_cast<gbce_run_params*>(pData);
+
+    ia_aiq* aiq = reinterpret_cast<ia_aiq*>(params->aiq_handle);
+    if (mIntelAiqs.find(aiq) == mIntelAiqs.end()) {
+        LOGE("@%s, aiq:%p doesn't exist", __func__, aiq);
+        return UNKNOWN_ERROR;
+    }
+
+    ia_aiq_gbce_results* gbceResults = nullptr;
+    ia_err err = mIntelAiqs[aiq]->gbceRun(&params->base, &gbceResults);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, gbceRun fails %d", __func__, err);
+
+    bool ret = mIpc.serverFlattenGbce(*gbceResults, params);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenGbce fails", __func__);
+
+    return OK;
+}
+
+status_t IntelAiqServer::paRunV1(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(!pData, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(pa_run_v1_params), UNKNOWN_ERROR, "@%s, buffer is small",
+               __func__);
+
+    pa_run_v1_params* params = static_cast<pa_run_v1_params*>(pData);
+
+    ia_aiq_pa_input_params* paParams = nullptr;
+    bool ret = mIpc.serverUnflattenPaV1(params, &paParams);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenPa fails", __func__);
+
+    ia_aiq* aiq = reinterpret_cast<ia_aiq*>(params->aiq_handle);
+    if (mIntelAiqs.find(aiq) == mIntelAiqs.end()) {
+        LOGE("@%s, aiq:%p doesn't exist", __func__, aiq);
+        return UNKNOWN_ERROR;
+    }
+
+    //    ia_aiq_pa_results
+    ia_aiq_pa_results_v1* paResults = nullptr;
+    ia_err err = mIntelAiqs[aiq]->paRunV1(paParams, &paResults);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, paRunV1 fails %d", __func__, err);
+
+    ret = mIpc.serverFlattenPaV1(*paResults, params);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenPa fails", __func__);
+
+    return OK;
+}
+
+status_t IntelAiqServer::saRunV2(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(!pData, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(sa_run_v2_params), UNKNOWN_ERROR, "@%s, buffer is small",
+               __func__);
+
+    sa_run_v2_params* params = static_cast<sa_run_v2_params*>(pData);
+
+    ia_aiq_sa_input_params_v1* saParams = nullptr;
+    bool ret = mIpc.serverUnflattenSaV2(*params, &saParams);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenSaV2 fails", __func__);
+
+    ia_aiq* aiq = reinterpret_cast<ia_aiq*>(params->aiq_handle);
+    if (mIntelAiqs.find(aiq) == mIntelAiqs.end()) {
+        LOGE("@%s, aiq:%p doesn't exist", __func__, aiq);
+        return UNKNOWN_ERROR;
+    }
+
+    ia_aiq_sa_results_v1* saResults = nullptr;
+    ia_err err = mIntelAiqs[aiq]->saRunV2(saParams, &saResults);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, saRunV2 fails %d", __func__, err);
+
+    ret = mIpc.serverFlattenSaV2(*saResults, params);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenSaV2 fails", __func__);
+
+    return OK;
+}
+
+status_t IntelAiqServer::statisticsSetV4(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(!pData, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(set_statistics_set_v4_params), UNKNOWN_ERROR,
+               "@%s, buffer is small", __func__);
+
+    set_statistics_set_v4_params* params = static_cast<set_statistics_set_v4_params*>(pData);
+
+    ia_aiq_statistics_input_params_v4* stat = nullptr;
+    bool ret = mIpc.serverUnflattenStatSetV4(params, &stat);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenStatSetV4 fails", __func__);
+
+    if (stat->faces) {
+        LOGIPC("@%s, num_faces:%d", __func__, stat->faces->num_faces);
+        if (stat->faces->num_faces > 0) {
+            ia_rectangle& rect = stat->faces->faces[0].face_area;
+            LOGIPC("@%s, left:%d, top:%d, right:%d, bottom:%d", __func__, rect.left, rect.top,
+                   rect.right, rect.bottom);
+        }
+    }
+
+    ia_aiq* aiq = reinterpret_cast<ia_aiq*>(params->ia_aiq);
+    if (mIntelAiqs.find(aiq) == mIntelAiqs.end()) {
+        LOGE("@%s, aiq:%p doesn't exist", __func__, aiq);
+        return UNKNOWN_ERROR;
+    }
+
+    ia_err err = mIntelAiqs[aiq]->statisticsSetV4(stat);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, statisticsSetV4 fails %d", __func__, err);
+
+    return OK;
+}
+
+status_t IntelAiqServer::getAiqdData(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(!pData, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(ia_binary_data_params), UNKNOWN_ERROR, "@%s, buffer is small",
+               __func__);
+
+    ia_binary_data binaryData = {nullptr, 0};
+
+    ia_binary_data_params* params = static_cast<ia_binary_data_params*>(pData);
+
+    ia_aiq* aiq = reinterpret_cast<ia_aiq*>(params->aiq_handle);
+    if (mIntelAiqs.find(aiq) == mIntelAiqs.end()) {
+        LOGE("@%s, aiq:%p doesn't exist", __func__, aiq);
+        return UNKNOWN_ERROR;
+    }
+
+    ia_err err = mIntelAiqs[aiq]->getAiqdData(&binaryData);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, getAiqdData fails %d", __func__, err);
+    LOGIPC("@%s, binary_data, data:%p, size:%d", __func__, binaryData.data, binaryData.size);
+
+    MEMCPY_S(params->data, sizeof(params->data), binaryData.data, binaryData.size);
+    params->size = binaryData.size;
+
+    return OK;
+}
+
+status_t IntelAiqServer::deinit(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(!pData, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(aiq_deinit_params), UNKNOWN_ERROR, "@%s, buffer is small",
+               __func__);
+
+    aiq_deinit_params* params = static_cast<aiq_deinit_params*>(pData);
+
+    ia_aiq* aiq = reinterpret_cast<ia_aiq*>(params->aiq_handle);
+    if (mIntelAiqs.find(aiq) == mIntelAiqs.end()) {
+        LOGE("@%s, aiq:%p doesn't exist", __func__, aiq);
+        return UNKNOWN_ERROR;
+    }
+
+    mIntelAiqs[aiq]->deinit();
+
+    return OK;
+}
+
+status_t IntelAiqServer::getVersion(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(!pData, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(ia_aiq_version_params), UNKNOWN_ERROR, "@%s, buffer is small",
+               __func__);
+
+    ia_aiq_version_params* params = static_cast<ia_aiq_version_params*>(pData);
+
+    ia_aiq* aiq = reinterpret_cast<ia_aiq*>(params->aiq_handle);
+    if (mIntelAiqs.find(aiq) == mIntelAiqs.end()) {
+        LOGE("@%s, aiq:%p doesn't exist", __func__, aiq);
+        return UNKNOWN_ERROR;
+    }
+
+    std::string version;
+    mIntelAiqs[aiq]->getVersion(&version);
+    snprintf(params->data, sizeof(params->data), "%s", version.c_str());
+    params->size = std::min(version.size(), sizeof(params->data));
+    LOGIPC("@%s, aiq version:%s, size:%d", __func__, version.c_str(), params->size);
+
+    return OK;
+}
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelAiqServer.h b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelAiqServer.h
new file mode 100644
index 000000000000..4fe58770d237
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelAiqServer.h
@@ -0,0 +1,49 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+#include <unordered_map>
+
+#include "modules/algowrapper/IntelAiq.h"
+#include "modules/sandboxing/IPCCommon.h"
+#include "modules/sandboxing/IPCIntelAiq.h"
+
+namespace icamera {
+class IntelAiqServer {
+ public:
+    IntelAiqServer();
+    virtual ~IntelAiqServer();
+
+    status_t init(void* pData, int dataSize);
+    status_t aeRun(void* pData, int dataSize);
+    status_t afRun(void* pData, int dataSize);
+    status_t awbRun(void* pData, int dataSize);
+    status_t gbceRun(void* pData, int dataSize);
+    status_t paRunV1(void* pData, int dataSize);
+    status_t saRunV2(void* pData, int dataSize);
+    status_t statisticsSetV4(void* pData, int dataSize);
+    status_t getAiqdData(void* pData, int dataSize);
+    status_t deinit(void* pData, int dataSize);
+    status_t getVersion(void* pData, int dataSize);
+
+ private:
+    IPCIntelAiq mIpc;
+    std::unordered_map<ia_aiq*, std::unique_ptr<IntelAiq>> mIntelAiqs;
+};
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelAlgoServer.cpp b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelAlgoServer.cpp
new file mode 100644
index 000000000000..6166b3125cba
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelAlgoServer.cpp
@@ -0,0 +1,241 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelAlgoServer"
+
+#include "modules/sandboxing/server/IntelAlgoServer.h"
+
+#include <base/logging.h>
+#include <ia_log.h>
+#include <stdlib.h>
+#include <sys/mman.h>
+#include <sys/stat.h>
+
+#include <memory>
+#include <string>
+
+#include "iutils/Utils.h"
+#ifndef GPU_ALGO_SERVER
+#include "modules/sandboxing/server/IntelCPUAlgoServer.h"
+#else
+#include "modules/sandboxing/server/IntelGPUAlgoServer.h"
+#endif
+
+namespace icamera {
+
+IntelAlgoServer* IntelAlgoServer::mInstance = nullptr;
+
+void IntelAlgoServer::init() {
+    LOGIPC("@%s", __func__);
+
+    if (mInstance == nullptr) {
+        mInstance = new IntelAlgoServer;
+    }
+}
+
+void IntelAlgoServer::deInit() {
+    LOGIPC("@%s", __func__);
+
+    delete mInstance;
+    mInstance = nullptr;
+}
+
+IntelAlgoServer::IntelAlgoServer() : mCallback(nullptr) {
+    LOGIPC("@%s", __func__);
+
+    ia_env env = {&Log::ccaPrintDebug, &Log::ccaPrintError, &Log::ccaPrintInfo};
+    ia_log_init(&env);
+
+    for (int i = 0; i < kThreadNum; i++) {
+        std::string name = kServerName + std::to_string(i) + std::string(" Thread");
+        mThreads[i] = std::unique_ptr<base::Thread>(new base::Thread(name));
+        mThreads[i]->Start();
+    }
+#ifndef GPU_ALGO_SERVER
+    mRequestHandler = std::unique_ptr<RequestHandler>(new IntelCPUAlgoServer(this));
+#else
+    mRequestHandler = std::unique_ptr<RequestHandler>(new IntelGPUAlgoServer(this));
+#endif
+
+    for (int32_t i = 1; i <= HANDLE_INDEX_MAX_VALUE; i++) {
+        mHandlesQueue.push(i);
+    }
+}
+
+IntelAlgoServer::~IntelAlgoServer() {
+    LOGIPC("@%s", __func__);
+    ia_log_deinit();
+}
+
+int32_t IntelAlgoServer::initialize(const camera_algorithm_callback_ops_t* callback_ops) {
+    LOGIPC("@%s, callback_ops:%p", __func__, callback_ops);
+
+    CheckError((!callback_ops), -EINVAL, "@%s, the callback_ops is nullptr", __func__);
+
+    mCallback = callback_ops;
+
+    return 0;
+}
+
+int32_t IntelAlgoServer::registerBuffer(int buffer_fd) {
+    LOGIPC("@%s, buffer_fd:%d", __func__, buffer_fd);
+
+    std::lock_guard<std::mutex> l(mRegisterBufMutex);
+    CheckError((mHandles.find(buffer_fd) != mHandles.end()), -EINVAL,
+               "@%s, Buffer already registered", __func__);
+    CheckError(mHandlesQueue.empty(), -EBADFD, "@%s, Failed to get buffer handle index", __func__);
+
+    struct stat sb;
+    int ret = fstat(buffer_fd, &sb);
+    CheckError((ret == -1), -EBADFD, "@%s, Failed to get buffer status", __func__);
+
+    void* addr = mmap(0, sb.st_size, PROT_WRITE, MAP_SHARED, buffer_fd, 0);
+    CheckError((!addr), -EBADFD, "@%s, Failed to map buffer", __func__);
+
+    int32_t handle = mHandlesQueue.front();
+    mHandlesQueue.pop();
+    mHandles[buffer_fd] = handle;
+
+    mShmInfoMap[handle].fd = buffer_fd;
+    mShmInfoMap[handle].addr = addr;
+    mShmInfoMap[handle].size = sb.st_size;
+
+    return handle;
+}
+
+int IntelAlgoServer::parseReqHeader(const uint8_t req_header[], uint32_t size) {
+    LOGIPC("@%s, size:%d", __func__, size);
+
+    CheckError(size < IPC_REQUEST_HEADER_USED_NUM || req_header[0] != IPC_MATCHING_KEY, -1,
+               "@%s, fails, req_header[0]:%d, size:%d", __func__, req_header[0], size);
+
+    return 0;
+}
+
+void IntelAlgoServer::returnCallback(uint32_t req_id, status_t status, int32_t buffer_handle) {
+    LOGIPC("@%s, req_id:%d:%s, status:%d", __func__, req_id,
+           IntelAlgoIpcCmdToString(static_cast<IPC_CMD>(req_id)), status);
+    (*mCallback->return_callback)(mCallback, req_id, (status == OK ? 0 : 1), buffer_handle);
+}
+
+status_t IntelAlgoServer::getShmInfo(const int32_t buffer_handle, ShmInfo* memInfo) {
+    CheckError(!memInfo, UNKNOWN_ERROR, "%s, memInfo is nullptr", __func__);
+    if (buffer_handle == -1) return OK;
+
+    CheckError(mShmInfoMap.find(buffer_handle) == mShmInfoMap.end(), UNKNOWN_ERROR,
+               "%s, Invalid buffer handle", __func__);
+    *memInfo = mShmInfoMap[buffer_handle];
+
+    LOGIPC("@%s, fd:%d, size:%zu, addr: %p", __func__, memInfo->fd, memInfo->size, memInfo->addr);
+
+    return OK;
+}
+
+void IntelAlgoServer::handleRequest(const MsgReq& msg) {
+    LOGIPC("@%s", __func__);
+    CheckError(!mRequestHandler, VOID_VALUE, "@%s, handler is null", __func__);
+    mRequestHandler->handleRequest(msg);
+}
+
+void IntelAlgoServer::request(uint32_t req_id, const uint8_t req_header[], uint32_t size,
+                              int32_t buffer_handle) {
+    LOGIPC("@%s, size:%d, buffer_handle:%d", __func__, size, buffer_handle);
+    LOGIPC("@%s, req_id:%d:%s", __func__, req_id,
+           IntelAlgoIpcCmdToString(static_cast<IPC_CMD>(req_id)));
+
+    IPC_GROUP group = IntelAlgoIpcCmdToGroup(static_cast<IPC_CMD>(req_id));
+    LOGIPC("@%s, group:%d", __func__, group);
+
+    int ret = parseReqHeader(req_header, size);
+    if (ret != 0) {
+        returnCallback(req_id, UNKNOWN_ERROR, buffer_handle);
+        return;
+    }
+
+    MsgReq msg = {req_id, buffer_handle};
+
+#ifndef GPU_ALGO_SERVER
+    int threadId = group;
+#else
+    // GPU server thread id start from IPC_GROUP_GPU
+    int threadId = group - IPC_GROUP_GPU;
+#endif
+    if (threadId >= 0 && threadId < kThreadNum) {
+        if (mThreads[threadId] && mThreads[threadId]->task_runner()) {
+            mThreads[threadId]->task_runner()->PostTask(FROM_HERE,
+                base::Bind(&IntelAlgoServer::handleRequest, base::Unretained(this), msg));
+        }
+    }
+}
+
+void IntelAlgoServer::deregisterBuffers(const int32_t buffer_handles[], uint32_t size) {
+    LOGIPC("@%s, size:%d", __func__, size);
+
+    std::lock_guard<std::mutex> l(mRegisterBufMutex);
+    for (uint32_t i = 0; i < size; i++) {
+        int32_t handle = buffer_handles[i];
+        if (mShmInfoMap.find(handle) == mShmInfoMap.end()) {
+            continue;
+        }
+
+        mHandles.erase(mShmInfoMap[handle].fd);
+
+        munmap(mShmInfoMap[handle].addr, mShmInfoMap[handle].size);
+        close(mShmInfoMap[handle].fd);
+        mShmInfoMap.erase(handle);
+        mHandlesQueue.push(handle);
+    }
+}
+
+static int32_t initialize(const camera_algorithm_callback_ops_t* callback_ops) {
+    LOGIPC("@%s, callback_ops:%p", __func__, callback_ops);
+    return IntelAlgoServer::getInstance()->initialize(callback_ops);
+}
+
+static int32_t registerBuffer(int32_t buffer_fd) {
+    LOGIPC("@%s, buffer_fd:%d", __func__, buffer_fd);
+    return IntelAlgoServer::getInstance()->registerBuffer(buffer_fd);
+}
+
+static void request(uint32_t req_id, const uint8_t req_header[], uint32_t size,
+                    int32_t buffer_handle) {
+    LOGIPC("@%s, size:%d, buffer_handle:%d", __func__, size, buffer_handle);
+    IntelAlgoServer::getInstance()->request(req_id, req_header, size, buffer_handle);
+}
+
+static void deregisterBuffers(const int32_t buffer_handles[], uint32_t size) {
+    LOGIPC("@%s, size:%d", __func__, size);
+    return IntelAlgoServer::getInstance()->deregisterBuffers(buffer_handles, size);
+}
+
+extern "C" {
+camera_algorithm_ops_t CAMERA_ALGORITHM_MODULE_INFO_SYM
+    __attribute__((__visibility__("default"))) = {.initialize = initialize,
+                                                  .register_buffer = registerBuffer,
+                                                  .request = request,
+                                                  .deregister_buffers = deregisterBuffers};
+}
+
+__attribute__((constructor)) void initIntelAlgoServer() {
+    icamera::Log::setDebugLevel();
+    IntelAlgoServer::init();
+}
+
+__attribute__((destructor)) void deinitIntelAlgoServer() {
+    IntelAlgoServer::deInit();
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelAlgoServer.h b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelAlgoServer.h
new file mode 100644
index 000000000000..50952801a7fe
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelAlgoServer.h
@@ -0,0 +1,106 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <base/bind.h>
+#include <base/threading/thread.h>
+
+#include <memory>
+#include <queue>
+#include <string>
+#include <unordered_map>
+
+#include "CameraLog.h"
+#include "cros-camera/camera_algorithm.h"
+#include "iutils/Errors.h"
+#include "iutils/Thread.h"
+#include "modules/sandboxing/IPCCommon.h"
+
+namespace icamera {
+
+#define HANDLE_INDEX_MAX_VALUE 1024
+struct MsgReq {
+    uint32_t req_id;
+    int32_t buffer_handle;
+};
+
+typedef struct {
+    int32_t fd;
+    void* addr;
+    size_t size;
+} ShmInfo;
+
+class IntelAlgoServer;
+class RequestHandler {
+ public:
+    explicit RequestHandler(IntelAlgoServer* server) { mIntelAlgoServer = server; }
+    virtual ~RequestHandler() {}
+    virtual void handleRequest(const MsgReq& msg) = 0;
+    IntelAlgoServer* getIntelAlgoServer() { return mIntelAlgoServer; }
+
+ private:
+    IntelAlgoServer* mIntelAlgoServer;
+};
+
+class IntelAlgoServer {
+ public:
+    static void init();
+    static void deInit();
+
+    static IntelAlgoServer* getInstance() { return mInstance; }
+
+    int32_t initialize(const camera_algorithm_callback_ops_t* callback_ops);
+    int32_t registerBuffer(int buffer_fd);
+    void request(uint32_t req_id, const uint8_t req_header[], uint32_t size, int32_t buffer_handle);
+    void deregisterBuffers(const int32_t buffer_handles[], uint32_t size);
+
+    void handleRequest(const MsgReq& msg);
+    status_t getShmInfo(const int32_t buffer_handle, ShmInfo* memInfo);
+    void returnCallback(uint32_t req_id, status_t status, int32_t buffer_handle);
+
+ private:
+    IntelAlgoServer();
+    ~IntelAlgoServer();
+    int parseReqHeader(const uint8_t req_header[], uint32_t size);
+
+ private:
+    static IntelAlgoServer* mInstance;
+#ifndef GPU_ALGO_SERVER
+    static const int kThreadNum = IPC_CPU_GROUP_NUM;
+    const std::string kServerName = "IntelCPUAlgoServer";
+#else
+    static const int kThreadNum = IPC_GPU_GROUP_NUM;
+    const std::string kServerName = "IntelGPUAlgoServer";
+#endif
+    std::unique_ptr<base::Thread> mThreads[kThreadNum];
+    std::unique_ptr<RequestHandler> mRequestHandler;
+
+    const camera_algorithm_callback_ops_t* mCallback;
+
+    // key: shared memory fd from client
+    // value: handle that returns from RegisterBuffer()
+    std::unordered_map<int32_t, int32_t> mHandles;
+
+    // key: handle that returns from RegisterBuffer()
+    // value: shared memory fd and mapped address
+    std::unordered_map<int32_t, ShmInfo> mShmInfoMap;
+    std::queue<int32_t> mHandlesQueue;
+    std::mutex mRegisterBufMutex;
+
+    DISALLOW_COPY_AND_ASSIGN(IntelAlgoServer);
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelCPUAlgoServer.cpp b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelCPUAlgoServer.cpp
new file mode 100644
index 000000000000..ab9354b62594
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelCPUAlgoServer.cpp
@@ -0,0 +1,307 @@
+/*
+ * Copyright (C) 2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelCPUAlgoServer"
+
+#include "modules/sandboxing/server/IntelCPUAlgoServer.h"
+
+#include <base/logging.h>
+#include <ia_log.h>
+#include <stdlib.h>
+#include <sys/mman.h>
+#include <sys/stat.h>
+
+#include <memory>
+#include <string>
+
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+IntelCPUAlgoServer::IntelCPUAlgoServer(IntelAlgoServer* server) : RequestHandler(server) {
+    LOGIPC("@%s", __func__);
+}
+
+void IntelCPUAlgoServer::handleRequest(const MsgReq& msg) {
+    uint32_t req_id = msg.req_id;
+    int32_t buffer_handle = msg.buffer_handle;
+
+    ShmInfo info = {};
+    status_t status = getIntelAlgoServer()->getShmInfo(buffer_handle, &info);
+    if (status != OK) {
+        LOGE("@%s, Invalid buffer handle", __func__);
+        getIntelAlgoServer()->returnCallback(req_id, UNKNOWN_ERROR, buffer_handle);
+        return;
+    }
+
+    size_t requestSize = info.size;
+    void* addr = info.addr;
+    LOGIPC("@%s, req_id:%d:%s, requestSize:%zu, addr:%p, buffer_handle:%d", __func__, req_id,
+           IntelAlgoIpcCmdToString(static_cast<IPC_CMD>(req_id)), requestSize, addr, buffer_handle);
+
+    switch (req_id) {
+        case IPC_LARD_INIT:
+            status = mLard.init(addr, requestSize);
+            break;
+        case IPC_LARD_GET_TAG_LIST:
+            status = mLard.getTagList(addr, requestSize);
+            break;
+        case IPC_LARD_RUN:
+            status = mLard.run(addr, requestSize);
+            break;
+        case IPC_LARD_DEINIT:
+            status = mLard.deinit(addr, requestSize);
+            break;
+        case IPC_FD_INIT:
+            status = mFaceDetection.init(addr, requestSize);
+            break;
+        case IPC_FD_RUN: {
+            FaceDetectionRunParams* palParams = static_cast<FaceDetectionRunParams*>(addr);
+            void* addrImage = nullptr;
+            if (palParams->bufferHandle >= 0) {
+                ShmInfo imageDataInfo;
+                status = getIntelAlgoServer()->getShmInfo(palParams->bufferHandle, &imageDataInfo);
+                if (status != OK) {
+                    LOGE("%s, the buffer handle for image data is invalid", __func__);
+                    break;
+                }
+                addrImage = imageDataInfo.addr;
+            }
+            status = mFaceDetection.run(addr, requestSize, addrImage);
+            break;
+        }
+        case IPC_FD_DEINIT:
+            status = mFaceDetection.deinit();
+            break;
+        case IPC_GRAPH_ADD_KEY:
+            mGraph.addCustomKeyMap();
+            break;
+        case IPC_GRAPH_PARSE:
+            mGraph.parse(addr, requestSize);
+            break;
+        case IPC_GRAPH_RELEASE_NODES:
+            mGraph.releaseGraphNodes();
+            break;
+        case IPC_GRAPH_CONFIG_STREAMS:
+            mGraph.configStreams(addr, requestSize);
+            break;
+        case IPC_GRAPH_GET_CONFIG_DATA:
+            mGraph.getGraphConfigData(addr, requestSize);
+            break;
+        case IPC_GRAPH_GET_CONNECTION:
+            mGraph.pipelineGetConnections(addr, requestSize);
+            break;
+        case IPC_GRAPH_GET_PG_ID:
+            mGraph.getPgIdForKernel(addr, requestSize);
+            break;
+        case IPC_CMC_INIT:
+            status = mCmc.init(addr, requestSize);
+            break;
+        case IPC_CMC_DEINIT:
+            status = mCmc.deinit(addr, requestSize);
+            break;
+        case IPC_MKN_INIT:
+            status = mMkn.init(addr, requestSize);
+            break;
+        case IPC_MKN_ENABLE:
+            status = mMkn.enable(addr, requestSize);
+            break;
+        case IPC_MKN_PREPARE:
+            status = mMkn.prepare(addr, requestSize);
+            break;
+        case IPC_MKN_DEINIT:
+            status = mMkn.deinit(addr, requestSize);
+            break;
+        case IPC_LTM_INIT:
+            status = mLtm.init(addr, requestSize);
+            break;
+        case IPC_LTM_RUN:
+            status = mLtm.run(addr, requestSize);
+            break;
+        case IPC_LTM_DEINIT:
+            status = mLtm.deinit(addr, requestSize);
+            break;
+        case IPC_AIQ_INIT:
+            status = mAiq.init(addr, requestSize);
+            break;
+        case IPC_AIQ_AE_RUN:
+            status = mAiq.aeRun(addr, requestSize);
+            break;
+        case IPC_AIQ_AF_RUN:
+            status = mAiq.afRun(addr, requestSize);
+            break;
+        case IPC_AIQ_AWB_RUN:
+            status = mAiq.awbRun(addr, requestSize);
+            break;
+        case IPC_AIQ_GBCE_RUN:
+            status = mAiq.gbceRun(addr, requestSize);
+            break;
+        case IPC_AIQ_PA_RUN_V1:
+            status = mAiq.paRunV1(addr, requestSize);
+            break;
+        case IPC_AIQ_SA_RUN_V2:
+            status = mAiq.saRunV2(addr, requestSize);
+            break;
+        case IPC_AIQ_STATISTICS_SET_V4:
+            status = mAiq.statisticsSetV4(addr, requestSize);
+            break;
+        case IPC_AIQ_GET_AIQD_DATA:
+            status = mAiq.getAiqdData(addr, requestSize);
+            break;
+        case IPC_AIQ_DEINIT:
+            status = mAiq.deinit(addr, requestSize);
+            break;
+        case IPC_AIQ_GET_VERSION:
+            status = mAiq.getVersion(addr, requestSize);
+            break;
+        case IPC_DVS_INIT:
+            status = mDvs.init(addr, requestSize);
+            break;
+        case IPC_DVS_CONFIG:
+            status = mDvs.config(addr, requestSize);
+            break;
+        case IPC_DVS_SET_NONE_BLANK_RATION:
+            status = mDvs.setNonBlankRatio(addr, requestSize);
+            break;
+        case IPC_DVS_SET_DIGITAL_ZOOM_MODE:
+            status = mDvs.setDigitalZoomMode(addr, requestSize);
+            break;
+        case IPC_DVS_SET_DIGITAL_ZOOM_REGION:
+            status = mDvs.setDigitalZoomRegion(addr, requestSize);
+            break;
+        case IPC_DVS_SET_DIGITAL_ZOOM_COORDINATE:
+            status = mDvs.setDigitalZoomCoordinate(addr, requestSize);
+            break;
+        case IPC_DVS_SET_DIGITAL_ZOOM_MAGNITUDE:
+            status = mDvs.setDigitalZoomMagnitude(addr, requestSize);
+            break;
+        case IPC_DVS_FREE_MORPH_TABLE:
+            status = mDvs.freeMorphTable(addr, requestSize);
+            break;
+        case IPC_DVS_ALLOCATE_MORPH_TABLE:
+            status = mDvs.allocateMorphTalbe(addr, requestSize);
+            break;
+        case IPC_DVS_GET_MORPH_TABLE:
+            status = mDvs.getMorphTalbe(addr, requestSize);
+            break;
+        case IPC_DVS_SET_STATISTICS:
+            status = mDvs.setStatistics(addr, requestSize);
+            break;
+        case IPC_DVS_EXECUTE:
+            status = mDvs.execute(addr, requestSize);
+            break;
+        case IPC_DVS_GET_IMAGE_TRANSFORMATION:
+            status = mDvs.getImageTransformation(addr, requestSize);
+            break;
+        case IPC_DVS_DEINIT:
+            status = mDvs.deinit(addr, requestSize);
+            break;
+        case IPC_ISP_ADAPTOR_INIT:
+            status = mIspAdaptor.init(addr, requestSize);
+            break;
+        case IPC_ISP_ADAPTOR_DEINIT:
+            status = mIspAdaptor.deInit(addr, requestSize);
+            break;
+        case IPC_ISP_GET_PAL_SIZE:
+            status = mIspAdaptor.getPalDataSize(addr, requestSize);
+            break;
+        case IPC_ISP_CONVERT_STATS: {
+            ConvertStatsParam* params = static_cast<ConvertStatsParam*>(addr);
+            ShmInfo statsDataInfo = {};
+            status = getIntelAlgoServer()->getShmInfo(params->statsHandle, &statsDataInfo);
+            if (status == OK) {
+                status = mIspAdaptor.queryAndConvertStats(addr, requestSize, statsDataInfo.addr);
+            } else {
+                LOGE("%s, the buffer handle for stats data is invalid", __func__);
+            }
+            break;
+        }
+        case IPC_ISP_RUN_PAL: {
+            RunPalParam* palParams = static_cast<RunPalParam*>(addr);
+            ShmInfo palDataInfo;
+            status = getIntelAlgoServer()->getShmInfo(palParams->palDataHandle, &palDataInfo);
+            if (status != OK) {
+                LOGE("%s, the buffer handle for pal data is invalid", __func__);
+                break;
+            }
+            LOGIPC("@%s, pal data info: fd:%d, size:%zu, addr: %p", __func__, palDataInfo.fd,
+                   palDataInfo.size, palDataInfo.addr);
+
+            status = mIspAdaptor.runPal(addr, requestSize, palDataInfo.addr);
+            break;
+        }
+        case IPC_PG_PARAM_INIT:
+            status = mPGParam.init(addr, requestSize);
+            break;
+        case IPC_PG_PARAM_PREPARE: {
+            pg_param_prepare_params* prepareParams = static_cast<pg_param_prepare_params*>(addr);
+            ShmInfo palDataInfo = {};
+            status = getIntelAlgoServer()->getShmInfo(prepareParams->ipuParamHandle, &palDataInfo);
+            if (status != OK) {
+                LOGE("%s, the buffer handle for pal data is invalid", __func__);
+                break;
+            }
+            status = mPGParam.prepare(addr, requestSize, palDataInfo.addr);
+            break;
+        }
+        case IPC_PG_PARAM_ALLOCATE_PG:
+            status = mPGParam.allocatePGBuffer(addr, requestSize);
+            break;
+        case IPC_PG_PARAM_GET_FRAG_DESCS:
+            status = mPGParam.getFragmentDescriptors(addr, requestSize);
+            break;
+        case IPC_PG_PARAM_PREPARE_PROGRAM:
+            status = mPGParam.setPGAndPrepareProgram(addr, requestSize);
+            break;
+        case IPC_PG_PARAM_ALLOCATE_PAYLOADS:
+            status = mPGParam.allocatePayloads(addr, requestSize);
+            break;
+        case IPC_PG_PARAM_ENCODE: {
+            pg_param_encode_params* encodeParams = static_cast<pg_param_encode_params*>(addr);
+            ShmInfo palDataInfo = {};
+            status = getIntelAlgoServer()->getShmInfo(encodeParams->ipuParamHandle, &palDataInfo);
+            if (status != OK) {
+                LOGE("%s, the buffer handle for pal data is invalid", __func__);
+                break;
+            }
+            status = mPGParam.updatePALAndEncode(addr, requestSize, palDataInfo.addr);
+            break;
+        }
+        case IPC_PG_PARAM_DECODE: {
+            pg_param_decode_params* decodeParams = static_cast<pg_param_decode_params*>(addr);
+            ShmInfo decodeInfo = {};
+            status = getIntelAlgoServer()->getShmInfo(decodeParams->clientStatsHandle, &decodeInfo);
+            if (status == OK) {
+                status = mPGParam.decode(addr, requestSize, decodeInfo.addr);
+            } else {
+                LOGE("%s, the buffer handle for stats data is invalid", __func__);
+            }
+            break;
+        }
+        case IPC_PG_PARAM_DEINIT:
+            mPGParam.deinit(addr, requestSize);
+            break;
+        default:
+            LOGE("@%s, req_id:%d is not defined", __func__, req_id);
+            status = UNKNOWN_ERROR;
+            break;
+    }
+
+    LOGIPC("@%s, req_id:%d:%s, status:%d", __func__, req_id,
+           IntelAlgoIpcCmdToString(static_cast<IPC_CMD>(req_id)), status);
+    getIntelAlgoServer()->returnCallback(req_id, status, buffer_handle);
+}
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelCPUAlgoServer.h b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelCPUAlgoServer.h
new file mode 100644
index 000000000000..f026af3f9f17
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelCPUAlgoServer.h
@@ -0,0 +1,63 @@
+/*
+ * Copyright (C) 2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <base/bind.h>
+#include <base/threading/thread.h>
+
+#include <memory>
+#include <queue>
+#include <unordered_map>
+
+#include "CameraLog.h"
+#include "GraphConfigServer.h"
+#include "IntelAiqServer.h"
+#include "IntelAlgoServer.h"
+#include "IntelCmcServer.h"
+#include "IntelDvsServer.h"
+#include "IntelFDServer.h"
+#include "IntelLardServer.h"
+#include "IntelLtmServer.h"
+#include "IntelMknServer.h"
+#include "IntelPGParamServer.h"
+#include "IspParamAdaptorServer.h"
+#include "cros-camera/camera_algorithm.h"
+#include "iutils/Errors.h"
+#include "iutils/Thread.h"
+#include "modules/sandboxing/IPCCommon.h"
+
+namespace icamera {
+
+class IntelCPUAlgoServer : public RequestHandler {
+ public:
+    explicit IntelCPUAlgoServer(IntelAlgoServer* server);
+    virtual ~IntelCPUAlgoServer() {}
+    void handleRequest(const MsgReq& msg);
+
+ private:
+    IntelLardServer mLard;
+    IntelFDServer mFaceDetection;
+    GraphConfigServer mGraph;
+    IntelCmcServer mCmc;
+    IntelMknServer mMkn;
+    IntelLtmServer mLtm;
+    IntelAiqServer mAiq;
+    IntelDvsServer mDvs;
+    IspParamAdaptorServer mIspAdaptor;
+    IntelPGParamServer mPGParam;
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelCmcServer.cpp b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelCmcServer.cpp
new file mode 100644
index 000000000000..c0d64aa75eda
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelCmcServer.cpp
@@ -0,0 +1,91 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelCmcServer"
+
+#include "modules/sandboxing/server/IntelCmcServer.h"
+
+#include <ia_cmc_parser.h>
+#include <string.h>
+
+#include <utility>
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+IntelCmcServer::IntelCmcServer() {
+    LOGIPC("@%s", __func__);
+}
+
+IntelCmcServer::~IntelCmcServer() {
+    LOGIPC("@%s", __func__);
+
+    mIntelCmcs.clear();
+}
+
+status_t IntelCmcServer::init(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(!pData, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(cmc_init_params), UNKNOWN_ERROR, "@%s, buffer is small", __func__);
+
+    cmc_init_params* params = static_cast<cmc_init_params*>(pData);
+    ia_binary_data aiqbData = {nullptr, 0};
+
+    bool ret = mIpc.serverUnflattenInit(*params, &aiqbData);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenInit fails", __func__);
+
+    std::unique_ptr<IntelCmc> intelCmc = std::make_unique<IntelCmc>();
+
+    ret = intelCmc->init(&aiqbData, nullptr);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, intelCmc->init fails", __func__);
+
+    ia_cmc_t* cmc = intelCmc->getCmc();
+    CheckError(!cmc, UNKNOWN_ERROR, "@%s, cmc is nullptr", __func__);
+    LOGIPC("@%s, cmc:%p", __func__, cmc);
+
+    ret = mIpc.serverFlattenInit(*cmc, params);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenInit fails", __func__);
+
+    mIntelCmcs[cmc] = std::move(intelCmc);
+
+    return OK;
+}
+
+status_t IntelCmcServer::deinit(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(!pData, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(cmc_deinit_params), UNKNOWN_ERROR, "@%s, buffer is small",
+               __func__);
+
+    cmc_deinit_params* params = static_cast<cmc_deinit_params*>(pData);
+
+    ia_cmc_t* cmc = reinterpret_cast<ia_cmc_t*>(params->cmc_handle);
+    LOGIPC("@%s, cmc:%p", __func__, cmc);
+
+    if (mIntelCmcs.find(cmc) == mIntelCmcs.end()) {
+        LOGE("@%s, cmc:%p doesn't exist", __func__, cmc);
+        return UNKNOWN_ERROR;
+    }
+
+    mIntelCmcs[cmc]->deinit();
+
+    mIntelCmcs.erase(cmc);
+
+    return OK;
+}
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelCmcServer.h b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelCmcServer.h
new file mode 100644
index 000000000000..5df41e6c5157
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelCmcServer.h
@@ -0,0 +1,40 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+#include <unordered_map>
+
+#include "modules/algowrapper/IntelCmc.h"
+#include "modules/sandboxing/IPCCommon.h"
+#include "modules/sandboxing/IPCIntelCmc.h"
+
+namespace icamera {
+class IntelCmcServer {
+ public:
+    IntelCmcServer();
+    virtual ~IntelCmcServer();
+
+    status_t init(void* pData, int dataSize);
+    status_t deinit(void* pData, int dataSize);
+
+ private:
+    std::unordered_map<ia_cmc_t*, std::unique_ptr<IntelCmc>> mIntelCmcs;
+    IPCIntelCmc mIpc;
+};
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelDvsServer.cpp b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelDvsServer.cpp
new file mode 100644
index 000000000000..a32b5dd86bea
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelDvsServer.cpp
@@ -0,0 +1,312 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelDvsServer"
+
+#include "modules/sandboxing/server/IntelDvsServer.h"
+
+#include <utility>
+
+#include "CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelDvsServer::IntelDvsServer() {
+    LOGIPC("@%s", __func__);
+}
+
+IntelDvsServer::~IntelDvsServer() {
+    LOGIPC("@%s", __func__);
+
+    mIntelDvss.clear();
+}
+
+status_t IntelDvsServer::init(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_binary_data* dvsDataPtr = nullptr;
+    ia_cmc_t* cmc = nullptr;
+    bool ret = mIpc.serverUnflattenInit(pData, dataSize, &dvsDataPtr, &cmc);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenInit fails", __func__);
+
+    std::unique_ptr<IntelDvs> intelDvs = std::make_unique<IntelDvs>();
+
+    ia_dvs_state* dvs = nullptr;
+    ia_err err = intelDvs->init(*dvsDataPtr, cmc, &dvs);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, intelDvs->init fails", __func__);
+
+    ret = mIpc.serverFlattenInit(pData, dataSize, dvs);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverflattenInit fails", __func__);
+
+    mIntelDvss[dvs] = std::move(intelDvs);
+
+    return OK;
+}
+
+status_t IntelDvsServer::deinit(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_dvs_state* dvs = nullptr;
+    bool ret = mIpc.serverUnflattenDeinit(pData, dataSize, &dvs);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenDeinit fails", __func__);
+
+    IntelDvs* intelDvs = getIntelDvs(dvs);
+    CheckError(intelDvs == nullptr, UNKNOWN_ERROR, "@%s, dvs:%p doesn't exist", __func__, dvs);
+
+    intelDvs->deinit(dvs);
+    return OK;
+}
+
+status_t IntelDvsServer::config(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_dvs_state* dvs = nullptr;
+    ia_dvs_configuration* config = nullptr;
+    float zoomRatio = 0;
+    bool ret = mIpc.serverUnflattenConfig(pData, dataSize, &dvs, &config, &zoomRatio);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenConfig fails", __func__);
+
+    IntelDvs* intelDvs = getIntelDvs(dvs);
+    CheckError(intelDvs == nullptr, UNKNOWN_ERROR, "@%s, dvs:%p doesn't exist", __func__, dvs);
+
+    ia_err err = intelDvs->config(dvs, config, zoomRatio);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, config fails", __func__);
+    return OK;
+}
+
+status_t IntelDvsServer::setNonBlankRatio(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_dvs_state* dvs = nullptr;
+    float nonBlankingRatio = 0;
+    bool ret = mIpc.serverUnflattenNoneBlanckRation(pData, dataSize, &dvs, &nonBlankingRatio);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenNoneBlanckRation fails", __func__);
+
+    IntelDvs* intelDvs = getIntelDvs(dvs);
+    CheckError(intelDvs == nullptr, UNKNOWN_ERROR, "@%s, dvs:%p doesn't exist", __func__, dvs);
+
+    ia_err err = intelDvs->setNonBlankRatio(dvs, nonBlankingRatio);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, setNonBlankRatio fails", __func__);
+    return OK;
+}
+
+status_t IntelDvsServer::setDigitalZoomMode(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_dvs_state* dvs = nullptr;
+    ia_dvs_zoom_mode zoomMode = ia_dvs_zoom_mode_center;
+    bool ret = mIpc.serverUnflattenDigitalZoomMode(pData, dataSize, &dvs, &zoomMode);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenDigitalZoomMode fails", __func__);
+
+    IntelDvs* intelDvs = getIntelDvs(dvs);
+    CheckError(intelDvs == nullptr, UNKNOWN_ERROR, "@%s, dvs:%p doesn't exist", __func__, dvs);
+
+    ia_err err = intelDvs->setDigitalZoomMode(dvs, zoomMode);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, setDigitalZoomMode fails", __func__);
+    return OK;
+}
+
+status_t IntelDvsServer::setDigitalZoomRegion(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_dvs_state* dvs = nullptr;
+    ia_rectangle* zoomRegion = nullptr;
+    bool ret = mIpc.serverUnflattenDigitalZoomRegion(pData, dataSize, &dvs, &zoomRegion);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenDigitalZoomRegion fails",
+               __func__);
+
+    IntelDvs* intelDvs = getIntelDvs(dvs);
+    CheckError(intelDvs == nullptr, UNKNOWN_ERROR, "@%s, dvs:%p doesn't exist", __func__, dvs);
+
+    ia_err err = intelDvs->setDigitalZoomRegion(dvs, zoomRegion);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, setDigitalZoomRegion fails", __func__);
+    return OK;
+}
+
+status_t IntelDvsServer::setDigitalZoomCoordinate(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_dvs_state* dvs = nullptr;
+    ia_coordinate* zoomCoordinate = nullptr;
+    bool ret = mIpc.serverUnflattenDigitalZoomCoordinate(pData, dataSize, &dvs, &zoomCoordinate);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenDigitalZoomCoordinate fails",
+               __func__);
+
+    IntelDvs* intelDvs = getIntelDvs(dvs);
+    CheckError(intelDvs == nullptr, UNKNOWN_ERROR, "@%s, dvs:%p doesn't exist", __func__, dvs);
+
+    ia_err err = intelDvs->setDigitalZoomCoordinate(dvs, zoomCoordinate);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, setDigitalZoomCoordinate fails", __func__);
+    return OK;
+}
+
+status_t IntelDvsServer::setDigitalZoomMagnitude(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_dvs_state* dvs = nullptr;
+    float zoomRatio = 0;
+    bool ret = mIpc.serverUnflattenDigitalZoomMagnitude(pData, dataSize, &dvs, &zoomRatio);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenDigitalZoomMagnitude fails",
+               __func__);
+
+    IntelDvs* intelDvs = getIntelDvs(dvs);
+    CheckError(intelDvs == nullptr, UNKNOWN_ERROR, "@%s, dvs:%p doesn't exist", __func__, dvs);
+
+    ia_err err = intelDvs->setDigitalZoomMagnitude(dvs, zoomRatio);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, setDigitalZoomMagnitude fails", __func__);
+    return OK;
+}
+
+status_t IntelDvsServer::freeMorphTable(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_dvs_morph_table* morph = nullptr;
+    ia_dvs_state* dvs = nullptr;
+    bool ret = mIpc.serverUnflattenFreeMorphTable(pData, dataSize, &dvs, &morph);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenFreeMorphTable fails", __func__);
+
+    IntelDvs* intelDvs = getIntelDvs(dvs);
+    CheckError(intelDvs == nullptr, UNKNOWN_ERROR, "@%s, dvs:%p doesn't exist", __func__, dvs);
+
+    intelDvs->freeMorphTable(dvs, morph);
+    return OK;
+}
+
+status_t IntelDvsServer::allocateMorphTalbe(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_dvs_state* dvs = nullptr;
+    bool ret = mIpc.serverUnflattenAllocateMorphTalbe(pData, dataSize, &dvs);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenAllocateMorphTalbe fails",
+               __func__);
+
+    IntelDvs* intelDvs = getIntelDvs(dvs);
+    CheckError(intelDvs == nullptr, UNKNOWN_ERROR, "@%s, dvs:%p doesn't exist", __func__, dvs);
+
+    ia_dvs_morph_table* morph = intelDvs->allocateMorphTalbe(dvs);
+
+    ret = mIpc.serverFlattenAllocateMorphTalbe(pData, dataSize, morph);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenAllocateMorphTalbe fails", __func__);
+
+    return OK;
+}
+
+status_t IntelDvsServer::getMorphTalbe(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_dvs_state* dvs = nullptr;
+    ia_dvs_morph_table* morph = nullptr;
+    bool ret = mIpc.serverUnflattenGetMorphTalbe(pData, dataSize, &dvs, &morph);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenGetMorphTalbe fails", __func__);
+
+    IntelDvs* intelDvs = getIntelDvs(dvs);
+    CheckError(intelDvs == nullptr, UNKNOWN_ERROR, "@%s, dvs:%p doesn't exist", __func__, dvs);
+
+    int err = intelDvs->getMorphTable(dvs, morph);
+    CheckError(err != OK, UNKNOWN_ERROR, "@%s, getMorphTalbe fails", __func__);
+
+    ret = mIpc.serverFlattenGetMorphTalbe(pData, dataSize, morph);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenGetMorphTalbe fails", __func__);
+
+    return OK;
+}
+
+status_t IntelDvsServer::setStatistics(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_dvs_state* dvs = nullptr;
+    ia_dvs_statistics* statistics = nullptr;
+    ia_aiq_ae_results* aeResults = nullptr;
+    ia_aiq_af_results* afResults = nullptr;
+    ia_aiq_sensor_events* sensorEvents = nullptr;
+    uint64_t frameReadoutStart = 0;
+    uint64_t frameReadoutEnd = 0;
+    bool ret = mIpc.serverUnflattenSetStatistics(pData, dataSize, &dvs, &statistics, &aeResults,
+                                                 &afResults, &sensorEvents, &frameReadoutStart,
+                                                 &frameReadoutEnd);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenSetStatistics fails", __func__);
+
+    IntelDvs* intelDvs = getIntelDvs(dvs);
+    CheckError(intelDvs == nullptr, UNKNOWN_ERROR, "@%s, dvs:%p doesn't exist", __func__, dvs);
+
+    ia_err err = intelDvs->setStatistics(dvs, statistics, aeResults, afResults, sensorEvents,
+                                         frameReadoutStart, frameReadoutEnd);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, execute fails", __func__);
+    return OK;
+}
+
+status_t IntelDvsServer::execute(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_dvs_state* dvs = nullptr;
+    uint16_t focusPosition = 0;
+    bool ret = mIpc.serverUnflattenExecute(pData, dataSize, &dvs, &focusPosition);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenExecute fails", __func__);
+
+    IntelDvs* intelDvs = getIntelDvs(dvs);
+    CheckError(intelDvs == nullptr, UNKNOWN_ERROR, "@%s, dvs:%p doesn't exist", __func__, dvs);
+
+    ia_err err = intelDvs->execute(dvs, focusPosition);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, execute fails", __func__);
+    return OK;
+}
+
+status_t IntelDvsServer::getImageTransformation(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_dvs_state* dvs = nullptr;
+    ia_dvs_image_transformation imageTransformation = {0};
+    bool ret = mIpc.serverUnflattenImageTransformation(pData, dataSize, &dvs);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenImageTransformation fails",
+               __func__);
+
+    IntelDvs* intelDvs = getIntelDvs(dvs);
+    CheckError(intelDvs == nullptr, UNKNOWN_ERROR, "@%s, dvs:%p doesn't exist", __func__, dvs);
+
+    ia_err err = intelDvs->getImageTransformation(dvs, &imageTransformation);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "@%s, getImageTransformation fails", __func__);
+
+    ret = mIpc.serverFlattenImageTransformation(pData, dataSize, &imageTransformation);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenImageTransformation fails",
+               __func__);
+    return OK;
+}
+
+IntelDvs* IntelDvsServer::getIntelDvs(ia_dvs_state* dvs) {
+    LOGIPC("@%s, dvs:%p", __func__, dvs);
+
+    if (mIntelDvss.find(dvs) == mIntelDvss.end()) {
+        LOGE("@%s, dvs:%p doesn't exist", __func__, dvs);
+        return nullptr;
+    }
+
+    return mIntelDvss[dvs].get();
+}
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelDvsServer.h b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelDvsServer.h
new file mode 100644
index 000000000000..4016cc5f8b0b
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelDvsServer.h
@@ -0,0 +1,54 @@
+/*
+ * Copyright (C) 2019 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+#include <unordered_map>
+
+#include "iutils/Errors.h"
+#include "modules/algowrapper/IntelDvs.h"
+#include "modules/sandboxing/IPCIntelDvs.h"
+
+namespace icamera {
+class IntelDvsServer {
+ public:
+    IntelDvsServer();
+    virtual ~IntelDvsServer();
+
+    status_t init(void* pData, int dataSize);
+    status_t deinit(void* pData, int dataSize);
+    status_t config(void* pData, int dataSize);
+    status_t setNonBlankRatio(void* pData, int dataSize);
+    status_t setDigitalZoomMode(void* pData, int dataSize);
+    status_t setDigitalZoomRegion(void* pData, int dataSize);
+    status_t setDigitalZoomCoordinate(void* pData, int dataSize);
+    status_t setDigitalZoomMagnitude(void* pData, int dataSize);
+    status_t freeMorphTable(void* pData, int dataSize);
+    status_t allocateMorphTalbe(void* pData, int dataSize);
+    status_t getMorphTalbe(void* pData, int dataSize);
+    status_t setStatistics(void* pData, int dataSize);
+    status_t execute(void* pData, int dataSize);
+    status_t getImageTransformation(void* pData, int dataSize);
+
+ private:
+    IntelDvs* getIntelDvs(ia_dvs_state* dvs);
+
+ private:
+    std::unordered_map<ia_dvs_state*, std::unique_ptr<IntelDvs>> mIntelDvss;
+    IPCIntelDvs mIpc;
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelFDServer.cpp b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelFDServer.cpp
new file mode 100644
index 000000000000..73da85c48293
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelFDServer.cpp
@@ -0,0 +1,63 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelFDServer"
+
+#include "modules/sandboxing/server/IntelFDServer.h"
+
+#include <pvl_types.h>
+
+#include "CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelFDServer::IntelFDServer() {
+    mFaceDetection = std::unique_ptr<IntelFaceDetection>(new IntelFaceDetection());
+
+    LOG1("@%s", __func__);
+}
+
+IntelFDServer::~IntelFDServer() {
+    LOG1("@%s", __func__);
+}
+
+status_t IntelFDServer::init(void* pData, int dataSize) {
+    LOG1("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < static_cast<int>(sizeof(FaceDetectionInitParams)), UNKNOWN_ERROR,
+               "buffer is small");
+
+    return mFaceDetection->init(static_cast<FaceDetectionInitParams*>(pData), dataSize);
+}
+
+status_t IntelFDServer::run(void* pData, int dataSize, void* imageData) {
+    LOG1("@%s, pData:%p, dataSize:%d, imageData:%p", __func__, pData, dataSize, imageData);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < static_cast<int>(sizeof(FaceDetectionRunParams)), UNKNOWN_ERROR,
+               "buffer is small");
+
+    pvl_image image;
+    FaceDetectionRunParams* pFdRunParams = static_cast<FaceDetectionRunParams*>(pData);
+    mIpcFD.serverUnflattenRun(*pFdRunParams, imageData, &image);
+
+    return mFaceDetection->run(&image, &pFdRunParams->results);
+}
+
+status_t IntelFDServer::deinit() {
+    LOG1("@%s", __func__);
+    return mFaceDetection->deinit();
+}
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelFDServer.h b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelFDServer.h
new file mode 100644
index 000000000000..053f5bbbefc3
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelFDServer.h
@@ -0,0 +1,39 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+
+#include "iutils/Errors.h"
+#include "modules/algowrapper/IntelFaceDetection.h"
+#include "modules/sandboxing/IPCIntelFD.h"
+
+namespace icamera {
+class IntelFDServer {
+ public:
+    IntelFDServer();
+    virtual ~IntelFDServer();
+
+    status_t init(void* pData, int dataSize);
+    status_t run(void* pData, int dataSize, void* imageData);
+    status_t deinit();
+
+ private:
+    std::unique_ptr<IntelFaceDetection> mFaceDetection;
+    IPCIntelFD mIpcFD;
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelGPUAlgoServer.cpp b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelGPUAlgoServer.cpp
new file mode 100644
index 000000000000..c665f5e522c9
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelGPUAlgoServer.cpp
@@ -0,0 +1,110 @@
+/*
+ * Copyright (C) 2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelGPUAlgoServer"
+
+#include "modules/sandboxing/server/IntelGPUAlgoServer.h"
+
+#include <base/logging.h>
+#include <ia_log.h>
+#include <stdlib.h>
+#include <sys/mman.h>
+#include <sys/stat.h>
+
+#include <memory>
+#include <string>
+
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+IntelGPUAlgoServer::IntelGPUAlgoServer(IntelAlgoServer* server) : RequestHandler(server) {
+    LOGIPC("@%s", __func__);
+}
+
+void IntelGPUAlgoServer::handleRequest(const MsgReq& msg) {
+    uint32_t req_id = msg.req_id;
+    int32_t buffer_handle = msg.buffer_handle;
+
+    ShmInfo info = {};
+    status_t status = getIntelAlgoServer()->getShmInfo(buffer_handle, &info);
+    if (status != OK) {
+        LOGE("@%s, Invalid buffer handle", __func__);
+        getIntelAlgoServer()->returnCallback(req_id, UNKNOWN_ERROR, buffer_handle);
+        return;
+    }
+
+    size_t requestSize = info.size;
+    void* addr = info.addr;
+    LOGIPC("@%s, req_id:%d:%s, requestSize:%zu, buffer_handle:%d addr:%p", __func__, req_id,
+           IntelAlgoIpcCmdToString(static_cast<IPC_CMD>(req_id)), requestSize, buffer_handle, addr);
+    switch (req_id) {
+#ifdef TNR7_CM
+        case IPC_GPU_TNR_INIT:
+            status = mTNR.init(addr, requestSize);
+            break;
+        case IPC_GPU_TNR_PREPARE_SURFACE:
+            status = mTNR.prepareSurface(addr, requestSize);
+            break;
+        case IPC_GPU_TNR_RUN_FRAME: {
+            TnrRunInfo* runInfo = static_cast<TnrRunInfo*>(addr);
+            ShmInfo inBuffer = {};
+            ShmInfo outBuffer = {};
+            ShmInfo paramBuffer = {};
+            if (runInfo->inHandle >= 0) {
+                status = getIntelAlgoServer()->getShmInfo(runInfo->inHandle, &inBuffer);
+                if (status != OK) {
+                    LOGE("%s, the buffer handle for inBuffer data is invalid", __func__);
+                    break;
+                }
+            }
+            if (runInfo->outHandle >= 0) {
+                status = getIntelAlgoServer()->getShmInfo(runInfo->outHandle, &outBuffer);
+                if (status != OK) {
+                    LOGE("%s, the buffer handle for inBuffer data is invalid", __func__);
+                    break;
+                }
+            }
+            if (runInfo->paramHandle >= 0) {
+                status = getIntelAlgoServer()->getShmInfo(runInfo->paramHandle, &paramBuffer);
+                if (status != OK) {
+                    LOGE("%s, the buffer handle for inBuffer data is invalid", __func__);
+                    break;
+                }
+            }
+
+            status = mTNR.runTnrFrame(inBuffer.addr, outBuffer.addr, inBuffer.size, outBuffer.size,
+                                      paramBuffer.addr, false);
+            break;
+        }
+        case IPC_GPU_TNR_PARAM_UPDATE: {
+            TnrRunInfo* runInfo = static_cast<TnrRunInfo*>(addr);
+            status = mTNR.asyncParamUpdate(runInfo->gain);
+            break;
+        }
+        case IPC_GPU_TNR_DEINIT:
+            status = mTNR.deInit();
+            break;
+#endif
+        default:
+            LOGE("@%s, req_id:%d is not defined", __func__, req_id);
+            status = UNKNOWN_ERROR;
+            break;
+    }
+
+    getIntelAlgoServer()->returnCallback(req_id, status, buffer_handle);
+}
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelGPUAlgoServer.h b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelGPUAlgoServer.h
new file mode 100644
index 000000000000..6447306cdcd3
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelGPUAlgoServer.h
@@ -0,0 +1,50 @@
+/*
+ * Copyright (C) 2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <base/bind.h>
+#include <base/threading/thread.h>
+
+#include <memory>
+#include <queue>
+#include <unordered_map>
+
+#include "CameraLog.h"
+#include "IntelAlgoServer.h"
+#include "cros-camera/camera_algorithm.h"
+#include "iutils/Errors.h"
+#include "iutils/Thread.h"
+#include "modules/sandboxing/IPCCommon.h"
+#ifdef TNR7_CM
+#include "modules/sandboxing/server/IntelTNRServer.h"
+#endif
+
+namespace icamera {
+
+class IntelGPUAlgoServer : public RequestHandler {
+ public:
+    explicit IntelGPUAlgoServer(IntelAlgoServer* server);
+    virtual ~IntelGPUAlgoServer() {}
+    void handleRequest(const MsgReq& msg);
+
+#ifdef TNR7_CM
+ private:
+    IntelTNRServer mTNR;
+#endif
+};
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelLardServer.cpp b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelLardServer.cpp
new file mode 100644
index 000000000000..f462795152e5
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelLardServer.cpp
@@ -0,0 +1,112 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelLardServer"
+
+#include "modules/sandboxing/server/IntelLardServer.h"
+
+#include <memory>
+
+#include "CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelLardServer::IntelLardServer() {
+    mLard = std::unique_ptr<IntelLard>(new IntelLard());
+    LOGIPC("@%s", __func__);
+}
+
+IntelLardServer::~IntelLardServer() {
+    LOGIPC("@%s", __func__);
+}
+
+status_t IntelLardServer::init(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_binary_data binaryData = {nullptr, 0};
+    bool ret = mIpc.serverUnflattenInit(pData, dataSize, &binaryData);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenInit fails", __func__);
+
+    ia_lard* lard = mLard->init(&binaryData);
+    CheckError(lard == nullptr, UNKNOWN_ERROR, "@%s, mLard.init fails", __func__);
+
+    ret = mIpc.serverFlattenInit(pData, dataSize, lard);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverflattenInit fails", __func__);
+
+    return OK;
+}
+
+status_t IntelLardServer::getTagList(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_lard* lard = nullptr;
+    unsigned int mode_tag = LCMC_TAG;
+
+    bool ret = mIpc.serverUnflattenGetTagList(pData, dataSize, &lard, &mode_tag);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenGetTagList fails", __func__);
+    CheckError(lard == nullptr, UNKNOWN_ERROR, "@%s, serverUnflattenGetTagList fails", __func__);
+
+    unsigned int num_tags = 0;
+    const unsigned int* tags = nullptr;
+
+    ia_err ret1 = mLard->getTagList(lard, mode_tag, &num_tags, &tags);
+    CheckError(ret1 != ia_err_none, UNKNOWN_ERROR, "@%s, mLard.getTagList fails", __func__);
+
+    mIpc.serverFlattenGetTagList(pData, dataSize, num_tags, tags);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenGetTagList fails", __func__);
+    CheckError(lard == nullptr, UNKNOWN_ERROR, "@%s, serverFlattenGetTagList fails", __func__);
+
+    return OK;
+}
+
+status_t IntelLardServer::run(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_lard* lard = nullptr;
+    ia_lard_input_params* inputParams = nullptr;
+
+    bool ret = mIpc.serverUnflattenRun(pData, dataSize, &lard, &inputParams);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenRun fails", __func__);
+    CheckError(lard == nullptr, UNKNOWN_ERROR, "@%s, serverUnflattenRun fails", __func__);
+    CheckError(inputParams == nullptr, UNKNOWN_ERROR, "@%s, serverUnflattenRun fails", __func__);
+
+    ia_lard_results* result = nullptr;
+    ia_err ret1 = mLard->run(lard, inputParams, &result);
+    CheckError(ret1 != ia_err_none, UNKNOWN_ERROR, "@%s, mLard.run fails", __func__);
+
+    ret = mIpc.serverFlattenRun(pData, dataSize, result);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenRun fails", __func__);
+
+    return OK;
+}
+
+status_t IntelLardServer::deinit(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+
+    ia_lard* lard = nullptr;
+    bool ret = mIpc.serverUnflattenDeinit(pData, dataSize, &lard);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenDeinit fails", __func__);
+    CheckError(lard == nullptr, UNKNOWN_ERROR, "@%s, serverUnflattenDeinit fails", __func__);
+
+    mLard->deinit(lard);
+
+    return OK;
+}
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelLardServer.h b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelLardServer.h
new file mode 100644
index 000000000000..aca05cf9c241
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelLardServer.h
@@ -0,0 +1,39 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "iutils/Errors.h"
+#include "memory"
+#include "modules/algowrapper/IntelLard.h"
+#include "modules/sandboxing/IPCIntelLard.h"
+
+namespace icamera {
+class IntelLardServer {
+ public:
+    IntelLardServer();
+    virtual ~IntelLardServer();
+
+    status_t init(void* pData, int dataSize);
+    status_t getTagList(void* pData, int dataSize);
+    status_t run(void* pData, int dataSize);
+    status_t deinit(void* pData, int dataSize);
+
+ private:
+    std::unique_ptr<IntelLard> mLard;
+    IPCIntelLard mIpc;
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelLtmServer.cpp b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelLtmServer.cpp
new file mode 100644
index 000000000000..31310e12b69c
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelLtmServer.cpp
@@ -0,0 +1,105 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelLtmServer"
+
+#include "modules/sandboxing/server/IntelLtmServer.h"
+
+#include <utility>
+
+#include "CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelLtmServer::IntelLtmServer() {
+    LOGIPC("@%s", __func__);
+}
+
+IntelLtmServer::~IntelLtmServer() {
+    LOGIPC("@%s", __func__);
+}
+
+int IntelLtmServer::init(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(!pData, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(LtmInitParams), UNKNOWN_ERROR, "@%s, buffer is small", __func__);
+
+    LtmInitParams* params = static_cast<LtmInitParams*>(pData);
+    ia_binary_data inData;
+    uintptr_t mkn_hanlde;
+    bool ret = mIpc.serverUnflattenInit(params, &inData, &mkn_hanlde);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenInit fails", __func__);
+
+    std::unique_ptr<IntelLtm> intelLtm = std::unique_ptr<IntelLtm>(new IntelLtm());
+    ia_mkn* mkn = reinterpret_cast<ia_mkn*>(mkn_hanlde);
+    ia_ltm* ltm = intelLtm->init(&inData, mkn);
+
+    ret = mIpc.serverFlattenInit(params, ltm);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenInit fails", __func__);
+
+    mIntelLtms[ltm] = std::move(intelLtm);
+
+    return OK;
+}
+
+int IntelLtmServer::deinit(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(!pData, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(LtmDeinitParams), UNKNOWN_ERROR, "@%s, buffer is small", __func__);
+
+    LtmDeinitParams* params = static_cast<LtmDeinitParams*>(pData);
+    ia_ltm* ltm = reinterpret_cast<ia_ltm*>(params->ltm_handle);
+    LOGIPC("@%s, params->ltm_handle:%p", __func__, ltm);
+
+    if (mIntelLtms.find(ltm) == mIntelLtms.end()) {
+        LOGE("@%s, ltm:%p doesn't exist", __func__, ltm);
+        return UNKNOWN_ERROR;
+    }
+
+    mIntelLtms[ltm]->deinit(ltm);
+    mIntelLtms.erase(ltm);
+
+    return OK;
+}
+
+int IntelLtmServer::run(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(!pData, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(LtmRunParams), UNKNOWN_ERROR, "@%s, buffer is small", __func__);
+
+    ia_ltm_input_params* inputParams = nullptr;
+    ia_ltm* ltm = nullptr;
+    bool ret = mIpc.serverUnflattenRun(pData, &ltm, &inputParams);
+    CheckError(!ret, UNKNOWN_ERROR, "@%s, serverUnflattenRun fails", __func__);
+
+    ia_ltm_results* ltmResults = nullptr;
+    ia_ltm_drc_params* drcResults = nullptr;
+
+    if (mIntelLtms.find(ltm) == mIntelLtms.end()) {
+        LOGE("@%s, ltm:%p doesn't exist", __func__, ltm);
+        return UNKNOWN_ERROR;
+    }
+
+    ia_err retErr = mIntelLtms[ltm]->run(ltm, inputParams, &ltmResults, &drcResults);
+    CheckError(retErr != ia_err_none, UNKNOWN_ERROR, "@%s, mIntelLtms->run fails", __func__);
+
+    LtmRunParams* params = static_cast<LtmRunParams*>(pData);
+    ret = mIpc.serverFlattenRun(*ltmResults, *drcResults, ltm, params);
+    CheckError(!ret, UNKNOWN_ERROR, "@%s, serverFlattenRun fails", __func__);
+
+    return OK;
+}
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelLtmServer.h b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelLtmServer.h
new file mode 100644
index 000000000000..65a7e99b2576
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelLtmServer.h
@@ -0,0 +1,40 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+#include <unordered_map>
+
+#include "iutils/Errors.h"
+#include "modules/algowrapper/IntelLtm.h"
+#include "modules/sandboxing/IPCIntelLtm.h"
+
+namespace icamera {
+class IntelLtmServer {
+ public:
+    IntelLtmServer();
+    virtual ~IntelLtmServer();
+
+    int init(void* pData, int dataSize);
+    int deinit(void* pData, int dataSize);
+    int run(void* pData, int dataSize);
+
+ private:
+    std::unordered_map<ia_ltm*, std::unique_ptr<IntelLtm>> mIntelLtms;
+    IPCIntelLtm mIpc;
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelMknServer.cpp b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelMknServer.cpp
new file mode 100644
index 000000000000..f34cb60b9c81
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelMknServer.cpp
@@ -0,0 +1,124 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelMknServer"
+
+#include "modules/sandboxing/server/IntelMknServer.h"
+
+#include <utility>
+
+#include "CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelMknServer::IntelMknServer() {
+    LOGIPC("@%s", __func__);
+}
+
+IntelMknServer::~IntelMknServer() {
+    LOGIPC("@%s", __func__);
+
+    mIntelMkns.clear();
+}
+
+int IntelMknServer::init(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(MknInitParams), UNKNOWN_ERROR, "@%s, buffer is small", __func__);
+
+    MknInitParams* params = static_cast<MknInitParams*>(pData);
+
+    std::unique_ptr<IntelMkn> intelMnk = std::make_unique<IntelMkn>();
+    ia_mkn* mkn = intelMnk->init(params->mkn_config_bits, params->mkn_section_1_size,
+                                 params->mkn_section_2_size);
+    CheckError(mkn == nullptr, UNKNOWN_ERROR, "@%s, mkn.init fails", __func__);
+
+    params->results = reinterpret_cast<uintptr_t>(mkn);
+    LOGIPC("@%s, mkn:%p, params->results:%", __func__, mkn, params->results);
+
+    mIntelMkns[mkn] = std::move(intelMnk);
+
+    return OK;
+}
+
+int IntelMknServer::deinit(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(MknDeinitParams), UNKNOWN_ERROR, "@%s, buffer is small", __func__);
+
+    MknDeinitParams* params = static_cast<MknDeinitParams*>(pData);
+    ia_mkn* mkn = reinterpret_cast<ia_mkn*>(params->mkn_handle);
+    LOGIPC("@%s, params->mkn_handle:%p", __func__, mkn);
+
+    IntelMkn* intelMnk = getIntelMkn(mkn);
+    CheckError(intelMnk == nullptr, UNKNOWN_ERROR, "@%s, mkn:%p doesn't exist", __func__, mkn);
+
+    intelMnk->deinit(mkn);
+    mIntelMkns.erase(mkn);
+
+    return OK;
+}
+
+int IntelMknServer::prepare(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(MknPrepareParams), UNKNOWN_ERROR, "@%s, buffer is small",
+               __func__);
+
+    MknPrepareParams* params = static_cast<MknPrepareParams*>(pData);
+
+    ia_mkn* mkn = reinterpret_cast<ia_mkn*>(params->mkn_handle);
+    ia_binary_data data;
+
+    IntelMkn* intelMnk = getIntelMkn(mkn);
+    CheckError(intelMnk == nullptr, UNKNOWN_ERROR, "@%s, mkn:%p doesn't exist", __func__, mkn);
+
+    int ret = intelMnk->prepare(mkn, params->data_target, &data);
+    CheckError(ret != OK, NO_MEMORY, "Failed to prepare makernote");
+    LOGIPC("@%s, data.size:%d, data.data:%p", __func__, data.size, data.data);
+
+    bool retFlag = mIpc.serverFlattenPrepare(data, params);
+    CheckError(retFlag == false, UNKNOWN_ERROR, "@%s, serverFlattenPrepare fails", __func__);
+
+    return OK;
+}
+
+int IntelMknServer::enable(void* pData, int dataSize) {
+    LOGIPC("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < sizeof(MknEnableParams), UNKNOWN_ERROR, "@%s, buffer is small", __func__);
+
+    MknEnableParams* params = static_cast<MknEnableParams*>(pData);
+    ia_mkn* mkn = reinterpret_cast<ia_mkn*>(params->mkn_handle);
+
+    IntelMkn* intelMnk = getIntelMkn(mkn);
+    CheckError(intelMnk == nullptr, UNKNOWN_ERROR, "@%s, mkn:%p doesn't exist", __func__, mkn);
+
+    return intelMnk->enable(mkn, params->enable_data_collection);
+}
+
+IntelMkn* IntelMknServer::getIntelMkn(ia_mkn* mkn) {
+    LOGIPC("@%s, mkn:%p", __func__, mkn);
+
+    if (mIntelMkns.find(mkn) == mIntelMkns.end()) {
+        LOGE("@%s, mkn:%p doesn't exist", __func__, mkn);
+        return nullptr;
+    }
+
+    return mIntelMkns[mkn].get();
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelMknServer.h b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelMknServer.h
new file mode 100644
index 000000000000..2f2947e34dcb
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelMknServer.h
@@ -0,0 +1,45 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+#include <unordered_map>
+
+#include "iutils/Errors.h"
+#include "modules/algowrapper/IntelMkn.h"
+#include "modules/sandboxing/IPCIntelMkn.h"
+
+namespace icamera {
+class IntelMknServer {
+ public:
+    IntelMknServer();
+    virtual ~IntelMknServer();
+
+    int init(void* pData, int dataSize);
+    int deinit(void* pData, int dataSize);
+
+    int enable(void* pData, int dataSize);
+    int prepare(void* pData, int dataSize);
+
+ private:
+    IntelMkn* getIntelMkn(ia_mkn* mkn);
+
+ private:
+    std::unordered_map<ia_mkn*, std::unique_ptr<IntelMkn>> mIntelMkns;
+    IPCIntelMkn mIpc;
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelPGParamServer.cpp b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelPGParamServer.cpp
new file mode 100644
index 000000000000..5c1f3620d58e
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelPGParamServer.cpp
@@ -0,0 +1,229 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelPGParamS"
+
+#include "modules/sandboxing/server/IntelPGParamServer.h"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+IntelPGParamServer::IntelPGParamServer() {}
+
+IntelPGParamServer::~IntelPGParamServer() {}
+
+int IntelPGParamServer::init(void* pData, int dataSize) {
+    int pgId = 0;
+    uintptr_t client = 0;
+    ia_p2p_platform_t platform = IA_P2P_PLATFORM_IPU6;
+    PgConfiguration pgConfig;
+
+    bool ret = mIpc.serverUnflattenInit(pData, dataSize, &pgId, &client, &platform, &pgConfig);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenInit fails", __func__);
+
+    PGParamPackage package;
+    package.pgId = pgId;
+    package.mPayloadCount = 0;
+    CLEAR(package.mPayloads);
+    package.mPGBuffer = nullptr;
+    mPGParamPackages[client] = package;
+    mPGParamPackages[client].mPGParamAdapt = std::shared_ptr<IntelPGParam>(new IntelPGParam(pgId));
+    int result = mPGParamPackages[client].mPGParamAdapt->init(platform, pgConfig);
+    CheckError(result != OK, result, "@%s, init fails", __func__);
+
+    return OK;
+}
+
+int IntelPGParamServer::prepare(void* pData, int dataSize, void* palDataAddr) {
+    uintptr_t client = 0;
+    ia_binary_data ipuParameters = {nullptr, 0};
+    ia_css_rbm_t* rbm = nullptr;
+    ia_css_kernel_bitmap_t* bitmap = nullptr;
+    uint32_t* maxStatsSize = nullptr;
+    bool ret = mIpc.serverUnflattenPrepare(pData, dataSize, &client, palDataAddr, &ipuParameters,
+                                           &rbm, &bitmap, &maxStatsSize);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenPrepare fails", __func__);
+
+    CheckError((mPGParamPackages.find(client) == mPGParamPackages.end()), UNKNOWN_ERROR,
+               "%s, the pg doesn't exist in the table", __func__);
+
+    int result =
+        mPGParamPackages[client].mPGParamAdapt->prepare(&ipuParameters, rbm, bitmap, maxStatsSize);
+    CheckError(result != OK, result, "@%s, prepare fails", __func__);
+
+    return OK;
+}
+
+int IntelPGParamServer::allocatePGBuffer(void* pData, int dataSize) {
+    uintptr_t client = 0;
+    int pgSize = 0;
+    bool ret = mIpc.serverUnflattenAllocatePGBuffer(pData, dataSize, &client, &pgSize);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenAllocatePGBuffer fails", __func__);
+
+    CheckError((mPGParamPackages.find(client) == mPGParamPackages.end()), UNKNOWN_ERROR,
+               "%s, the pg doesn't exist in the table", __func__);
+
+    // Get server data pointer of PGBuffer
+    void* pgBuffer = nullptr;
+    ret = mIpc.assignPGBuffer(pData, dataSize, pgSize, &pgBuffer);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, assignPGBuffer fails", __func__);
+
+    mPGParamPackages[client].mPGBuffer = reinterpret_cast<ia_css_process_group_t*>(pgBuffer);
+    return OK;
+}
+
+int IntelPGParamServer::getFragmentDescriptors(void* pData, int dataSize) {
+    uintptr_t client = 0;
+    int descCount = 0;
+    ia_p2p_fragment_desc* descs = nullptr;
+    bool ret = mIpc.serverUnflattenGetFragDescs(pData, dataSize, &client, &descCount, &descs);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenGetFragDescs fails", __func__);
+
+    CheckError((mPGParamPackages.find(client) == mPGParamPackages.end()), UNKNOWN_ERROR,
+               "%s, the pg doesn't exist in the table", __func__);
+
+    int count = mPGParamPackages[client].mPGParamAdapt->getFragmentDescriptors(descCount, descs);
+    CheckError(count <= 0, count, "@%s, getFragmentDescriptors fails", __func__);
+
+    ret = mIpc.serverFlattenGetFragDescs(pData, dataSize, count);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenGetFragDescs fails", __func__);
+
+    return OK;
+}
+
+int IntelPGParamServer::setPGAndPrepareProgram(void* pData, int dataSize) {
+    uintptr_t client = 0;
+    bool ret = mIpc.serverUnflattenPrepareProgram(pData, dataSize, &client);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenPrepareProgram fails", __func__);
+
+    CheckError((mPGParamPackages.find(client) == mPGParamPackages.end()), UNKNOWN_ERROR,
+               "%s, the pg doesn't exist in the table", __func__);
+    PGParamPackage& package = mPGParamPackages[client];
+
+    int result = package.mPGParamAdapt->setPGAndPrepareProgram(package.mPGBuffer);
+    CheckError(result != OK, result, "@%s, setPGAndPrepareProgram fails", __func__);
+
+    // Get payload size here
+    package.mPayloadCount =
+        package.mPGParamAdapt->getPayloadSizes(ARRAY_SIZE(package.mPayloads), package.mPayloads);
+    CheckError(!package.mPayloadCount, UNKNOWN_ERROR, "@%s, getPayloadSizes fails", __func__);
+
+    ret =
+        mIpc.serverFlattenPrepareProgram(pData, dataSize, package.mPayloadCount, package.mPayloads);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenPrepareProgram fails", __func__);
+
+    return OK;
+}
+
+int IntelPGParamServer::allocatePayloads(void* pData, int dataSize) {
+    uintptr_t client = 0;
+    int clientPayloadCount = 0;
+    ia_binary_data* clientPayloads = nullptr;
+    bool ret = mIpc.serverUnflattenAllocatePayloads(pData, dataSize, &client, &clientPayloadCount,
+                                                    &clientPayloads);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenAllocatePayloads fails", __func__);
+
+    CheckError((mPGParamPackages.find(client) == mPGParamPackages.end()), UNKNOWN_ERROR,
+               "%s, the pg doesn't exist in the table", __func__);
+    PGParamPackage& package = mPGParamPackages[client];
+
+    // Check size here only because shared memory is allocated in client
+    CheckError(clientPayloadCount != package.mPayloadCount, UNKNOWN_ERROR,
+               "@%s, payloadCount errror", __func__);
+    for (int i = 0; i < clientPayloadCount; i++) {
+        CheckError(clientPayloads[i].size != package.mPayloads[i].size, UNKNOWN_ERROR,
+                   "@%s, payload size error for term %d", __func__, i);
+    }
+
+    // Get server data pointer of payloads
+    package.mPayloadMemory.size = dataSize;
+    package.mPayloadMemory.data = pData;
+
+    return OK;
+}
+
+int IntelPGParamServer::updatePALAndEncode(void* pData, int dataSize, void* palDataAddr) {
+    uintptr_t client = 0;
+    ia_binary_data ipuParameters = {nullptr, 0};
+    int32_t* payloadOffsets = nullptr;
+    ia_binary_data* payloads = nullptr;
+    int32_t payloadCount = 0;
+
+    bool ret = mIpc.serverUnflattenEncode(pData, dataSize, &client, palDataAddr, &ipuParameters,
+                                          &payloadCount, &payloads, &payloadOffsets);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenEncode fails", __func__);
+
+    CheckError((mPGParamPackages.find(client) == mPGParamPackages.end()), UNKNOWN_ERROR,
+               "%s, the pg doesn't exist in the table", __func__);
+    PGParamPackage& package = mPGParamPackages[client];
+    CheckError(payloadCount != package.mPayloadCount, UNKNOWN_ERROR, "@%s, wrong payloadCount",
+               __func__);
+    ret = mIpc.getPayloadData(package.mPayloadMemory.data, package.mPayloadMemory.size,
+                              payloadCount, payloadOffsets, payloads);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, getPayloadData fails", __func__);
+
+    int result = package.mPGParamAdapt->updatePALAndEncode(&ipuParameters, payloadCount, payloads);
+    CheckError(result != OK, result, "@%s, updatePALAndEncode fails", __func__);
+
+    return OK;
+}
+
+int IntelPGParamServer::decode(void* pData, int dataSize, void* statsAddr) {
+    uintptr_t client = 0;
+    ia_binary_data statistics = {statsAddr, 0};
+    int32_t* payloadOffsets = nullptr;
+    ia_binary_data* payloads = nullptr;
+    int32_t payloadCount = 0;
+
+    bool ret = mIpc.serverUnflattenDecode(pData, dataSize, &client, &payloadCount, &payloads,
+                                          &payloadOffsets);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenDecode fails", __func__);
+
+    CheckError((mPGParamPackages.find(client) == mPGParamPackages.end()), UNKNOWN_ERROR,
+               "%s, the pg doesn't exist in the table", __func__);
+    PGParamPackage& package = mPGParamPackages[client];
+    CheckError(payloadCount != package.mPayloadCount, UNKNOWN_ERROR, "@%s, wrong payloadCount",
+               __func__);
+    ret = mIpc.getPayloadData(package.mPayloadMemory.data, package.mPayloadMemory.size,
+                              payloadCount, payloadOffsets, payloads);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, getPayloadData fails", __func__);
+
+    int result = package.mPGParamAdapt->decode(payloadCount, payloads, &statistics);
+    CheckError(result != OK, result, "@%s, decode fails", __func__);
+
+    ret = mIpc.serverFlattenDecode(pData, dataSize, statistics);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenDecode fails", __func__);
+
+    return OK;
+}
+
+void IntelPGParamServer::deinit(void* pData, int dataSize) {
+    uintptr_t client = 0;
+
+    bool ret = mIpc.serverUnflattenDeinit(pData, dataSize, &client);
+    CheckError(ret == false, VOID_VALUE, "@%s, serverUnflattenDeinit fails", __func__);
+
+    CheckError((mPGParamPackages.find(client) == mPGParamPackages.end()), VOID_VALUE,
+               "%s, the pg doesn't exist in the table", __func__);
+
+    mPGParamPackages[client].mPGParamAdapt->deinit();
+    mPGParamPackages.erase(client);
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelPGParamServer.h b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelPGParamServer.h
new file mode 100644
index 000000000000..fdd11337a0a2
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelPGParamServer.h
@@ -0,0 +1,56 @@
+/*
+ * Copyright (C) 2019 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+#include <unordered_map>
+
+#include "modules/algowrapper/IntelPGParam.h"
+#include "modules/sandboxing/IPCIntelPGParam.h"
+
+namespace icamera {
+
+class IntelPGParamServer {
+ public:
+    IntelPGParamServer();
+    ~IntelPGParamServer();
+
+    int init(void* pData, int dataSize);
+    int prepare(void* pData, int dataSize, void* palDataAddr);
+    int allocatePGBuffer(void* pData, int dataSize);
+    int getFragmentDescriptors(void* pData, int dataSize);
+    int setPGAndPrepareProgram(void* pData, int dataSize);
+    int allocatePayloads(void* pData, int dataSize);
+    int updatePALAndEncode(void* pData, int dataSize, void* palDataAddr);
+    int decode(void* pData, int dataSize, void* statsAddr);
+    void deinit(void* pData, int dataSize);
+
+ private:
+    struct PGParamPackage {
+        int pgId;
+        std::shared_ptr<IntelPGParam> mPGParamAdapt;
+        ia_binary_data mPayloads[IPU_MAX_TERMINAL_COUNT];
+        int mPayloadCount;
+        ia_binary_data mPayloadMemory;
+        ia_css_process_group_t* mPGBuffer;
+    };
+
+    IPCIntelPGParam mIpc;
+    std::unordered_map<uintptr_t, PGParamPackage> mPGParamPackages;
+};
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelTNRServer.cpp b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelTNRServer.cpp
new file mode 100644
index 000000000000..44ffee804f51
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelTNRServer.cpp
@@ -0,0 +1,71 @@
+/*
+ * Copyright (C) 2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IntelTNRServer"
+
+#include "modules/sandboxing/server/IntelTNRServer.h"
+
+#include <pvl_types.h>
+
+#include "CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+IntelTNRServer::IntelTNRServer() {
+    LOG1("@%s", __func__);
+}
+
+IntelTNRServer::~IntelTNRServer() {
+    LOG1("@%s", __func__);
+}
+
+int IntelTNRServer::init(void* pData, int dataSize) {
+    LOG1("@%s, pData:%p, dataSize:%d", __func__, pData, dataSize);
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(dataSize < static_cast<int>(sizeof(TnrResolution)), UNKNOWN_ERROR,
+               "buffer is small");
+    TnrResolution* res = static_cast<TnrResolution*>(pData);
+    if (!mIntelTNR) {
+        mIntelTNR = std::unique_ptr<IntelTNR7US>(new IntelTNR7US());
+    }
+    return mIntelTNR->init(res->width, res->height);
+}
+
+int IntelTNRServer::deInit() {
+    LOG1("@%s", __func__);
+    mIntelTNR = nullptr;
+    return OK;
+}
+
+int IntelTNRServer::prepareSurface(void* pData, int dataSize) {
+    CheckError(pData == nullptr, UNKNOWN_ERROR, "@%s, pData is nullptr", __func__);
+    CheckError(mIntelTNR == nullptr, UNKNOWN_ERROR, "@%s, invalid IntelTNR object", __func__);
+    return mIntelTNR->prepareSurface(pData, dataSize);
+}
+
+int IntelTNRServer::runTnrFrame(const void* inBufAddr, void* outBufAddr, uint32_t inBufSize,
+                                uint32_t outBufSize, void* tnrParam, bool isUsrPtr) {
+    CheckError(mIntelTNR == nullptr, UNKNOWN_ERROR, "@%s, mIntelTNR is nullptr", __func__);
+    return mIntelTNR->runTnrFrame(inBufAddr, outBufAddr, inBufSize, outBufSize,
+                                  static_cast<Tnr7Param*>(tnrParam), isUsrPtr);
+}
+
+int IntelTNRServer::asyncParamUpdate(int gain) {
+    CheckError(mIntelTNR == nullptr, UNKNOWN_ERROR, "@%s, mIntelTNR is nullptr", __func__);
+    return mIntelTNR->asyncParamUpdate(gain);
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IntelTNRServer.h b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelTNRServer.h
new file mode 100644
index 000000000000..33f129939e9b
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IntelTNRServer.h
@@ -0,0 +1,41 @@
+/*
+ * Copyright (C) 2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+
+#include "iutils/Errors.h"
+#include "modules/algowrapper/IntelTNR7US.h"
+
+namespace icamera {
+
+class IntelTNRServer {
+ public:
+    IntelTNRServer();
+    virtual ~IntelTNRServer();
+
+    int init(void* pData, int dataSize);
+    int deInit();
+    int prepareSurface(void* pData, int dataSize);
+    int runTnrFrame(const void* inBufAddr, void* outBufAddr, uint32_t inBufSize,
+                    uint32_t outBufSize, void* tnrParam, bool isUsrPtr = true);
+    int asyncParamUpdate(int gain);
+
+ private:
+    std::unique_ptr<IntelTNR7US> mIntelTNR;
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IspParamAdaptorServer.cpp b/camera/hal/intel/ipu6/modules/sandboxing/server/IspParamAdaptorServer.cpp
new file mode 100644
index 000000000000..3e69120009e9
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IspParamAdaptorServer.cpp
@@ -0,0 +1,155 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IspParamAdaptorServer"
+
+#include "modules/sandboxing/server/IspParamAdaptorServer.h"
+
+#include <utility>
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+IspParamAdaptorServer::IspParamAdaptorServer() {
+    LOGIPC("@%s", __func__);
+}
+
+IspParamAdaptorServer::~IspParamAdaptorServer() {
+    LOGIPC("@%s", __func__);
+    mIspParamAdaptors.clear();
+}
+
+status_t IspParamAdaptorServer::init(void* pData, int size) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, UNKNOWN_ERROR, "%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(IspBxtInitParam), UNKNOWN_ERROR, "%s, buffer is small", __func__);
+
+    IspBxtInitParam* params = static_cast<IspBxtInitParam*>(pData);
+
+    ia_binary_data* ispData = nullptr;
+    ia_cmc_t* cmcData = nullptr;
+    bool ret = mIpc.serverUnflattenInit(params, size, &ispData, &cmcData);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenInit fails", __func__);
+
+    std::unique_ptr<IntelIspParamAdaptor> adaptor = std::make_unique<IntelIspParamAdaptor>();
+    ia_isp_bxt* ispHandle =
+        adaptor->init(ispData, cmcData, params->maxStatsWidth, params->maxStatsHeight,
+                      params->maxStatsIn, params->iaMkn);
+    CheckError(!ispHandle, UNKNOWN_ERROR, "@%s, init isp param adaptor failed", __func__);
+
+    params->ispRemoteHandle = reinterpret_cast<uintptr_t>(ispHandle);
+    LOGIPC("@%s ispHandle %p: %d", __func__, ispHandle, params->ispRemoteHandle);
+
+    mIspParamAdaptors[ispHandle] = std::move(adaptor);
+
+    return OK;
+}
+
+status_t IspParamAdaptorServer::deInit(void* pData, int size) {
+    LOGIPC("@%s", __func__);
+    CheckError(!pData, UNKNOWN_ERROR, "%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(IspBxtDeInitParam), UNKNOWN_ERROR, "%s, buffer is small", __func__);
+
+    ia_isp_bxt* ispHandle = nullptr;
+    bool ret = mIpc.serverUnflattenDeInit(pData, size, &ispHandle);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenDeInit fails", __func__);
+
+    CheckError((mIspParamAdaptors.find(ispHandle) == mIspParamAdaptors.end()), UNKNOWN_ERROR,
+               "%s, the isp handle doesn't exist in the table", __func__);
+    CheckError(!mIspParamAdaptors[ispHandle], UNKNOWN_ERROR, "%s, IntelIspParamAdaptor is nullptr",
+               __func__);
+
+    mIspParamAdaptors[ispHandle]->deInit(ispHandle);
+    mIspParamAdaptors.erase(ispHandle);
+
+    return OK;
+}
+
+int IspParamAdaptorServer::getPalDataSize(void* pData, int size) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, UNKNOWN_ERROR, "%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(PalDataSizeParam), UNKNOWN_ERROR, "%s, buffer is small", __func__);
+
+    ia_isp_bxt_program_group* programGroup = nullptr;
+    bool ret = mIpc.serverUnflattenGetPalSize(pData, size, &programGroup);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenGetPalSize fails", __func__);
+
+    CheckError(mIspParamAdaptors.empty(), UNKNOWN_ERROR, "%s, mIspParamAdaptors is empty",
+               __func__);
+    int palSize = mIspParamAdaptors.begin()->second->getPalDataSize(programGroup);
+    LOGIPC("%s, The pal data size: %d", __func__, palSize);
+
+    PalDataSizeParam* params = static_cast<PalDataSizeParam*>(pData);
+    params->palDataSize = palSize;
+
+    return OK;
+}
+
+status_t IspParamAdaptorServer::queryAndConvertStats(void* pData, int size, void* statsAddr) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, UNKNOWN_ERROR, "%s, pData is nullptr", __func__);
+    CheckError(size < sizeof(ConvertStatsParam), UNKNOWN_ERROR, "%s, buffer is small", __func__);
+
+    ia_isp_bxt* ispHandle = nullptr;
+    ConvertInputParam inputParams = {};
+    ConvertResult result = {};
+
+    bool ret =
+        mIpc.serverUnflattenConvertStats(pData, size, &ispHandle, &inputParams, &result, statsAddr);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenConvertStats fails", __func__);
+
+    CheckError((mIspParamAdaptors.find(ispHandle) == mIspParamAdaptors.end()), UNKNOWN_ERROR,
+               "%s, the isp handle doesn't exist in the table", __func__);
+    CheckError(!mIspParamAdaptors[ispHandle], UNKNOWN_ERROR, "%s, IntelIspParamAdaptor is nullptr",
+               __func__);
+    int res = mIspParamAdaptors[ispHandle]->queryAndConvertStats(ispHandle, &inputParams, &result);
+    CheckError(res != OK, res, "%s, Failed to convert the status", __func__);
+
+    ret = mIpc.serverFlattenConvertStats(pData, size, result);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverFlattenConvertStats fails", __func__);
+
+    return OK;
+}
+
+status_t IspParamAdaptorServer::runPal(void* pData, int size, void* palDataAddr) {
+    LOGIPC("@%s, pData:%p, size:%d", __func__, pData, size);
+    CheckError(!pData, UNKNOWN_ERROR, "%s, pData is nullptr", __func__);
+    CheckError(!palDataAddr, UNKNOWN_ERROR, "%s, palDataAddr is nullptr", __func__);
+
+    ia_isp_bxt* ispHandle = nullptr;
+    ia_isp_bxt_input_params_v2* inputParams = nullptr;
+    ia_binary_data* palOutput = nullptr;
+
+    bool ret = mIpc.serverUnflattenRunPal(pData, size, &ispHandle, &inputParams, &palOutput);
+    CheckError(ret == false, UNKNOWN_ERROR, "@%s, serverUnflattenRunPal fails", __func__);
+    palOutput->data = palDataAddr;
+    LOGIPC("%s, palDataAddr: %p, size: %d", __func__, palDataAddr, palOutput->size);
+
+    CheckError((mIspParamAdaptors.find(ispHandle) == mIspParamAdaptors.end()), UNKNOWN_ERROR,
+               "%s, the isp handle doesn't exist in the table", __func__);
+    CheckError(!mIspParamAdaptors[ispHandle], UNKNOWN_ERROR, "%s, IntelIspParamAdaptor is nullptr",
+               __func__);
+
+    int res = mIspParamAdaptors[ispHandle]->runPal(ispHandle, inputParams, palOutput);
+    CheckError(res != OK, res, "%s, Failed to run pal", __func__);
+    LOGIPC("%s, the pal data size is: %d after running", __func__, palOutput->size);
+
+    return OK;
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/modules/sandboxing/server/IspParamAdaptorServer.h b/camera/hal/intel/ipu6/modules/sandboxing/server/IspParamAdaptorServer.h
new file mode 100644
index 000000000000..15fc4530ed83
--- /dev/null
+++ b/camera/hal/intel/ipu6/modules/sandboxing/server/IspParamAdaptorServer.h
@@ -0,0 +1,44 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+#include <unordered_map>
+
+#include "iutils/Errors.h"
+#include "modules/algowrapper/IntelIspParamAdaptor.h"
+#include "modules/sandboxing/IPCIspParamAdaptor.h"
+
+namespace icamera {
+
+class IspParamAdaptorServer {
+ public:
+    IspParamAdaptorServer();
+    virtual ~IspParamAdaptorServer();
+
+    status_t init(void* pData, int size);
+    status_t deInit(void* pData, int size);
+    status_t getPalDataSize(void* pData, int size);
+    status_t runPal(void* pData, int size, void* palDataAddr);
+    status_t queryAndConvertStats(void* pData, int size, void* statsAddr);
+
+ private:
+    IPCIspParamAdaptor mIpc;
+    std::unordered_map<ia_isp_bxt*, std::unique_ptr<IntelIspParamAdaptor> > mIspParamAdaptors;
+};
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/3a/AiqCore.cpp b/camera/hal/intel/ipu6/src/3a/AiqCore.cpp
new file mode 100644
index 000000000000..5bc25fd3c4af
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/AiqCore.cpp
@@ -0,0 +1,961 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "AiqCore"
+
+#include <math.h>
+
+#include <memory>
+#include <string>
+
+#include "PlatformData.h"
+#include "AiqUtils.h"
+#include "Parameters.h"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+#include "AiqCore.h"
+
+namespace icamera {
+#define VALID_COLOR_GAINS(colorGains) (colorGains[0] > 0 && colorGains[1] > 0 && \
+                                       colorGains[2] > 0 && colorGains[3] > 0)
+AiqCore::AiqCore(int cameraId) :
+    mCameraId(cameraId),
+    mTimestamp(0),
+    mSensorPixelClock(0.0),
+    mAeForceLock(false),
+    mAwbForceLock(false),
+    mAfForceLock(false),
+    mLastAeResult(nullptr),
+    mLastAwbResult(nullptr),
+    mLastAfResult(nullptr),
+    mAeRunTime(0),
+    mAwbRunTime(0),
+    mAiqState(AIQ_NOT_INIT),
+    mUseManualColorMatrix(false),
+    mHyperFocalDistance(0.0f),
+    mTuningMode(TUNING_MODE_MAX),
+    mShadingMode(SHADING_MODE_FAST),
+    mLensShadingMapMode(LENS_SHADING_MAP_MODE_OFF),
+    mLastEvShift(0.0f) {
+    LOG3A("@%s", __func__);
+
+    mIntel3AParameter = std::unique_ptr<Intel3AParameter>(new Intel3AParameter(cameraId));
+
+    CLEAR(mFrameParams);
+    CLEAR(mGbceParams);
+    CLEAR(mPaParams);
+    CLEAR(mSaParams);
+    CLEAR(mColorMatrix);
+    CLEAR(mColorGains);
+    CLEAR(mIntelAiqHandle);
+    CLEAR(mIntelAiqHandleStatus);
+    CLEAR(mPaColorGains);
+
+    CLEAR(mResizeLscGridR);
+    CLEAR(mResizeLscGridGr);
+    CLEAR(mResizeLscGridGb);
+    CLEAR(mResizeLscGridB);
+
+    CLEAR(mLensShadingMapSize);
+
+    // init LscOffGrid to 1.0f
+    std::fill(std::begin(mLscOffGrid), std::end(mLscOffGrid), 1.0f);
+
+}
+
+AiqCore::~AiqCore() {
+    LOG3A("@%s", __func__);
+
+    for (int i = 0; i < TUNING_MODE_MAX; i++) {
+        if (mIntelAiqHandle[i]) {
+            delete mIntelAiqHandle[i];
+        }
+    }
+}
+
+int AiqCore::initAiqPlusParams() {
+    LOG3A("@%s", __func__);
+
+    CLEAR(mFrameParams);
+    CLEAR(mGbceParams);
+    CLEAR(mPaParams);
+    CLEAR(mPaColorGains);
+    CLEAR(mSaParams);
+    CLEAR(mColorMatrix);
+    CLEAR(mColorGains);
+
+    mUseManualColorMatrix = false;
+
+    mGbceParams.gbce_level = ia_aiq_gbce_level_use_tuning;
+    mGbceParams.frame_use = ia_aiq_frame_use_video;
+    mGbceParams.ev_shift = 0;
+    mGbceParams.tone_map_level = ia_aiq_tone_map_level_use_tuning;
+
+    mPaParams.color_gains = nullptr;
+
+    mSaParams.sensor_frame_params = &mFrameParams;
+    /* use convergence time from tunings */
+    mSaParams.manual_convergence_time = -1.0;
+
+    return OK;
+}
+
+int AiqCore::init() {
+    LOG3A("@%s", __func__);
+
+    initAiqPlusParams();
+
+#ifndef ENABLE_SANDBOXING
+    ia_env env = {&Log::ccaPrintDebug, &Log::ccaPrintError, &Log::ccaPrintInfo};
+    ia_log_init(&env);
+#endif
+
+    mAiqState = AIQ_INIT;
+
+    int ret = mIntel3AParameter->init();
+    CheckError(ret != OK, ret, "@%s, Init 3a parameter failed ret: %d", __func__, ret);
+
+    mLastAeResult = nullptr;
+    mLastAwbResult = nullptr;
+    mLastAfResult = nullptr;
+    mAeRunTime = 0;
+    mAwbRunTime = 0;
+
+    return OK;
+}
+
+void AiqCore::deinitIntelAiqHandle() {
+    LOG3A("@%s", __func__);
+
+    for (auto mode = 0; mode < TUNING_MODE_MAX; mode++) {
+        IntelAiq* aiq = mIntelAiqHandle[mode];
+        if (!aiq) continue;
+
+        if (PlatformData::isAiqdEnabled(mCameraId)) {
+            ia_binary_data data = {nullptr, 0};
+            ia_err iaErr = aiq->getAiqdData(&data);
+            if (AiqUtils::convertError(iaErr) == OK) {
+                PlatformData::saveAiqd(mCameraId, static_cast<TuningMode>(mode), data);
+            } else {
+                LOGW("@%s, failed to get aiqd data, iaErr %d", __func__, iaErr);
+            }
+        }
+        aiq->deinit();
+        delete aiq;
+        mIntelAiqHandle[mode] = nullptr;
+    }
+    CLEAR(mIntelAiqHandleStatus);
+}
+
+int AiqCore::deinit() {
+    LOG3A("@%s", __func__);
+
+#ifndef ENABLE_SANDBOXING
+    ia_log_deinit();
+#endif
+
+    deinitIntelAiqHandle();
+
+    mAiqState = AIQ_NOT_INIT;
+
+    return OK;
+}
+
+int AiqCore::initIntelAiqHandle(const std::vector<TuningMode>& tuningModes) {
+    LOG3A("@%s", __func__);
+
+    ia_mkn* mkn = static_cast<ia_mkn*>(PlatformData::getMknHandle(mCameraId));
+    ia_binary_data *nvmData = PlatformData::getNvm(mCameraId);
+    ia_binary_data aiqData = {nullptr, 0};
+    // Initialize mIntelAiqHandle array based on different cpf data
+    for (auto & mode : tuningModes) {
+        uintptr_t cmcHandle = reinterpret_cast<uintptr_t>(nullptr);
+        int ret = PlatformData::getCpfAndCmc(mCameraId, nullptr, &aiqData, nullptr,
+                                             &cmcHandle, mode);
+        CheckError(ret != OK, BAD_VALUE, "@%s, getDataAndCmc fails", __func__);
+
+        ia_binary_data* aiqd = nullptr;
+        if (PlatformData::PlatformData::isAiqdEnabled(mCameraId)) {
+            aiqd = PlatformData::getAiqd(mCameraId, mode);
+        }
+
+        int statsNum = PlatformData::getExposureNum(mCameraId,
+                           CameraUtils::isMultiExposureCase(mode));
+        {
+            IntelAiq* intelAiq = new IntelAiq();
+            PERF_CAMERA_ATRACE_PARAM1_IMAGING("intelAiq->init", 1);
+            ia_aiq* aiq = intelAiq->init(&(aiqData),
+                                        nvmData,
+                                        aiqd,
+                                        MAX_STATISTICS_WIDTH,
+                                        MAX_STATISTICS_HEIGHT,
+                                        statsNum,
+                                        reinterpret_cast<ia_cmc_t*>(cmcHandle),
+                                        mkn);
+            if (aiq) {
+                mIntelAiqHandle[mode] = intelAiq;
+
+                std::string aiqVersion;
+                intelAiq->getVersion(&aiqVersion);
+                LOGI("@%s, AIQ VERSION: %s", __func__, aiqVersion.c_str());
+            } else {
+                mIntelAiqHandle[mode] = nullptr;
+                delete intelAiq;
+            }
+        }
+        CheckError(!mIntelAiqHandle[mode], NO_INIT, "@%s: init aiq failed!", __func__);
+        mIntelAiqHandleStatus[mode] = true;
+    }
+
+    return OK;
+}
+
+int AiqCore::configure(const std::vector<ConfigMode>& configModes) {
+    LOG3A("@%s", __func__);
+
+    int ret = OK;
+    bool allTuningModeConfiged = true;
+    std::vector<TuningMode> tuningModes;
+    for (auto cfg : configModes) {
+        TuningMode mode;
+        ret = PlatformData::getTuningModeByConfigMode(mCameraId, cfg, mode);
+        CheckError(ret != OK, ret, "%s: getTuningModeByConfigMode fails, cfg:%d", __func__, cfg);
+        tuningModes.push_back(mode);
+
+        if (!mIntelAiqHandle[mode]) {
+            allTuningModeConfiged = false;
+        }
+    }
+
+    if (mAiqState == AIQ_CONFIGURED && allTuningModeConfiged) {
+        return OK;
+    }
+
+    deinitIntelAiqHandle();
+
+    ret = initIntelAiqHandle(tuningModes);
+    if (ret == OK) {
+        mAiqState = AIQ_CONFIGURED;
+    }
+
+    return OK;
+}
+
+int AiqCore::setSensorInfo(const ia_aiq_frame_params &frameParams,
+                           const ia_aiq_exposure_sensor_descriptor &descriptor) {
+    LOG3A("@%s", __func__);
+
+    mFrameParams = frameParams;
+    mSensorPixelClock = descriptor.pixel_clock_freq_mhz;
+    mIntel3AParameter->setSensorInfo(descriptor);
+
+    return OK;
+}
+
+/**
+ *  Hyperfocal distance is the closest distance at which a lens can be focused
+ *  while keeping objects at infinity acceptably sharp. When the lens is focused
+ *  at this distance, all objects at distances from half of the hyperfocal
+ *  distance out to infinity will be acceptably sharp.
+ *
+ *  The equation used for this is:
+ *        f*f
+ *  H = -------
+ *        N*c
+ *
+ *  where:
+ *   f is the focal length
+ *   N is the f-number (f/D for aperture diameter D)
+ *   c is the Circle Of Confusion (COC)
+ *
+ *   the COC is calculated as the pixel width of 2 pixels
+ *
+ *  The hyperfocal distance in mm. It is ensured it will never be 0 to
+ *  avoid division by 0. If any of the required CMC items is missing
+ *  it will return the default value 5m
+ */
+int AiqCore::calculateHyperfocalDistance(TuningMode mode) {
+    LOG3A("@%s, tuning mode: %d", __func__, mode);
+
+    ia_cmc_t *cmcData = nullptr;
+    int ret = PlatformData::getCpfAndCmc(mCameraId, nullptr, nullptr, nullptr,
+                                         nullptr, mode, &cmcData);
+    CheckError(ret != OK || !cmcData, BAD_VALUE, "@%s get cmc data failed", __func__);
+
+    mHyperFocalDistance = AiqUtils::calculateHyperfocalDistance(*cmcData);
+
+    return OK;
+}
+
+/**
+ *
+ * Calculate the Depth of field (DOF) for a given AF Result.
+ *
+ * The Formulas to calculate the near and afar DOF are:
+ *          H * s
+ * Dn = ---------------
+ *         H + (s-f)
+ *
+ *          H * s
+ * Df =  ------------
+ *         H - (s-f)
+ *
+ * Where:
+ * H is the hyperfocal distance (that we get from CPF) (it cannot be 0)
+ * s is the distance to focused object (current focus distance)
+ * f is the focal length
+ *
+ * \param[in] afResults with current focus distance in mm
+ * \param[out] dof info: DOF for near and far limit in diopters
+ */
+int AiqCore::calculateDepthOfField(const ia_aiq_af_results &afResults,
+                                   camera_range_t *focusRange) {
+    LOG3A("@%s, afResults:%p, focusRange:%p", __func__, afResults, focusRange);
+    CheckError(!focusRange, BAD_VALUE, "@%s, Bad input values", __func__);
+
+    const float DEFAULT_DOF = 5000.0f;
+    focusRange->min = 1000.0f / DEFAULT_DOF;
+    focusRange->max = 1000.0f / DEFAULT_DOF;
+
+    float focusDistance = 1.0f * afResults.current_focus_distance;
+    if (focusDistance == 0.0f) {
+        // Not reporting error since this may be normal in fixed focus sensors
+        return OK;
+    }
+
+    ia_cmc_t *cmcData = nullptr;
+    cmc_optomechanics_t *optoInfo = nullptr;
+    PlatformData::getCpfAndCmc(mCameraId, nullptr, nullptr, nullptr,
+                               nullptr, mTuningMode, &cmcData);
+    if (cmcData) {
+        optoInfo = cmcData->cmc_parsed_optics.cmc_optomechanics;
+    }
+
+    float focalLengthMillis = 2.3f;
+    if (optoInfo) {
+        // focal length is stored in CMC in hundreds of millimeters
+        focalLengthMillis = static_cast<float>(optoInfo->effect_focal_length) / 100;
+    }
+
+    float num = mHyperFocalDistance * focusDistance;
+    float denom = (mHyperFocalDistance + focusDistance - focalLengthMillis);
+    if (denom != 0.0f) {
+        focusRange->min = num / denom;
+    }
+
+    denom = (mHyperFocalDistance - focusDistance + focalLengthMillis);
+    if (denom != 0.0f) {
+        focusRange->max = num / denom;
+    }
+
+    focusRange->min = 1000.0f / focusRange->min;
+    focusRange->max = 1000.0f / focusRange->max;
+
+    return OK;
+}
+
+int AiqCore::updateParameter(const aiq_parameter_t &param) {
+    LOG3A("@%s", __func__);
+
+    mUseManualColorMatrix = (param.awbMode == AWB_MODE_MANUAL_COLOR_TRANSFORM);
+    mColorMatrix = param.manualColorMatrix;
+    mColorGains = param.manualColorGains;
+
+    if (mTuningMode != param.tuningMode) {
+        int ret = calculateHyperfocalDistance(param.tuningMode);
+        CheckError(ret != OK, ret, "%s calculateHyperfocalDistance fails", __func__);
+        mTuningMode = param.tuningMode;
+    }
+    mShadingMode = param.shadingMode;
+    mLensShadingMapMode = param.lensShadingMapMode;
+    mLensShadingMapSize = param.lensShadingMapSize;
+
+    mGbceParams.frame_use = AiqUtils::convertFrameUsageToIaFrameUsage(param.frameUsage);
+    mGbceParams.ev_shift = param.evShift;
+
+    // In still frame use force update by setting convergence time to 0.
+    // in other cases use tunings.
+    mSaParams.manual_convergence_time = (param.frameUsage == FRAME_USAGE_STILL) ? 0.0 : -1.0;
+
+    mIntel3AParameter->updateParameter(param);
+    mAeForceLock = param.aeForceLock;
+    mAwbForceLock = param.awbForceLock;
+    mAfForceLock = mIntel3AParameter->mAfForceLock;
+
+    return OK;
+}
+
+int AiqCore::setStatistics(const ia_aiq_statistics_input_params_v4 *ispStatistics) {
+    LOG3A("@%s, ispStatistics:%p", __func__, ispStatistics);
+    CheckError(!ispStatistics, BAD_VALUE, "@%s, ispStatistics is nullptr", __func__);
+
+    int ret = OK;
+
+    CheckError(mTuningMode >= TUNING_MODE_MAX, UNKNOWN_ERROR, "mTuningMode overflow!");
+    IntelAiq* intelAiq = mIntelAiqHandle[mTuningMode];
+    CheckError(!intelAiq, UNKNOWN_ERROR, "%s, aiq is nullptr, mode:%d", __func__, mTuningMode);
+    {
+        PERF_CAMERA_ATRACE_PARAM1_IMAGING("statisticsSetV4", 1);
+        ia_err iaErr = intelAiq->statisticsSetV4(ispStatistics);
+        ret = AiqUtils::convertError(iaErr);
+        CheckError(ret != OK, ret, "Error setting statistics, ret = %d", ret);
+    }
+
+    mTimestamp = ispStatistics->frame_timestamp;
+    return ret;
+}
+
+int AiqCore::runAiq(AiqResult *aiqResult) {
+    LOG3A("@%s, aiqResult:%p", __func__, aiqResult);
+    CheckError(!aiqResult, BAD_VALUE, "@%s, aiqResult is nullptr", __func__);
+
+    int ret = run3A(aiqResult);
+    CheckError(ret != OK, ret, "run3A failed, ret: %d", ret);
+
+    ret = runAiqPlus(aiqResult);
+    CheckError(ret != OK, ret, "runAiqPlus failed, ret: %d", ret);
+
+    aiqResult->mTimestamp = mTimestamp;
+    return OK;
+}
+
+int AiqCore::run3A(AiqResult *aiqResult) {
+    LOG3A("@%s, aiqResult:%p", __func__, aiqResult);
+    CheckError(!aiqResult, BAD_VALUE, "@%s, aiqResult is nullptr", __func__);
+
+    int ret = OK;
+    int aaaType = IMAGING_ALGO_AE | IMAGING_ALGO_AWB;
+    if (PlatformData::getLensHwType(mCameraId) == LENS_VCM_HW) {
+        aaaType |= IMAGING_ALGO_AF;
+    }
+
+    if (aaaType & IMAGING_ALGO_AE) {
+        ret |= runAe(&aiqResult->mAeResults);
+    }
+    if (aaaType & IMAGING_ALGO_AWB) {
+        ret |= runAwb(&aiqResult->mAwbResults);
+    }
+    if (aaaType & IMAGING_ALGO_AF) {
+        ret |= runAf(aiqResult);
+    }
+
+    uint16_t pixelInLine = aiqResult->mAeResults.exposures[0].sensor_exposure->line_length_pixels;
+    uint16_t lineInFrame = aiqResult->mAeResults.exposures[0].sensor_exposure->frame_length_lines;
+    aiqResult->mFrameDuration = pixelInLine * lineInFrame / mSensorPixelClock;
+    aiqResult->mRollingShutter = pixelInLine * (mFrameParams.cropped_image_height - 1)
+                                 / mSensorPixelClock;
+    return ret;
+}
+
+int AiqCore::runAiqPlus(AiqResult *aiqResult) {
+    LOG3A("@%s, aiqResult:%p", __func__, aiqResult);
+    CheckError(!aiqResult, BAD_VALUE, "@%s, aiqResult is nullptr", __func__);
+
+    int algoType = IMAGING_ALGO_GBCE | IMAGING_ALGO_PA | IMAGING_ALGO_SA;
+
+    int ret = OK;
+    if (algoType & IMAGING_ALGO_GBCE) {
+        ret |= runGbce(&aiqResult->mGbceResults);
+    }
+    if (algoType & IMAGING_ALGO_PA) {
+        ret |= runPa(&aiqResult->mPaResults, &aiqResult->mAwbResults,
+                     aiqResult->mAeResults.exposures[0].exposure,
+                     &aiqResult->mPreferredAcm);
+    }
+    if ((algoType & IMAGING_ALGO_SA) && (mShadingMode != SHADING_MODE_OFF)) {
+        ret |= runSa(&aiqResult->mSaResults, &aiqResult->mAwbResults,
+                     aiqResult->mAiqParam.lensShadingMap);
+    }
+
+    return ret;
+}
+
+int AiqCore::runAe(ia_aiq_ae_results* aeResults) {
+    LOG3A("@%s, aeResults:%p", __func__, aeResults);
+    CheckError(!aeResults, BAD_VALUE, "@%s, aeResults is nullptr", __func__);
+    PERF_CAMERA_ATRACE();
+
+    int ret = OK;
+    ia_aiq_ae_results *newAeResults = mLastAeResult;
+    bool aeForceRun = mIntel3AParameter->mAeParams.ev_shift != mLastEvShift ||
+                      (!mAeForceLock && (mAeRunTime % mIntel3AParameter->mAePerTicks == 0));
+
+    if (aeForceRun) {
+        LOG3A("AEC frame_use: %d", mIntel3AParameter->mAeParams.frame_use);
+
+        IntelAiq* intelAiq = mIntelAiqHandle[mTuningMode];
+        CheckError(!intelAiq, UNKNOWN_ERROR, "%s, aiq is nullptr, mode:%d", __func__, mTuningMode);
+        {
+            PERF_CAMERA_ATRACE_PARAM1_IMAGING("intelAiq->aeRun", 1);
+            ia_err iaErr = intelAiq->aeRun(&mIntel3AParameter->mAeParams, &newAeResults);
+            ret = AiqUtils::convertError(iaErr);
+            CheckError(ret != OK || !newAeResults, ret, "Error running AE, ret: %d", ret);
+        }
+
+        if (newAeResults->exposures[0].converged) {
+            mLastEvShift = mIntel3AParameter->mAeParams.ev_shift;
+        }
+    }
+
+    mIntel3AParameter->updateAeResult(newAeResults);
+    if (newAeResults) {
+        ret = AiqUtils::deepCopyAeResults(*newAeResults, aeResults);
+    }
+    mLastAeResult = aeResults;
+    ++mAeRunTime;
+
+    return ret;
+}
+
+int AiqCore::runAf(AiqResult *aiqResult) {
+    LOG3A("@%s, aiqResult:%p", __func__, aiqResult);
+    CheckError(!aiqResult, BAD_VALUE, "@%s, aiqResult is nullptr", __func__);
+    PERF_CAMERA_ATRACE();
+
+    ia_aiq_af_results *afResults = &aiqResult->mAfResults;
+    ia_aiq_af_results *newAfResults = mLastAfResult;
+
+    int ret = OK;
+    if (!mAfForceLock) {
+        IntelAiq* intelAiq = mIntelAiqHandle[mTuningMode];
+        CheckError(!intelAiq, UNKNOWN_ERROR, "@%s, aiq is nullptr, mode:%d", __func__, mTuningMode);
+        {
+            PERF_CAMERA_ATRACE_PARAM1_IMAGING("intelAiq->afRun", 1);
+            ia_err iaErr = intelAiq->afRun(&mIntel3AParameter->mAfParams, &newAfResults);
+            ret = AiqUtils::convertError(iaErr);
+            CheckError(ret != OK || !newAfResults, ret, "Error running AF, ret: %d", ret);
+        }
+    }
+
+    focusDistanceResult(newAfResults, &aiqResult->mAfDistanceDiopters, &aiqResult->mFocusRange);
+    ret = AiqUtils::deepCopyAfResults(*newAfResults, afResults);
+
+    mLastAfResult = afResults;
+    mIntel3AParameter->fillAfTriggerResult(newAfResults);
+    return ret;
+}
+
+void AiqCore::focusDistanceResult(const ia_aiq_af_results *afResults,
+                                  float *afDistanceDiopters,
+                                  camera_range_t *focusRange) {
+    LOG3A("@%s, afResults:%p, afDistanceDiopters:%p, focusRange:%p", __func__,
+          afResults, afDistanceDiopters, focusRange);
+    CheckError(!afResults || !afDistanceDiopters || !focusRange, VOID_VALUE,
+               "@%s, Bad input values", __func__);
+
+    *afDistanceDiopters = 1.2f;
+    if (mIntel3AParameter->mAfParams.focus_mode == ia_aiq_af_operation_mode_infinity) {
+        // infinity mode is special: we need to report 0.0f (1/inf = 0)
+        *afDistanceDiopters = 0.0f;
+    } else if (afResults->current_focus_distance != 0) {
+        // In AIQ, 'current_focus_distance' is in millimeters
+        // For rounding multiply by extra 100.
+        // This allows the diopters to have 2 decimal values
+        *afDistanceDiopters = 100 * 1000 * (1.0 / afResults->current_focus_distance);
+        *afDistanceDiopters = ceil(*afDistanceDiopters);
+        // Divide by 100 for final result.
+        *afDistanceDiopters = *afDistanceDiopters / 100;
+    }
+    LOG3A("%s, Zero focus distance in AF result, reporting %f", __func__, *afDistanceDiopters);
+
+    calculateDepthOfField(*afResults, focusRange);
+    LOG3A("%s, focus distance with diopters: %f %f", __func__, focusRange->min, focusRange->max);
+}
+
+int AiqCore::runAwb(ia_aiq_awb_results* awbResults) {
+    LOG3A("@%s, awbResults:%p", __func__, awbResults);
+    CheckError(!awbResults, BAD_VALUE, "@%s, awbResults is nullptr", __func__);
+    PERF_CAMERA_ATRACE();
+
+    int ret = OK;
+    ia_aiq_awb_results *newAwbResults = mLastAwbResult;
+
+    if (!mAwbForceLock && (mAwbRunTime % mIntel3AParameter->mAwbPerTicks == 0)) {
+        IntelAiq* intelAiq = mIntelAiqHandle[mTuningMode];
+        CheckError(!intelAiq, UNKNOWN_ERROR, "%s, aiq is nullptr, mode:%d", __func__, mTuningMode);
+        {
+            PERF_CAMERA_ATRACE_PARAM1_IMAGING("intelAiq->awbRun", 1);
+            ia_err iaErr = intelAiq->awbRun(&mIntel3AParameter->mAwbParams, &newAwbResults);
+            ret = AiqUtils::convertError(iaErr);
+            CheckError(ret != OK || !newAwbResults, ret, "Error running AWB, ret: %d", ret);
+        }
+    }
+
+    CheckError(!newAwbResults, BAD_VALUE, "newAwbResults is nullptr");
+
+    if (!PlatformData::isIsysEnabled(mCameraId)) {
+        // Fix AWB gain to 1 for none-ISYS cases
+        newAwbResults->accurate_r_per_g = 1;
+        newAwbResults->accurate_b_per_g = 1;
+        newAwbResults->final_r_per_g = 1;
+        newAwbResults->final_b_per_g = 1;
+    }
+
+    mIntel3AParameter->updateAwbResult(newAwbResults);
+
+    ret = AiqUtils::deepCopyAwbResults(*newAwbResults, awbResults);
+
+    mLastAwbResult = awbResults;
+    ++mAwbRunTime;
+
+    return ret;
+}
+
+int AiqCore::runGbce(ia_aiq_gbce_results *gbceResults) {
+    LOG3A("%s, gbceResults:%p", __func__, gbceResults);
+    CheckError(!gbceResults, BAD_VALUE, "@%s, gbceResults is nullptr", __func__);
+
+    PERF_CAMERA_ATRACE();
+    ia_aiq_gbce_results *newGbceResults = nullptr;
+
+    IntelAiq* intelAiq = mIntelAiqHandle[mTuningMode];
+    CheckError(!intelAiq, UNKNOWN_ERROR, "%s, aiq is nullptr, mode:%d", __func__, mTuningMode);
+    {
+        PERF_CAMERA_ATRACE_PARAM1_IMAGING("intelAiq->gbceRun", 1);
+        ia_err iaErr = intelAiq->gbceRun(&mGbceParams, &newGbceResults);
+        int ret = AiqUtils::convertError(iaErr);
+        CheckError(ret != OK, ret, "@%s, gbceRun fails, ret: %d", __func__, ret);
+    }
+
+    return AiqUtils::deepCopyGbceResults(*newGbceResults, gbceResults);
+}
+
+int AiqCore::runPa(ia_aiq_pa_results_v1 *paResults,
+                   ia_aiq_awb_results *awbResults,
+                   ia_aiq_exposure_parameters *exposureParams,
+                   ia_aiq_advanced_ccm_t *preferredAcm) {
+    LOG3A("%s, paResults:%p, awbResults:%p, exposureParams:%p, preferredAcm:%p", __func__,
+          paResults, awbResults, exposureParams, preferredAcm);
+    CheckError(!paResults || !awbResults || !exposureParams || !preferredAcm, BAD_VALUE,
+               "@%s, Bad input values", __func__);
+
+    PERF_CAMERA_ATRACE();
+    ia_aiq_pa_results_v1 *newPaResults = nullptr;
+
+    mPaParams.awb_results = awbResults;
+    mPaParams.exposure_params = exposureParams;
+    mPaParams.color_gains = nullptr;
+
+    IntelAiq* intelAiq = mIntelAiqHandle[mTuningMode];
+    CheckError(!intelAiq, UNKNOWN_ERROR, "%s, aiq is nullptr, mode:%d", __func__, mTuningMode);
+    {
+        PERF_CAMERA_ATRACE_PARAM1_IMAGING("intelAiq->paRunV1", 1);
+        ia_err iaErr = intelAiq->paRunV1(&mPaParams, &newPaResults);
+        int ret = AiqUtils::convertError(iaErr);
+        CheckError(ret != OK || !newPaResults, ret, "Error running PA, ret: %d", ret);
+    }
+
+    dumpPaResult(newPaResults);
+
+    // Override color_conversion_matrix and color_gains
+    // when application requires manual color transform.
+    if (mUseManualColorMatrix) {
+        MEMCPY_S(&newPaResults->color_conversion_matrix,
+                 sizeof(newPaResults->color_conversion_matrix),
+                 &mColorMatrix.color_transform,
+                 sizeof(mColorMatrix.color_transform));
+
+        if (VALID_COLOR_GAINS(mColorGains.color_gains_rggb)) {
+            newPaResults->color_gains.r  = mColorGains.color_gains_rggb[0];
+            newPaResults->color_gains.gr = mColorGains.color_gains_rggb[1];
+            newPaResults->color_gains.gb = mColorGains.color_gains_rggb[2];
+            newPaResults->color_gains.b  = mColorGains.color_gains_rggb[3];
+        }
+
+        // Override advanced color conversion matrix also if it enabled
+        if (newPaResults->preferred_acm) {
+            for (unsigned int i = 0; i < newPaResults->preferred_acm->sector_count; i++) {
+                MEMCPY_S(&newPaResults->preferred_acm->advanced_color_conversion_matrices[i],
+                         sizeof(newPaResults->preferred_acm->advanced_color_conversion_matrices[0]),
+                         &mColorMatrix.color_transform,
+                         sizeof(mColorMatrix.color_transform));
+            }
+        }
+    }
+
+    return AiqUtils::deepCopyPaResults(*newPaResults, paResults, preferredAcm);
+}
+
+int AiqCore::checkColorOrder(cmc_bayer_order bayerOrder, ColorOrder *colorOrder) {
+    LOG3A("@%s, bayerOrder = %d, colorOrder:%p", __func__, bayerOrder, colorOrder);
+    CheckError(!colorOrder, BAD_VALUE, "@%s, colorOrder is nullptr", __func__);
+
+    int ret = OK;
+    switch (bayerOrder) {
+    case cmc_bayer_order_grbg:
+    /* use gr r b gb constitute 2X2 array
+     * gr    r
+     * b     gb
+     * The four channel use x y coordinate to indicate
+     * gr(0, 0) r(1, 0) b(0, 1) gb(1, 1)
+    */
+        colorOrder->r[0] = 1;
+        colorOrder->r[1] = 0;
+        colorOrder->b[0] = 0;
+        colorOrder->b[1] = 1;
+        colorOrder->gr[0] = 0;
+        colorOrder->gr[1] = 0;
+        colorOrder->gb[0] = 1;
+        colorOrder->gb[1] = 1;
+        break;
+    case cmc_bayer_order_rggb:
+        colorOrder->r[0] = 0;
+        colorOrder->r[1] = 0;
+        colorOrder->b[0] = 1;
+        colorOrder->b[1] = 1;
+        colorOrder->gr[0] = 1;
+        colorOrder->gr[1] = 0;
+        colorOrder->gb[0] = 0;
+        colorOrder->gb[1] = 1;
+        break;
+    case cmc_bayer_order_bggr:
+        colorOrder->r[0] = 1;
+        colorOrder->r[1] = 1;
+        colorOrder->b[0] = 0;
+        colorOrder->b[1] = 0;
+        colorOrder->gr[0] = 0;
+        colorOrder->gr[1] = 1;
+        colorOrder->gb[0] = 1;
+        colorOrder->gb[1] = 0;
+        break;
+    case cmc_bayer_order_gbrg:
+        colorOrder->r[0] = 0;
+        colorOrder->r[1] = 1;
+        colorOrder->b[0] = 1;
+        colorOrder->b[1] = 0;
+        colorOrder->gr[0] = 1;
+        colorOrder->gr[1] = 1;
+        colorOrder->gb[0] = 0;
+        colorOrder->gb[1] = 0;
+        break;
+    default:
+        ret = BAD_VALUE;
+        break;
+    }
+    return ret;
+}
+
+int AiqCore::reFormatLensShadingMap(const LSCGrid &inputLscGrid, float *dstLscGridRGGB) {
+    LOG3A("@%s, width %d, height %d", __func__, inputLscGrid.width, inputLscGrid.height);
+
+    CheckError(inputLscGrid.isBad() || !dstLscGridRGGB, BAD_VALUE,
+               "@%s, Bad input values for lens shading map reformatting", __func__);
+
+    // Metadata spec request order [R, Geven, Godd, B]
+    // the lensShading from ISP is 4 width * height block,
+    // for ia_aiq_bayer_order_grbg, the four block is G, R, B, G
+    size_t size = inputLscGrid.height * inputLscGrid.width;
+    for (size_t i = 0; i < size; i++) {
+        *dstLscGridRGGB++ = inputLscGrid.gridR[i];
+        *dstLscGridRGGB++ = inputLscGrid.gridGr[i];
+        *dstLscGridRGGB++ = inputLscGrid.gridGb[i];
+        *dstLscGridRGGB++ = inputLscGrid.gridB[i];
+    }
+
+    return OK;
+}
+
+int AiqCore::storeLensShadingMap(const LSCGrid &inputLscGrid,
+                                 const LSCGrid &resizeLscGrid, float *dstLscGridRGGB) {
+    LOG3A("@%s", __func__);
+    CheckError(inputLscGrid.isBad() || resizeLscGrid.isBad() || !dstLscGridRGGB, BAD_VALUE,
+               "@%s, Bad input values for lens shading map storing", __func__);
+
+    int destWidth = resizeLscGrid.width;
+    int destHeight = resizeLscGrid.height;
+    int width = inputLscGrid.width;
+    int height = inputLscGrid.height;
+
+    if (width != destWidth || height != destHeight) {
+        // requests lensShadingMapSize must be smaller than 64*64
+        // and it is a constant size.
+        // Our lensShadingMapSize is dynamic based on the resolution, so need
+        // to do resize for 4 channels separately
+
+        AiqUtils::resize2dArray(inputLscGrid.gridR,  width, height,
+                      resizeLscGrid.gridR,  destWidth, destHeight);
+        AiqUtils::resize2dArray(inputLscGrid.gridGr,  width, height,
+                      resizeLscGrid.gridGr,  destWidth, destHeight);
+        AiqUtils::resize2dArray(inputLscGrid.gridGb,  width, height,
+                      resizeLscGrid.gridGb,  destWidth, destHeight);
+        AiqUtils::resize2dArray(inputLscGrid.gridB,  width, height,
+                      resizeLscGrid.gridB,  destWidth, destHeight);
+
+        LOG3A("resize lens shading map from [%d,%d] to [%d,%d]",
+              width, height, destWidth, destHeight);
+    } else {
+        size_t size = destWidth * destHeight * sizeof(resizeLscGrid.gridR[0]);
+        STDCOPY((int8_t *) resizeLscGrid.gridR,  (int8_t *) inputLscGrid.gridR,  size);
+        STDCOPY((int8_t *) resizeLscGrid.gridGr, (int8_t *) inputLscGrid.gridGr, size);
+        STDCOPY((int8_t *) resizeLscGrid.gridGb, (int8_t *) inputLscGrid.gridGb, size);
+        STDCOPY((int8_t *) resizeLscGrid.gridB,  (int8_t *) inputLscGrid.gridB,  size);
+    }
+
+    return reFormatLensShadingMap(resizeLscGrid, dstLscGridRGGB);
+}
+
+int AiqCore::processSAResults(ia_aiq_sa_results_v1 *saResults, float *lensShadingMap) {
+    LOG3A("@%s, saResults:%p, lensShadingMap:%p", __func__, saResults, lensShadingMap);
+    CheckError(!saResults || !lensShadingMap, BAD_VALUE, "@%s, Bad input values", __func__);
+
+    if (!saResults->lsc_update || (mLensShadingMapMode == LENS_SHADING_MAP_MODE_OFF)) {
+        return OK;
+    }
+
+    ColorOrder co_ind = {};
+    int ret = checkColorOrder(saResults->color_order, &co_ind);
+    CheckError(ret != OK, BAD_VALUE, "Failed to checkColorOrder, ret: %d", ret);
+
+    LSCGrid inputGrid;
+    inputGrid.gridB = saResults->lsc_grid[co_ind.b[0]][co_ind.b[1]];
+    inputGrid.gridR = saResults->lsc_grid[co_ind.r[0]][co_ind.r[1]];
+    inputGrid.gridGr = saResults->lsc_grid[co_ind.gr[0]][co_ind.gr[1]];
+    inputGrid.gridGb = saResults->lsc_grid[co_ind.gb[0]][co_ind.gb[1]];
+    inputGrid.width = saResults->width;
+    inputGrid.height = saResults->height;
+
+    LSCGrid resizeGrid;
+    resizeGrid.gridB = mResizeLscGridB;
+    resizeGrid.gridR = mResizeLscGridR;
+    resizeGrid.gridGr = mResizeLscGridGr;
+    resizeGrid.gridGb = mResizeLscGridGb;
+    resizeGrid.width = mLensShadingMapSize.x;
+    resizeGrid.height = mLensShadingMapSize.y;
+
+    float lscGridRGGB[DEFAULT_LSC_GRID_SIZE * 4];
+    storeLensShadingMap(inputGrid, resizeGrid, lscGridRGGB);
+
+    size_t size = resizeGrid.width * resizeGrid.height * 4;
+    size_t errCount = 0;
+    for (size_t i = 0; i < size; i++) {
+        if (lscGridRGGB[i] < 1.0f) {
+            lscGridRGGB[i] = 1.0f;
+            errCount++;
+        }
+    }
+    if (errCount) {
+        LOGW("Error - SA produced too small values (%d/%d)!", errCount, size);
+    }
+
+    float *lsm = (mShadingMode != SHADING_MODE_OFF) ? lscGridRGGB : mLscOffGrid;
+    for (size_t i = 0; i < size; i++) {
+        lensShadingMap[i] = lsm[i];
+    }
+
+    return OK;
+}
+
+int AiqCore::runSa(ia_aiq_sa_results_v1 *saResults,
+                   ia_aiq_awb_results *awbResults,
+                   float *lensShadingMap) {
+    LOG3A("%s, saResults:%p, awbResults:%p, lensShadingMap:%p", __func__,
+          saResults, awbResults, lensShadingMap);
+    CheckError(!saResults || !awbResults || !lensShadingMap, BAD_VALUE,
+               "@%s, Bad input values", __func__);
+
+    PERF_CAMERA_ATRACE();
+    int ret = OK;
+    ia_aiq_sa_results_v1 *newSaResults = nullptr;
+
+    mSaParams.awb_results = awbResults;
+
+    IntelAiq* intelAiq = mIntelAiqHandle[mTuningMode];
+    CheckError(!intelAiq, UNKNOWN_ERROR, "%s, aiq is nullptr, mode:%d", __func__, mTuningMode);
+    {
+        PERF_CAMERA_ATRACE_PARAM1_IMAGING("intelAiq->saRunV2", 1);
+        ia_err iaErr = intelAiq->saRunV2(&mSaParams, &newSaResults);
+        ret = AiqUtils::convertError(iaErr);
+        CheckError(ret != OK || !newSaResults, ret, "intelAiq->saRunV2 fails, ret: %d", ret);
+    }
+
+    dumpSaResult(newSaResults);
+    ret = AiqUtils::deepCopySaResults(*newSaResults, saResults);
+    CheckError(ret != OK, ret, "Error deepCopySaResults, ret: %d", ret);
+
+    return processSAResults(saResults, lensShadingMap);
+}
+
+int AiqCore::dumpPaResult(const ia_aiq_pa_results_v1 *paResult) {
+    LOG3A("%s, paResult:%p", __func__, paResult);
+    CheckError(!paResult, BAD_VALUE, "@%s, paResult is nullptr", __func__);
+
+    if (!Log::isDebugLevelEnable(CAMERA_DEBUG_LOG_AIQ)) return OK;
+
+    LOG3A("   PA results brightness %f saturation %f",
+          paResult->brightness_level,
+          paResult->saturation_factor);
+    LOG3A("   PA results black level row 0: %f %f %f  %f ",
+          paResult->black_level_4x4[0][0],
+          paResult->black_level_4x4[0][1],
+          paResult->black_level_4x4[0][2],
+          paResult->black_level_4x4[0][3]);
+    LOG3A("   PA results black level row 1: %f %f %f  %f ",
+          paResult->black_level_4x4[1][0],
+          paResult->black_level_4x4[1][1],
+          paResult->black_level_4x4[1][2],
+          paResult->black_level_4x4[1][3]);
+    LOG3A("   PA results black level row 2: %f %f %f  %f ",
+          paResult->black_level_4x4[2][0],
+          paResult->black_level_4x4[2][1],
+          paResult->black_level_4x4[2][2],
+          paResult->black_level_4x4[2][3]);
+    LOG3A("   PA results black level row 3: %f %f %f  %f ",
+          paResult->black_level_4x4[3][0],
+          paResult->black_level_4x4[3][1],
+          paResult->black_level_4x4[3][2],
+          paResult->black_level_4x4[3][3]);
+    LOG3A("   PA results color gains %f %f %f  %f ",
+          paResult->color_gains.r,
+          paResult->color_gains.gr,
+          paResult->color_gains.gb,
+          paResult->color_gains.b);
+    LOG3A("   PA results linearization table size %d",
+          paResult->linearization.size);
+
+    for (int i = 0; i < 3; i++) {
+        LOG3A("   PA results color matrix  [%.3f %.3f %.3f] ",
+              paResult->color_conversion_matrix[i][0],
+              paResult->color_conversion_matrix[i][1],
+              paResult->color_conversion_matrix[i][2]);
+    }
+
+    if (paResult->preferred_acm) {
+        LOG3A("   PA results advanced ccm sector count %d ",
+              paResult->preferred_acm->sector_count);
+    }
+    if (paResult->ir_weight) {
+        LOG3A("   PA results ir weight grid [ %d x %d ] ",
+              paResult->ir_weight->width, paResult->ir_weight->height);
+    }
+
+    return OK;
+}
+
+int AiqCore::dumpSaResult(const ia_aiq_sa_results_v1 *saResult) {
+    LOG3A("%s, saResult:%p", __func__, saResult);
+    CheckError(!saResult, BAD_VALUE, "@%s, saResult is nullptr", __func__);
+
+    if (!Log::isDebugLevelEnable(CAMERA_DEBUG_LOG_AIQ)) return OK;
+
+    LOG3A("   SA results lsc Update %d size %dx%d",
+          saResult->lsc_update, saResult->width,  saResult->height);
+
+    return OK;
+}
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/AiqCore.h b/camera/hal/intel/ipu6/src/3a/AiqCore.h
new file mode 100644
index 000000000000..931c4f7e1b65
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/AiqCore.h
@@ -0,0 +1,214 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+
+#include "ia_aiq.h"
+#include "ia_ltm.h"
+#include "ia_cmc_types.h"
+#ifndef ENABLE_SANDBOXING
+#include "ia_log.h"
+#endif
+
+#include "AiqSetting.h"
+#include "AiqResult.h"
+
+#include "Intel3AParameter.h"
+
+#ifdef ENABLE_SANDBOXING
+#include "modules/sandboxing/client/IntelAiq.h"
+#else
+#include "modules/algowrapper/IntelAiq.h"
+#endif
+
+namespace icamera {
+
+/*
+ * \class AiqCore
+ * This class is used to set parameter, statistics and run Ae,
+ * Af, Awb, Gbce, Pa, Sa.
+ */
+class AiqCore {
+
+public:
+    AiqCore(int cameraId);
+    ~AiqCore();
+
+    /**
+     * \brief AiqCore init
+     *
+     * Init AiqPlus and AAAObject
+     */
+    int init();
+
+    /**
+     * \brief AiqCore deinit
+     *
+     * Deinit AiqPlus and AAAObject
+     */
+    int deinit();
+
+    /**
+     * \brief AiqCore configure
+     *
+     * Configure AiqPlus ConfigMode
+     */
+    int configure(const std::vector<ConfigMode>& configModes);
+
+    /**
+     * \brief Set sensor and frame info
+     *
+     * \param frameParams: the frame info parameter
+     * \param descriptor: the sensor info parameter
+     */
+    int setSensorInfo(const ia_aiq_frame_params &frameParams,
+                      const ia_aiq_exposure_sensor_descriptor &descriptor);
+
+    /**
+     * \brief update param and set converge speed
+     *
+     * \param param: the parameter update to AiqPlus and Aiq3A or custom 3A
+     */
+    int updateParameter(const aiq_parameter_t &param);
+
+    /**
+     * \brief Set ispStatistics to AiqPlus and Aiq3A or custom 3A
+     */
+    int setStatistics(const ia_aiq_statistics_input_params_v4 *ispStatistics);
+
+    /**
+     * \brief run 3A and AiqPlus
+     *
+     * \return OK if succeed, other value indicates failed
+     */
+    int runAiq(AiqResult *aiqResult);
+
+private:
+    DISALLOW_COPY_AND_ASSIGN(AiqCore);
+
+    // LSC data
+    typedef struct ColorOrder {
+        uint8_t r[2];
+        uint8_t gr[2];
+        uint8_t gb[2];
+        uint8_t b[2];
+    } ColorOrder;
+
+    class LSCGrid {
+     public: /* this was a struct: class just to satisfy a static code scanner */
+        uint16_t width;
+        uint16_t height;
+        uint16_t *gridR;
+        uint16_t *gridGr;
+        uint16_t *gridGb;
+        uint16_t *gridB;
+
+        bool isBad() const {
+            return (gridB == NULL || gridGb == NULL || gridR == NULL ||
+                    gridGr == NULL || width == 0 || height == 0);
+        }
+        LSCGrid(): width(0), height(0), gridR(NULL), gridGr(NULL),
+            gridGb(NULL), gridB(NULL) {}
+    };
+
+    int run3A(AiqResult *aiqResult);
+    int runAiqPlus(AiqResult *aiqResult);
+    int runAe(ia_aiq_ae_results* aeResults);
+    int runAf(AiqResult *aiqResult);
+    void focusDistanceResult(const ia_aiq_af_results *afResults,
+                             float *afDistanceDiopters,
+                             camera_range_t *focusRange);
+    int runAwb(ia_aiq_awb_results* awbResults);
+    int runGbce(ia_aiq_gbce_results *gbceResults);
+    int runPa(ia_aiq_pa_results_v1 *paResults,
+              ia_aiq_awb_results *awbResults,
+              ia_aiq_exposure_parameters *exposureParams,
+              ia_aiq_advanced_ccm_t *preferredAcm);
+    int runSa(ia_aiq_sa_results_v1 *saResults,
+              ia_aiq_awb_results *awbResults,
+              float *lensShadingMap);
+    int processSAResults(ia_aiq_sa_results_v1 *saResults, float *lensShadingMap);
+    int checkColorOrder(cmc_bayer_order bayerOrder, ColorOrder *colorOrder);
+    int storeLensShadingMap(const LSCGrid &inputLscGrid,
+                            const LSCGrid &resizeLscGrid, float *dstLscGridRGGB);
+    int reFormatLensShadingMap(const LSCGrid &inputLscGrid, float *dstLscGridRGGB);
+
+    int calculateHyperfocalDistance(TuningMode mode);
+    int calculateDepthOfField(const ia_aiq_af_results &afResults, camera_range_t *focusRange);
+    int initAiqPlusParams();
+    int initIntelAiqHandle(const std::vector<TuningMode>& tuningModes);
+    void deinitIntelAiqHandle(void);
+    // debug dumpers
+    int dumpPaResult(const ia_aiq_pa_results_v1* paResult);
+    int dumpSaResult(const ia_aiq_sa_results_v1* saResult);
+
+private:
+    int mCameraId;
+    unsigned long long mTimestamp;  // Latest statistics timestamp
+    float mSensorPixelClock;
+
+    bool mAeForceLock;
+    bool mAwbForceLock;
+    bool mAfForceLock;
+
+    std::unique_ptr<Intel3AParameter> mIntel3AParameter;
+
+    // Original AeResult class arrays are kept in 3a engine which is safely used here.
+    ia_aiq_ae_results *mLastAeResult;
+    ia_aiq_awb_results *mLastAwbResult;
+    ia_aiq_af_results *mLastAfResult;
+
+    int mAeRunTime;
+    int mAwbRunTime;
+
+    IntelAiq *mIntelAiqHandle[TUNING_MODE_MAX];
+    bool mIntelAiqHandleStatus[TUNING_MODE_MAX];
+
+    enum AiqState {
+        AIQ_NOT_INIT = 0,
+        AIQ_INIT,
+        AIQ_CONFIGURED,
+        AIQ_MAX
+    } mAiqState;
+
+    ia_aiq_frame_params mFrameParams;
+
+    ia_aiq_gbce_input_params mGbceParams;
+    ia_aiq_pa_input_params mPaParams;
+    ia_aiq_color_channels mPaColorGains;
+    ia_aiq_sa_input_params_v1 mSaParams;
+
+    bool mUseManualColorMatrix;
+    camera_color_transform_t mColorMatrix;
+    camera_color_gains_t mColorGains;
+    float mHyperFocalDistance;  // in millimeters
+
+    TuningMode mTuningMode;
+    camera_shading_mode_t mShadingMode;
+    camera_lens_shading_map_mode_type_t mLensShadingMapMode;
+    camera_coordinate_t mLensShadingMapSize;
+    uint16_t mResizeLscGridR[DEFAULT_LSC_GRID_SIZE];
+    uint16_t mResizeLscGridGr[DEFAULT_LSC_GRID_SIZE];
+    uint16_t mResizeLscGridGb[DEFAULT_LSC_GRID_SIZE];
+    uint16_t mResizeLscGridB[DEFAULT_LSC_GRID_SIZE];
+    float mLscOffGrid[DEFAULT_LSC_GRID_SIZE * 4];
+    float mLastEvShift;
+
+};
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/AiqEngine.cpp b/camera/hal/intel/ipu6/src/3a/AiqEngine.cpp
new file mode 100644
index 000000000000..8fe8efcc4f71
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/AiqEngine.cpp
@@ -0,0 +1,522 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "AiqEngine"
+
+#include "iutils/Utils.h"
+#include "iutils/Errors.h"
+#include "iutils/CameraLog.h"
+#include "PlatformData.h"
+#include "AiqEngine.h"
+#include "FaceDetection.h"
+
+namespace icamera {
+
+AiqEngine::AiqEngine(int cameraId, SensorHwCtrl *sensorHw, LensHw *lensHw, AiqSetting *setting) :
+    mCameraId(cameraId),
+    mAiqSetting(setting),
+    mFirstAiqRunning(true),
+    mFirstExposureSetting(true),
+    mAiqRunningForPerframe(false),
+    m3ACadenceSequence(0),
+    mLastStatsSequence(-1)
+{
+    LOG1("%s, mCameraId = %d", __func__, mCameraId);
+
+    CLEAR(mRgbsGridArray);
+    CLEAR(mAfGridArray);
+
+    mAiqCore = new AiqCore(mCameraId);
+
+    mSensorManager = new SensorManager(mCameraId, sensorHw);
+
+    mLensManager = new LensManager(mCameraId, lensHw);
+
+    // Should consider better place to maintain the life cycle of AiqResultStorage
+    mAiqResultStorage = AiqResultStorage::getInstance(mCameraId);
+    mCurrentStatsSequence = 0;
+}
+
+AiqEngine::~AiqEngine()
+{
+    LOG1("%s, mCameraId = %d", __func__, mCameraId);
+
+    delete mLensManager;
+
+    delete mSensorManager;
+
+    delete mAiqCore;
+
+    AiqResultStorage::releaseAiqResultStorage(mCameraId);
+}
+
+int AiqEngine::init()
+{
+    AutoMutex l(mEngineLock);
+    LOG1("%s, mCameraId = %d", __func__, mCameraId);
+
+    if (mAiqCore->init() != OK) {
+        return UNKNOWN_ERROR;
+    }
+
+    mSensorManager->init();
+
+    LOG1("%s, end mCameraId = %d", __func__, mCameraId);
+    return OK;
+}
+
+int AiqEngine::deinit()
+{
+    AutoMutex l(mEngineLock);
+    LOG1("%s, mCameraId = %d", __func__, mCameraId);
+
+    mSensorManager->deinit();
+
+    mAiqCore->deinit();
+
+    LOG1("%s, end mCameraId = %d", __func__, mCameraId);
+    return OK;
+}
+
+int AiqEngine::configure(const std::vector<ConfigMode>& configModes)
+{
+    AutoMutex l(mEngineLock);
+    LOG1("%s, mCameraId = %d", __func__, mCameraId);
+
+    mAiqCore->configure(configModes);
+
+    return OK;
+}
+
+int AiqEngine::startEngine()
+{
+    AutoMutex l(mEngineLock);
+    LOG1("%s, mCameraId = %d", __func__, mCameraId);
+
+    mFirstAiqRunning = true;
+    mFirstExposureSetting = true;
+
+    mSensorManager->reset();
+
+    mLensManager->start();
+
+    m3ACadenceSequence = 0;
+
+    LOG1("%s, end mCameraId = %d", __func__, mCameraId);
+
+    return OK;
+}
+
+int AiqEngine::stopEngine()
+{
+    AutoMutex l(mEngineLock);
+    LOG1("%s, end mCameraId = %d", __func__, mCameraId);
+
+    mLensManager->stop();
+
+    return OK;
+}
+
+int AiqEngine::run3A(long *settingSequence)
+{
+    LOG3A("%s", __func__);
+    // Run 3A in call thread
+    AutoMutex l(mEngineLock);
+
+    // Handle 3A cadence logic
+    int run3ACadence = mAiqParam.run3ACadence;
+    if (run3ACadence < 1) {
+        LOGW("Invalid 3A cadence %d, use default 1.", run3ACadence);
+        run3ACadence = 1;
+    }
+    LOG2("%s: run3ACadence is %d", __func__, run3ACadence);
+
+    if (m3ACadenceSequence % run3ACadence != 0) {
+        // Skip 3A per cadence
+        m3ACadenceSequence ++;
+        return OK;
+    }
+    LOG2("%s: run 3A for cadence sequence %ld", __func__, m3ACadenceSequence);
+    m3ACadenceSequence ++;
+
+    mAiqRunningForPerframe = (settingSequence != nullptr);
+    AiqState state = prepareInputParam();
+
+    AiqResult *aiqResult = mAiqResultStorage->acquireAiqResult();
+    aiqResult->mTuningMode = mAiqParam.tuningMode;
+
+    if (state == AIQ_STATE_RUN) {
+        state = runAiq(aiqResult);
+    }
+    if (state == AIQ_STATE_RESULT_SET) {
+        state = handleAiqResult(aiqResult);
+    }
+    if (state == AIQ_STATE_DONE) {
+        done(aiqResult);
+    }
+
+    mAiqResultStorage->unLockAiqStatistics();
+
+    if (settingSequence) {
+        *settingSequence = mAiqResultStorage->getAiqResult()->mSequence;
+        LOG3A("%s, sequence %ld, mLastStatsSequence %ld", __func__, *settingSequence,
+               mLastStatsSequence);
+    }
+
+    PlatformData::saveMakernoteData(mCameraId, mAiqParam.makernoteMode,
+                                    mAiqResultStorage->getAiqResult()->mSequence);
+
+    return (state == AIQ_STATE_DONE || state == AIQ_STATE_WAIT) ? 0 : UNKNOWN_ERROR;
+}
+
+EventListener *AiqEngine::getSofEventListener()
+{
+    AutoMutex l(mEngineLock);
+    LOG1("%s, mCameraId = %d", __func__, mCameraId);
+
+    return mSensorManager->getSofEventListener();
+}
+
+int AiqEngine::saveAfGridData(const ia_aiq_af_grid* afGrid, ia_aiq_af_grid* dst)
+{
+    LOG3A("%s", __func__);
+    if (afGrid == nullptr
+        || afGrid->filter_response_1 == nullptr
+        || afGrid->filter_response_2 == nullptr
+        || afGrid->grid_width == 0
+        || afGrid->grid_height == 0) {
+        LOGE("%s, af grids are invalid", __func__);
+        return BAD_VALUE;
+    }
+
+    size_t gridSize = afGrid->grid_width * afGrid->grid_height;
+    if (afGrid->grid_width != dst->grid_width || afGrid->grid_height != dst->grid_height) {
+        if (dst->filter_response_1 != nullptr) {
+            delete [] dst->filter_response_1;
+        }
+        dst->filter_response_1 = new int[gridSize];
+        if (dst->filter_response_2 != nullptr) {
+            delete [] dst->filter_response_2;
+        }
+        dst->filter_response_2 = new int[gridSize];
+    }
+
+    dst->grid_width = afGrid->grid_width;
+    dst->grid_height = afGrid->grid_height;
+    dst->block_width = afGrid->block_width;
+    dst->block_height = afGrid->block_height;
+    MEMCPY_S(dst->filter_response_1, gridSize * sizeof(int),
+             afGrid->filter_response_1, gridSize * sizeof(int));
+    MEMCPY_S(dst->filter_response_2, gridSize * sizeof(int),
+             afGrid->filter_response_2, gridSize * sizeof(int));
+
+    LOG3A("%s, grid size=[%dx%d]", __func__, dst->grid_width, dst->grid_height);
+    return OK;
+}
+
+int AiqEngine::saveRgbsGridData(const ia_aiq_rgbs_grid* rgbsGrid, ia_aiq_rgbs_grid* dst)
+{
+    LOG3A("%s", __func__);
+    if (rgbsGrid == nullptr
+        || rgbsGrid->blocks_ptr == nullptr
+        || rgbsGrid->grid_width == 0
+        || rgbsGrid->grid_height == 0) {
+        LOGE("%s, rgbs grids are invalid", __func__);
+        return BAD_VALUE;
+    }
+
+    size_t gridSize = rgbsGrid->grid_width * rgbsGrid->grid_height;
+    if (rgbsGrid->grid_width != dst->grid_width || rgbsGrid->grid_height != dst->grid_height) {
+        if (dst->blocks_ptr != nullptr) {
+            delete [] dst->blocks_ptr;
+        }
+        dst->blocks_ptr = new rgbs_grid_block[gridSize];
+    }
+
+    dst->grid_width = rgbsGrid->grid_width;
+    dst->grid_height = rgbsGrid->grid_height;
+    MEMCPY_S(dst->blocks_ptr, gridSize * sizeof(rgbs_grid_block),
+             rgbsGrid->blocks_ptr, gridSize * sizeof(rgbs_grid_block));
+
+    dst->shading_correction = rgbsGrid->shading_correction;
+
+    LOG3A("%s, grid size=[%dx%d]", __func__, dst->grid_width, dst->grid_height);
+    return OK;
+}
+
+int AiqEngine::prepareStats(ia_aiq_statistics_input_params_v4 *statsParam,
+                            ia_aiq_gbce_results *gbceResults,
+                            AiqStatistics *aiqStatistics)
+{
+    mLastStatsSequence = aiqStatistics->mSequence;
+    LOG3A("%s, sequence %ld", __func__, aiqStatistics->mSequence);
+
+    statsParam->rgbs_grids = mRgbsGridArray;
+    statsParam->af_grids = mAfGridArray;
+
+    int ret = OK;
+    do {
+
+        // The statistics timestamp is incorrect. If possible, use SOF timestamp instead.
+        unsigned long long timestamp = mSensorManager->getSofTimestamp(aiqStatistics->mSequence);
+        if (timestamp == 0) {
+            LOG2("The sof sequence was not found %ld", aiqStatistics->mSequence);
+            timestamp = aiqStatistics->mTimestamp;
+        }
+
+        statsParam->frame_id = aiqStatistics->mSequence;
+        statsParam->frame_timestamp = timestamp;
+        statsParam->num_rgbs_grids = PlatformData::getExposureNum(mCameraId,
+                                         CameraUtils::isMultiExposureCase(mAiqParam.tuningMode));
+
+        if (statsParam->num_rgbs_grids > 1) {
+            for (unsigned int i = 0; i < statsParam->num_rgbs_grids; i++) {
+                statsParam->rgbs_grids[i] = &(aiqStatistics->mRgbsGridArray[i]);
+            }
+        } else {
+            statsParam->rgbs_grids[0] = &(aiqStatistics->mRgbsGridArray[0]);
+        }
+        statsParam->af_grids[0] = &(aiqStatistics->mAfGridArray[0]);
+        statsParam->num_af_grids = 1;
+        statsParam->external_histograms = nullptr;
+        statsParam->num_external_histograms = 0;
+        statsParam->camera_orientation = ia_aiq_camera_orientation_unknown;
+
+        const AiqResult *feedback = mAiqResultStorage->getAiqResult(aiqStatistics->mSequence);
+        if (feedback == nullptr) {
+            LOGW("%s: no feed back result for sequence %ld! use the latest instead",
+                    __func__, aiqStatistics->mSequence);
+            feedback = mAiqResultStorage->getAiqResult();
+        }
+
+        statsParam->frame_ae_parameters = &feedback->mAeResults;
+        statsParam->frame_af_parameters = &feedback->mAfResults;
+        statsParam->frame_pa_parameters = &feedback->mPaResults;
+        statsParam->awb_results = &feedback->mAwbResults;
+        statsParam->frame_sa_parameters = &feedback->mSaResults;
+
+        *gbceResults = feedback->mGbceResults;
+    } while (0);
+    LOG3A("%s end", __func__);
+    return ret;
+}
+
+void AiqEngine::setAiqResult(AiqResult *aiqResult, bool skip)
+{
+    SensorExpGroup sensorExposures;
+    for (unsigned int i = 0; i < aiqResult->mAeResults.num_exposures; i++) {
+        SensorExposure exposure;
+        exposure.sensorParam = *aiqResult->mAeResults.exposures[i].sensor_exposure;
+        exposure.realDigitalGain = (aiqResult->mAeResults.exposures[i].exposure)->digital_gain;
+        sensorExposures.push_back(exposure);
+    }
+    bool useSof = !mFirstExposureSetting;
+    aiqResult->mSequence = mSensorManager->updateSensorExposure(sensorExposures, useSof);
+    if (mFirstExposureSetting) {
+        mFirstExposureSetting = false;
+    }
+    aiqResult->mSkip = skip;
+
+    if (skip) {
+        LOG3A("%s, skipping frame aiqResult->mSequence = %ld", __func__, aiqResult->mSequence);
+    }
+
+    mLensManager->setLensResult(aiqResult->mAeResults, aiqResult->mAfResults);
+
+    aiqResult->mAiqParam = mAiqParam;
+}
+
+int AiqEngine::getSkippingNum(AiqResult *aiqResult)
+{
+    LOG3A("%s", __func__);
+    int skipNum = 0;
+
+    if (!mFirstAiqRunning) {
+        const AiqResult *lastResult = mAiqResultStorage->getAiqResult();
+        if (lastResult->mTuningMode != aiqResult->mTuningMode) {
+            // Skip 3 frames when pipe switching
+            skipNum = 3;
+        }
+    } else if (mAiqRunningForPerframe) {
+        // The 1st result takes effect @ frame (initialSkip) (applied before stream on)
+        skipNum = PlatformData::getInitialSkipFrame(mCameraId);
+    }
+
+    return skipNum;
+}
+
+bool AiqEngine::needRun3A(AiqStatistics *aiqStatistics)
+{
+    LOG3A("%s", __func__);
+
+    // Force to run 3a for per-frame control case
+    if (mAiqRunningForPerframe) {
+        return true;
+    }
+
+    // Force to run 3a for the first time running
+    if (mFirstAiqRunning) {
+        return true;
+    }
+
+    if (aiqStatistics == nullptr) {
+        LOG3A("no stats and not need to re-run 3A");
+        return false;
+    }
+
+    if (mLastStatsSequence == aiqStatistics->mSequence) {
+        LOG3A("no new stats skip, mLastStatsSequence = %ld", mLastStatsSequence);
+        return false;
+    } else if (mSensorManager->getCurrentExposureAppliedDelay() > kMaxExposureAppliedDelay) {
+        LOG3A("exposure setting applied delay is too larger, skip it");
+        return false;
+    }
+
+    return true;
+}
+
+AiqEngine::AiqState AiqEngine::prepareInputParam(void)
+{
+    // set Aiq Params
+    int ret = mAiqSetting->getAiqParameter(mAiqParam);
+    if (ret != OK)
+        return AIQ_STATE_ERROR;
+
+    // Update sensor info for the first-run of AIQ
+    if (mFirstAiqRunning) {
+        mSensorManager->setFrameRate(mAiqParam.fps);
+        // set sensor info if needed
+        ia_aiq_exposure_sensor_descriptor sensorDescriptor;
+        ia_aiq_frame_params frameParams;
+        CLEAR(sensorDescriptor);
+        CLEAR(frameParams);
+        ret = mSensorManager->getSensorInfo(frameParams, sensorDescriptor);
+        CheckError((ret != OK), AIQ_STATE_ERROR, "Get sensor info failed:%d", ret);
+        mAiqCore->setSensorInfo(frameParams, sensorDescriptor);
+    }
+
+    // update lens related parameters
+    mLensManager->getLensInfo(mAiqParam);
+
+    mAiqCore->updateParameter(mAiqParam);
+    // set Stats
+    ia_aiq_statistics_input_params_v4 statsParam;
+    CLEAR(statsParam);
+    ia_aiq_gbce_results gbceResults;
+    CLEAR(gbceResults);
+
+    AiqStatistics *aiqStats =
+        const_cast<AiqStatistics*>(mAiqResultStorage->getAndLockAiqStatistics());
+
+    if (!needRun3A(aiqStats)) {
+        return AIQ_STATE_WAIT;
+    }
+
+    if (aiqStats == nullptr) {
+        LOG3A("%s: run aiq without stats data", __func__);
+        return AIQ_STATE_RUN;
+    }
+
+    // update face detection related parameters
+    ia_atbx_face faces[MAX_FACES_DETECTABLE];
+    ia_atbx_face_state facesState;
+    if (PlatformData::isFaceAeEnabled(mCameraId)) {
+        facesState.num_faces = 0;
+        facesState.faces = faces;
+        int ret = icamera::FaceDetection::getResult(mCameraId, &facesState);
+        if (ret == OK && facesState.num_faces > 0) {
+            ia_rectangle rect = facesState.faces[0].face_area;
+            LOG3A("@%s, face number:%d, left:%d, top:%d, right:%d, bottom:%d", __func__,
+                   facesState.num_faces, rect.left, rect.top, rect.right, rect.bottom);
+            statsParam.faces = &facesState;
+        }
+    }
+
+    ret = prepareStats(&statsParam, &gbceResults, aiqStats);
+
+    if (ret != OK) {
+        LOG3A("%s: no useful stats", __func__);
+        return AIQ_STATE_RUN;
+    }
+
+    mAiqCore->setStatistics(&statsParam);
+
+    return AIQ_STATE_RUN;
+}
+
+AiqEngine::AiqState AiqEngine::runAiq(AiqResult *aiqResult)
+{
+    int ret = mAiqCore->runAiq(aiqResult);
+    if (ret != OK) {
+        return AIQ_STATE_ERROR;
+    }
+
+    return AIQ_STATE_RESULT_SET;
+}
+
+AiqEngine::AiqState AiqEngine::handleAiqResult(AiqResult *aiqResult)
+{
+    LOG2("%s: aiqResult->mTuningMode = %d", __func__, aiqResult->mTuningMode);
+
+    applyManualTonemaps(aiqResult);
+
+    return AIQ_STATE_DONE;
+}
+
+int AiqEngine::applyManualTonemaps(AiqResult *aiqResult)
+{
+    /*
+     * Normal use-case is the automatic modes, and we need not do anything here
+     */
+    if (mAiqParam.tonemapMode == TONEMAP_MODE_FAST ||
+        mAiqParam.tonemapMode == TONEMAP_MODE_HIGH_QUALITY) {
+        return OK;
+    }
+
+    if (mAiqParam.tonemapMode == TONEMAP_MODE_GAMMA_VALUE) {
+        AiqUtils::applyTonemapGamma(mAiqParam.tonemapGamma, &aiqResult->mGbceResults);
+    } else if (mAiqParam.tonemapMode == TONEMAP_MODE_PRESET_CURVE) {
+        if (mAiqParam.tonemapPresetCurve == TONEMAP_PRESET_CURVE_SRGB) {
+            AiqUtils::applyTonemapSRGB(&aiqResult->mGbceResults);
+        } else if (mAiqParam.tonemapPresetCurve == TONEMAP_PRESET_CURVE_REC709) {
+            AiqUtils::applyTonemapREC709(&aiqResult->mGbceResults);
+        }
+    }
+    return OK;
+}
+
+AiqEngine::AiqState AiqEngine::done(AiqResult *aiqResult)
+{
+    int skipNum = getSkippingNum(aiqResult);
+    AiqResult *tmp = aiqResult;
+
+    for (int i = 0; i < skipNum; i++) {
+        // Increase the sensor settings sequence id, so for any frame that
+        // its sequence id is bigger than the user expected id will be discarded.
+        setAiqResult(tmp, true);
+        mAiqResultStorage->updateAiqResult(tmp->mSequence);
+        tmp = mAiqResultStorage->acquireAiqResult();
+        *tmp = *aiqResult;
+    }
+
+    setAiqResult(tmp, false);
+    mAiqResultStorage->updateAiqResult(tmp->mSequence);
+
+    mFirstAiqRunning = false;
+    return AIQ_STATE_WAIT;
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/AiqEngine.h b/camera/hal/intel/ipu6/src/3a/AiqEngine.h
new file mode 100644
index 000000000000..d4ce931e337d
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/AiqEngine.h
@@ -0,0 +1,149 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "AiqSetting.h"
+#include "AiqCore.h"
+#include "AiqResult.h"
+#include "AiqStatistics.h"
+#include "AiqResultStorage.h"
+#include "SensorManager.h"
+#include "LensManager.h"
+
+namespace icamera {
+
+/*
+ * \class AiqEngine
+ * This class is used to parse Stats, control \class AiqEngine
+ * This class is used to parse Stats, control running AIQ algorithms
+ * and set result to HW layer.
+ * This is sub thread class.
+ */
+class AiqEngine {
+
+public:
+    AiqEngine(int cameraId, SensorHwCtrl *sensorHw, LensHw *lensHw, AiqSetting *setting);
+    ~AiqEngine();
+
+    /**
+     * \brief Init AiqResult, AiqCore and SensorManager
+     */
+    int init();
+
+    /**
+     * \brief Deinit AiqResult, AiqCore and SensorManager
+     */
+    int deinit();
+
+    /**
+     * \brief configure with ConfigMode
+     */
+    int configure(const std::vector<ConfigMode>& configModes);
+
+    /**
+     * \brief Calculate and set frame and sensor info, and run 3a with default setting.
+     */
+    int startEngine();
+
+    /**
+     * \brief Run 3a to get new 3a settings.
+     * Return 0 if the operation succeeds, and output settingSequence to
+     * indicate the frame that settings are applied.
+     * settingSequence -1 means uncertain frame for this settings.
+     */
+    int run3A(long *settingSequence);
+
+    /**
+     * \brief Stop 3a thrad and LensManager.
+     */
+    int stopEngine();
+
+    /**
+     * \brief Get software EventListener
+     */
+    EventListener *getSofEventListener();
+
+private:
+    DISALLOW_COPY_AND_ASSIGN(AiqEngine);
+
+    int saveAfGridData(const ia_aiq_af_grid* afGrid, ia_aiq_af_grid* dst);
+    int saveRgbsGridData(const ia_aiq_rgbs_grid* rgbsGrid, ia_aiq_rgbs_grid* dst);
+
+    int prepareStats(ia_aiq_statistics_input_params_v4 *statsParami,
+                     ia_aiq_gbce_results *gbceResults,
+                     AiqStatistics *aiqStatistics);
+
+    void setAiqResult(AiqResult *aiqResult, bool skip);
+
+    int getSkippingNum(AiqResult *aiqResult);
+
+    bool needRun3A(AiqStatistics *aiqStatistics);
+
+    enum AiqState {
+        AIQ_STATE_IDLE = 0,
+        AIQ_STATE_WAIT,
+        AIQ_STATE_INPUT_PREPARE,
+        AIQ_STATE_RUN,
+        AIQ_STATE_RESULT_SET,
+        AIQ_STATE_DONE,
+        AIQ_STATE_ERROR,
+        AIQ_STATE_MAX
+    };
+
+    AiqState prepareInputParam(void);
+    AiqState runAiq(AiqResult *aiqResult);
+    AiqState handleAiqResult(AiqResult *aiqResult);
+    AiqState done(AiqResult *aiqResult);
+
+    int run();
+
+    // For manual ISP settings
+    int applyManualTonemaps(AiqResult *aiqResult);
+
+private:
+    static const nsecs_t kWaitDuration = 1000000000; //1000ms
+    static const int kMaxStatisticsDataSize = 3;
+    static const int kMaxExposureAppliedDelay = 5;
+
+private:
+    int mCameraId;
+    AiqResultStorage* mAiqResultStorage;
+    AiqSetting *mAiqSetting;
+    AiqCore *mAiqCore;
+    SensorManager *mSensorManager;
+    LensManager *mLensManager;
+    bool mFirstAiqRunning;
+    bool mFirstExposureSetting;
+    bool mAiqRunningForPerframe;
+
+    // Guard for public API of AiqEngine.
+    Mutex mEngineLock;
+    Condition mStatsAvailableSignal;
+
+    uint32_t mCurrentStatsSequence;
+
+    const ia_aiq_rgbs_grid* mRgbsGridArray[MAX_EXPOSURES_NUM];
+    const ia_aiq_af_grid* mAfGridArray[MAX_EXPOSURES_NUM];
+
+    aiq_parameter_t mAiqParam;
+
+    long m3ACadenceSequence;
+    long mLastStatsSequence;
+};
+
+} /* namespace icamera */
+
diff --git a/camera/hal/intel/ipu6/src/3a/AiqResult.cpp b/camera/hal/intel/ipu6/src/3a/AiqResult.cpp
new file mode 100644
index 000000000000..d67b1f1a54e1
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/AiqResult.cpp
@@ -0,0 +1,182 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "AiqResult"
+
+#include "iutils/Errors.h"
+#include "iutils/CameraLog.h"
+
+#include "AiqResult.h"
+
+namespace icamera {
+
+AiqResult::AiqResult(int cameraId) :
+    mCameraId(cameraId),
+    mTimestamp(0),
+    mSequence(-1),
+    mTuningMode(TUNING_MODE_VIDEO),
+    mAfDistanceDiopters(0.0f),
+    mSkip(false),
+    mFrameDuration(0),
+    mRollingShutter(0)
+{
+    LOG3A("@%s", __func__);
+
+    CLEAR(mGrid);
+    CLEAR(mFlashes);
+    CLEAR(mRGammaLut);
+    CLEAR(mGGammaLut);
+    CLEAR(mBGammaLut);
+    CLEAR(mToneMapLut);
+    CLEAR(mHueSectors);
+    CLEAR(mAdvancedCCM);
+    CLEAR(mIrWeightGridR);
+    CLEAR(mIrWeightGridG);
+    CLEAR(mIrWeightGridB);
+    CLEAR(mCustomControls);
+    CLEAR(mCustomControlsParams);
+    CLEAR(mSaResults);
+    CLEAR(mAwbResults);
+    CLEAR(mIrWeight);
+    CLEAR(mApertureControl);
+    CLEAR(mGbceResults);
+    CLEAR(mWeightGrid);
+    CLEAR(mPreferredAcm);
+    CLEAR(mPaResults);
+    CLEAR(mAeResults);
+    CLEAR(mAfResults);
+    CLEAR(mFocusRange);
+}
+
+AiqResult::~AiqResult()
+{
+    LOG3A("@%s", __func__);
+    deinit();
+}
+
+int AiqResult::init()
+{
+    LOG3A("@%s", __func__);
+
+    CLEAR(mAeResults);
+    CLEAR(mAfResults);
+    CLEAR(mAwbResults);
+    CLEAR(mGbceResults);
+    CLEAR(mSaResults);
+    CLEAR(mPaResults);
+
+    CLEAR(mExposureResults);
+    CLEAR(mWeightGrid);
+    CLEAR(mGrid);
+    CLEAR(mFlashes);
+    CLEAR(mGenericExposure);
+    CLEAR(mSensorExposure);
+    CLEAR(mApertureControl);
+    CLEAR(mPreferredAcm);
+    CLEAR(mIrWeight);
+
+    mAiqParam.reset();
+
+    /*AE results init */
+    mAeResults.num_exposures = 1;
+    mAeResults.exposures = mExposureResults;
+    mAeResults.aperture_control = &mApertureControl;
+    mAeResults.weight_grid = &mWeightGrid;
+    mAeResults.weight_grid->weights = mGrid;
+    mAeResults.flashes = mFlashes;
+    for (unsigned int i = 0; i< MAX_EXPOSURES_NUM; i++) {
+        mAeResults.exposures[i].exposure = &mGenericExposure[i];
+        mAeResults.exposures[i].sensor_exposure = &mSensorExposure[i];
+    }
+    /* GBCE results init */
+    mGbceResults.r_gamma_lut = mRGammaLut;
+    mGbceResults.g_gamma_lut = mGGammaLut;
+    mGbceResults.b_gamma_lut = mBGammaLut;
+    mGbceResults.tone_map_lut = mToneMapLut;
+
+    /* SA results init */
+    mSaResults.width = MAX_LSC_WIDTH;
+    mSaResults.height = MAX_LSC_HEIGHT;
+
+    for (int i = 0; i < MAX_BAYER_ORDER_NUM; i++) {
+        for (int j = 0; j < MAX_BAYER_ORDER_NUM; j++) {
+            mSaResults.lsc_grid[i][j] = new unsigned short[MAX_LSC_WIDTH * MAX_LSC_HEIGHT];
+            memset(mSaResults.lsc_grid[i][j], 0,
+                   sizeof(unsigned short) * MAX_LSC_WIDTH * MAX_LSC_HEIGHT);
+        }
+    }
+
+    /* PA results init */
+    mPaResults.ir_weight = &mIrWeight;
+
+    mPaResults.ir_weight->ir_weight_grid_R = mIrWeightGridR;
+    mPaResults.ir_weight->ir_weight_grid_G = mIrWeightGridG;
+    mPaResults.ir_weight->ir_weight_grid_B = mIrWeightGridB;
+
+    mPreferredAcm.hue_of_sectors = mHueSectors;
+    mPreferredAcm.advanced_color_conversion_matrices = mAdvancedCCM;
+
+    /* Custom Controls init */
+    mCustomControls.parameters = mCustomControlsParams;
+
+    return OK;
+}
+
+int AiqResult::deinit()
+{
+    LOG3A("@%s", __func__);
+
+    for (int i = 0; i < MAX_BAYER_ORDER_NUM; ++i) {
+        for (int j = 0; j < MAX_BAYER_ORDER_NUM; ++j) {
+            delete []  mSaResults.lsc_grid[i][j];
+            mSaResults.lsc_grid[i][j] = nullptr;
+        }
+    }
+
+    return OK;
+}
+
+AiqResult &AiqResult::operator=(const AiqResult &other)
+{
+    deepCopyAiqResult(other, this);
+    mSequence = other.mSequence;
+    mTimestamp = other.mTimestamp;
+    mTuningMode = other.mTuningMode;
+    mSkip = other.mSkip;
+    mCustomControls.count = other.mCustomControls.count;
+    for (int i = 0; i < mCustomControls.count; i++) {
+        mCustomControlsParams[i] = other.mCustomControlsParams[i];
+    }
+    mAiqParam = other.mAiqParam;
+    mFrameDuration = other.mFrameDuration;
+    mRollingShutter = other.mRollingShutter;
+
+    return *this;
+}
+
+int AiqResult::deepCopyAiqResult(const AiqResult &src, AiqResult *dst)
+{
+    int ret = AiqUtils::deepCopyAeResults(src.mAeResults, &dst->mAeResults);
+    ret |= AiqUtils::deepCopyAfResults(src.mAfResults, &dst->mAfResults);
+    ret |= AiqUtils::deepCopyAwbResults(src.mAwbResults, &dst->mAwbResults);
+    ret |= AiqUtils::deepCopyGbceResults(src.mGbceResults, &dst->mGbceResults);
+    ret |= AiqUtils::deepCopyPaResults(src.mPaResults, &dst->mPaResults, &dst->mPreferredAcm);
+    ret |= AiqUtils::deepCopySaResults(src.mSaResults, &dst->mSaResults);
+
+    return ret;
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/AiqResult.h b/camera/hal/intel/ipu6/src/3a/AiqResult.h
new file mode 100644
index 000000000000..e9e117e89218
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/AiqResult.h
@@ -0,0 +1,109 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "ia_aiq.h"
+#include "ia_ltm.h"
+#include "ia_isp_types.h"
+
+#include "AiqUtils.h"
+#include "AiqSetting.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+/**
+ * \class AiqResult
+ * The private structs are part of AE, AF, AWB, PA and SA results.
+ * They need to be separately introduced to store the contents of the results
+ * that the AIQ algorithms return as pointers.
+ * Then we can do deep copy of the results
+ */
+class AiqResult {
+
+public:
+    AiqResult(int cameraId);
+    ~AiqResult();
+
+    int init();
+    int deinit();
+
+    AiqResult& operator=(const AiqResult& other);
+
+private:
+    int deepCopyAiqResult(const AiqResult &src, AiqResult *dst);
+
+public:
+    int mCameraId;
+    unsigned long long mTimestamp;
+    long mSequence;
+    TuningMode mTuningMode;
+    float mAfDistanceDiopters;
+    bool mSkip;
+    camera_range_t mFocusRange;
+
+    ia_aiq_ae_results mAeResults;
+    ia_aiq_awb_results mAwbResults;
+    ia_aiq_af_results mAfResults;
+    ia_aiq_gbce_results mGbceResults;
+    ia_aiq_pa_results_v1 mPaResults;
+    ia_aiq_sa_results_v1 mSaResults;
+
+    ia_aiq_advanced_ccm_t mPreferredAcm;
+
+    ia_isp_custom_controls mCustomControls;
+
+    aiq_parameter_t mAiqParam;
+
+    int64_t mFrameDuration;   // us
+    int64_t mRollingShutter;  // us
+
+private:
+    /*!< ia_aiq_ae_results pointer contents */
+    ia_aiq_ae_exposure_result mExposureResults[MAX_EXPOSURES_NUM];
+    ia_aiq_aperture_control   mApertureControl;
+    ia_aiq_hist_weight_grid   mWeightGrid;
+    unsigned char mGrid[MAX_AE_GRID_SIZE];
+    ia_aiq_flash_parameters   mFlashes[NUM_FLASH_LEDS];
+
+    /*!< ia_aiq_ae_exposure_result pointer contents */
+    ia_aiq_exposure_parameters        mGenericExposure[MAX_EXPOSURES_NUM];
+    ia_aiq_exposure_sensor_parameters mSensorExposure[MAX_EXPOSURES_NUM];
+
+    /*!< ia_aiq_gbce results */
+    /* The actual size of this table can be calculated by running cold
+     * GBCE, it will provide those tables.
+     */
+    float mRGammaLut[MAX_GAMMA_LUT_SIZE];
+    float mGGammaLut[MAX_GAMMA_LUT_SIZE];
+    float mBGammaLut[MAX_GAMMA_LUT_SIZE];
+    float mToneMapLut[MAX_TONEMAP_LUT_SIZE];
+
+    /*!< ia_aiq_pa_results pointer content */
+    unsigned int mHueSectors[MAX_NUM_SECTORS];
+    float mAdvancedCCM[MAX_NUM_SECTORS][3][3];
+    ia_aiq_ir_weight_t mIrWeight;
+    unsigned short mIrWeightGridR[MAX_IR_WEIGHT_GRID_SIZE];
+    unsigned short mIrWeightGridG[MAX_IR_WEIGHT_GRID_SIZE];
+    unsigned short mIrWeightGridB[MAX_IR_WEIGHT_GRID_SIZE];
+
+    /*!< ia_isp_custom_controls pointer content */
+    float mCustomControlsParams[MAX_CUSTOM_CONTROLS_PARAM_SIZE];
+
+};
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/AiqResultStorage.cpp b/camera/hal/intel/ipu6/src/3a/AiqResultStorage.cpp
new file mode 100644
index 000000000000..5d98a639c3c4
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/AiqResultStorage.cpp
@@ -0,0 +1,286 @@
+/*
+ * Copyright (C) 2016-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "AiqResultStorage"
+
+#include "AiqResultStorage.h"
+
+#include "iutils/CameraLog.h"
+
+namespace icamera {
+
+std::map<int, AiqResultStorage*> AiqResultStorage::sInstances;
+Mutex AiqResultStorage::sLock;
+
+AiqResultStorage* AiqResultStorage::getInstance(int cameraId)
+{
+    AutoMutex lock(sLock);
+    return getInstanceLocked(cameraId);
+}
+
+void AiqResultStorage::releaseAiqResultStorage(int cameraId)
+{
+    AutoMutex lock(sLock);
+    AiqResultStorage* storage = getInstanceLocked(cameraId);
+    sInstances.erase(cameraId);
+    delete storage;
+}
+
+AiqResultStorage::AiqResultStorage(int cameraId) :
+    mCameraId(cameraId)
+{
+    LOG1("AiqResultStorage created for id:%d", mCameraId);
+
+    for (int i = 0; i < kStorageSize; i++) {
+        mAiqResults[i] = new AiqResult(mCameraId);
+        mAiqResults[i]->init();
+    }
+    // INTEL_DVS_S
+    for (int i = 0; i < kDvsStorageSize; i++) {
+        mDvsResults[i] = new DvsResult();
+    }
+    // INTEL_DVS_E
+    // LOCAL_TONEMAP_S
+    for (int i = 0; i < kLtmStorageSize; i++) {
+        mLtmResults[i] = new ltm_result_t;
+    }
+    // LOCAL_TONEMAP_E
+}
+
+AiqResultStorage::~AiqResultStorage()
+{
+    LOG1("AiqResultStorage released for id:%d", mCameraId);
+
+    for (int i = 0; i < kStorageSize; i++) {
+        delete mAiqResults[i];
+    }
+    // INTEL_DVS_S
+    for (int i = 0; i < kDvsStorageSize; i++) {
+        delete mDvsResults[i];
+    }
+    // INTEL_DVS_E
+    // LOCAL_TONEMAP_S
+    for (int i = 0; i < kLtmStorageSize; i++) {
+        delete mLtmResults[i];
+    }
+    // LOCAL_TONEMAP_E
+}
+
+// LOCAL_TONEMAP_S
+ltm_result_t* AiqResultStorage::acquireLtmResult()
+{
+    AutoWMutex rlock(mDataLock);
+
+    int index = mCurrentLtmIndex + 1;
+    index %= kLtmStorageSize;
+
+    mLtmResults[index]->sequence = -1;
+
+    return mLtmResults[index];
+}
+
+void AiqResultStorage::updateLtmResult(long sequence)
+{
+    AutoWMutex wlock(mDataLock);
+
+    mCurrentLtmIndex++;
+    mCurrentLtmIndex %= kLtmStorageSize;
+
+    mLtmResults[mCurrentLtmIndex]->sequence = sequence;
+}
+
+const ltm_result_t* AiqResultStorage::getLtmResult(long sequence)
+{
+    AutoRMutex rlock(mDataLock);
+
+    if (mCurrentLtmIndex == -1)
+        return nullptr;
+
+    // Sequence is -1 means to get the latest result
+    if (sequence == -1) {
+        return mLtmResults[mCurrentLtmIndex];
+    }
+
+    // Try to find the matched result
+    for (int i = 0; i < kLtmStorageSize; i++) {
+        int tmpIdx = (mCurrentLtmIndex + kLtmStorageSize - i) % kLtmStorageSize;
+        if (mLtmResults[tmpIdx]->sequence >= 0 && sequence >= mLtmResults[tmpIdx]->sequence) {
+            LOG2("%s, find the ltm result (expect: %ld actual: %ld)",
+                    __func__, sequence, mLtmResults[tmpIdx]->sequence);
+            return mLtmResults[tmpIdx];
+        }
+    }
+
+    return nullptr;
+}
+// LOCAL_TONEMAP_E
+
+// INTEL_DVS_S
+DvsResult* AiqResultStorage::acquireDvsResult()
+{
+    AutoWMutex rlock(mDataLock);
+
+    int index = mCurrentDvsIndex + 1;
+    index %= kDvsStorageSize;
+
+    mDvsResults[index]->mSequence = -1;
+
+    return mDvsResults[index];
+}
+
+void AiqResultStorage::updateDvsResult(long sequence)
+{
+    AutoWMutex wlock(mDataLock);
+
+    mCurrentDvsIndex++;
+    mCurrentDvsIndex %= kDvsStorageSize;
+
+    mDvsResults[mCurrentDvsIndex]->mSequence = sequence;
+}
+
+const DvsResult* AiqResultStorage::getDvsResult(long sequence)
+{
+    AutoRMutex rlock(mDataLock);
+
+    if (mCurrentDvsIndex == -1)
+        return nullptr;
+
+    CheckError(mDvsResults[mCurrentDvsIndex]->mSequence == -1, nullptr, "invalid sequence id -1");
+
+    if (sequence == -1)
+        return mDvsResults[mCurrentDvsIndex];
+
+    // Try to find the matched result
+    for (int i = 0; i < kDvsStorageSize; i++) {
+        int tmpIdx = (mCurrentDvsIndex + kDvsStorageSize - i) % kDvsStorageSize;
+        if (mDvsResults[tmpIdx]->mSequence >= 0 && sequence >= mDvsResults[tmpIdx]->mSequence) {
+            LOG2("%s, find the DVS result (expect: %ld actual: %ld)",
+                    __func__, sequence, mDvsResults[tmpIdx]->mSequence);
+            return mDvsResults[tmpIdx];
+        }
+    }
+
+    return nullptr;
+}
+// INTEL_DVS_E
+
+AiqStatistics* AiqResultStorage::acquireAiqStatistics()
+{
+    AutoWMutex rlock(mDataLock);
+
+    int index = (mCurrentAiqStatsIndex + 1) % kAiqStatsStorageSize;
+    if (mAiqStatistics[index].mInUse) {
+        // The "next" storage is still in use, skip it.
+        mCurrentAiqStatsIndex = index;
+        index = (mCurrentAiqStatsIndex + 1) % kAiqStatsStorageSize;
+    }
+
+    mAiqStatistics[index].mSequence = -1;
+
+    return &mAiqStatistics[index];
+}
+
+void AiqResultStorage::updateAiqStatistics(long sequence)
+{
+    AutoWMutex wlock(mDataLock);
+
+    mCurrentAiqStatsIndex++;
+    mCurrentAiqStatsIndex %= kAiqStatsStorageSize;
+
+    mAiqStatistics[mCurrentAiqStatsIndex].mSequence = sequence;
+}
+
+const AiqStatistics* AiqResultStorage::getAndLockAiqStatistics()
+{
+    AutoRMutex rlock(mDataLock);
+
+    if (mCurrentAiqStatsIndex == -1)
+        return nullptr;
+
+    CheckError(mAiqStatistics[mCurrentAiqStatsIndex].mSequence == -1,
+          nullptr, "Invalid sequence id -1 of stored aiq statistics");
+
+    mAiqStatistics[mCurrentAiqStatsIndex].mInUse = true;
+    return &mAiqStatistics[mCurrentAiqStatsIndex];
+}
+
+void AiqResultStorage::unLockAiqStatistics()
+{
+    AutoRMutex rlock(mDataLock);
+    for (int i = 0; i < kAiqStatsStorageSize; i++) {
+        mAiqStatistics[i].mInUse = false;
+    }
+}
+
+AiqResult* AiqResultStorage::acquireAiqResult()
+{
+    AutoWMutex rlock(mDataLock);
+
+    int index = mCurrentIndex + 1;
+    index %= kStorageSize;
+
+    mAiqResults[index]->mSequence = -1;
+
+    return mAiqResults[index];
+}
+
+void AiqResultStorage::updateAiqResult(long sequence)
+{
+    AutoWMutex wlock(mDataLock);
+
+    mCurrentIndex++;
+    mCurrentIndex %= kStorageSize;
+
+    mAiqResults[mCurrentIndex]->mSequence = sequence;
+}
+
+const AiqResult* AiqResultStorage::getAiqResult(long sequence)
+{
+    AutoRMutex rlock(mDataLock);
+
+    // Sequence id is -1 means user wants get the latest result.
+    if (sequence == -1) {
+        // If mCurrentIndex is -1, that means no result is saved to the storage yet,
+        // just return the first one in this case.
+        return mAiqResults[(mCurrentIndex == -1) ? 0 : mCurrentIndex];
+    }
+
+    for (int i = 0; i < kStorageSize; i++) {
+        // Search from the newest result
+        int tmpIdx = (mCurrentIndex + kStorageSize - i) % kStorageSize;
+        if (mAiqResults[tmpIdx]->mSequence >= 0 && sequence >= mAiqResults[tmpIdx]->mSequence) {
+            return mAiqResults[tmpIdx];
+        }
+    }
+
+    return nullptr;
+}
+
+/**
+ * Private function with no lock in it, must be called with lock protection
+ */
+AiqResultStorage* AiqResultStorage::getInstanceLocked(int cameraId)
+{
+    if (sInstances.find(cameraId) != sInstances.end()) {
+        return sInstances[cameraId];
+    }
+
+    sInstances[cameraId] = new AiqResultStorage(cameraId);
+    return sInstances[cameraId];
+}
+
+} //namespace icamera
+
diff --git a/camera/hal/intel/ipu6/src/3a/AiqResultStorage.h b/camera/hal/intel/ipu6/src/3a/AiqResultStorage.h
new file mode 100644
index 000000000000..432ecc1632aa
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/AiqResultStorage.h
@@ -0,0 +1,246 @@
+/*
+ * Copyright (C) 2016-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <map>
+
+#include "AiqResult.h"
+
+// LOCAL_TONEMAP_S
+#include "Ltm.h"
+// LOCAL_TONEMAP_E
+// INTEL_DVS_S
+#include "Dvs.h"
+// INTEL_DVS_E
+#include "AiqStatistics.h"
+
+#include "iutils/Utils.h"
+#include "iutils/Thread.h"
+#include "iutils/RWLock.h"
+
+namespace icamera {
+
+/**
+ * \class AiqResultStorage
+ *
+ * This class provides interfaces for setting and getting AiqResult, and a storage space
+ * which is able to contain at most `kStorageSize` AiqResults at same time.
+ *
+ * It's a singleton based on camera id, and its life cycle can be maintained by
+ * its static methods getInstance and releaseAiqResultStorage.
+ */
+class AiqResultStorage {
+public:
+    /**
+     * \brief Get internal instance for cameraId.
+     *
+     * param[in] int camera id: only one instance for one particular camera id.
+     *
+     * return the instance of AiqResultStorage for cameraId
+     */
+    static AiqResultStorage* getInstance(int cameraId);
+
+    /**
+     * \brief Release the static instance of AiqResultStorage for cameraId.
+     */
+    static void releaseAiqResultStorage(int cameraId);
+
+    /**
+     * \brief Acquire Aiq result.
+     *
+     * The function will return one Aiq result pointer which is kept by Aiq algo.
+     * The sequence id is set to -1 which indicates the Aiq result is invalid.
+     *
+     * return Aiq result pointer to be kept by Aiq algo.
+     */
+    AiqResult* acquireAiqResult();
+
+    /**
+     * \brief Update mCurrentIndex and set sequence id into internal storage.
+     */
+    void updateAiqResult(long sequence);
+
+    /**
+     * \brief Get the pointer of aiq result to internal storage by given sequence id.
+     *
+     * The function will return the internal pointer of AiqResult, the caller MUST use this
+     * pointer quickly, let's say less than 10ms. For any time-consuming operations, it's
+     * the caller's responsibility to do a deep-copy, otherwise the data in returned AiqResult
+     * may not be consistent.
+     *
+     * param[in] long sequence: specify which aiq result is needed.
+     *
+     * return 1. when sequence id is -1 or not provided, the lastest result will be returned.
+     *        2. when sequence id is larger than -1, the result with gaven sequence id will be returned.
+     *        3. if cannot find in result storage, it means either sequence id is too old and its
+     *           result was overrided, or the sequence id is too new, and its result has not been
+     *           saved into storage yet. For both cases, nullptr will be returned.
+     */
+    const AiqResult* getAiqResult(long sequence = -1);
+
+    // LOCAL_TONEMAP_S
+    /**
+     * \brief Acquire Ltm result.
+     *
+     * The function will return one Ltm result pointer which is kept by LTM algo.
+     * The sequence id is set to -1 which indicates the Ltm result is invalid.
+     *
+     * return Ltm result pointer to be kept by LTM algo.
+     */
+    ltm_result_t* acquireLtmResult();
+
+    /**
+     * \brief Update mCurrentLtmIndex and set sequence id in internal storage.
+     */
+    void updateLtmResult(long sequence);
+
+    /**
+     * \brief Get the pointer of ltm_result_t.
+     *
+     * The function will return the latest Ltm result.
+     *
+     * return the latest Ltm result.
+     */
+    const ltm_result_t* getLtmResult(long sequence = -1);
+    // LOCAL_TONEMAP_E
+
+    // INTEL_DVS_S
+    /**
+     * \brief Acquire Dvs result.
+     *
+     * The function will return one Dvs result pointer which is kept by Dvs algo.
+     * The sequence id is set to -1 which indicates the Dvs result is invalid.
+     *
+     * return Dvs result pointer to be kept by Dvs algo.
+     */
+    DvsResult* acquireDvsResult();
+
+    /**
+     * \brief Update mCurrentDvsIndex and set sequence id in internal storage.
+     */
+    void updateDvsResult(long sequence);
+
+    /**
+     * \brief Get the pointer of DvsResult to internal storage.
+     *
+     * The function will return the latest DVS result.
+     *
+     * param[in] long sequence: specify which aiq result is needed.
+     *
+     * return the latest dvs result if sequence is -1, otherwise return the result of sequence.
+     */
+    const DvsResult* getDvsResult(long sequence = -1);
+    // INTEL_DVS_E
+
+    /**
+     * \brief Acquire AIQ statistics.
+     *
+     * The function will return one AIQ statistics pointer which is kept by AIQ statistics decoder.
+     * The sequence id is set to -1 which indicates the AIQ statistics is invalid.
+     *
+     * return AIQ statistics pointer to be kept by AIQ statistics decoder..
+     */
+    AiqStatistics* acquireAiqStatistics();
+
+    /**
+     * \brief Update mCurrentAiqStatsIndex and set sequence id in internal storage.
+     */
+    void updateAiqStatistics(long sequence);
+
+    /**
+     * \brief Get the pointer of AIQ statistics to internal storage.
+     *
+     * The function will return the latest AIQ statistics, and set the mInUse flag to true.
+     *
+     * return the latest AIQ statistics.
+     */
+    const AiqStatistics* getAndLockAiqStatistics();
+
+    /**
+     * \brief Clear the mInUse flag of all the AIQ statitics in internal storage.
+     */
+    void unLockAiqStatistics();
+
+    /**
+     * DVS statistics storage.
+     * A pair of {pointer to ia_dvs_statistics, sequence} is stored.
+     * The function updateDvsStatistics() is called by PipeExecutor, while getDvsStatistics()
+     * called by Dvs. They are called in the same thread on PSys statistics available.
+     */
+    /**
+     * \brief Update the dvs statistics in internal storage.
+     */
+    void updateDvsStatistics(const DvsStatistics &dvsStats) { mDvsStatistics = dvsStats; }
+    /**
+     * \brief Get the pointer of dvs statistics to internal storage.
+     */
+    DvsStatistics* getDvsStatistics() { return &mDvsStatistics; }
+
+    /**
+     * LTM statistics storage.
+     * A pair of {pointer to ia_isp_bxt_hdr_yv_grid_t, sequence} is stored.
+     * The function updateLtmStatistics() is called by PipeExecutor, while getLtmStatistics()
+     * called by Ltm. They are called in the same thread on PSys statistics available.
+     */
+    /**
+     * \brief Update the ltm statistics in internal storage.
+     */
+    void updateLtmStatistics(const LtmStatistics &ltmStats) { mLtmStatistics = ltmStats; }
+    /**
+     * \brief Get the pointer of ltm statistics to internal storage.
+     */
+    LtmStatistics* getLtmStatistics() { return &mLtmStatistics; }
+
+private:
+    AiqResultStorage(int cameraId);
+    ~AiqResultStorage();
+
+    static AiqResultStorage* getInstanceLocked(int cameraId);
+
+private:
+    static std::map<int, AiqResultStorage*> sInstances;
+    // Guard for singleton creation.
+    static Mutex sLock;
+
+    int mCameraId;
+    RWLock mDataLock;   // lock for all the data storage below
+
+    static const int kStorageSize = MAX_SETTING_COUNT; // Should > MAX_BUFFER_COUNT + sensorLag
+    int mCurrentIndex = -1;
+    AiqResult* mAiqResults[kStorageSize];
+
+    // LOCAL_TONEMAP_S
+    static const int kLtmStorageSize = MAX_SETTING_COUNT; // Should > MAX_BUFFER_COUNT + ltmLag
+    int mCurrentLtmIndex = -1;
+    ltm_result_t* mLtmResults[kLtmStorageSize];
+    // LOCAL_TONEMAP_E
+
+    // INTEL_DVS_S
+    static const int kDvsStorageSize = MAX_SETTING_COUNT;
+    int mCurrentDvsIndex = -1;
+    DvsResult* mDvsResults[kDvsStorageSize];
+    // INTEL_DVS_E
+
+    static const int kAiqStatsStorageSize = 3; // Always use the latest, but may hold for long time
+    int mCurrentAiqStatsIndex = -1;
+    AiqStatistics mAiqStatistics[kAiqStatsStorageSize];
+
+    DvsStatistics mDvsStatistics;
+    LtmStatistics mLtmStatistics;
+};
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/3a/AiqSetting.cpp b/camera/hal/intel/ipu6/src/3a/AiqSetting.cpp
new file mode 100644
index 000000000000..35f793f27e31
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/AiqSetting.cpp
@@ -0,0 +1,379 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "AiqSetting"
+
+#include "iutils/Errors.h"
+#include "iutils/CameraLog.h"
+
+#include "AiqSetting.h"
+#include "PlatformData.h"
+#include "ParameterHelper.h"
+
+namespace icamera {
+
+AiqSetting::AiqSetting(int cameraId) :
+    mCameraId(cameraId),
+    mPipeSwitchFrameCount(0)
+{
+    LOG3A("@%s", __func__);
+}
+
+AiqSetting::~AiqSetting()
+{
+    LOG3A("@%s", __func__);
+}
+
+int AiqSetting::init(void)
+{
+    LOG3A("@%s", __func__);
+    AutoWMutex wlock(mParamLock);
+
+    mPipeSwitchFrameCount = 0;
+
+    mAiqParam.reset();
+
+    camera_info_t info = {};
+    PlatformData::getCameraInfo(mCameraId, info);
+    info.capability->getAeCompensationRange(mAiqParam.evRange);
+    info.capability->getAeCompensationStep(mAiqParam.evStep);
+
+    return OK;
+}
+
+int AiqSetting::deinit(void)
+{
+    LOG3A("@%s", __func__);
+    AutoWMutex wlock(mParamLock);
+
+    return OK;
+}
+
+int AiqSetting::configure(const stream_config_t *streamList)
+{
+    LOG3A("@%s", __func__);
+    AutoWMutex wlock(mParamLock);
+
+    camera_resolution_t resolution = {streamList->streams[0].width, streamList->streams[0].height};
+    for (int i = 0; i < streamList->num_streams; i++) {
+        if (streamList->streams[i].usage == CAMERA_STREAM_PREVIEW) {
+            resolution = {streamList->streams[i].width, streamList->streams[i].height};
+            break;
+        }
+    }
+
+    updateFrameUsage(streamList);
+
+    mAiqParam.tuningMode = TUNING_MODE_MAX;
+    mAiqParam.resolution = resolution;
+
+    mTuningModes.clear();
+    std::vector<ConfigMode> configModes;
+    PlatformData::getConfigModesByOperationMode(mCameraId, streamList->operation_mode, configModes);
+    for (auto cfg : configModes) {
+        TuningMode tuningMode;
+        if (PlatformData::getTuningModeByConfigMode(mCameraId, cfg, tuningMode) == OK) {
+            mTuningModes.push_back(tuningMode);
+        }
+    }
+    if (!mTuningModes.empty()) {
+        mAiqParam.tuningMode = mTuningModes[0];
+    }
+    LOG3A("%s, tuningMode %d, configMode %x", __func__, mAiqParam.tuningMode, configModes[0]);
+
+    return OK;
+}
+
+void AiqSetting::updateFrameUsage(const stream_config_t *streamList)
+{
+    bool preview = false, still = false, video = false;
+    for (int i = 0; i < streamList->num_streams; i++) {
+        if (streamList->streams[i].usage == CAMERA_STREAM_VIDEO_CAPTURE) {
+            video = true;
+        } else if (streamList->streams[i].usage == CAMERA_STREAM_STILL_CAPTURE) {
+            still = true;
+        } else if (streamList->streams[i].usage == CAMERA_STREAM_PREVIEW
+                   || streamList->streams[i].usage == CAMERA_STREAM_APP) {
+            preview = true;
+        }
+    }
+
+    mAiqParam.frameUsage = FRAME_USAGE_PREVIEW;
+    if (video) {
+        mAiqParam.frameUsage = FRAME_USAGE_VIDEO;
+    } else if (preview && still) {
+        mAiqParam.frameUsage = FRAME_USAGE_CONTINUOUS;
+    } else if (still) {
+        mAiqParam.frameUsage = FRAME_USAGE_STILL;
+    }
+}
+
+int AiqSetting::setParameters(const Parameters& params)
+{
+    LOG3A("@%s", __func__);
+    AutoWMutex wlock(mParamLock);
+
+    // Update AE related parameters
+    params.getAeMode(mAiqParam.aeMode);
+    params.getAeLock(mAiqParam.aeForceLock);
+    params.getExposureTime(mAiqParam.manualExpTimeUs);
+    params.getSensitivityGain(mAiqParam.manualGain);
+    params.getSensitivityIso(mAiqParam.manualIso);
+    params.getBlcAreaMode(mAiqParam.blcAreaMode);
+    params.getAeRegions(mAiqParam.aeRegions);
+    params.getAeConvergeSpeedMode(mAiqParam.aeConvergeSpeedMode);
+    params.getAeConvergeSpeed(mAiqParam.aeConvergeSpeed);
+    params.getRun3ACadence(mAiqParam.run3ACadence);
+
+    int ev = 0;
+    params.getAeCompensation(ev);
+    if (mAiqParam.evStep.denominator == 0) {
+        mAiqParam.evShift = 0.0;
+    } else {
+        ev = CLIP(ev, mAiqParam.evRange.max, mAiqParam.evRange.min);
+        mAiqParam.evShift = static_cast<float>(ev) *
+            mAiqParam.evStep.numerator / mAiqParam.evStep.denominator;
+    }
+
+    params.getFrameRate(mAiqParam.fps);
+    params.getFpsRange(mAiqParam.aeFpsRange);
+    params.getAntiBandingMode(mAiqParam.antibandingMode);
+    // Update AWB related parameters
+    params.getAwbMode(mAiqParam.awbMode);
+    params.getAwbLock(mAiqParam.awbForceLock);
+    params.getAwbCctRange(mAiqParam.cctRange);
+    params.getAwbGains(mAiqParam.awbManualGain);
+    params.getAwbWhitePoint(mAiqParam.whitePoint);
+    params.getAwbGainShift(mAiqParam.awbGainShift);
+    params.getColorTransform(mAiqParam.manualColorMatrix);
+    params.getColorGains(mAiqParam.manualColorGains);
+    params.getAwbConvergeSpeedMode(mAiqParam.awbConvergeSpeedMode);
+    params.getAwbConvergeSpeed(mAiqParam.awbConvergeSpeed);
+
+    // Update AF related parameters
+    params.getAfMode(mAiqParam.afMode);
+    params.getAfRegions(mAiqParam.afRegions);
+    params.getAfTrigger(mAiqParam.afTrigger);
+
+    params.getWeightGridMode(mAiqParam.weightGridMode);
+    params.getSceneMode(mAiqParam.sceneMode);
+
+    params.getAeDistributionPriority(mAiqParam.aeDistributionPriority);
+
+    params.getWdrLevel(mAiqParam.ltmStrength);
+
+    unsigned int length = sizeof(mAiqParam.customAicParam.data);
+    if (params.getCustomAicParam(mAiqParam.customAicParam.data, &length) == OK) {
+        mAiqParam.customAicParam.length = length;
+    }
+
+    params.getYuvColorRangeMode(mAiqParam.yuvColorRangeMode);
+
+    params.getExposureTimeRange(mAiqParam.exposureTimeRange);
+    params.getSensitivityGainRange(mAiqParam.sensitivityGainRange);
+
+    params.getVideoStabilizationMode(mAiqParam.videoStabilizationMode);
+    params.getLdcMode(mAiqParam.ldcMode);
+    params.getRscMode(mAiqParam.rscMode);
+    params.getFlipMode(mAiqParam.flipMode);
+    params.getDigitalZoomRatio(mAiqParam.digitalZoomRatio);
+
+    int ret = params.getMakernoteMode(mAiqParam.makernoteMode);
+    if (ret == NAME_NOT_FOUND) mAiqParam.makernoteMode = MAKERNOTE_MODE_OFF;
+
+    CameraMetadata meta;
+    ParameterHelper::copyMetadata(params, &meta);
+
+    uint32_t tag = CAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE;
+    icamera_metadata_entry entry = meta.find(tag);
+    if (entry.count == 1) {
+        mAiqParam.minFocusDistance = entry.data.f[0];
+    }
+
+    params.getFocusDistance(mAiqParam.focusDistance);
+    params.getShadingMode(mAiqParam.shadingMode);
+    params.getLensShadingMapMode(mAiqParam.lensShadingMapMode);
+    params.getLensInfoShadingMapSize(mAiqParam.lensShadingMapSize);
+
+    params.getTonemapMode(mAiqParam.tonemapMode);
+    params.getTonemapPresetCurve(mAiqParam.tonemapPresetCurve);
+    params.getTonemapGamma(mAiqParam.tonemapGamma);
+
+    uint8_t captureIntent = 0;
+    if (params.getCaptureIntent(captureIntent) == OK) {
+        switch (captureIntent) {
+        case CAMERA_CONTROL_CAPTUREINTENT_STILL_CAPTURE:
+            mAiqParam.frameUsage = FRAME_USAGE_STILL;
+            break;
+        case CAMERA_CONTROL_CAPTUREINTENT_VIDEO_RECORD:
+        case CAMERA_CONTROL_CAPTUREINTENT_VIDEO_SNAPSHOT:
+            mAiqParam.frameUsage = FRAME_USAGE_VIDEO;
+            break;
+        case CAMERA_CONTROL_CAPTUREINTENT_PREVIEW:
+            mAiqParam.frameUsage = FRAME_USAGE_PREVIEW;
+            break;
+        default:
+            mAiqParam.frameUsage = FRAME_USAGE_CONTINUOUS;
+            break;
+        }
+    }
+
+    mAiqParam.dump();
+
+    return OK;
+}
+
+int AiqSetting::getAiqParameter(aiq_parameter_t &param)
+{
+    LOG3A("@%s", __func__);
+    AutoRMutex rlock(mParamLock);
+
+    param = mAiqParam;
+    return OK;
+}
+
+void aiq_parameter_t::reset()
+{
+    frameUsage = FRAME_USAGE_VIDEO;
+    aeMode = AE_MODE_AUTO;
+    aeForceLock = false;
+    awbMode = AWB_MODE_AUTO;
+    awbForceLock = false;
+    afMode = AF_MODE_AUTO;
+    afTrigger = AF_TRIGGER_IDLE;
+    sceneMode = SCENE_MODE_AUTO;
+    manualExpTimeUs = -1;
+    manualGain = -1;
+    manualIso = 0;
+    evShift = 0;
+    evStep = {1, 3};
+    evRange = {-6, 6};
+    fps = 0;
+    aeFpsRange = { 0.0, 0.0 };
+    antibandingMode = ANTIBANDING_MODE_AUTO;
+    cctRange = { 0, 0 };
+    whitePoint = { 0, 0 };
+    awbManualGain = { 0, 0, 0 };
+    awbGainShift = { 0, 0, 0 };
+    CLEAR(manualColorMatrix);
+    CLEAR(manualColorGains);
+    aeRegions.clear();
+    blcAreaMode = BLC_AREA_MODE_OFF;
+    aeConvergeSpeedMode = CONVERGE_SPEED_MODE_AIQ;
+    awbConvergeSpeedMode = CONVERGE_SPEED_MODE_AIQ;
+    aeConvergeSpeed = CONVERGE_NORMAL;
+    awbConvergeSpeed = CONVERGE_NORMAL;
+    run3ACadence = 1;
+    ltmStrength = 0;
+    weightGridMode = WEIGHT_GRID_AUTO;
+    aeDistributionPriority = DISTRIBUTION_AUTO;
+    CLEAR(customAicParam);
+    yuvColorRangeMode = CAMERA_FULL_MODE_YUV_COLOR_RANGE;
+    exposureTimeRange.min = -1;
+    exposureTimeRange.max = -1;
+    sensitivityGainRange.min = -1;
+    sensitivityGainRange.max = -1;
+    videoStabilizationMode = VIDEO_STABILIZATION_MODE_OFF;
+    tuningMode = TUNING_MODE_MAX;
+    ldcMode = LDC_MODE_OFF;
+    rscMode = RSC_MODE_OFF;
+    flipMode = FLIP_MODE_NONE;
+    digitalZoomRatio = 1.0f;
+
+    lensPosition = 0;
+    lensMovementStartTimestamp = 0;
+    makernoteMode = MAKERNOTE_MODE_OFF;
+    minFocusDistance = 0.0f;
+    focusDistance = 0.0f;
+    shadingMode = SHADING_MODE_FAST;
+    lensShadingMapMode = LENS_SHADING_MAP_MODE_OFF;
+    lensShadingMapSize = {0, 0};
+    CLEAR(lensShadingMap);
+
+    tonemapMode = TONEMAP_MODE_FAST;
+    tonemapPresetCurve = TONEMAP_PRESET_CURVE_SRGB;
+    tonemapGamma = 0.0f;
+
+    CLEAR(resolution);
+}
+
+void aiq_parameter_t::dump()
+{
+    // Log only printed when 3a log enabled.
+    if (!Log::isDebugLevelEnable(CAMERA_DEBUG_LOG_AIQ)) return;
+
+    LOG3A("Application parameters:");
+    LOG3A("frame usage mode %d", frameUsage);
+    LOG3A("ae mode:%d, awb mode:%d, af mode:%d, scene mode:%d", aeMode, awbMode, afMode, sceneMode);
+    LOG3A("ae lock:%d, awb lock:%d, af trigger:%d", aeForceLock, awbForceLock, afTrigger);
+    LOG3A("EV:%f, manualExpTimeUs:%ld, manualGain:%f, manualIso %d",
+          evShift, manualExpTimeUs, manualGain, manualIso);
+    LOG3A("FPS:%f", fps);
+    LOG3A("FPS range:(%f-%f)", aeFpsRange.min, aeFpsRange.max);
+    LOG3A("Antibanding mode:%d", antibandingMode);
+    LOG3A("cctRange:(%f-%f)", cctRange.min, cctRange.max);
+    LOG3A("manual white point:(%d,%d)", whitePoint.x, whitePoint.y);
+    LOG3A("manual awb gain:(%d,%d,%d)", awbManualGain.r_gain, awbManualGain.g_gain, awbManualGain.b_gain);
+    LOG3A("manual awb gain shift:(%d,%d,%d)", awbGainShift.r_gain, awbGainShift.g_gain, awbGainShift.b_gain);
+    for (int i = 0; i < 3; i++) {
+        LOG3A("manual color matrix:  [%.3f %.3f %.3f]",
+            manualColorMatrix.color_transform[i][0],
+            manualColorMatrix.color_transform[i][1],
+            manualColorMatrix.color_transform[i][2]);
+    }
+    LOG3A("manual color gains in rggb:(%.3f,%.3f,%.3f,%.3f)",
+        manualColorGains.color_gains_rggb[0], manualColorGains.color_gains_rggb[1],
+        manualColorGains.color_gains_rggb[2], manualColorGains.color_gains_rggb[3]);
+    LOG3A("ae region size:%zu, blc area mode:%d", aeRegions.size(), blcAreaMode);
+    for (auto &region : aeRegions) {
+        LOG3A("ae region (%d, %d, %d, %d, %d)",
+            region.left, region.top, region.right, region.bottom, region.weight);
+    }
+    LOG3A("af region size:%zu", aeRegions.size());
+    for (auto &region : afRegions) {
+        LOG3A("af region (%d, %d, %d, %d, %d)",
+            region.left, region.top, region.right, region.bottom, region.weight);
+    }
+    LOG3A("manual focus distance: %f, min focus distance: %f", focusDistance, minFocusDistance);
+
+    LOG3A("ae converge speed mode:(%d) awb converge speed mode:(%d)", aeConvergeSpeedMode, awbConvergeSpeedMode);
+    LOG3A("ae converge speed:(%d) awb converge speed:(%d)", aeConvergeSpeed, awbConvergeSpeed);
+    LOG3A("custom AIC parameter length:%d", customAicParam.length);
+    if (customAicParam.length > 0) {
+        LOG3A("custom AIC parameter data:%s", customAicParam.data);
+    }
+    if (tuningMode != TUNING_MODE_MAX) {
+        LOG3A("camera mode:%d", tuningMode);
+    }
+    LOG3A("ltm strength:(%d)", ltmStrength);
+    LOG3A("weight grid mode:%d", weightGridMode);
+    LOG3A("AE Distribution Priority:%d", aeDistributionPriority);
+    LOG3A("Yuv Color Range Mode:%d", yuvColorRangeMode);
+    LOG3A("AE exposure time range, min %f, max %f", exposureTimeRange.min, exposureTimeRange.max);
+    LOG3A("AE sensitivity gain range, min %.2f, max %.2f", sensitivityGainRange.min, sensitivityGainRange.max);
+    LOG3A("DVS mode %d", videoStabilizationMode);
+
+    LOG3A("Focus position %d, start timestamp %llu", lensPosition, lensMovementStartTimestamp);
+    LOG3A("makernoteMode %d", makernoteMode);
+    LOG3A("shadingMode %d", shadingMode);
+    LOG3A("lensShadingMapMode %d", lensShadingMapMode);
+    LOG3A("lensShadingMapSize x:%d, y:%d", lensShadingMapSize.x, lensShadingMapSize.y);
+
+    LOG3A("tonemap mode %d, preset curve %d, gamma %f",
+          tonemapMode, tonemapPresetCurve, tonemapGamma);
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/AiqSetting.h b/camera/hal/intel/ipu6/src/3a/AiqSetting.h
new file mode 100644
index 000000000000..3e6ddf2ef3d0
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/AiqSetting.h
@@ -0,0 +1,155 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "iutils/Utils.h"
+#include "iutils/RWLock.h"
+#include "Parameters.h"
+
+#include "AiqUtils.h"
+
+namespace icamera {
+
+#define DEFAULT_LSC_GRID_SIZE (64 * 64)
+
+// Imaging algorithms that are supported
+typedef enum {
+    IMAGING_ALGO_NONE = 0,
+    IMAGING_ALGO_AE   = 1,
+    IMAGING_ALGO_AWB  = 1 << 1,
+    IMAGING_ALGO_AF   = 1 << 2,
+    IMAGING_ALGO_GBCE = 1 << 3,
+    IMAGING_ALGO_PA   = 1 << 4,
+    IMAGING_ALGO_SA   = 1 << 5
+} imaging_algorithm_t;
+
+typedef struct {
+    char data[MAX_CUSTOM_CONTROLS_PARAM_SIZE];
+    unsigned int length;
+} custom_aic_param_t;
+
+typedef enum {
+    FRAME_USAGE_PREVIEW,
+    FRAME_USAGE_VIDEO,
+    FRAME_USAGE_STILL,
+    FRAME_USAGE_CONTINUOUS,
+} frame_usage_mode_t;
+
+/*
+ * aiq related parameters
+ */
+struct aiq_parameter_t {
+    frame_usage_mode_t frameUsage;
+    camera_ae_mode_t aeMode;
+    bool aeForceLock;
+    camera_awb_mode_t awbMode;
+    bool awbForceLock;
+    camera_af_mode_t afMode;
+    camera_af_trigger_t afTrigger;
+    camera_scene_mode_t sceneMode;
+    int64_t manualExpTimeUs;
+    float manualGain;
+    int32_t manualIso;
+    float evShift;
+    float fps;
+    camera_range_t aeFpsRange;
+    camera_antibanding_mode_t antibandingMode;
+    camera_range_t cctRange;
+    camera_coordinate_t whitePoint;
+    camera_awb_gains_t awbManualGain;
+    camera_awb_gains_t awbGainShift;
+    camera_color_transform_t manualColorMatrix;
+    camera_color_gains_t manualColorGains;
+    camera_window_list_t aeRegions;
+    camera_window_list_t afRegions;
+    camera_blc_area_mode_t blcAreaMode;
+    camera_converge_speed_mode_t aeConvergeSpeedMode;
+    camera_converge_speed_mode_t awbConvergeSpeedMode;
+    camera_converge_speed_t aeConvergeSpeed;
+    camera_converge_speed_t awbConvergeSpeed;
+    int run3ACadence;
+    uint8_t ltmStrength;
+    camera_weight_grid_mode_t weightGridMode;
+    camera_ae_distribution_priority_t aeDistributionPriority;
+    custom_aic_param_t customAicParam;
+    camera_yuv_color_range_mode_t yuvColorRangeMode;
+    camera_range_t exposureTimeRange;
+    camera_range_t sensitivityGainRange;
+    camera_video_stabilization_mode_t videoStabilizationMode;
+    camera_resolution_t resolution;
+    camera_ldc_mode_t ldcMode;
+    camera_rsc_mode_t rscMode;
+    camera_flip_mode_t flipMode;
+    float digitalZoomRatio;
+    camera_range_t evRange;
+    camera_rational_t evStep;
+
+    TuningMode tuningMode;
+
+    int lensPosition;
+    unsigned long long lensMovementStartTimestamp;
+    camera_makernote_mode_t makernoteMode;
+    float minFocusDistance;
+    float focusDistance;
+    camera_shading_mode_t shadingMode;
+    camera_lens_shading_map_mode_type_t lensShadingMapMode;
+    camera_coordinate_t lensShadingMapSize;
+    float lensShadingMap[DEFAULT_LSC_GRID_SIZE * 4];
+
+    camera_tonemap_mode_t tonemapMode;
+    camera_tonemap_preset_curve_t tonemapPresetCurve;
+    float tonemapGamma;
+
+    aiq_parameter_t() { reset(); }
+    void reset();
+    void dump();
+};
+
+/*
+ * \class AiqSetting
+ * This class is used for setting parameters to other aiq class
+ * and return some useful status of aiq results
+ */
+class AiqSetting {
+
+public:
+    AiqSetting(int cameraId);
+    ~AiqSetting();
+
+    int init(void);
+    int deinit(void);
+    int configure(const stream_config_t *streamList);
+
+    int setParameters(const Parameters& params);
+
+    int getAiqParameter(aiq_parameter_t &param);
+
+private:
+    void updateFrameUsage(const stream_config_t *streamList);
+
+public:
+    int mCameraId;
+
+private:
+    std::vector<TuningMode> mTuningModes;
+    unsigned int mPipeSwitchFrameCount;
+    aiq_parameter_t mAiqParam;
+
+    RWLock mParamLock;
+};
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/AiqStatistics.cpp b/camera/hal/intel/ipu6/src/3a/AiqStatistics.cpp
new file mode 100644
index 000000000000..a5e55db45ed0
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/AiqStatistics.cpp
@@ -0,0 +1,147 @@
+/*
+ * Copyright (C) 2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "AiqStatistics"
+
+#include "iutils/Errors.h"
+#include "iutils/CameraLog.h"
+
+#include "AiqStatistics.h"
+
+namespace icamera {
+
+AiqStatistics::AiqStatistics()
+{
+    CLEAR(mRgbsGridArray);
+    CLEAR(mAfGridArray);
+}
+
+AiqStatistics::~AiqStatistics()
+{
+    for (unsigned int i = 0; i < MAX_EXPOSURES_NUM; i++) {
+        delete [] mRgbsGridArray[i].blocks_ptr;
+        mRgbsGridArray[i].blocks_ptr = nullptr;
+
+        delete [] mAfGridArray[i].filter_response_1;
+        mAfGridArray[i].filter_response_1 = nullptr;
+        delete [] mAfGridArray[i].filter_response_2;
+        mAfGridArray[i].filter_response_2 = nullptr;
+    }
+}
+
+AiqStatistics &AiqStatistics::operator=(const AiqStatistics &other)
+{
+    mSequence = other.mSequence;
+    mTimestamp = other.mTimestamp;
+    mTuningMode = other.mTuningMode;
+
+    const ia_aiq_rgbs_grid *rgbs_grid[MAX_EXPOSURES_NUM];
+    for (int i = 0; i < other.mExposureNum; i++) {
+        rgbs_grid[i] = &other.mRgbsGridArray[i];
+    }
+    saveRgbsGridData(rgbs_grid, other.mExposureNum);
+
+    saveAfGridData(&other.mAfGridArray[0]);
+
+    return *this;
+}
+
+#define GRID_SIZE_UNEQUAL(g1, g2) \
+        ((g1)->grid_width != (g2)->grid_width || (g1)->grid_height != (g2)->grid_height)
+
+int AiqStatistics::saveRgbsGridData(const ia_aiq_rgbs_grid* const *rgbsGrids, int exposureNum)
+{
+    CheckError(rgbsGrids == nullptr, BAD_VALUE, "Rgbs grid is null");
+
+    for (int i = 0; i < exposureNum; i++) {
+        int ret = copyRgbsGridData(rgbsGrids[i], &mRgbsGridArray[i]);
+        CheckError(ret != OK, ret, "save Rgbs grid %d failed ret %d", i, ret);
+    }
+    mExposureNum = exposureNum;
+    return OK;
+}
+
+int AiqStatistics::copyRgbsGridData(const ia_aiq_rgbs_grid *src, ia_aiq_rgbs_grid *dst)
+{
+    LOG3A("%s", __func__);
+    CheckError(src == nullptr || dst == nullptr, BAD_VALUE, "src or dst rgbs grid is null");
+
+    // Release the old memory if size changes.
+    if (GRID_SIZE_UNEQUAL(src, dst)) {
+        if (dst->blocks_ptr) delete [] dst->blocks_ptr;
+        CLEAR(*dst);
+    }
+
+    if (src->blocks_ptr == nullptr || src->grid_width == 0 || src->grid_height == 0) {
+        return OK;
+    }
+
+    size_t gridSize = src->grid_width * src->grid_height;
+    if (dst->blocks_ptr == nullptr) {
+        dst->blocks_ptr = new rgbs_grid_block[gridSize];
+    }
+
+    dst->grid_width = src->grid_width;
+    dst->grid_height = src->grid_height;
+    MEMCPY_S(dst->blocks_ptr, gridSize * sizeof(rgbs_grid_block),
+             src->blocks_ptr, gridSize * sizeof(rgbs_grid_block));
+
+    dst->shading_correction = src->shading_correction;
+
+    LOG3A("%s, grid size=[%dx%d]", __func__, dst->grid_width, dst->grid_height);
+    return OK;
+}
+
+int AiqStatistics::saveAfGridData(const ia_aiq_af_grid *afGrid)
+{
+    LOG3A("%s", __func__);
+    CheckError(afGrid == nullptr, BAD_VALUE, "AF grid is null");
+
+    // Release the old memory if size changes.
+    if (GRID_SIZE_UNEQUAL(afGrid, &mAfGridArray[0])) {
+        if (mAfGridArray[0].filter_response_1) delete [] mAfGridArray[0].filter_response_1;
+        if (mAfGridArray[0].filter_response_2) delete [] mAfGridArray[0].filter_response_2;
+        CLEAR(mAfGridArray);
+    }
+
+    if (afGrid->filter_response_1 == nullptr || afGrid->filter_response_2 == nullptr
+        || afGrid->grid_width == 0 || afGrid->grid_height == 0) {
+        return OK;
+    }
+
+    size_t gridSize = afGrid->grid_width * afGrid->grid_height;
+    if (mAfGridArray[0].filter_response_1 == nullptr) {
+        mAfGridArray[0].filter_response_1 = new int[gridSize];
+    }
+    if (mAfGridArray[0].filter_response_2 == nullptr) {
+        mAfGridArray[0].filter_response_2 = new int[gridSize];
+    }
+
+    mAfGridArray[0].grid_width = afGrid->grid_width;
+    mAfGridArray[0].grid_height = afGrid->grid_height;
+    mAfGridArray[0].block_width = afGrid->block_width;
+    mAfGridArray[0].block_height = afGrid->block_height;
+    MEMCPY_S(mAfGridArray[0].filter_response_1, gridSize * sizeof(int),
+             afGrid->filter_response_1, gridSize * sizeof(int));
+    MEMCPY_S(mAfGridArray[0].filter_response_2, gridSize * sizeof(int),
+             afGrid->filter_response_2, gridSize * sizeof(int));
+
+    LOG3A("%s, grid size=[%dx%d]", __func__, mAfGridArray[0].grid_width, mAfGridArray[0].grid_height);
+    return OK;
+}
+
+} /* namespace icamera */
+
diff --git a/camera/hal/intel/ipu6/src/3a/AiqStatistics.h b/camera/hal/intel/ipu6/src/3a/AiqStatistics.h
new file mode 100644
index 000000000000..da4a2ec34ef3
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/AiqStatistics.h
@@ -0,0 +1,58 @@
+/*
+ * Copyright (C) 2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "ia_aiq_types.h"
+#include "ia_isp_bxt_types.h"
+#include "ia_dvs_types.h"
+#include "ia_isp_bxt_statistics_types.h"
+
+#include "AiqUtils.h"
+
+namespace icamera {
+
+/*
+ * \class AiqStatistics
+ *
+ * This class is used to envelop AIQ statistics.
+ */
+class AiqStatistics {
+public:
+    AiqStatistics();
+    ~AiqStatistics();
+
+    AiqStatistics& operator=(const AiqStatistics& other);
+
+    int saveRgbsGridData(const ia_aiq_rgbs_grid* const *rgbsGrid, int exposureNum);
+    int saveAfGridData(const ia_aiq_af_grid *afGrid);
+
+private:
+    int copyRgbsGridData(const ia_aiq_rgbs_grid *src, ia_aiq_rgbs_grid *dst);
+
+public:
+    long mSequence = -1;
+    unsigned long long mTimestamp = 0;
+    TuningMode mTuningMode = TUNING_MODE_MAX;
+    bool mInUse = false;
+
+    int mExposureNum = 0;
+    ia_aiq_rgbs_grid mRgbsGridArray[MAX_EXPOSURES_NUM];
+    ia_aiq_af_grid mAfGridArray[MAX_EXPOSURES_NUM];
+};
+
+} /* namespace icamera */
+
diff --git a/camera/hal/intel/ipu6/src/3a/AiqUnit.cpp b/camera/hal/intel/ipu6/src/3a/AiqUnit.cpp
new file mode 100644
index 000000000000..9bc6cfe67c82
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/AiqUnit.cpp
@@ -0,0 +1,257 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "AiqUnit"
+
+#include "iutils/Errors.h"
+#include "iutils/CameraLog.h"
+
+#include "AiqUnit.h"
+
+namespace icamera {
+
+AiqUnit::AiqUnit(int cameraId, SensorHwCtrl *sensorHw, LensHw *lensHw) :
+    mCameraId(cameraId),
+    // LOCAL_TONEMAP_S
+    mLtm(nullptr),
+    // LOCAL_TONEMAP_E
+    mAiqUnitState(AIQ_UNIT_NOT_INIT)
+{
+    LOG1("@%s mCameraId = %d", __func__, mCameraId);
+
+    mAiqSetting = new AiqSetting(cameraId);
+
+    mAiqEngine = new AiqEngine(cameraId, sensorHw, lensHw, mAiqSetting);
+
+    // INTEL_DVS_S
+    mDvs = new Dvs(cameraId, mAiqSetting);
+    // INTEL_DVS_E
+    // LOCAL_TONEMAP_S
+    if (PlatformData::isLtmEnabled(mCameraId)) {
+        mLtm = new Ltm(cameraId);
+    }
+    // LOCAL_TONEMAP_E
+}
+
+AiqUnit::~AiqUnit()
+{
+    LOG1("@%s mCameraId = %d", __func__, mCameraId);
+
+    if (mAiqUnitState == AIQ_UNIT_START) {
+        stop();
+    }
+
+    if (mAiqUnitState == AIQ_UNIT_INIT) {
+        deinit();
+    }
+
+    // LOCAL_TONEMAP_S
+    delete mLtm;
+    // LOCAL_TONEMAP_E
+    // INTEL_DVS_S
+    delete mDvs;
+    // INTEL_DVS_E
+    delete mAiqEngine;
+    delete mAiqSetting;
+}
+
+int AiqUnit::init()
+{
+    AutoMutex l(mAiqUnitLock);
+    LOG1("@%s mCameraId = %d", __func__, mCameraId);
+
+    int ret = mAiqSetting->init();
+    if (ret != OK) {
+        mAiqSetting->deinit();
+        return ret;
+    }
+
+    if (mAiqUnitState == AIQ_UNIT_NOT_INIT) {
+        ret = mAiqEngine->init();
+        if (ret != OK) {
+            mAiqEngine->deinit();
+            return ret;
+        }
+
+        // INTEL_DVS_S
+        mDvs->init();
+        // INTEL_DVS_E
+        // LOCAL_TONEMAP_S
+        if (mLtm) {
+            mLtm->init();
+        }
+        // LOCAL_TONEMAP_E
+    }
+
+    mAiqUnitState = AIQ_UNIT_INIT;
+
+    return OK;
+}
+
+int AiqUnit::deinit()
+{
+    AutoMutex l(mAiqUnitLock);
+    LOG1("@%s mCameraId = %d", __func__, mCameraId);
+
+    // LOCAL_TONEMAP_S
+    if (mLtm) {
+        mLtm->deinit();
+    }
+    // LOCAL_TONEMAP_E
+    // INTEL_DVS_S
+    mDvs->deinit();
+    // INTEL_DVS_E
+    mAiqEngine->deinit();
+
+    mAiqSetting->deinit();
+
+    mAiqUnitState = AIQ_UNIT_NOT_INIT;
+
+    return OK;
+}
+
+int AiqUnit::configure(const stream_config_t *streamList)
+{
+    CheckError(streamList == nullptr, BAD_VALUE, "streamList is nullptr");
+
+    AutoMutex l(mAiqUnitLock);
+    LOG1("@%s mCameraId = %d", __func__, mCameraId);
+
+    if (mAiqUnitState != AIQ_UNIT_INIT && mAiqUnitState != AIQ_UNIT_STOP) {
+        LOGW("%s: configure in wrong state: %d", __func__, mAiqUnitState);
+        return BAD_VALUE;
+    }
+
+    std::vector<ConfigMode> configModes;
+    PlatformData::getConfigModesByOperationMode(mCameraId, streamList->operation_mode, configModes);
+
+    int ret = mAiqSetting->configure(streamList);
+    CheckError(ret != OK, ret, "configure AIQ settings error: %d", ret);
+
+    ret = mAiqEngine->configure(configModes);
+    CheckError(ret != OK, ret, "configure AIQ engine error: %d", ret);
+    // INTEL_DVS_S
+    ret = mDvs->configure(configModes);
+    CheckError(ret != OK, ret, "configure DVS engine error: %d", ret);
+    // INTEL_DVS_E
+    // LOCAL_TONEMAP_S
+    if (mLtm) {
+        ret = mLtm->configure(configModes);
+        CheckError(ret != OK, ret, "configure LTM engine error: %d", ret);
+    }
+    // LOCAL_TONEMAP_E
+
+    mAiqUnitState = AIQ_UNIT_CONFIGURED;
+    return OK;
+}
+
+int AiqUnit::start()
+{
+    AutoMutex l(mAiqUnitLock);
+    LOG1("@%s mCameraId = %d", __func__, mCameraId);
+
+    if (mAiqUnitState != AIQ_UNIT_CONFIGURED && mAiqUnitState != AIQ_UNIT_STOP) {
+        LOGW("%s: configure in wrong state: %d", __func__, mAiqUnitState);
+        return BAD_VALUE;
+    }
+
+    // LOCAL_TONEMAP_S
+    if (mLtm) {
+        mLtm->start();
+    }
+    // LOCAL_TONEMAP_E
+    int ret = mAiqEngine->startEngine();
+    if (ret == OK) {
+        mAiqUnitState = AIQ_UNIT_START;
+    }
+
+    return OK;
+}
+
+int AiqUnit::stop()
+{
+    AutoMutex l(mAiqUnitLock);
+    LOG1("@%s mCameraId = %d", __func__, mCameraId);
+
+    if (mAiqUnitState == AIQ_UNIT_START) {
+        mAiqEngine->stopEngine();
+        // LOCAL_TONEMAP_S
+        if (mLtm) {
+            mLtm->stop();
+        }
+        // LOCAL_TONEMAP_E
+    }
+
+    mAiqUnitState = AIQ_UNIT_STOP;
+
+    return OK;
+}
+
+int AiqUnit::run3A(long *settingSequence)
+{
+    AutoMutex l(mAiqUnitLock);
+    TRACE_LOG_PROCESS("AiqUnit", "run3A");
+
+    if (settingSequence)
+       *settingSequence = -1;
+
+    if (mAiqUnitState != AIQ_UNIT_START) {
+        LOGW("%s: AIQ is not started: %d", __func__, mAiqUnitState);
+        return BAD_VALUE;
+    }
+
+    int ret = mAiqEngine->run3A(settingSequence);
+    CheckError(ret != OK, ret, "run 3A failed.");
+
+    return OK;
+}
+
+std::vector<EventListener*> AiqUnit::getSofEventListener()
+{
+    AutoMutex l(mAiqUnitLock);
+    LOG1("@%s mCameraId = %d", __func__, mCameraId);
+
+    std::vector<EventListener*> eventListenerList;
+    eventListenerList.push_back(mAiqEngine->getSofEventListener());
+    return eventListenerList;
+}
+
+std::vector<EventListener*> AiqUnit::getStatsEventListener()
+{
+    AutoMutex l(mAiqUnitLock);
+    LOG1("@%s mCameraId = %d", __func__, mCameraId);
+
+    std::vector<EventListener*> eventListenerList;
+    // LOCAL_TONEMAP_S
+    if (mLtm) {
+        eventListenerList.push_back(mLtm);
+    }
+    // LOCAL_TONEMAP_E
+    // INTEL_DVS_S
+    eventListenerList.push_back(mDvs);
+    // INTEL_DVS_E
+    return eventListenerList;
+}
+
+int AiqUnit::setParameters(const Parameters &params)
+{
+    AutoMutex l(mAiqUnitLock);
+    LOG1("@%s mCameraId = %d", __func__, mCameraId);
+
+    return mAiqSetting->setParameters(params);
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/AiqUnit.h b/camera/hal/intel/ipu6/src/3a/AiqUnit.h
new file mode 100644
index 000000000000..7584e48581f0
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/AiqUnit.h
@@ -0,0 +1,155 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "CameraEvent.h"
+
+#include "AiqSetting.h"
+#include "AiqEngine.h"
+// INTEL_DVS_S
+#include "Dvs.h"
+// INTEL_DVS_E
+// LOCAL_TONEMAP_S
+#include "Ltm.h"
+// LOCAL_TONEMAP_E
+
+namespace icamera {
+
+class SensorHwCtrl;
+class LensHw;
+
+/*
+ * \class AiqUnit
+ * This class is used for upper layer to control 3a engine.
+ */
+
+class AiqUnitBase{
+
+public:
+    AiqUnitBase() {}
+    virtual ~AiqUnitBase() {}
+
+    virtual int init() { return OK; }
+    virtual int deinit() { return OK; }
+    virtual int configure(const stream_config_t * /*streamList*/) { return OK; }
+    virtual int start() { return OK; }
+    virtual int stop() { return OK; }
+    virtual int run3A(long * /*settingSequence*/)  { return OK; }
+
+    virtual std::vector<EventListener*> getSofEventListener()
+    {
+        std::vector<EventListener*> eventListenerList;
+        return eventListenerList;
+    }
+    virtual std::vector<EventListener*> getStatsEventListener()
+    {
+        std::vector<EventListener*> eventListenerList;
+        return eventListenerList;
+    }
+
+    virtual int setParameters(const Parameters & /*params*/) { return OK; }
+
+private:
+    DISALLOW_COPY_AND_ASSIGN(AiqUnitBase);
+
+};
+
+class AiqUnit : public AiqUnitBase {
+
+public:
+    AiqUnit(int cameraId, SensorHwCtrl *sensorHw, LensHw *lensHw);
+    ~AiqUnit();
+
+    /**
+     * \brief Init 3a related objects
+     */
+    int init();
+
+    /**
+     * \brief Deinit 3a related objects
+     */
+    int deinit();
+
+    /**
+     * \brief configure 3a engine with stream configuration
+     */
+    int configure(const stream_config_t *streamList);
+
+    /**
+     * \brief Start 3a Engine
+     */
+    int start();
+
+    /**
+     * \brief Stop 3a Engine
+     */
+    int stop();
+
+    /**
+     * \brief Run 3a to get new 3a settings.
+     * Return 0 if the operation succeeds, and output settingSequence to
+     * indicate the frame that settings are applied.
+     * settingSequence -1 means uncertain frame for this settings.
+     */
+    int run3A(long *settingSequence);
+
+    /**
+     * \brief Get software EventListener
+     */
+    std::vector<EventListener*> getSofEventListener();
+
+    /**
+     * \brief Get stats EventListener
+     */
+    std::vector<EventListener*> getStatsEventListener();
+
+    /**
+     * \brief Set 3A Parameters
+     *
+     * \param params: the Parameters update to 3A
+     */
+    int setParameters(const Parameters &params);
+
+private:
+    DISALLOW_COPY_AND_ASSIGN(AiqUnit);
+
+private:
+    int mCameraId;
+    // LOCAL_TONEMAP_S
+    Ltm *mLtm;
+    // LOCAL_TONEMAP_E
+    enum AiqUnitState {
+        AIQ_UNIT_NOT_INIT = 0,
+        AIQ_UNIT_INIT,
+        AIQ_UNIT_CONFIGURED,
+        AIQ_UNIT_START,
+        AIQ_UNIT_STOP,
+        AIQ_UNIT_MAX
+    } mAiqUnitState;
+
+    // INTEL_DVS_S
+    Dvs *mDvs;
+    // INTEL_DVS_E
+    AiqEngine *mAiqEngine;
+    AiqSetting *mAiqSetting;
+
+    // Guard for AiqUnit public API.
+    Mutex mAiqUnitLock;
+};
+
+} /* namespace icamera */
+
diff --git a/camera/hal/intel/ipu6/src/3a/AiqUtils.cpp b/camera/hal/intel/ipu6/src/3a/AiqUtils.cpp
new file mode 100644
index 000000000000..b55692db6723
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/AiqUtils.cpp
@@ -0,0 +1,660 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "AiqUtils"
+
+#include <math.h>
+
+#include "iutils/Utils.h"
+#include "iutils/Errors.h"
+#include "iutils/CameraLog.h"
+#include "AiqUtils.h"
+#include "AiqSetting.h"
+
+namespace icamera {
+
+#define TONEMAP_MIN_POINTS 64
+
+int AiqUtils::dumpAeResults(const ia_aiq_ae_results &aeResult)
+{
+    if (!Log::isDebugLevelEnable(CAMERA_DEBUG_LOG_AIQ)) {
+        return OK;
+    }
+
+    LOG3A("@%s", __func__);
+
+    if (aeResult.exposures) {
+        for (unsigned int i = 0; i < aeResult.num_exposures; i++) {
+            if (aeResult.exposures[i].exposure) {
+                LOG3A("AE exp[%d] ag %f dg %f Fn %f time %dus total %d filter[%s] iso %d", i,
+                      aeResult.exposures[i].exposure->analog_gain,
+                      aeResult.exposures[i].exposure->digital_gain,
+                      aeResult.exposures[i].exposure->aperture_fn,
+                      aeResult.exposures[i].exposure->exposure_time_us,
+                      aeResult.exposures[i].exposure->total_target_exposure,
+                      aeResult.exposures[i].exposure->nd_filter_enabled? "YES": "NO",
+                      aeResult.exposures[i].exposure->iso);
+            }
+            if (aeResult.exposures[i].sensor_exposure) {
+                LOG3A("AE sensor exp[%d] result ag %d dg %d coarse: %d fine: %d llp:%d fll:%d", i,
+                      aeResult.exposures[i].sensor_exposure->analog_gain_code_global,
+                      aeResult.exposures[i].sensor_exposure->digital_gain_global,
+                      aeResult.exposures[i].sensor_exposure->coarse_integration_time,
+                      aeResult.exposures[i].sensor_exposure->fine_integration_time,
+                      aeResult.exposures[i].sensor_exposure->line_length_pixels,
+                      aeResult.exposures[i].sensor_exposure->frame_length_lines);
+            }
+            LOG3A(" AE Converged : %s", aeResult.exposures[i].converged ? "YES" : "NO");
+        }
+    } else {
+        LOGE("nullptr in StatsInputParams->frame_ae_parameters->exposures");
+    }
+    LOG3A("AE bracket mode = %d %s", aeResult.multiframe,
+          aeResult.multiframe == ia_aiq_bracket_mode_ull ? "ULL" : "none-ULL");
+
+    if (aeResult.weight_grid &&
+        aeResult.weight_grid->width != 0 &&
+        aeResult.weight_grid->height != 0) {
+        LOG3A("AE weight grid [%dx%d]", aeResult.weight_grid->width, aeResult.weight_grid->height);
+        if (aeResult.weight_grid->weights) {
+            for (int i = 0; i < 5 && i < aeResult.weight_grid->height; i++) {
+                LOG3A("AE weight_grid[%d] = %d ", aeResult.weight_grid->width/2,
+                      aeResult.weight_grid->weights[aeResult.weight_grid->width/2]);
+            }
+        }
+    }
+
+    if (aeResult.aperture_control) {
+        LOG3A("AE aperture fn = %f, iris command = %d, code = %d",
+              aeResult.aperture_control->aperture_fn,
+              aeResult.aperture_control->dc_iris_command,
+              aeResult.aperture_control->code);
+    }
+
+    return OK;
+}
+
+int AiqUtils::dumpAfResults(const ia_aiq_af_results &afResult)
+{
+    if (!Log::isDebugLevelEnable(CAMERA_DEBUG_LOG_AIQ)) {
+        return OK;
+    }
+
+    LOG3A("@%s", __func__);
+
+    LOG3A("AF results current_focus_distance %d final_position_reached %s",
+          afResult.current_focus_distance,
+          afResult.final_lens_position_reached ? "TRUE":"FALSE");
+    LOG3A("AF results driver_action %d, next_lens_position %d",
+          afResult.lens_driver_action,
+          afResult.next_lens_position);
+    LOG3A("AF results use_af_assist %s",
+          afResult.use_af_assist? "TRUE":"FALSE");
+
+    switch (afResult.status) {
+    case ia_aiq_af_status_local_search:
+        LOG3A("AF result state _local_search");
+        break;
+    case ia_aiq_af_status_extended_search:
+        LOG3A("AF result state extended_search");
+        break;
+    case ia_aiq_af_status_success:
+        LOG3A("AF state success");
+        break;
+    case ia_aiq_af_status_fail:
+        LOG3A("AF state fail");
+        break;
+    case ia_aiq_af_status_idle:
+    default:
+        LOG3A("AF state idle");
+    }
+
+    return OK;
+}
+
+int AiqUtils::dumpAwbResults(const ia_aiq_awb_results &awbResult)
+{
+    if (!Log::isDebugLevelEnable(CAMERA_DEBUG_LOG_AIQ)) {
+        return OK;
+    }
+
+    LOG3A("@%s", __func__);
+
+    LOG3A("AWB result: accurate_r/g %f, accurate_b/g %f final_r/g %f final_b/g %f",
+          awbResult.accurate_r_per_g,
+          awbResult.accurate_b_per_g,
+          awbResult.final_r_per_g,
+          awbResult.final_b_per_g);
+    LOG3A("AWB result: cct_estimate %d, distance_from_convergence %f",
+          awbResult.cct_estimate,
+          awbResult.distance_from_convergence);
+
+    return OK;
+}
+
+int AiqUtils::deepCopyAeResults(const ia_aiq_ae_results& src, ia_aiq_ae_results* dst)
+{
+    LOG3A("@%s", __func__);
+    dumpAeResults(src);
+
+    /**
+     * lets check that all the pointers are there
+     * in the source and in the destination
+     */
+    CheckError(!dst||!dst->exposures||!dst->flashes||!dst->weight_grid||!dst->weight_grid->weights
+        ,BAD_VALUE ,"Failed to deep copy AE result- invalid destination");
+
+    CheckError(!src.exposures||!src.flashes||!src.weight_grid||!src.weight_grid->weights
+        ,BAD_VALUE ,"Failed to deep copy AE result- invalid source");
+
+    dst->lux_level_estimate = src.lux_level_estimate;
+    dst->flicker_reduction_mode = src.flicker_reduction_mode;
+    dst->multiframe = src.multiframe;
+    dst->num_flashes = src.num_flashes;
+    dst->num_exposures = src.num_exposures;
+    if (src.aperture_control) {
+        *dst->aperture_control = *src.aperture_control;
+    }
+    for (unsigned int i = 0; i < dst->num_exposures; i++)
+    {
+        dst->exposures[i].converged = src.exposures[i].converged;
+        dst->exposures[i].distance_from_convergence = src.exposures[i].distance_from_convergence;
+        dst->exposures[i].exposure_index = src.exposures[i].exposure_index;
+        if (src.exposures[i].exposure) {
+            *dst->exposures[i].exposure = *src.exposures[i].exposure;
+        }
+        if (src.exposures[i].sensor_exposure) {
+            *dst->exposures[i].sensor_exposure = *src.exposures[i].sensor_exposure;
+        }
+    }
+
+    // Copy weight grid
+    dst->weight_grid->width = src.weight_grid->width;
+    dst->weight_grid->height = src.weight_grid->height;
+
+    unsigned int gridElements  = src.weight_grid->width *
+                                 src.weight_grid->height;
+    gridElements = CLIP(gridElements, MAX_AE_GRID_SIZE, 1);
+    MEMCPY_S(dst->weight_grid->weights, gridElements*sizeof(char),
+             src.weight_grid->weights, gridElements*sizeof(char));
+
+    // Copy the flash info structure
+    MEMCPY_S(dst->flashes, NUM_FLASH_LEDS*sizeof(ia_aiq_flash_parameters),
+             src.flashes, NUM_FLASH_LEDS*sizeof(ia_aiq_flash_parameters));
+
+    return OK;
+}
+
+int AiqUtils::deepCopyAfResults(const ia_aiq_af_results& src, ia_aiq_af_results* dst)
+{
+    LOG3A("@%s", __func__);
+    dumpAfResults(src);
+
+    CheckError(!dst, BAD_VALUE, "Failed to deep copy Af result- invalid destination or Source");
+
+    MEMCPY_S(dst, sizeof(ia_aiq_af_results), &src, sizeof(ia_aiq_af_results));
+    return OK;
+}
+
+int AiqUtils::deepCopyAwbResults(const ia_aiq_awb_results& src, ia_aiq_awb_results* dst)
+{
+    LOG3A("@%s", __func__);
+    dumpAwbResults(src);
+
+    CheckError(!dst, BAD_VALUE, "Failed to deep copy Awb result- invalid destination or Source");
+
+    MEMCPY_S(dst, sizeof(ia_aiq_awb_results), &src, sizeof(ia_aiq_awb_results));
+    return OK;
+}
+
+int AiqUtils::deepCopyGbceResults(const ia_aiq_gbce_results& src, ia_aiq_gbce_results* dst)
+{
+    LOG3A("%s", __func__);
+
+    CheckError(!dst||!dst->r_gamma_lut||!dst->g_gamma_lut||!dst->b_gamma_lut||!dst->tone_map_lut
+        ,BAD_VALUE ,"Failed to deep copy GBCE result- invalid destination");
+    CheckError(!src.r_gamma_lut||!src.g_gamma_lut||!src.b_gamma_lut
+        ,BAD_VALUE ,"Failed to deep copy GBCE result- invalid source");
+
+    MEMCPY_S(dst->r_gamma_lut, src.gamma_lut_size*sizeof(float),
+             src.r_gamma_lut, src.gamma_lut_size*sizeof(float));
+
+    MEMCPY_S(dst->g_gamma_lut, src.gamma_lut_size*sizeof(float),
+             src.g_gamma_lut, src.gamma_lut_size*sizeof(float));
+
+    MEMCPY_S(dst->b_gamma_lut, src.gamma_lut_size*sizeof(float),
+             src.b_gamma_lut, src.gamma_lut_size*sizeof(float));
+
+    dst->gamma_lut_size = src.gamma_lut_size;
+
+    // Copy tone mapping table
+    if (src.tone_map_lut != nullptr)
+    {
+        MEMCPY_S(dst->tone_map_lut, src.tone_map_lut_size * sizeof(float),
+                 src.tone_map_lut, src.tone_map_lut_size * sizeof(float));
+
+    }
+    dst->tone_map_lut_size = src.tone_map_lut_size; // zero indicates GBCE is ineffective.
+
+    return OK;
+}
+
+int AiqUtils::deepCopyPaResults(const ia_aiq_pa_results_v1& src, ia_aiq_pa_results_v1* dst,
+                                ia_aiq_advanced_ccm_t* preferredAcm)
+{
+    LOG3A("%s", __func__);
+
+    CheckError(!dst, BAD_VALUE ,"Failed to deep copy PA result- invalid destination");
+
+    MEMCPY_S(dst->color_conversion_matrix, sizeof(dst->color_conversion_matrix),
+             src.color_conversion_matrix, sizeof(src.color_conversion_matrix));
+    for (unsigned int i = 0; i < 4; i++)
+        for (unsigned int j = 0; j < 4; j++)
+            dst->black_level_4x4[i][j] = src.black_level_4x4[i][j];
+    dst->color_gains = src.color_gains;
+    dst->saturation_factor = src.saturation_factor;
+    dst->brightness_level = src.brightness_level;
+
+    if (src.ir_weight) {
+        unsigned long int irSize = src.ir_weight->width * src.ir_weight->height;
+        if (irSize) {
+            LOG3A("%s irSize = %ld", __func__, irSize);
+            MEMCPY_S(dst->ir_weight->ir_weight_grid_R, irSize * sizeof(unsigned short),
+                     src.ir_weight->ir_weight_grid_R, irSize * sizeof(unsigned short));
+            MEMCPY_S(dst->ir_weight->ir_weight_grid_G, irSize * sizeof(unsigned short),
+                     src.ir_weight->ir_weight_grid_G, irSize * sizeof(unsigned short));
+            MEMCPY_S(dst->ir_weight->ir_weight_grid_B, irSize * sizeof(unsigned short),
+                     src.ir_weight->ir_weight_grid_B, irSize * sizeof(unsigned short));
+            dst->ir_weight->width = src.ir_weight->width;
+            dst->ir_weight->height = src.ir_weight->height;
+        }
+    }
+
+    if (src.preferred_acm && src.preferred_acm->sector_count) {
+        dst->preferred_acm = preferredAcm;
+
+        LOG3A("%s advanced ccm sector count = %d", __func__, src.preferred_acm->sector_count);
+        MEMCPY_S(dst->preferred_acm->hue_of_sectors,
+                 src.preferred_acm->sector_count * sizeof(unsigned int),
+                 src.preferred_acm->hue_of_sectors,
+                 src.preferred_acm->sector_count * sizeof(unsigned int));
+        MEMCPY_S(dst->preferred_acm->advanced_color_conversion_matrices,
+                 src.preferred_acm->sector_count * sizeof(float[3][3]),
+                 src.preferred_acm->advanced_color_conversion_matrices,
+                 src.preferred_acm->sector_count  * sizeof(float[3][3]));
+        dst->preferred_acm->sector_count = src.preferred_acm->sector_count;
+    } else {
+        dst->preferred_acm = nullptr;
+    }
+
+    /* current linearization.size is zero, set related pointers to nullptr */
+    dst->linearization.r = nullptr;
+    dst->linearization.gr = nullptr;
+    dst->linearization.gb = nullptr;
+    dst->linearization.b = nullptr;
+    dst->linearization.size = 0;
+
+    return OK;
+}
+
+int AiqUtils::deepCopyLtmResults(const ia_ltm_results& src, ia_ltm_results* dst)
+{
+    LOG3A("%s", __func__);
+
+    CheckError(!dst, BAD_VALUE ,"Failed to deep copy LTM result- invalid destination");
+
+    MEMCPY_S(dst, sizeof(ia_ltm_results), &src, sizeof(ia_ltm_results));
+
+    return OK;
+}
+
+int AiqUtils::deepCopyLtmDRCParams(const ia_ltm_drc_params& src, ia_ltm_drc_params* dst)
+{
+    LOG3A("%s", __func__);
+
+    CheckError(!dst, BAD_VALUE ,"Failed to deep copy LTM DRC params- invalid destination");
+
+    MEMCPY_S(dst, sizeof(ia_ltm_drc_params), &src, sizeof(ia_ltm_drc_params));
+
+    return OK;
+}
+
+int AiqUtils::deepCopySaResults(const ia_aiq_sa_results_v1& src, ia_aiq_sa_results_v1* dst)
+{
+    LOG3A("%s", __func__);
+
+    CheckError(!dst, BAD_VALUE, "Failed to deep copy SA result- invalid destination");
+
+    const size_t gridSize = src.width * src.height;
+    if ((size_t)(dst->width * dst->height) < gridSize) {
+        LOG3A("%s: increases the size of LSC table from %dx%d to %dx%d.",
+              __func__, dst->width, dst->height, src.width, src.height);
+
+        // allocated buffer is too small to accomodate what SA returns.
+        for (int i = 0; i < MAX_BAYER_ORDER_NUM; ++i) {
+            for (int j = 0; j < MAX_BAYER_ORDER_NUM; ++j) {
+                // re-allocate
+                delete [] dst->lsc_grid[i][j];
+                dst->lsc_grid[i][j] = new unsigned short[gridSize];
+
+                // copy a table
+                if (src.lsc_grid[i][j]) {
+                    MEMCPY_S(dst->lsc_grid[i][j], gridSize * sizeof(unsigned short),
+                             src.lsc_grid[i][j], gridSize * sizeof(unsigned short));
+                }
+            }
+        }
+    } else {
+        // copy tables
+        for (int i = 0; i < MAX_BAYER_ORDER_NUM; i++) {
+            for (int j = 0; j < MAX_BAYER_ORDER_NUM; j++) {
+                if (dst->lsc_grid[i][j] && src.lsc_grid[i][j]) {
+                    MEMCPY_S(dst->lsc_grid[i][j], gridSize * sizeof(unsigned short),
+                             src.lsc_grid[i][j], gridSize * sizeof(unsigned short));
+                }
+            }
+        }
+    }
+
+    dst->width = src.width;
+    dst->height = src.height;
+    dst->lsc_update = src.lsc_update;
+    dst->fraction_bits = src.fraction_bits;
+    dst->color_order = src.color_order;
+
+    MEMCPY_S(dst->light_source, sizeof(dst->light_source), src.light_source, sizeof(src.light_source));
+    MEMCPY_S(&dst->frame_params, sizeof(dst->frame_params), &src.frame_params, sizeof(src.frame_params));
+
+    return OK;
+}
+
+int AiqUtils::deepCopyDvsResults(const ia_dvs_morph_table& src, ia_dvs_morph_table* dst)
+{
+    LOG3A("%s", __func__);
+
+    CheckError(!dst || !dst->xcoords_y || !dst->ycoords_y
+          || !dst->xcoords_uv || !dst->ycoords_uv
+          || !dst->xcoords_uv_float || !dst->ycoords_uv_float
+          ,BAD_VALUE ,"Failed to deep copy DVS result- invalid destination");
+
+    CheckError(!src.xcoords_y || !src.ycoords_y
+          || !src.xcoords_uv || !src.ycoords_uv
+          || !src.xcoords_uv_float || !src.ycoords_uv_float
+          ,BAD_VALUE ,"Failed to deep copy DVS result- invalid source");
+
+    CheckError(src.width_y == 0 || src.height_y == 0 || src.width_uv == 0 || src.height_uv == 0
+          ,BAD_VALUE ,"Failed to deep copy DVS result- invalid source size y[%dx%d] uv[%dx%d]",
+          src.width_y, src.height_y, src.width_uv, src.height_uv);
+
+    dst->width_y = src.width_y;
+    dst->height_y = src.height_y;
+    dst->width_uv = src.width_uv;
+    dst->height_uv = src.height_uv;
+    dst->morph_table_changed = src.morph_table_changed;
+    unsigned int SizeY = dst->width_y  * dst->height_y * sizeof(int32_t);
+    unsigned int SizeUV = dst->width_uv * dst->height_uv * sizeof(int32_t);
+    MEMCPY_S(dst->xcoords_y, SizeY, src.xcoords_y, SizeY);
+    MEMCPY_S(dst->ycoords_y, SizeY, src.ycoords_y, SizeY);
+    MEMCPY_S(dst->xcoords_uv, SizeUV, src.xcoords_uv, SizeUV);
+    MEMCPY_S(dst->ycoords_uv, SizeUV, src.ycoords_uv, SizeUV);
+
+    SizeUV = dst->width_uv * dst->height_uv * sizeof(float);
+    MEMCPY_S(dst->xcoords_uv_float, SizeUV, src.xcoords_uv_float, SizeUV);
+    MEMCPY_S(dst->ycoords_uv_float, SizeUV, src.ycoords_uv_float, SizeUV);
+
+    return OK;
+}
+
+int AiqUtils::deepCopyDvsResults(const ia_dvs_image_transformation& src, ia_dvs_image_transformation* dst)
+{
+    LOG3A("%s", __func__);
+
+    CheckError(!dst,BAD_VALUE ,"Failed to deep copy DVS result- invalid destination");
+
+    dst->num_homography_matrices = src.num_homography_matrices;
+    MEMCPY_S(dst->matrices, sizeof(dst->matrices), src.matrices, sizeof(src.matrices));
+
+    return OK;
+}
+
+int AiqUtils::convertError(ia_err iaErr)
+{
+    LOG3A("%s, iaErr = %d", __func__, iaErr);
+    switch (iaErr) {
+    case ia_err_none:
+        return OK;
+    case ia_err_general:
+        return UNKNOWN_ERROR;
+    case ia_err_nomemory:
+        return NO_MEMORY;
+    case ia_err_data:
+        return BAD_VALUE;
+    case ia_err_internal:
+        return INVALID_OPERATION;
+    case ia_err_argument:
+        return BAD_VALUE;
+    default:
+        return UNKNOWN_ERROR;
+    }
+}
+
+/**
+ * Convert SensorFrameParams defined in PlatformData to ia_aiq_frame_params in aiq
+ */
+void AiqUtils::convertToAiqFrameParam(const SensorFrameParams &sensor, ia_aiq_frame_params &aiq)
+{
+    aiq.cropped_image_height = sensor.cropped_image_height;
+    aiq.cropped_image_width = sensor.cropped_image_width;
+    aiq.horizontal_crop_offset = sensor.horizontal_crop_offset;
+    aiq.horizontal_scaling_denominator = sensor.horizontal_scaling_denominator;
+    aiq.horizontal_scaling_numerator = sensor.horizontal_scaling_numerator;
+    aiq.vertical_crop_offset = sensor.vertical_crop_offset;
+    aiq.vertical_scaling_denominator = sensor.vertical_scaling_denominator;
+    aiq.vertical_scaling_numerator = sensor.vertical_scaling_numerator;
+}
+
+camera_coordinate_t AiqUtils::convertCoordinateSystem(const camera_coordinate_system_t& srcSystem,
+                                                      const camera_coordinate_system_t& dstSystem,
+                                                      const camera_coordinate_t& srcCoordinate)
+{
+    int dstWidth = dstSystem.right - dstSystem.left;
+    int dstHeight = dstSystem.bottom - dstSystem.top;
+    int srcWidth = srcSystem.right - srcSystem.left;
+    int srcHeight = srcSystem.bottom - srcSystem.top;
+
+    camera_coordinate_t result;
+    result.x = (srcCoordinate.x - srcSystem.left) * dstWidth / srcWidth + dstSystem.left;
+    result.y = (srcCoordinate.y - srcSystem.top) * dstHeight / srcHeight + dstSystem.top;
+
+    return result;
+}
+
+camera_coordinate_t AiqUtils::convertToIaCoordinate(const camera_coordinate_system_t& srcSystem,
+                                                    const camera_coordinate_t& srcCoordinate)
+{
+    camera_coordinate_system_t iaCoordinate = {IA_COORDINATE_LEFT, IA_COORDINATE_TOP,
+                                               IA_COORDINATE_RIGHT, IA_COORDINATE_BOTTOM};
+
+    return convertCoordinateSystem(srcSystem, iaCoordinate, srcCoordinate);
+}
+
+camera_window_t AiqUtils::convertToIaWindow(const camera_coordinate_system_t& srcSystem,
+                                            const camera_window_t& srcWindow)
+{
+    camera_coordinate_t leftTop;
+    camera_coordinate_t rightBottom;
+    leftTop.x     = srcWindow.left;
+    leftTop.y     = srcWindow.top;
+    rightBottom.x = srcWindow.right;
+    rightBottom.y = srcWindow.bottom;
+    leftTop       = convertToIaCoordinate(srcSystem, leftTop);
+    rightBottom   = convertToIaCoordinate(srcSystem, rightBottom);
+
+    camera_window_t result;
+    result.left   = leftTop.x;
+    result.top    = leftTop.y;
+    result.right  = rightBottom.x;
+    result.bottom = rightBottom.y;
+    result.weight = srcWindow.weight;
+    return result;
+}
+
+/**
+ * Map user input manual gain(0, 255) to (AWB_GAIN_NORMALIZED_START, AWB_GAIN_NORMALIZED_END)
+ */
+float AiqUtils::normalizeAwbGain(int gain)
+{
+    gain = CLIP(gain, AWB_GAIN_MAX, AWB_GAIN_MIN);
+    return AWB_GAIN_NORMALIZED_START + (float)(gain - AWB_GAIN_MIN) * \
+                                       AWB_GAIN_RANGE_NORMALIZED / AWB_GAIN_RANGE_USER;
+}
+
+int AiqUtils::convertToUserAwbGain(float normalizedGain)
+{
+    normalizedGain = CLIP(normalizedGain, AWB_GAIN_NORMALIZED_START, AWB_GAIN_NORMALIZED_END);
+    return AWB_GAIN_MIN + (normalizedGain - AWB_GAIN_NORMALIZED_START) * \
+                          AWB_GAIN_RANGE_USER / AWB_GAIN_RANGE_NORMALIZED;
+}
+
+float AiqUtils::convertSpeedModeToTime(camera_converge_speed_t mode)
+{
+    float convergenceTime = -1;
+    /*
+     * The unit of manual_convergence_time is second, and 3.0 means 3 seconds.
+     * The default value can be changed based on customer requirement.
+     */
+    switch (mode) {
+        case CONVERGE_MID:
+            convergenceTime = 3.0;
+            break;
+        case CONVERGE_LOW:
+            convergenceTime = 5.0;
+            break;
+        case CONVERGE_NORMAL:
+        default:
+            convergenceTime = -1;
+            break;
+    }
+    return convergenceTime;
+}
+
+/*
+ * Get ia_aiq_frame_use
+ *
+ * Convert frame usage to ia_aiq_frame_use
+ */
+ia_aiq_frame_use AiqUtils::convertFrameUsageToIaFrameUsage(int frameUsage)
+{
+    switch (frameUsage) {
+        case FRAME_USAGE_VIDEO:
+            return ia_aiq_frame_use_video;
+        case FRAME_USAGE_STILL:
+            return ia_aiq_frame_use_still;
+        case FRAME_USAGE_CONTINUOUS:
+            return ia_aiq_frame_use_continuous;
+    }
+    return ia_aiq_frame_use_preview;
+}
+
+void AiqUtils::applyTonemapGamma(float gamma, ia_aiq_gbce_results* results) {
+    CheckError(gamma < EPSILON, VOID_VALUE, "Bad gamma %f", gamma);
+    CheckError(!results, VOID_VALUE, "gbce results nullptr");
+
+    int lutSize = results->gamma_lut_size;
+    CheckError(lutSize < TONEMAP_MIN_POINTS, VOID_VALUE, "Bad gamma lut size (%d) in gbce results", lutSize);
+    for (int i = 0; i < lutSize; i++) {
+        results->g_gamma_lut[i] = pow(i / static_cast<float>(lutSize), 1 / gamma);
+    }
+
+    MEMCPY_S(results->b_gamma_lut, lutSize * sizeof(float),
+             results->g_gamma_lut, lutSize * sizeof(float));
+    MEMCPY_S(results->r_gamma_lut, lutSize * sizeof(float),
+             results->g_gamma_lut, lutSize * sizeof(float));
+}
+
+void AiqUtils::applyTonemapSRGB(ia_aiq_gbce_results* results) {
+    CheckError(!results, VOID_VALUE, "gbce results nullptr");
+
+    int lutSize = results->gamma_lut_size;
+    CheckError(lutSize < TONEMAP_MIN_POINTS, VOID_VALUE, "Bad gamma lut size (%d) in gbce results", lutSize);
+    for (int i = 0; i < lutSize; i++) {
+        if (i / (lutSize - 1)  < 0.0031308)
+            results->g_gamma_lut[i] = 12.92 * (i / (lutSize - 1));
+        else
+            results->g_gamma_lut[i] =
+                    1.055 * pow(i / static_cast<float>(lutSize - 1), 1 / 2.4) - 0.055;
+    }
+
+    MEMCPY_S(results->b_gamma_lut, lutSize * sizeof(float),
+             results->g_gamma_lut, lutSize * sizeof(float));
+    MEMCPY_S(results->r_gamma_lut, lutSize * sizeof(float),
+             results->g_gamma_lut, lutSize * sizeof(float));
+}
+
+void AiqUtils::applyTonemapREC709(ia_aiq_gbce_results* results) {
+    CheckError(!results, VOID_VALUE, "gbce results nullptr");
+
+    int lutSize = results->gamma_lut_size;
+    CheckError(lutSize < TONEMAP_MIN_POINTS, VOID_VALUE, "Bad gamma lut size (%d) in gbce results", lutSize);
+    for (int i = 0; i < lutSize; i++) {
+        if (i / (lutSize - 1) < 0.018)
+            results->g_gamma_lut[i] = 4.5 * (i / (lutSize - 1));
+        else
+            results->g_gamma_lut[i] =
+                    1.099 * pow(i / static_cast<float>(lutSize - 1), 0.45) - 0.099;
+    }
+
+    MEMCPY_S(results->b_gamma_lut, lutSize * sizeof(float),
+             results->g_gamma_lut, lutSize * sizeof(float));
+    MEMCPY_S(results->r_gamma_lut, lutSize * sizeof(float),
+             results->g_gamma_lut, lutSize * sizeof(float));
+}
+
+float AiqUtils::calculateHyperfocalDistance(const ia_cmc_t &cmcData) {
+    LOG2("@%s", __func__);
+
+    float pixelSizeMicro = 100.0f;  // size of pixels in um, default to avoid division by 0
+    float focalLengthMillis = 0.0f;
+    const float DEFAULT_HYPERFOCAL_DISTANCE = 5000.0f;
+
+    cmc_optomechanics_t *optoInfo = cmcData.cmc_parsed_optics.cmc_optomechanics;
+    if (optoInfo) {
+        // Pixel size is stored in CMC in hundreds of micrometers
+        pixelSizeMicro = optoInfo->sensor_pix_size_h / 100;
+        // focal length is stored in CMC in hundreds of millimeters
+        focalLengthMillis = static_cast<float>(optoInfo->effect_focal_length) / 100;
+    }
+
+    // fixed aperture, the fn should be divided 100 because the value
+    // is multiplied 100 in cmc avoid division by 0
+    if (!cmcData.cmc_parsed_optics.lut_apertures ||
+        cmcData.cmc_parsed_optics.lut_apertures[0] == 0) {
+        LOG2("lut apertures is not provided or zero in the cmc. Using default");
+        return DEFAULT_HYPERFOCAL_DISTANCE;
+    }
+
+    float fNumber = static_cast<float>(cmcData.cmc_parsed_optics.lut_apertures[0]) / 100;
+    // assuming square pixel
+    const int CIRCLE_OF_CONFUSION_IN_PIXELS = 2;
+    float cocMicros = pixelSizeMicro * CIRCLE_OF_CONFUSION_IN_PIXELS;
+    float hyperfocalDistanceMillis = 1000 * (focalLengthMillis * focalLengthMillis) /
+                                     (fNumber * cocMicros);
+
+    return (hyperfocalDistanceMillis == 0.0f) ? DEFAULT_HYPERFOCAL_DISTANCE :
+                                                hyperfocalDistanceMillis;
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/AiqUtils.h b/camera/hal/intel/ipu6/src/3a/AiqUtils.h
new file mode 100644
index 000000000000..7def8de88ddb
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/AiqUtils.h
@@ -0,0 +1,185 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "PlatformData.h"
+#include "Parameters.h"
+#include "ia_aiq.h"
+#include "ia_ltm_types.h"
+#include "ia_dvs_types.h"
+#include "ia_isp_bxt_statistics_types.h"
+
+namespace icamera {
+
+/*!> Top limit for the RGBS grid size */
+static const unsigned int MAX_AE_GRID_SIZE = 2048;
+/*!> Number of leds AEC algorithm provides output for */
+static const unsigned int MAX_EXPOSURES_NUM = 3;
+static const unsigned int NUM_FLASH_LEDS = 1;
+static const unsigned int MAX_GAMMA_LUT_SIZE = 2048;
+static const unsigned int MAX_TONEMAP_LUT_SIZE = 2048;
+
+static const unsigned int MAX_STATISTICS_WIDTH = BXT_RGBS_GRID_MAX_WIDTH;
+static const unsigned int MAX_STATISTICS_HEIGHT = BXT_RGBS_GRID_MAX_HEIGHT;
+
+static const unsigned int MAX_LSC_WIDTH = 100;
+static const unsigned int MAX_LSC_HEIGHT = 100;
+
+static const unsigned int MAX_IR_WEIGHT_GRID_SIZE = 480;
+static const unsigned int MAX_NUM_SECTORS = 36;
+
+static const int MAX_BAYER_ORDER_NUM = 4;
+
+/*! \brief Definitions of IA imaging coordinate system. */
+static const unsigned int IA_COORDINATE_TOP = 0;
+static const unsigned int IA_COORDINATE_LEFT = 0;
+static const unsigned int IA_COORDINATE_BOTTOM = 8192;
+static const unsigned int IA_COORDINATE_RIGHT = 8192;
+static const unsigned int IA_COORDINATE_WIDTH = (IA_COORDINATE_RIGHT - IA_COORDINATE_LEFT);
+static const unsigned int IA_COORDINATE_HEIGHT = (IA_COORDINATE_BOTTOM - IA_COORDINATE_TOP);
+
+/**
+ *  The normalized awb gain range is (4.0, 1.0) which is just experimental.
+ *  TODO: Maybe need put them in configuration file later.
+ */
+static const int AWB_GAIN_NORMALIZED_START = 4.0;
+static const int AWB_GAIN_NORMALIZED_END = 1.0;
+static const int AWB_GAIN_RANGE_NORMALIZED = AWB_GAIN_NORMALIZED_END - AWB_GAIN_NORMALIZED_START;
+
+static const float AWB_GAIN_MIN = 0;
+static const float AWB_GAIN_MAX = 255;
+static const float AWB_GAIN_RANGE_USER = AWB_GAIN_MAX - AWB_GAIN_MIN;
+
+static const int MAX_CUSTOM_CONTROLS_PARAM_SIZE = 1024;
+
+namespace AiqUtils {
+int dumpAeResults(const ia_aiq_ae_results &aeResult);
+int dumpAfResults(const ia_aiq_af_results &afResult);
+int dumpAwbResults(const ia_aiq_awb_results &awbResult);
+
+int deepCopyAeResults(const ia_aiq_ae_results& src, ia_aiq_ae_results* dst);
+int deepCopyAfResults(const ia_aiq_af_results& src, ia_aiq_af_results* dst);
+int deepCopyAwbResults(const ia_aiq_awb_results& src, ia_aiq_awb_results* dst);
+int deepCopyGbceResults(const ia_aiq_gbce_results& src, ia_aiq_gbce_results* dst);
+int deepCopyPaResults(const ia_aiq_pa_results_v1& src, ia_aiq_pa_results_v1* dst,
+                      ia_aiq_advanced_ccm_t* preferredAcm);
+int deepCopySaResults(const ia_aiq_sa_results_v1& src, ia_aiq_sa_results_v1* dst);
+int deepCopyLtmResults(const ia_ltm_results& src, ia_ltm_results* dst);
+int deepCopyLtmDRCParams(const ia_ltm_drc_params& src, ia_ltm_drc_params* dst);
+int deepCopyDvsResults(const ia_dvs_morph_table& src, ia_dvs_morph_table* dst);
+int deepCopyDvsResults(const ia_dvs_image_transformation& src, ia_dvs_image_transformation* dst);
+
+int convertError(ia_err iaErr);
+
+void convertToAiqFrameParam(const SensorFrameParams& sensor, ia_aiq_frame_params& aiq);
+
+camera_coordinate_t convertCoordinateSystem(const camera_coordinate_system_t& srcSystem,
+                                            const camera_coordinate_system_t& dstSystem,
+                                            const camera_coordinate_t& srcCoordinate);
+camera_coordinate_t convertToIaCoordinate(const camera_coordinate_system_t& srcSystem,
+                                          const camera_coordinate_t& srcCoordinate);
+camera_window_t convertToIaWindow(const camera_coordinate_system_t& srcSystem,
+                                  const camera_window_t& srcWindow);
+float normalizeAwbGain(int gain);
+int convertToUserAwbGain(float normalizedGain);
+float convertSpeedModeToTime(camera_converge_speed_t mode);
+
+ia_aiq_frame_use convertFrameUsageToIaFrameUsage(int frameUsage);
+
+void applyTonemapGamma(float gamma, ia_aiq_gbce_results* results);
+void applyTonemapSRGB(ia_aiq_gbce_results* results);
+void applyTonemapREC709(ia_aiq_gbce_results* results);
+
+// Resize a 2D array with linear interpolation
+// For some cases, we need to upscale or downscale a 2D array.
+// For example, Android requests lensShadingMapSize must be smaller than 64*64,
+// but for some sensors, the lens shading map is bigger than this, so need to do resize.
+/* Value of 8 is maximum in order to avoid overflow with 16-bit inputs */
+#define FRAC_BITS_CURR_LOC 8
+#define FRAC_BASE (short)(1) << FRAC_BITS_CURR_LOC
+
+/*!
+ * \brief Resize a 2D array with linear interpolation.
+ *
+ * @param[in,out]
+ *  in a_src                pointer to input array (width-major)
+ *  in a_src_w              width of the input array
+ *  in a_src_h              height of the input array
+ *  in a_dst                pointer to output array (width-major)
+ *  in a_dst_w              width of the output array
+ *  in a_dst_h              height of the output array
+ */
+template <typename T> int resize2dArray(
+    const T* a_src, int a_src_w, int a_src_h,
+    T* a_dst, int a_dst_w, int a_dst_h) {
+    int i, j, step_size_w, step_size_h, rounding_term;
+
+    if (a_src_w < 2 || a_dst_w < 2 || a_src_h < 2 || a_dst_h < 2) {
+        return  -1;
+    }
+    nsecs_t startTime = CameraUtils::systemTime();
+    step_size_w = ((a_src_w - 1) << FRAC_BITS_CURR_LOC) / (a_dst_w - 1);
+    step_size_h = ((a_src_h - 1) << FRAC_BITS_CURR_LOC) / (a_dst_h - 1);
+    rounding_term = (1 << (2 * FRAC_BITS_CURR_LOC - 1));
+    for (j = 0; j < a_dst_h; ++j) {
+        unsigned int curr_loc_h, curr_loc_lower_h;
+        curr_loc_h = j * step_size_h;
+        curr_loc_lower_h = (curr_loc_h > 0) ? (curr_loc_h - 1) >> FRAC_BITS_CURR_LOC : 0;
+
+        for (i = 0; i < a_dst_w; ++i) {
+            unsigned int curr_loc_w, curr_loc_lower_w;
+
+            curr_loc_w = i * step_size_w;
+            curr_loc_lower_w = (curr_loc_w > 0) ? (curr_loc_w - 1) >> FRAC_BITS_CURR_LOC : 0;
+
+            a_dst[a_dst_w * j + i] =
+                (a_src[curr_loc_lower_w + curr_loc_lower_h * a_src_w]  *
+                        (((curr_loc_lower_w + 1) << FRAC_BITS_CURR_LOC) - curr_loc_w) *
+                        (((curr_loc_lower_h + 1) << FRAC_BITS_CURR_LOC) - curr_loc_h) +
+                a_src[curr_loc_lower_w + 1 + curr_loc_lower_h * a_src_w] *
+                        (curr_loc_w-((curr_loc_lower_w) << FRAC_BITS_CURR_LOC))   *
+                        (((curr_loc_lower_h + 1) << FRAC_BITS_CURR_LOC) - curr_loc_h) +
+                a_src[curr_loc_lower_w + (curr_loc_lower_h + 1) * a_src_w]  *
+                        (((curr_loc_lower_w + 1) << FRAC_BITS_CURR_LOC) - curr_loc_w) *
+                        (curr_loc_h - ((curr_loc_lower_h) << FRAC_BITS_CURR_LOC)) +
+                a_src[curr_loc_lower_w + 1 + (curr_loc_lower_h + 1) * a_src_w] *
+                        (curr_loc_w - ((curr_loc_lower_w) << FRAC_BITS_CURR_LOC))   *
+                        (curr_loc_h - ((curr_loc_lower_h) << FRAC_BITS_CURR_LOC))
+                + rounding_term) / (FRAC_BASE * FRAC_BASE);
+        }
+    }
+    LOG2("resize the 2D array cost %dus",
+         (unsigned)((CameraUtils::systemTime() - startTime) / 1000));
+
+    return 0;
+}
+
+template int resize2dArray<float>(
+    const float* a_src, int a_src_w, int a_src_h,
+    float* a_dst, int a_dst_w, int a_dst_h);
+template int resize2dArray<unsigned short>(
+    const unsigned short* a_src, int a_src_w, int a_src_h,
+    unsigned short* a_dst, int a_dst_w, int a_dst_h);
+template int resize2dArray<int>(
+    const int* a_src, int a_src_w, int a_src_h,
+    int* a_dst, int a_dst_w, int a_dst_h);
+
+float calculateHyperfocalDistance(const ia_cmc_t &cmcData);
+
+} // namespace AiqUtils
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/3a/Dvs.cpp b/camera/hal/intel/ipu6/src/3a/Dvs.cpp
new file mode 100644
index 000000000000..2b881e526607
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/Dvs.cpp
@@ -0,0 +1,616 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Dvs"
+#include "src/3a/Dvs.h"
+
+#include <ia_cmc_parser.h>
+#include <ia_pal_types_isp_ids_autogen.h>
+
+#include <algorithm>
+#include "iutils/CameraLog.h"
+#include "iutils/CameraDump.h"
+#include "iutils/Utils.h"
+#include "PlatformData.h"
+#include "AiqUtils.h"
+#include "AiqResultStorage.h"
+#include "IGraphConfigManager.h"
+#include "IGraphConfig.h"
+
+namespace icamera {
+
+const int DVS_MIN_ENVELOPE = 12;
+const float QUATERNION_CLIP_MAX_ANGLE = 3.5f;
+
+const int DVS_OXDIM_Y = 128;
+const int DVS_OYDIM_Y = 32;
+const int DVS_OXDIM_UV  = 64;
+const int DVS_OYDIM_UV  = 16;
+
+Dvs::Dvs(int cameraId, AiqSetting *setting) :
+    mDvsHandle(nullptr),
+    mDvsEnabled(false),
+    mLdcEnabled(false),
+    mRscEnabled(false),
+    mDigitalZoomRatio(1.0f),
+    mCameraId(cameraId),
+    mFps(30),
+    mConfigMode(CAMERA_STREAM_CONFIGURATION_MODE_NORMAL),
+    mTuningMode(TUNING_MODE_VIDEO),
+    mAiqSetting(setting),
+    mKernelId(0),
+    mMorphTable(nullptr),
+    mStatistics(nullptr),
+    mConfigured(false) {
+    LOG1("@%s", __func__);
+
+    CLEAR(mSrcResolution);
+    CLEAR(mDstResolution);
+    CLEAR(mEnvelopeResolution);
+    CLEAR(mImage_transformation);
+
+    mIntelDvs = new IntelDvs();
+}
+
+Dvs::~Dvs() {
+    LOG1("@%s", __func__);
+    delete mIntelDvs;
+}
+
+int Dvs::initDvsHandle(TuningMode tuningMode) {
+    ia_binary_data aiqData;
+    uintptr_t cmcHandle = reinterpret_cast<uintptr_t>(nullptr);
+    int ret = PlatformData::getCpfAndCmc(mCameraId, nullptr, &aiqData, nullptr,
+                                         &cmcHandle, tuningMode);
+    CheckError(ret != OK, BAD_VALUE, "@%s, Get cpf data failed", __func__);
+
+    ia_err err = mIntelDvs->init(aiqData, reinterpret_cast<ia_cmc_t*>(cmcHandle), &mDvsHandle);
+    CheckError(err != ia_err_none, NO_INIT, "@%s, Failed to initilize IntelDvs", __func__);
+
+    return OK;
+}
+
+int Dvs::deinitDvsHandle() {
+    int status = deInitDVSTable();
+    if (mDvsHandle) {
+        mIntelDvs->deinit(mDvsHandle);
+        mDvsHandle = nullptr;
+    }
+
+    return status;
+}
+
+int Dvs::init() {
+    LOG1("@%s", __func__);
+
+    return OK;
+}
+
+int Dvs::deinit() {
+    LOG1("@%s", __func__);
+    AutoMutex l(mLock);
+
+    return deinitDvsHandle();
+}
+
+int Dvs::configure(const std::vector<ConfigMode>& configModes) {
+    LOG1("@%s, isDvsSupported:%d", __func__, PlatformData::isDvsSupported(mCameraId));
+    AutoMutex l(mLock);
+    mConfigured = false;
+    if (!PlatformData::isDvsSupported(mCameraId)) return OK;
+
+    if (configModes.empty()) {
+        return UNKNOWN_ERROR;
+    }
+    mConfigMode = configModes[0];
+
+    TuningMode tuningMode;
+    if (PlatformData::getTuningModeByConfigMode(mCameraId, mConfigMode, tuningMode) != OK) {
+        return UNKNOWN_ERROR;
+    }
+    mTuningMode = tuningMode;
+
+    mKernelId = 0;
+    CLEAR(mSrcResolution);
+    CLEAR(mDstResolution);
+
+    return reconfigure();
+}
+
+int Dvs::configure(TuningMode tuningMode, uint32_t kernelId,
+                   int srcWidth, int srcHeight, int dstWidth, int dstHeight) {
+    LOG1("@%s, isDvsSupported:%d", __func__, PlatformData::isDvsSupported(mCameraId));
+    AutoMutex l(mLock);
+    mConfigured = false;
+    if (!PlatformData::isDvsSupported(mCameraId)) return OK;
+
+    mTuningMode = tuningMode;
+    mKernelId = kernelId;
+    mSrcResolution.width = srcWidth;
+    mSrcResolution.height = srcHeight;
+    mDstResolution.width = dstWidth;
+    mDstResolution.height = dstHeight;
+
+    return reconfigure();
+}
+
+int Dvs::setDVSConfiguration(uint32_t kernelId, ia_dvs_configuration *config) {
+    config->num_axis = mDvsEnabled ? ia_dvs_algorithm_6_axis : ia_dvs_algorithm_0_axis;
+
+    /* General setting for dvs */
+    config->source_bq.width_bq = mSrcResolution.width / 2;
+    config->source_bq.height_bq = mSrcResolution.height / 2;
+    config->output_bq.width_bq = mDstResolution.width / 2;
+    config->output_bq.height_bq = mDstResolution.height / 2;
+    // if the DstResolution is valid, the output_bq from dstResolution.
+    if (mDstResolution.width != 0 && mDstResolution.height != 0) {
+        config->output_bq.width_bq = mDstResolution.width / 2;
+        config->output_bq.height_bq = mDstResolution.height / 2;
+    }
+    config->ispfilter_bq.width_bq = DVS_MIN_ENVELOPE / 2;
+    config->ispfilter_bq.height_bq = DVS_MIN_ENVELOPE / 2;
+
+    // config->num_axis = ia_dvs_algorithm_0_axis;
+    config->gdc_shift_x = 0;
+    config->gdc_shift_y = 0;
+
+    if (kernelId == ia_pal_uuid_isp_gdc3_1) {
+        config->oxdim_y = DVS_OXDIM_Y;
+        config->oydim_y = DVS_OYDIM_Y;
+        config->oxdim_uv = DVS_OXDIM_UV;
+        config->oydim_uv = DVS_OYDIM_UV;
+    } else {
+        config->oxdim_y = DVS_OXDIM_Y / 2;
+        config->oydim_y = DVS_OYDIM_Y;
+        config->oxdim_uv = DVS_OXDIM_UV;
+        config->oydim_uv = DVS_OYDIM_UV;
+    }
+
+    config->hw_config.scan_mode = ia_dvs_gdc_scan_mode_stb;
+    config->hw_config.interpolation = ia_dvs_gdc_interpolation_bci;
+    config->hw_config.performance_point = ia_dvs_gdc_performance_point_1x1;
+
+    config->gdc_buffer_config.x_offset = 0;
+    config->gdc_buffer_config.y_offset = 0;
+    config->gdc_buffer_config.width = config->source_bq.width_bq;
+    config->gdc_buffer_config.height = config->source_bq.height_bq;
+    config->frame_rate = mFps;
+    config->validate_morph_table = false;
+
+    /*
+     * cropping from the active pixel array, needs to be coming from history
+     */
+    config->crop_params.horizontal_crop_offset = 0;
+    config->crop_params.vertical_crop_offset = 0;
+    config->crop_params.cropped_width = 0;
+    config->crop_params.cropped_height = 0;
+    config->quaternion_clip_max_angle = QUATERNION_CLIP_MAX_ANGLE;
+
+    const float Max_Ratio = 1.45f;
+    int bq_max_width = static_cast<int>(Max_Ratio * static_cast<float>(config->output_bq.width_bq));
+    int bq_max_height =
+                      static_cast<int>(Max_Ratio * static_cast<float>(config->output_bq.height_bq));
+
+    config->envelope_bq.width_bq = mEnvelopeResolution.width / 2 - config->ispfilter_bq.width_bq;
+    config->envelope_bq.height_bq = mEnvelopeResolution.height / 2 - config->ispfilter_bq.height_bq;
+    // envelope should be larger than 0
+    config->envelope_bq.width_bq = max(0, config->envelope_bq.width_bq);
+    config->envelope_bq.height_bq = max(0, config->envelope_bq.height_bq);
+
+    if (config->source_bq.width_bq - config->envelope_bq.width_bq -
+        config->ispfilter_bq.width_bq > bq_max_width)
+        config->envelope_bq.width_bq =
+            config->source_bq.width_bq - config->ispfilter_bq.width_bq - bq_max_width;
+
+    if (config->source_bq.height_bq - config->envelope_bq.height_bq -
+        config->ispfilter_bq.height_bq > bq_max_height)
+        config->envelope_bq.height_bq =
+            config->source_bq.height_bq - config->ispfilter_bq.height_bq - bq_max_height;
+
+    if (mLdcEnabled) {
+        // The crop must be set in LDC function, or there is config dvs fail
+        config->crop_params.cropped_width = mDstResolution.width / 2;
+        config->crop_params.cropped_height = mDstResolution.height / 2;
+        // envelope bq is only for stabilization and it has to be set as 0 when ldc enabled.
+        // TODO: clear define the envelope_bq when ldc & video stabilization enabled together
+        config->envelope_bq.width_bq = 0;
+        config->envelope_bq.height_bq = 0;
+        config->use_lens_distortion_correction = true;
+    }
+
+    if (mRscEnabled) {
+        // TODO: set config.nonblanking_ratio to inputReadoutTime/framePeriod.
+    }
+    return 0;
+}
+
+int Dvs::reconfigure() {
+    LOG1("@%s", __func__);
+
+    int status = OK;
+    uint32_t gdcKernelId = mKernelId;
+
+    // If parameters are not valid, try to query them in GC.
+    if (gdcKernelId == 0 || mSrcResolution.width == 0 || mSrcResolution.height == 0) {
+        // update GC
+        std::shared_ptr<IGraphConfig> gc = nullptr;
+
+        if (PlatformData::getGraphConfigNodes(mCameraId)) {
+            IGraphConfigManager *GCM = IGraphConfigManager::getInstance(mCameraId);
+            if (GCM) {
+                gc = GCM->getGraphConfig(mConfigMode);
+            }
+        }
+        CheckWarning(gc == nullptr, OK, "Failed to get GC in DVS");
+
+        // update resolution infomation
+        ia_isp_bxt_resolution_info_t resolution;
+        status = gc->getGdcKernelSetting(&gdcKernelId, &resolution);
+        CheckWarning(status != OK, OK, "Failed to get GDC kernel setting, DVS disabled");
+        mSrcResolution.width = resolution.input_width;
+        mSrcResolution.height = resolution.input_height;
+        mDstResolution.width = resolution.output_width;
+        mDstResolution.height = resolution.output_height;
+        mEnvelopeResolution.width = resolution.input_crop.left + resolution.input_crop.right;
+        mEnvelopeResolution.height = resolution.input_crop.top + resolution.input_crop.bottom;
+    }
+    LOG1("%s, GDC kernel setting: id: %u, resolution:src: %dx%d, dst: %dx%d, envelope: %dx%d",
+         __func__, gdcKernelId, mSrcResolution.width, mSrcResolution.height, mDstResolution.width,
+         mDstResolution.height, mEnvelopeResolution.width, mEnvelopeResolution.height);
+
+    if (mDvsHandle) {
+        deinitDvsHandle();
+    }
+    status = initDvsHandle(mTuningMode);
+
+    if (!mDvsHandle)
+        return status;
+
+    ia_dvs_configuration config;
+    CLEAR(config);
+
+    setDVSConfiguration(gdcKernelId, &config);
+    dumpConfiguration(config);
+
+    CheckError(mSrcResolution.width <= (config.envelope_bq.width_bq * 2), UNKNOWN_ERROR,
+               "%s the mSrcResolution width: %d is too small", __func__, mSrcResolution.width);
+    CheckError(mSrcResolution.height <= (config.envelope_bq.height_bq * 2), UNKNOWN_ERROR,
+               "%s the mSrcResolution height: %d is too small", __func__, mSrcResolution.height);
+
+    float zoomHRatio = mSrcResolution.width /
+                       (mSrcResolution.width - config.envelope_bq.width_bq * 2);
+    float zoomVRatio = mSrcResolution.height /
+                       (mSrcResolution.height - config.envelope_bq.height_bq * 2);
+    ia_err err = mIntelDvs->config(mDvsHandle, &config,
+                                   (zoomHRatio > zoomVRatio) ? zoomHRatio : zoomVRatio);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "Configure DVS failed %d", err);
+
+    LOG2("Configure DVS success");
+    err = mIntelDvs->setNonBlankRatio(mDvsHandle, config.nonblanking_ratio);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "set non blank ratio failed %d", err);
+
+    status = initDVSTable();
+    CheckError(status != OK, UNKNOWN_ERROR, "Allocate dvs table failed");
+    mConfigured = true;
+
+    return status;
+}
+
+void Dvs::handleEvent(EventData eventData) {
+    if (eventData.type != EVENT_PSYS_STATS_BUF_READY) return;
+    if (!mConfigured) return;
+
+    LOG3A("%s: handle EVENT_PSYS_STATS_BUF_READY", __func__);
+    int64_t sequence = eventData.data.statsReady.sequence;
+
+    AiqResultStorage* aiqResultStorage = AiqResultStorage::getInstance(mCameraId);
+    DvsStatistics *dvsStatistics = aiqResultStorage->getDvsStatistics();
+    if (dvsStatistics->sequence != sequence || dvsStatistics->dvsStats == nullptr) return;
+
+    // Set dvs statistics
+    {
+    AutoMutex l(mLock);
+    mStatistics = dvsStatistics->dvsStats;
+    }
+
+    // Run dvs
+    if (mAiqSetting) {
+        aiq_parameter_t aiqParam;
+        mAiqSetting->getAiqParameter(aiqParam);
+        updateParameter(aiqParam);
+    }
+
+    DvsResult *dvsResult = aiqResultStorage->acquireDvsResult();
+
+    const AiqResult *feedback = aiqResultStorage->getAiqResult(sequence);
+    if (feedback == nullptr) {
+        LOGW("%s: no aiq result for sequence %ld! use the latest instead", __func__, sequence);
+        feedback = aiqResultStorage->getAiqResult();
+    }
+
+    const ia_aiq_af_results *afResults = nullptr;
+    if (PlatformData::getLensHwType(mCameraId) == LENS_VCM_HW) {
+        afResults = &feedback->mAfResults;
+    }
+    int ret = run(&feedback->mAeResults, afResults, dvsResult, sequence);
+    CheckError(ret != OK, VOID_VALUE, "Run DVS fail");
+
+    aiqResultStorage->updateDvsResult(sequence);
+}
+
+int Dvs::run(const ia_aiq_ae_results *aeResults, const ia_aiq_af_results *afResults,
+             DvsResult *result, int64_t sequence) {
+    LOG2("@%s", __func__);
+    if (!mConfigured) return OK;
+
+    PERF_CAMERA_ATRACE_IMAGING();
+    AutoMutex l(mLock);
+
+    runImpl(aeResults, afResults);
+
+    int dvsType = PlatformData::getDVSType(mCameraId);
+    switch (dvsType) {
+        case MORPH_TABLE: {
+            return getMorphTable(sequence, result);
+        }
+        case IMG_TRANS:
+            return getImageTrans(sequence, result);
+        default:
+            LOGE("not supportted dvs type");
+            return UNKNOWN_ERROR;
+    }
+}
+
+int Dvs::configureDigitalZoom(ia_dvs_zoom_mode zoom_mode, ia_rectangle *zoom_region,
+                              ia_coordinate *zoom_coordinate) {
+    LOG2("@%s zoom mode:%d", __func__, zoom_mode);
+    AutoMutex l(mLock);
+
+    ia_err err = mIntelDvs->setDigitalZoomMode(mDvsHandle, zoom_mode);
+    CheckError(err != ia_err_none, BAD_VALUE, "set zoom mode error: %d", err);
+
+    if (zoom_mode == ia_dvs_zoom_mode_region) {
+        err = mIntelDvs->setDigitalZoomRegion(mDvsHandle, zoom_region);
+    } else if (zoom_mode == ia_dvs_zoom_mode_coordinate) {
+        err = mIntelDvs->setDigitalZoomCoordinate(mDvsHandle, zoom_coordinate);
+    }
+
+    int ret = AiqUtils::convertError(err);
+    CheckError(ret != OK, ret, "Error config zoom: %d", ret);
+
+    return OK;
+}
+
+int Dvs::setZoomRatio(float zoomRatio) {
+    LOG2("@%s zoom:%4.2f", __func__, zoomRatio);
+    AutoMutex l(mLock);
+
+    ia_err err = mIntelDvs->setDigitalZoomMagnitude(mDvsHandle, zoomRatio);
+    CheckError(err != ia_err_none, UNKNOWN_ERROR, "set digital zoom magnitude error: %d", err);
+
+    return OK;
+}
+
+/**
+ * Private function implementations. mLock is assumed to be held.
+ */
+
+int Dvs::initDVSTable() {
+    LOG1("@%s", __func__);
+
+    int dvsType = PlatformData::getDVSType(mCameraId);
+    switch (dvsType) {
+        case MORPH_TABLE:
+            if (mMorphTable) {
+                mIntelDvs->freeMorphTable(mDvsHandle, mMorphTable);
+                mMorphTable = nullptr;
+            }
+            if (mDvsHandle) {
+                mMorphTable = mIntelDvs->allocateMorphTalbe(mDvsHandle);
+                CheckError(!mMorphTable, UNKNOWN_ERROR, "mMorphTable allocate failed");
+            }
+            break;
+        case IMG_TRANS:
+            LOG1("not need allocate MorphTable for image_transformation");
+            break;
+        default:
+            LOGE("not supportted dvs type");
+            return UNKNOWN_ERROR;
+    }
+    return OK;
+}
+
+int Dvs::deInitDVSTable() {
+    int status = OK;
+    if (mMorphTable) {
+        mIntelDvs->freeMorphTable(mDvsHandle, mMorphTable);
+        mMorphTable = nullptr;
+    }
+
+    return status;
+}
+
+int Dvs::runImpl(const ia_aiq_ae_results *aeResults, const ia_aiq_af_results *afResults) {
+    LOG2("@%s", __func__);
+    ia_err err = ia_err_none;
+    int ret = OK;
+
+    if (!mDvsHandle)
+        return UNKNOWN_ERROR;
+
+    if ((mDvsEnabled) && mStatistics && mStatistics->vector_count > 0) {
+        err = mIntelDvs->setStatistics(mDvsHandle, mStatistics,
+                                       aeResults, afResults, /*sensor events*/nullptr, 0, 0);
+        ret = AiqUtils::convertError(err);
+        CheckError(ret != OK, ret, "DVS set statistics failed: %d", ret);
+    } else if ((mDvsEnabled) && !mStatistics) {
+        return UNKNOWN_ERROR;
+    }
+
+    uint16_t focusPosition = 0;
+    if (afResults) {
+        focusPosition = static_cast<uint16_t>(afResults->next_lens_position);
+    }
+
+    err = mIntelDvs->execute(mDvsHandle, focusPosition);
+    ret = AiqUtils::convertError(err);
+    CheckError(ret != OK, ret, "DVS execution failed: %d", ret);
+
+    return OK;
+}
+
+int Dvs::getMorphTable(int64_t sequence, DvsResult *result) {
+    LOG2("@%s", __func__);
+    CheckError(!result, UNKNOWN_ERROR, "@%s, result is null", __func__);
+
+    int ret = mIntelDvs->getMorphTable(mDvsHandle, mMorphTable, result);
+    CheckError(ret != OK, ret, "Error geting DVS result: %d", ret);
+    return dumpDVSTable(&result->mMorphTable, sequence);
+}
+
+int Dvs::getImageTrans(int64_t sequence, DvsResult *result) {
+    LOG2("@%s", __func__);
+
+    ia_err err = mIntelDvs->getImageTransformation(mDvsHandle, &mImage_transformation);
+    int ret = AiqUtils::convertError(err);
+    CheckError(ret != OK, ret, "Error geting DVS result: %d", ret);
+    dumpDVSTable(&mImage_transformation, sequence);
+    return AiqUtils::deepCopyDvsResults(mImage_transformation, &result->mTransformation);
+}
+
+int Dvs::updateParameter(const aiq_parameter_t &param) {
+    LOG2("@%s", __func__);
+    if (!mConfigured) return OK;
+
+    bool dvsEnabled = (param.videoStabilizationMode == VIDEO_STABILIZATION_MODE_ON);
+    bool ldcEnabled = (param.ldcMode == LDC_MODE_ON);
+    bool rscEnabled = (param.rscMode == RSC_MODE_ON);
+    int digitalZoomRatio = param.digitalZoomRatio;
+
+    if ((param.fps > 0.01 && param.fps != mFps)
+        || param.tuningMode != mTuningMode
+        || dvsEnabled != mDvsEnabled || ldcEnabled != mLdcEnabled
+        || rscEnabled != mRscEnabled) {
+        mFps = param.fps > 0.01 ? param.fps : mFps;
+        mTuningMode = param.tuningMode;
+        mDvsEnabled = dvsEnabled;
+        mLdcEnabled = ldcEnabled;
+        mRscEnabled = rscEnabled;
+
+        LOG3A("%s: DVS fps = %f ", __func__, mFps);
+        LOG3A("%s: DVS tuning Mode = %d ", __func__, mTuningMode);
+        LOG3A("%s: DVS enabled = %d ", __func__, mDvsEnabled);
+        LOG3A("%s: LDC enabled = %d ", __func__, mLdcEnabled);
+        LOG3A("%s: RSC enabled = %d ", __func__, mRscEnabled);
+
+        return reconfigure();
+    }
+
+    if (param.digitalZoomRatio > 0 && param.digitalZoomRatio!= mDigitalZoomRatio) {
+        mDigitalZoomRatio = digitalZoomRatio;
+        setZoomRatio(mDigitalZoomRatio);
+        LOG3A("%s: digital zoom ratio = %f ", __func__, mDigitalZoomRatio);
+    }
+
+    return OK;
+}
+
+int Dvs::dumpDVSTable(ia_dvs_morph_table *table, int64_t sequence) {
+    if (!CameraDump::isDumpTypeEnable(DUMP_AIQ_DVS_RESULT)) return OK;
+
+    LOG3A("%s", __func__);
+
+    CheckError(!table, BAD_VALUE, "%s: morph table is nullptr, and nothing to dump.", __func__);
+
+    BinParam_t bParam;
+    bParam.bType = BIN_TYPE_GENERAL;
+    bParam.mType = M_PSYS;
+    bParam.sequence = sequence;
+    bParam.gParam.appendix = "dvs_morph_table_x_y";
+    CameraDump::dumpBinary(0, table->xcoords_y,
+                           table->width_y * table->height_y * sizeof(uint32_t), &bParam);
+    bParam.gParam.appendix = "dvs_morph_table_y_y";
+    CameraDump::dumpBinary(0, table->ycoords_y,
+                           table->width_y * table->height_y * sizeof(uint32_t), &bParam);
+    bParam.gParam.appendix = "dvs_morph_table_x_uv";
+    CameraDump::dumpBinary(0, table->xcoords_uv,
+                           table->width_uv * table->height_uv * sizeof(uint32_t), &bParam);
+    bParam.gParam.appendix = "dvs_morph_table_y_uv";
+    CameraDump::dumpBinary(0, table->ycoords_uv,
+                           table->width_uv * table->height_uv * sizeof(uint32_t), &bParam);
+
+    LOG3A("%s: DVS morph table y=[%d x %d], uv=[%d x %d] changed=%s", __func__,
+          table->width_y, table->height_y,
+          table->width_uv, table->height_uv,
+          table->morph_table_changed == true ? "TRUE" : "FALSE");
+    return OK;
+}
+
+int Dvs::dumpDVSTable(ia_dvs_image_transformation *trans, int64_t sequence) {
+    if (!CameraDump::isDumpTypeEnable(DUMP_AIQ_DVS_RESULT)) return OK;
+
+    LOG3A("%s", __func__);
+
+    CheckError(!trans, BAD_VALUE, "%s: trans table is nullptr, and nothing to dump.", __func__);
+
+    LOG3A("%s: DVS trans table num_homography_matrices=%d", __func__,
+            trans->num_homography_matrices);
+
+    BinParam_t bParam;
+    bParam.bType = BIN_TYPE_GENERAL;
+    bParam.mType = M_PSYS;
+    bParam.sequence = sequence;
+    for (int i = 0; i < DVS_HOMOGRAPHY_MATRIX_MAX_COUNT; i++) {
+        LOG3A("%s: DVS trans table %d start_row=%d", __func__,
+            i, trans->matrices[i].start_row);
+        bParam.gParam.appendix = "matrices";
+        CameraDump::dumpBinary(0, &trans->matrices[i].matrix, 3 * 3 * sizeof(float), &bParam);
+    }
+
+    return OK;
+}
+
+int Dvs::dumpConfiguration(ia_dvs_configuration config) {
+    LOG3A("%s", __func__);
+
+    LOG3A("config.num_axis %d", config.num_axis);
+    LOG3A("config.nonblanking_ratio %f", config.nonblanking_ratio);
+    LOG3A("config.source_bq.width_bq %d", config.source_bq.width_bq);
+    LOG3A("config.source_bq.height_bq %d", config.source_bq.height_bq);
+    LOG3A("config.output_bq.width_bq %d", config.output_bq.width_bq);
+    LOG3A("config.output_bq.height_bq %d", config.output_bq.height_bq);
+    LOG3A("config.envelope_bq.width_bq %d", config.envelope_bq.width_bq);
+    LOG3A("config.envelope_bq.height_bq %d", config.envelope_bq.height_bq);
+    LOG3A("config.ispfilter_bq.width_bq %d", config.ispfilter_bq.width_bq);
+    LOG3A("config.ispfilter_bq.height_bq %d", config.ispfilter_bq.height_bq);
+    LOG3A("config.gdc_shift_x %d", config.gdc_shift_x);
+    LOG3A("config.gdc_shift_y %d", config.gdc_shift_y);
+    LOG3A("config.oxdim_y %d", config.oxdim_y);
+    LOG3A("config.oydim_y %d", config.oydim_y);
+    LOG3A("config.oxdim_uv %d", config.oxdim_uv);
+    LOG3A("config.oydim_uv %d", config.oydim_uv);
+    LOG3A("config.hw_config.scan_mode %d", config.hw_config.scan_mode);
+    LOG3A("config.hw_config.interpolation %d", config.hw_config.interpolation);
+    LOG3A("config.hw_config.performance_point %d", config.hw_config.performance_point);
+    LOG3A("config.validate_morph_table = %s",
+          (config.validate_morph_table == true) ? "true" : "false");
+    LOG3A("config.use_lens_distortion_correction = %s",
+          (config.use_lens_distortion_correction == true) ? "true" : "false");
+
+    return OK;
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/3a/Dvs.h b/camera/hal/intel/ipu6/src/3a/Dvs.h
new file mode 100644
index 000000000000..60901c4818ae
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/Dvs.h
@@ -0,0 +1,119 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_dvs.h>
+#include <ia_dvs_types.h>
+#include <ia_isp_bxt.h>
+#include <memory>
+#include <vector>
+
+#include "iutils/Thread.h"
+#include "iutils/Errors.h"
+
+#include "AiqSetting.h"
+#include "CameraEvent.h"
+#include "DvsResult.h"
+
+#ifdef ENABLE_SANDBOXING
+#include "modules/sandboxing/client/IntelDvs.h"
+#else
+#include "modules/algowrapper/IntelDvs.h"
+#endif
+
+namespace icamera {
+struct DvsStatistics {
+    explicit DvsStatistics(ia_dvs_statistics *dvs = nullptr, int64_t seq = -1) {
+        dvsStats = dvs;
+        sequence = seq;
+    }
+    ia_dvs_statistics *dvsStats;
+    int64_t sequence;
+};
+
+/**
+ * \class Dvs
+ * Wrapper of the DVSx, provide 2 basic functionalities in video mode:
+ * 1. zoom (including center and freeform)
+ * 2. DVS
+ * The algorithm should generate the morph table to support the
+ * above functionalities.
+ */
+class Dvs : public EventListener {
+ public:
+    explicit Dvs(int cameraId, AiqSetting *setting = nullptr);
+    ~Dvs();
+
+    int init();
+    int deinit();
+    int configure(const std::vector<ConfigMode>& configMode);
+    int configure(TuningMode tuningMode, uint32_t kernelId,
+                  int srcWidth, int srcHeight, int dstWidth, int dstHeight);
+    int updateParameter(const aiq_parameter_t &param);
+    void handleEvent(EventData eventData);
+
+    int run(const ia_aiq_ae_results *aeResults, const ia_aiq_af_results *afResults,
+            DvsResult *result, int64_t sequence = 0);
+
+ private:
+    int configureDigitalZoom(ia_dvs_zoom_mode zoom_mode, ia_rectangle *zoom_region,
+                             ia_coordinate *zoom_coordinate);
+    int setZoomRatio(float zoom);
+
+    int initDvsHandle(TuningMode tuningMode);
+    int deinitDvsHandle();
+    int initDVSTable();
+    int deInitDVSTable();
+    int reconfigure();
+    int runImpl(const ia_aiq_ae_results *aeResults, const ia_aiq_af_results *afResults);
+    int getMorphTable(int64_t sequence, DvsResult *result);
+    int getImageTrans(int64_t sequence, DvsResult *result);
+    int setDVSConfiguration(uint32_t kernelId, ia_dvs_configuration *config);
+    int dumpDVSTable(ia_dvs_morph_table *table, int64_t sequence);
+    int dumpDVSTable(ia_dvs_image_transformation *trans, int64_t sequence);
+    int dumpConfiguration(ia_dvs_configuration config);
+
+ private:
+    // Guard for class Dvs public API
+    IntelDvs *mIntelDvs;
+    Mutex mLock;
+    ia_dvs_state *mDvsHandle;
+    bool mDvsEnabled;
+    bool mLdcEnabled;
+    bool mRscEnabled;
+    float mDigitalZoomRatio;
+    int mCameraId;
+    float mFps;
+    ConfigMode mConfigMode;
+    TuningMode mTuningMode;
+    AiqSetting *mAiqSetting;
+
+    uint32_t mKernelId;
+    camera_resolution_t mSrcResolution;
+    camera_resolution_t mDstResolution;
+    camera_resolution_t mEnvelopeResolution;
+
+    ia_dvs_morph_table *mMorphTable;
+    ia_dvs_image_transformation mImage_transformation;
+    ia_dvs_statistics *mStatistics;
+    bool mConfigured;
+
+    // prevent copy constructor and assignment operator
+    DISALLOW_COPY_AND_ASSIGN(Dvs);
+};
+
+}  /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/DvsResult.cpp b/camera/hal/intel/ipu6/src/3a/DvsResult.cpp
new file mode 100644
index 000000000000..c9a80301621e
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/DvsResult.cpp
@@ -0,0 +1,105 @@
+/*
+ * Copyright (C) 2019 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "DvsResult"
+
+#include "src/3a/DvsResult.h"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+#include "iutils/Errors.h"
+
+namespace icamera {
+DvsResult::DvsResult() :
+    mSequence(-1) {
+    LOG3A("@%s", __func__);
+
+    CLEAR(mTransformation);
+    CLEAR(mMorphTable);
+    CLEAR(mDvsXcoordsY);
+    CLEAR(mDvsYcoordsY);
+    CLEAR(mDvsXcoordsUV);
+    CLEAR(mDvsYcoordsUV);
+    CLEAR(mDvsXcoordsUVFloat);
+    CLEAR(mDvsYcoordsUVFloat);
+    mMorphTable.xcoords_y = mDvsXcoordsY;
+    mMorphTable.ycoords_y = mDvsYcoordsY;
+    mMorphTable.xcoords_uv = mDvsXcoordsUV;
+    mMorphTable.ycoords_uv = mDvsYcoordsUV;
+    mMorphTable.xcoords_uv_float = mDvsXcoordsUVFloat;
+    mMorphTable.ycoords_uv_float = mDvsYcoordsUVFloat;
+}
+
+DvsResult::~DvsResult() {
+    LOG3A("@%s", __func__);
+}
+
+int DvsResult::deepCopyDvsResults(const ia_dvs_morph_table& src, ia_dvs_morph_table* dst) {
+    LOG3A("%s", __func__);
+
+    CheckError(!dst || !dst->xcoords_y || !dst->ycoords_y
+               || !dst->xcoords_uv || !dst->ycoords_uv
+               || !dst->xcoords_uv_float || !dst->ycoords_uv_float,
+               BAD_VALUE , "Failed to deep copy DVS result- invalid destination");
+
+    CheckError(!src.xcoords_y || !src.ycoords_y
+               || !src.xcoords_uv || !src.ycoords_uv
+               || !src.xcoords_uv_float || !src.ycoords_uv_float,
+               BAD_VALUE , "Failed to deep copy DVS result- invalid source");
+
+    CheckError(src.width_y == 0 || src.height_y == 0 || src.width_uv == 0 || src.height_uv == 0,
+               BAD_VALUE , "Failed to deep copy DVS result- invalid source size y[%dx%d] uv[%dx%d]",
+               src.width_y, src.height_y, src.width_uv, src.height_uv);
+
+    dst->width_y = src.width_y;
+    dst->height_y = src.height_y;
+    dst->width_uv = src.width_uv;
+    dst->height_uv = src.height_uv;
+    dst->morph_table_changed = src.morph_table_changed;
+    unsigned int SizeY = dst->width_y  * dst->height_y * sizeof(int32_t);
+    unsigned int SizeUV = dst->width_uv * dst->height_uv * sizeof(int32_t);
+    MEMCPY_S(dst->xcoords_y, SizeY, src.xcoords_y, SizeY);
+    MEMCPY_S(dst->ycoords_y, SizeY, src.ycoords_y, SizeY);
+    MEMCPY_S(dst->xcoords_uv, SizeUV, src.xcoords_uv, SizeUV);
+    MEMCPY_S(dst->ycoords_uv, SizeUV, src.ycoords_uv, SizeUV);
+
+    SizeUV = dst->width_uv * dst->height_uv * sizeof(float);
+    MEMCPY_S(dst->xcoords_uv_float, SizeUV, src.xcoords_uv_float, SizeUV);
+    MEMCPY_S(dst->ycoords_uv_float, SizeUV, src.ycoords_uv_float, SizeUV);
+
+    return OK;
+}
+
+int DvsResult::deepCopyDvsResults(const ia_dvs_image_transformation& src,
+                                  ia_dvs_image_transformation* dst) {
+    LOG3A("%s", __func__);
+
+    CheckError(!dst, BAD_VALUE , "Failed to deep copy DVS result- invalid destination");
+
+    dst->num_homography_matrices = src.num_homography_matrices;
+    MEMCPY_S(dst->matrices, sizeof(dst->matrices), src.matrices, sizeof(src.matrices));
+
+    return OK;
+}
+
+DvsResult &DvsResult::operator=(const DvsResult &other) {
+    deepCopyDvsResults(other.mMorphTable, &this->mMorphTable);
+    deepCopyDvsResults(other.mTransformation, &this->mTransformation);
+    mSequence = other.mSequence;
+
+    return *this;
+}
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/3a/DvsResult.h b/camera/hal/intel/ipu6/src/3a/DvsResult.h
new file mode 100644
index 000000000000..1be7b5b3bec2
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/DvsResult.h
@@ -0,0 +1,48 @@
+/*
+ * Copyright (C) 2019 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <ia_dvs_types.h>
+
+namespace icamera {
+static const int MAX_DVS_COORDS_Y_SIZE = 33 * 69;
+static const int MAX_DVS_COORDS_UV_SIZE = 33 * 69;
+
+class DvsResult {
+ public:
+    DvsResult();
+    ~DvsResult();
+
+    DvsResult& operator=(const DvsResult& other);
+
+    ia_dvs_morph_table mMorphTable;
+    ia_dvs_image_transformation mTransformation;
+    int64_t mSequence;
+
+    static int deepCopyDvsResults(const ia_dvs_morph_table& src, ia_dvs_morph_table* dst);
+    static int deepCopyDvsResults(const ia_dvs_image_transformation& src,
+                                  ia_dvs_image_transformation* dst);
+
+ private:
+    uint32_t mDvsXcoordsY[MAX_DVS_COORDS_Y_SIZE];
+    uint32_t mDvsYcoordsY[MAX_DVS_COORDS_Y_SIZE];
+    uint32_t mDvsXcoordsUV[MAX_DVS_COORDS_UV_SIZE];
+    uint32_t mDvsYcoordsUV[MAX_DVS_COORDS_UV_SIZE];
+    float mDvsXcoordsUVFloat[MAX_DVS_COORDS_UV_SIZE];
+    float mDvsYcoordsUVFloat[MAX_DVS_COORDS_UV_SIZE];
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/3a/I3AControlFactory.cpp b/camera/hal/intel/ipu6/src/3a/I3AControlFactory.cpp
new file mode 100644
index 000000000000..8eaac4ab5145
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/I3AControlFactory.cpp
@@ -0,0 +1,35 @@
+/*
+ * Copyright (C) 2015-2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "I3AControlFactory"
+
+#include "iutils/CameraLog.h"
+#include "PlatformData.h"
+
+#include "I3AControlFactory.h"
+
+namespace icamera {
+
+AiqUnitBase *I3AControlFactory::createI3AControl(int cameraId, SensorHwCtrl *sensorHw, LensHw *lensHw)
+{
+    LOG1("@%s cameraId = %d", __func__, cameraId);
+    if (PlatformData::isEnableAIQ(cameraId)) {
+        return new AiqUnit(cameraId, sensorHw, lensHw);
+    }
+    return new AiqUnitBase();
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/I3AControlFactory.h b/camera/hal/intel/ipu6/src/3a/I3AControlFactory.h
new file mode 100644
index 000000000000..93482cc2996c
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/I3AControlFactory.h
@@ -0,0 +1,44 @@
+/*
+ * Copyright (C) 2015-2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "AiqUnit.h"
+#include "SensorHwCtrl.h"
+#include "LensHw.h"
+
+namespace icamera {
+
+/*
+ * \factory class I3AControlFactory
+ * This class is used to create the right instance of 3A unit
+ * automatically based on 3a enabled status
+ */
+class I3AControlFactory {
+public:
+    /**
+     * \brief Select the AIQ unit according to config file and compiling option
+     *
+     * \param cameraId: the camera id
+     * \param sensorHw: the hw sensor
+     * \param lensHw: the hw lens
+     *
+     * \return the AIQ unit base class
+     */
+    static AiqUnitBase *createI3AControl(int cameraId, SensorHwCtrl *sensorHw, LensHw *lensHw);
+};
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/ImagingControl.h b/camera/hal/intel/ipu6/src/3a/ImagingControl.h
new file mode 100644
index 000000000000..74fa2289a34c
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/ImagingControl.h
@@ -0,0 +1,46 @@
+/*
+ * Copyright (C) 2018-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "iutils/Errors.h"
+#include "AiqSetting.h"
+#include "AiqResult.h"
+
+namespace icamera {
+
+/*
+ * \interface class ImagingControl
+ * This is an interface class for imaging algorithm controllers.
+ */
+class ImagingControl {
+public:
+    virtual ~ImagingControl() {}
+
+    virtual int init() = 0;
+    virtual int deinit() = 0;
+    virtual int run(AiqResult *aiqResult, int algoType) = 0;
+
+    virtual int configure(const std::vector<ConfigMode>& configModes) { return OK; }
+    virtual int updateParameter(const aiq_parameter_t &param) { return OK; }
+    virtual int setFrameInfo(const ia_aiq_frame_params &frameParams) { return OK; }
+    virtual int setSensorInfo(const ia_aiq_exposure_sensor_descriptor &descriptor) { return OK; }
+    virtual int setStatistics(const ia_aiq_statistics_input_params_v4 *ispStatistics) { return OK; }
+    virtual int getSupportedAlgoType() { return IMAGING_ALGO_NONE; }
+
+};
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/3a/LensManager.cpp b/camera/hal/intel/ipu6/src/3a/LensManager.cpp
new file mode 100644
index 000000000000..7bc95176543b
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/LensManager.cpp
@@ -0,0 +1,101 @@
+/*
+ * Copyright (C) 2016-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "LensManager"
+
+#include "LensManager.h"
+#include "iutils/Utils.h"
+#include "iutils/Errors.h"
+#include "iutils/CameraLog.h"
+#include "PlatformData.h"
+
+namespace icamera {
+
+LensManager::LensManager(int cameraId, LensHw *lensHw) :
+    mCameraId(cameraId),
+    mLensHw(lensHw),
+    mDcIrisCommand(ia_aiq_aperture_control_dc_iris_close),
+    mFocusPosition(-1)
+{
+    LOG1("%s, mCameraId = %d", __func__, mCameraId);
+}
+
+LensManager::~LensManager()
+{
+    LOG1("%s, mCameraId = %d", __func__, mCameraId);
+}
+
+int LensManager::start()
+{
+    LOG1("%s, mCameraId = %d", __func__, mCameraId);
+    AutoMutex l(mLock);
+
+    mDcIrisCommand = ia_aiq_aperture_control_dc_iris_close;
+    mFocusPosition = -1;
+
+    return OK;
+}
+
+int LensManager::stop()
+{
+    LOG1("%s, mCameraId = %d", __func__, mCameraId);
+    AutoMutex l(mLock);
+
+    if (!mLensHw->isLensSubdevAvailable()) {
+        return OK;
+    }
+
+    return OK;
+}
+
+int LensManager::setLensResult(const ia_aiq_ae_results &aeResults,
+                               const ia_aiq_af_results &afResults)
+{
+    LOG3A("%s, mCameraId = %d", __func__, mCameraId);
+    AutoMutex l(mLock);
+
+    if (!mLensHw->isLensSubdevAvailable()) {
+        return OK;
+    }
+
+    int ret = OK;
+
+    int lensHwType = PlatformData::getLensHwType(mCameraId);
+    switch(lensHwType) {
+        case LENS_VCM_HW:
+            if (mFocusPosition != afResults.next_lens_position) {
+                ret = mLensHw->setFocusPosition(afResults.next_lens_position);
+                mFocusPosition = afResults.next_lens_position;
+                LOG3A("mFocusPosition = %d, camera id %d", mFocusPosition, mCameraId);
+                LOG2("SENSORCTRLINFO: vcm_step=%d", mFocusPosition);
+            }
+            break;
+        default:
+            LOGW("Not supported Lens HW type, lensHwType = %d", lensHwType);
+            break;
+    }
+
+    return ret;
+}
+
+void LensManager::getLensInfo(aiq_parameter_t &aiqParam) {
+
+    if (PlatformData::getLensHwType(mCameraId) == LENS_VCM_HW) {
+        mLensHw->getLatestPosition(aiqParam.lensPosition, aiqParam.lensMovementStartTimestamp);
+    }
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/LensManager.h b/camera/hal/intel/ipu6/src/3a/LensManager.h
new file mode 100644
index 000000000000..cbd1de401d28
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/LensManager.h
@@ -0,0 +1,77 @@
+/*
+ * Copyright (C) 2016-2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "ia_aiq.h"
+
+#include "LensHw.h"
+#include "AiqSetting.h"
+
+namespace icamera {
+
+/*
+ * \class LensManager
+ * This class is used to control focus and aperture related controls.
+ */
+class LensManager {
+
+public:
+    LensManager(int cameraId, LensHw *lensHw);
+    ~LensManager();
+
+    /**
+     * \brief Called when AIQ engine is started
+     */
+    int start();
+
+    /**
+     * \brief Called when AIQ engine is stopped.
+     */
+    int stop();
+
+    /**
+     * \brief Set Lens results
+     *
+     * \param[in] ia_aiq_ae_results includes aperture result
+     *            and ia_aiq_af_results includes focus result.
+     *
+     * \return OK if set successfully.
+     */
+    int setLensResult(const ia_aiq_ae_results &aeResults,
+                      const ia_aiq_af_results &afResults);
+    /**
+     * \brief Get Lens info
+     *
+     * \param[out] aiqParam: updating lens related parameters.
+     *
+     */
+    void getLensInfo(aiq_parameter_t &aiqParam);
+
+private:
+    DISALLOW_COPY_AND_ASSIGN(LensManager);
+
+private:
+    int mCameraId;
+    LensHw *mLensHw;
+    ia_aiq_aperture_control_dc_iris_command mDcIrisCommand;
+    int mFocusPosition;
+
+    // Guard for LensManager public API.
+    Mutex mLock;
+};
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/Ltm.cpp b/camera/hal/intel/ipu6/src/3a/Ltm.cpp
new file mode 100644
index 000000000000..e3c0667581a9
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/Ltm.cpp
@@ -0,0 +1,545 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Ltm"
+
+#include <cmath>
+#include <memory>
+
+#include "Ltm.h"
+
+#include "iutils/Utils.h"
+#include "iutils/Errors.h"
+#include "iutils/CameraLog.h"
+
+#include "PlatformData.h"
+#include "AiqResultStorage.h"
+#include "AiqUtils.h"
+
+#include "ia_log.h"
+
+namespace icamera {
+
+Ltm::Ltm(int cameraId) :
+    mCameraId(cameraId),
+    mLtm(nullptr),
+    mTuningMode(TUNING_MODE_MAX),
+    mLtmState(LTM_NOT_INIT),
+    mThreadRunning(false),
+    mInputParamIndex(-1)
+{
+    CLEAR(mLtmParams);
+    CLEAR(mSisBuffer);
+    CLEAR(mLtmBinParam);
+
+    mIntelLtm = std::unique_ptr<IntelLtm>(new IntelLtm());
+    mLtmThread = new LtmThread(this);
+    LOG3A("%s", __func__);
+}
+
+Ltm::~Ltm()
+{
+    LOG3A("%s", __func__);
+    mLtmThread->join();
+    delete mLtmThread;
+}
+
+int Ltm::initLtmParams()
+{
+    for (int i = 0; i < kMaxLtmParamsNum; i++) {
+        mLtmParams[i]->ltmParams.ltm_level = ia_ltm_level_use_tuning;
+        mLtmParams[i]->ltmParams.frame_use = ia_aiq_frame_use_video;
+        mLtmParams[i]->ltmParams.ev_shift = 0;
+        mLtmParams[i]->ltmParams.ltm_strength_manual = 100;
+        mLtmParams[i]->ltmParams.gtm_input_params_ptr = &(mLtmParams[i]->gtmParams);
+
+        mLtmParams[i]->gtmParams.manual_convergence_time = -1;
+        mLtmParams[i]->gtmParams.manual_gain = -1;
+        mLtmParams[i]->gtmParams.frame_timestamp = 0;
+    }
+
+    return OK;
+}
+
+int Ltm::init()
+{
+    LOG3A("%s", __func__);
+
+    AutoMutex l(mLtmLock);
+
+    for (int i = 0; i < kMaxLtmParamsNum; i++) {
+        mLtmParams[i] = new LtmInputParams;
+        mSisBuffer[i] = new SisBuffer;
+    }
+
+    initLtmParams();
+
+    mLtmState = LTM_INIT;
+
+    return OK;
+}
+
+int Ltm::deinit()
+{
+    LOG3A("%s", __func__);
+
+    deinitIaLtmHandle();
+
+    AutoMutex l(mLtmLock);
+
+    for (int i = 0; i < kMaxLtmParamsNum; i++) {
+        if (mSisBuffer[i]->sisImage.data) {
+            free(mSisBuffer[i]->sisImage.data);
+            mSisBuffer[i]->sisImage.data = nullptr;
+        }
+
+        if (mLtmParams[i]->ltmParams.input_image_ptr) {
+            free(mLtmParams[i]->ltmParams.input_image_ptr);
+            mLtmParams[i]->ltmParams.input_image_ptr = nullptr;
+        }
+
+        delete mSisBuffer[i];
+        mSisBuffer[i] = nullptr;
+
+        delete mLtmParams[i];
+        mLtmParams[i] = nullptr;
+    }
+    mLtmState = LTM_NOT_INIT;
+
+    return OK;
+}
+
+int Ltm::initIaLtmHandle(TuningMode tuningMode)
+{
+    LOG3A("%s", __func__);
+
+    ia_mkn *mkn = static_cast<ia_mkn *>(PlatformData::getMknHandle(mCameraId));
+    ia_binary_data otherData;
+    int ret = PlatformData::getCpfAndCmc(mCameraId, nullptr, nullptr, &otherData,
+                                         nullptr, tuningMode);
+    CheckError(ret != OK, BAD_VALUE, "@%s, Get cpf data failed", __func__);
+    {
+        PERF_CAMERA_ATRACE_PARAM1("ia_ltm_init", 0);
+        mLtm = mIntelLtm->init(&otherData, mkn);
+    }
+    CheckError(mLtm == nullptr, NO_INIT, "Failed to init ltm");
+
+    return OK;
+}
+
+int Ltm::deinitIaLtmHandle()
+{
+    LOG3A("%s", __func__);
+
+    if (mLtm) {
+        PERF_CAMERA_ATRACE_PARAM1("ia_ltm_deinit", 0);
+        mIntelLtm->deinit(mLtm);
+        mLtm = nullptr;
+    }
+
+    return OK;
+}
+
+int Ltm::configure(const std::vector<ConfigMode>& configModes)
+{
+    LOG3A("%s", __func__);
+
+    TuningMode tMode = TUNING_MODE_MAX;
+    for (auto cfg : configModes) {
+        // Only support the 1st tuning mode if multiple config mode is configured.
+        if (cfg == CAMERA_STREAM_CONFIGURATION_MODE_NORMAL) {
+            tMode = TUNING_MODE_VIDEO;
+            break;
+        }
+    }
+
+    if (tMode == TUNING_MODE_MAX) {
+        return OK;
+    }
+
+    if (mLtmState == LTM_CONFIGURED && mTuningMode == tMode) {
+        return OK;
+    }
+
+    deinitIaLtmHandle();
+
+    int ret = initIaLtmHandle(tMode);
+    CheckError((ret != OK), ret, "%s, configure LTM algo failed %d", __func__, ret);
+
+    mTuningMode = tMode;
+    mLtmState = LTM_CONFIGURED;
+
+    updateTuningData();
+
+    LOG3A("%s Ltm algo is Configured", __func__);
+    return OK;
+}
+
+int Ltm::start()
+{
+    LOG1("@%s", __func__);
+    AutoMutex l(mLtmLock);
+
+    if (PlatformData::isEnableLtmThread(mCameraId)) {
+        mThreadRunning = true;
+        mLtmThread->run("ltm_thread", PRIORITY_NORMAL);
+    }
+
+    return OK;
+}
+
+void Ltm::stop()
+{
+    LOG1("@%s", __func__);
+
+    if (PlatformData::isEnableLtmThread(mCameraId)) {
+        mLtmThread->requestExit();
+        {
+            AutoMutex l(mLtmLock);
+            mThreadRunning = false;
+            mParamAvailableSignal.signal();
+        }
+        mLtmThread->requestExitAndWait();
+    }
+
+    while (!mLtmParamsQ.empty()) {
+        mLtmParamsQ.pop();
+    }
+}
+
+void Ltm::handleEvent(EventData eventData)
+{
+    if (eventData.type == EVENT_PSYS_STATS_BUF_READY) {
+        LOG3A("%s: handle EVENT_PSYS_STATS_BUF_READY", __func__);
+        long sequence = eventData.data.statsReady.sequence;
+        unsigned long long timestamp = TIMEVAL2USECS(eventData.data.statsReady.timestamp);
+
+        LtmStatistics *ltmStatistics = AiqResultStorage::getInstance(mCameraId)->getLtmStatistics();
+        if (ltmStatistics->sequence != sequence || ltmStatistics->yvGrid == nullptr) return;
+
+        handleLtm(ltmStatistics->yvGrid, timestamp, sequence);
+    } else if (eventData.type == EVENT_PSYS_STATS_SIS_BUF_READY) {
+        LOG3A("%s: handle EVENT_PSYS_STATS_SIS_BUF_READY", __func__);
+        handleSisLtm(eventData.buffer);
+    }
+}
+
+AiqResult *Ltm::getAiqResult(long sequence)
+{
+    long ltmSequence = sequence;
+    AiqResultStorage *resultStorage = AiqResultStorage::getInstance(mCameraId);
+    if (ltmSequence > 0) {
+        ltmSequence += PlatformData::getLtmGainLag(mCameraId);
+    }
+
+    LOG3A("%s, ltmSequence %ld, sequence %ld", __func__, ltmSequence, sequence);
+    AiqResult* feedback = const_cast<AiqResult*>(resultStorage->getAiqResult(ltmSequence));
+    if (feedback == nullptr) {
+        LOGW("%s: no feed back result for sequence %ld! use the latest instead",
+                __func__, ltmSequence);
+        feedback = const_cast<AiqResult*>(resultStorage->getAiqResult());
+    }
+
+    return feedback;
+}
+
+int Ltm::handleLtm(ia_isp_bxt_hdr_yv_grid_t* inputYvGrid, unsigned long long timestamp, long sequence)
+{
+    LOG3A("@%s", __func__);
+    AutoMutex l(mLtmLock);
+
+    mInputParamIndex++;
+    mInputParamIndex %= kMaxLtmParamsNum;
+
+    if (inputYvGrid) {
+        mLtmParams[mInputParamIndex]->yvGrid = *inputYvGrid;
+        mLtmParams[mInputParamIndex]->ltmParams.yv_grid = &(mLtmParams[mInputParamIndex]->yvGrid);
+    } else {
+        mLtmParams[mInputParamIndex]->ltmParams.yv_grid = nullptr;
+    }
+
+    AiqResult* feedback = getAiqResult(sequence);
+    updateParameter(feedback->mAiqParam, timestamp);
+
+    if ((!PlatformData::isEnableLtmThread(mCameraId)) || sequence == 0) {
+        AiqResultStorage *resultStorage = AiqResultStorage::getInstance(mCameraId);
+        ltm_result_t *ltmResult = resultStorage->acquireLtmResult();
+
+        runLtm(&(feedback->mAeResults), ltmResult);
+        resultStorage->updateLtmResult(sequence);
+        updateTuningData();
+    } else {
+        mLtmParams[mInputParamIndex]->sequence = sequence;
+        bool needSignal = mLtmParamsQ.empty();
+        mLtmParamsQ.push(mLtmParams[mInputParamIndex]);
+        if (needSignal) {
+            mParamAvailableSignal.signal();
+        }
+    }
+
+    return OK;
+}
+
+int Ltm::handleSisLtm(const std::shared_ptr<CameraBuffer> &cameraBuffer)
+{
+    LOG3A("@%s", __func__);
+    AutoMutex l(mLtmLock);
+
+    ia_binary_data* sisFrame = (ia_binary_data*)cameraBuffer->getBufferAddr();
+    CheckError(sisFrame == nullptr, BAD_VALUE, "sis frame buffer is nullptr!");
+    int sisWidth = cameraBuffer->getWidth();
+    int sisHeight = cameraBuffer->getHeight();
+    int sequence = cameraBuffer->getSequence();
+
+    mInputParamIndex++;
+    mInputParamIndex %= kMaxLtmParamsNum;
+
+    AiqResult *feedback = getAiqResult(sequence);
+
+    int size = sisFrame->size;
+    CheckError((size <= 0), BAD_VALUE, "sis data size err!");
+
+    void *data = sisFrame->data;
+    CheckError((data == nullptr), BAD_VALUE, "sis data ptr err!");
+
+    ia_ltm_input_image *inputImagePtr = mLtmParams[mInputParamIndex]->ltmParams.input_image_ptr;
+    if (!inputImagePtr) {
+        inputImagePtr = (ia_ltm_input_image *)malloc(sizeof(ia_ltm_input_image));
+        CheckError((inputImagePtr == nullptr), NO_INIT, "Error in initing image ptr");
+
+        memset(inputImagePtr, 0, sizeof(ia_ltm_input_image));
+        mLtmParams[mInputParamIndex]->ltmParams.input_image_ptr = inputImagePtr;
+
+        inputImagePtr->image_info.raw_image.data_format = ia_image_data_format_rawplain16_interleaved;
+        inputImagePtr->image_info.raw_image.bayer_order = cmc_bayer_order_grbg;
+        inputImagePtr->image_info.raw_image.data_format_bpp = 16;
+        inputImagePtr->image_info.raw_image.data_bpp = 15;
+
+        mSisBuffer[mInputParamIndex]->sisPort = SIS_A;
+        mSisBuffer[mInputParamIndex]->sisImage.size = size;
+        mSisBuffer[mInputParamIndex]->sisImage.data = malloc(size);
+        CheckError((mSisBuffer[mInputParamIndex]->sisImage.data == nullptr),
+            NO_MEMORY, "sis buffer allocated failed!");
+
+        MEMCPY_S(mSisBuffer[mInputParamIndex]->sisImage.data, size, data, size);
+
+        inputImagePtr->image_data = &mSisBuffer[mInputParamIndex]->sisImage;
+        // width_cols and height_lines are quad count, need to divide 2 for them.
+        inputImagePtr->image_info.raw_image.width_cols = sisWidth / 2;
+        inputImagePtr->image_info.raw_image.height_lines = sisHeight / 2;
+
+        mLtmBinParam.sParam.gridWidth = sisWidth;
+        mLtmBinParam.sParam.gridHeight = sisHeight;
+    }
+
+    updateParameter(feedback->mAiqParam, 0);
+
+    if ((!PlatformData::isEnableLtmThread(mCameraId)) || sequence == 0) {
+        AiqResultStorage *resultStorage = AiqResultStorage::getInstance(mCameraId);
+        ltm_result_t *ltmResult = resultStorage->acquireLtmResult();
+
+        runLtm(&(feedback->mAeResults), ltmResult);
+        resultStorage->updateLtmResult(sequence);
+        updateTuningData();
+    } else {
+        mLtmParams[mInputParamIndex]->sequence = sequence;
+        bool needSignal = mLtmParamsQ.empty();
+        mLtmParamsQ.push(mLtmParams[mInputParamIndex]);
+        if (needSignal) {
+            mParamAvailableSignal.signal();
+        }
+    }
+
+    return OK;
+}
+
+int Ltm::runLtm()
+{
+    LtmInputParams *inputParams = NULL;
+
+    AiqResultStorage *resultStorage = AiqResultStorage::getInstance(mCameraId);
+    ConditionLock lock(mLtmLock);
+
+    while(mLtmParamsQ.empty()) {
+        // To prevent possible dead lock during stop of ltm thread.
+        if (!mThreadRunning) {
+            LOG2("%s, ltm thread is not active, no need to wait ltm stat", __func__);
+            return OK;
+        }
+
+        mParamAvailableSignal.wait(lock);
+
+        if (!mThreadRunning) {
+            LOG2("%s, ltm thread is not active while waiting ltm stat", __func__);
+            return OK;
+        }
+    }
+
+    CheckError(mLtmParamsQ.empty(), UNKNOWN_ERROR, "Failed to get ltm input params buffers");
+    inputParams = mLtmParamsQ.front();
+    mLtmParamsQ.pop();
+    CheckError(!inputParams, OK, "%s, the inputParams is NULL", __func__);
+
+    ltm_result_t *ltmResult = resultStorage->acquireLtmResult();
+    LOG1("@%s the sequence: %ld", __func__, inputParams->sequence);
+    AiqResult *feedback = getAiqResult(inputParams->sequence);
+
+    mLtmBinParam.sequence = inputParams->sequence;
+    runLtm(&(feedback->mAeResults), ltmResult, &(inputParams->ltmParams));
+    resultStorage->updateLtmResult(inputParams->sequence);
+
+    updateTuningData();
+
+    return OK;
+}
+
+int Ltm::runLtm(ia_aiq_ae_results* aeResult, ltm_result_t *ltmResult, ia_ltm_input_params *ltmParams)
+{
+    LOG3A("%s", __func__);
+    PERF_CAMERA_ATRACE();
+
+    if (mLtmState != LTM_CONFIGURED) {
+        return OK;
+    }
+
+    ia_ltm_results *tmpLtmResults = nullptr;
+    ia_ltm_drc_params *tmpLtmDrcParams = nullptr;
+    ia_ltm_input_params *tmpLtmParams = ltmParams;
+
+    if (tmpLtmParams == NULL) {
+        tmpLtmParams = &(mLtmParams[mInputParamIndex]->ltmParams);
+    }
+
+    if (!tmpLtmParams->yv_grid) {
+        // ltm can run without Yv grid and default ltm params will be used.
+        LOGD("mYvGrid is Null.");
+    }
+    tmpLtmParams->ae_results = aeResult;
+
+    LOG3A("%s: begin running LTM", __func__);
+    ia_err iaErr;
+    {
+        PERF_CAMERA_ATRACE_PARAM1_IMAGING("ia_ltm_run", 0);
+        iaErr = mIntelLtm->run(mLtm, tmpLtmParams, &tmpLtmResults, &tmpLtmDrcParams);
+    }
+
+    int ret = AiqUtils::convertError(iaErr);
+    CheckError(ret != OK, ret, "Error running LTM: %d", ret);
+
+    LOG3A("%s: LTM GAIN = %lf", __func__, tmpLtmResults->ltm_gain);
+
+    dumpLtmDrcParams(tmpLtmDrcParams);
+    dumpLtmResultsParams(tmpLtmResults);
+
+    {
+        PERF_CAMERA_ATRACE_PARAM1("deepCopyLtmResults", 0);
+        ret = AiqUtils::deepCopyLtmResults(*tmpLtmResults, &ltmResult->ltmResults);
+    }
+    CheckError(ret != OK, ret, "Error on copying LTM results: %d", ret);
+
+    {
+        PERF_CAMERA_ATRACE_PARAM1("deepCopyLtmDRCParams", 0);
+        ret = AiqUtils::deepCopyLtmDRCParams(*tmpLtmDrcParams, &ltmResult->ltmDrcParams);
+    }
+    CheckError(ret != OK, ret, "Error on copying DRC results: %d", ret);
+
+    ltmResult->yvGridInfo.width = tmpLtmParams->yv_grid ? tmpLtmParams->yv_grid->grid_width : 0;
+    ltmResult->yvGridInfo.height = tmpLtmParams->yv_grid ? tmpLtmParams->yv_grid->grid_height : 0;
+
+    return OK;
+}
+
+int Ltm::updateTuningData()
+{
+    LOG3A("%s", __func__);
+
+    if (mLtmState != LTM_CONFIGURED) {
+        return INVALID_OPERATION;
+    }
+
+    return OK;
+}
+
+int Ltm::updateParameter(const aiq_parameter_t &param, unsigned long long timestamp)
+{
+    LOG3A("%s: frame resolution %dx%d", __func__, param.resolution.width, param.resolution.height);
+
+    mLtmParams[mInputParamIndex]->ltmParams.ev_shift = param.evShift;
+    mLtmParams[mInputParamIndex]->ltmParams.ltm_strength_manual = param.ltmStrength;
+    mLtmParams[mInputParamIndex]->ltmParams.frame_width = param.resolution.width;
+    mLtmParams[mInputParamIndex]->ltmParams.frame_height = param.resolution.height;
+
+    mLtmParams[mInputParamIndex]->gtmParams.manual_convergence_time = -1;
+
+    if (param.manualGain >= 0) {
+        mLtmParams[mInputParamIndex]->gtmParams.manual_gain = pow(10, (param.manualGain / 20));
+    } else {
+        mLtmParams[mInputParamIndex]->gtmParams.manual_gain = -1;
+    }
+    mLtmParams[mInputParamIndex]->gtmParams.frame_timestamp = timestamp;
+
+    LOG3A("%s: Ltm EV shift %f strength %d", __func__,
+            mLtmParams[mInputParamIndex]->ltmParams.ev_shift,
+            mLtmParams[mInputParamIndex]->ltmParams.ltm_strength_manual);
+    LOG3A("%s: Gtm manual gain %f, manual convergence time %f, frame timestamp %llu",
+            __func__, mLtmParams[mInputParamIndex]->gtmParams.manual_gain,
+            mLtmParams[mInputParamIndex]->gtmParams.manual_convergence_time,
+            mLtmParams[mInputParamIndex]->gtmParams.frame_timestamp);
+
+    return OK;
+}
+
+int Ltm::dumpLtmDrcParams(const ia_ltm_drc_params* ltmDrcParams)
+{
+    if (!Log::isDebugLevelEnable(CAMERA_DEBUG_LOG_AIQ)) return OK;
+
+    LOG3A("%s", __func__);
+
+    if (!ltmDrcParams){
+        LOG2("%s: ltmDrcParams is nullptr, and nothing to dump.", __func__);
+        return BAD_VALUE;
+    }
+
+    //Only dump first 10 values.
+    for (unsigned int i = 0; i < 10; i++) {
+       LOG3A("   LTM DRC params matrix gain %u weight %u",
+            ltmDrcParams->gain_map[i], ltmDrcParams->weight_map[i]);
+    }
+
+    return OK;
+}
+
+int Ltm::dumpLtmResultsParams(const ia_ltm_results *ltmResults)
+{
+    if (!CameraDump::isDumpTypeEnable(DUMP_LTM_OUTPUT)) return OK;
+
+    LOG3A("%s", __func__);
+
+    if (!ltmResults){
+        LOG2("%s: ltmResults is nullptr, and nothing to dump.", __func__);
+        return BAD_VALUE;
+    }
+
+    char fileName[MAX_NAME_LEN] = {'\0'};
+    snprintf(fileName, (MAX_NAME_LEN-1), "ia_ltm_luts_%ld_w_%d_h_%d.bin",
+        mLtmBinParam.sequence, mLtmBinParam.sParam.gridWidth, mLtmBinParam.sParam.gridHeight);
+
+    CameraDump::writeData(ltmResults->ltm_luts, sizeof(ltmResults->ltm_luts), fileName);
+
+    return OK;
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/Ltm.h b/camera/hal/intel/ipu6/src/3a/Ltm.h
new file mode 100644
index 000000000000..1f97e1975f96
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/Ltm.h
@@ -0,0 +1,185 @@
+/*
+ * Copyright (C) 2016-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <queue>
+#include <memory>
+
+#ifdef ENABLE_SANDBOXING
+#include "modules/sandboxing/client/IntelLtm.h"
+#else
+#include "modules/algowrapper/IntelLtm.h"
+#endif
+
+#include "CameraEvent.h"
+#include "AiqSetting.h"
+#include "AiqResult.h"
+#include "iutils/Thread.h"
+#include "iutils/CameraDump.h"
+
+namespace icamera {
+
+struct ltm_result_t {
+    ia_ltm_drc_params ltmDrcParams;
+    ia_ltm_results ltmResults;
+
+    camera_resolution_t yvGridInfo;
+
+    long sequence;
+
+    ltm_result_t() {
+        CLEAR(ltmDrcParams);
+        CLEAR(ltmResults);
+        yvGridInfo = {0, 0};
+        sequence = -1;
+    }
+};
+
+struct LtmInputParams {
+    ia_isp_bxt_hdr_yv_grid_t yvGrid;
+    ia_ltm_input_params ltmParams;
+    ia_ltm_gtm_input_params gtmParams;
+    long sequence;
+
+    LtmInputParams() {
+        CLEAR(yvGrid);
+        CLEAR(ltmParams);
+        CLEAR(gtmParams);
+        sequence = -1;
+    }
+};
+/**
+ * There are two SIS port, SIS_A is for preview.
+ * SIS_B is for still and capture.
+ */
+enum SisPort
+{
+    SIS_A = 0,
+    SIS_B,
+    MAX_SIS_NUM
+};
+
+struct SisBuffer
+{
+    SisPort sisPort;
+    ia_binary_data sisImage;
+
+    SisBuffer() {
+        sisPort = SIS_A;
+        CLEAR(sisImage);
+    }
+};
+
+struct LtmStatistics {
+    LtmStatistics(ia_isp_bxt_hdr_yv_grid_t *inputYvGrid = nullptr, long seq = -1) {
+        yvGrid = inputYvGrid;
+        sequence = seq;
+    }
+    ia_isp_bxt_hdr_yv_grid_t *yvGrid;
+    long sequence;
+};
+
+/**
+ * \class Ltm
+ * This class is used to run Local tone mapping (Ltm) algorithm.
+ */
+class Ltm : public EventListener {
+ public:
+    Ltm(int cameraId);
+    ~Ltm();
+
+    int init();
+    int deinit();
+    int start();
+    void stop();
+
+    int configure(const std::vector<ConfigMode>& configModes);
+
+    /**
+     * \brief handle statistics event
+     */
+    void handleEvent(EventData eventData);
+
+    int handleLtm(ia_isp_bxt_hdr_yv_grid_t* yvGrid,
+                  unsigned long long timestamp = 0,
+                  long sequence = 0);
+    int handleSisLtm(const std::shared_ptr<CameraBuffer> &cameraBuffer);
+ private:
+    DISALLOW_COPY_AND_ASSIGN(Ltm);
+
+    int runLtm();
+    int initLtmParams();
+    int runLtm(ia_aiq_ae_results* aeResult, ltm_result_t *ltmResult, ia_ltm_input_params *ltmParams = NULL);
+
+    int initIaLtmHandle(TuningMode tuningMode);
+    int deinitIaLtmHandle();
+
+    int updateTuningData();
+    AiqResult *getAiqResult(long sequence);
+    int updateParameter(const aiq_parameter_t &param, unsigned long long timestamp);
+
+    int dumpLtmDrcParams(const ia_ltm_drc_params* ltmDrcParams);
+    int dumpLtmResultsParams(const ia_ltm_results *ltmResults);
+
+ private:
+    /**
+     * \brief The ltm thread
+     */
+    class LtmThread: public Thread {
+        Ltm *mLtmHandle;
+        public:
+        LtmThread(Ltm *pThis)
+            : mLtmHandle(pThis) { }
+
+        virtual bool threadLoop() {
+            int ret = mLtmHandle->runLtm();
+            return (ret == 0);
+        }
+    };
+
+    int mCameraId;
+    ia_ltm *mLtm;
+    TuningMode mTuningMode;
+
+    enum LtmState {
+        LTM_NOT_INIT = 0,
+        LTM_INIT,
+        LTM_CONFIGURED,
+        LTM_MAX
+    } mLtmState;
+
+    // Use it to lock mInputParamIndex and mLtmParams
+    // And use it for thread condition lock
+    Mutex  mLtmLock;
+    LtmThread *mLtmThread;
+    bool mThreadRunning;
+    Condition mParamAvailableSignal;
+    static const nsecs_t kWaitDuration = 2000000000;  // 2000ms
+    static const int kMaxLtmParamsNum = 2;  // 2 ltm input params
+
+    int mInputParamIndex;
+    LtmInputParams *mLtmParams[kMaxLtmParamsNum];
+    std::queue<LtmInputParams *> mLtmParamsQ;
+
+    SisBuffer *mSisBuffer[kMaxLtmParamsNum];
+    BinParam_t mLtmBinParam;
+
+    std::unique_ptr<IntelLtm> mIntelLtm;
+};
+
+}  /* namespace icamera */
+
diff --git a/camera/hal/intel/ipu6/src/3a/MakerNote.cpp b/camera/hal/intel/ipu6/src/3a/MakerNote.cpp
new file mode 100644
index 000000000000..cf93dd336345
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/MakerNote.cpp
@@ -0,0 +1,126 @@
+/*
+ * Copyright (C) 2018-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "MakerNote"
+
+#include <memory>
+
+#include "src/3a/MakerNote.h"
+#include "iutils/Errors.h"
+#include "iutils/CameraLog.h"
+
+namespace icamera {
+
+MakerNote::MakerNote() :
+    mMknState(UNINIT),
+    mMkn(nullptr) {
+    LOG1("@%s", __func__);
+
+    for (int i = 0; i < MAX_MAKER_NOTE_LIST_SIZE; i++) {
+        std::shared_ptr<MakernoteData> data = std::shared_ptr<MakernoteData>(new MakernoteData());
+        mMakernoteDataList.push_back(data);
+    }
+
+    mMkn = mIntelMkn.init(ia_mkn_cfg_compression,
+                          MAKERNOTE_SECTION1_SIZE, MAKERNOTE_SECTION2_SIZE);
+    CheckError(mMkn == nullptr, VOID_VALUE, "@%s, Failed to init mkn", __func__);
+
+    int ret = mIntelMkn.enable(mMkn, true);
+    CheckError(ret != ia_err_none, VOID_VALUE,
+               "@%s, Failed to enable mkn ret %d", __func__, ret);
+
+    mMknState = INIT;
+}
+
+MakerNote::~MakerNote() {
+    LOG1("@%s", __func__);
+
+    AutoMutex lock(mMknLock);
+    mIntelMkn.deinit(mMkn);
+
+    mMakernoteDataList.clear();
+}
+
+int MakerNote::saveMakernoteData(camera_makernote_mode_t makernoteMode, int64_t sequence) {
+    LOG1("@%s", __func__);
+    if (makernoteMode == MAKERNOTE_MODE_OFF) return OK;
+
+    AutoMutex lock(mMknLock);
+    CheckError(mMknState != INIT, BAD_VALUE, "@%s, mkn isn't initialized", __func__);
+
+    ia_mkn_trg mknTrg = (makernoteMode == MAKERNOTE_MODE_JPEG ? ia_mkn_trg_section_1 :
+                                                                ia_mkn_trg_section_2);
+    ia_binary_data makerNote;
+    int ret = mIntelMkn.prepare(mMkn, mknTrg, &makerNote);
+    CheckError(ret != OK, UNKNOWN_ERROR, "@%s, Failed to prepare makernote", __func__);
+    CheckError((makerNote.data == nullptr), UNKNOWN_ERROR,
+               "@%s, invalid makernote data pointer", __func__);
+    CheckError(makerNote.size == 0, UNKNOWN_ERROR, "@%s, 0 size makernote data saved", __func__);
+
+    std::shared_ptr<MakernoteData> data = mMakernoteDataList.front();
+    mMakernoteDataList.pop_front();
+
+    MEMCPY_S(data->section, sizeof(char) * (MAKERNOTE_SECTION1_SIZE + MAKERNOTE_SECTION2_SIZE),
+             makerNote.data, makerNote.size);
+
+    data->size = makerNote.size;
+    data->sequence = sequence;
+    data->timestamp = 0;
+    LOG2("@%s, saved makernote %d for sequence %ld", __func__, makernoteMode, sequence);
+
+    mMakernoteDataList.push_back(data);
+    return OK;
+}
+
+ia_mkn *MakerNote::getMknHandle() {
+    LOG1("@%s", __func__);
+    AutoMutex lock(mMknLock);
+    CheckError(mMknState != INIT, nullptr, "@%s, mkn isn't initialized", __func__);
+
+    return mMkn;
+}
+
+void MakerNote::updateTimestamp(int64_t sequence, uint64_t timestamp) {
+    LOG1("@%s", __func__);
+    AutoMutex lock(mMknLock);
+    CheckError(mMknState != INIT, VOID_VALUE, "@%s, mkn isn't initialized", __func__);
+
+    for (auto rit = mMakernoteDataList.rbegin(); rit != mMakernoteDataList.rend(); ++rit) {
+        if ((*rit)->sequence == sequence) {
+            LOG2("@%s, found sequence %ld for request sequence %ld, timestamp %ld", __func__,
+                 (*rit)->sequence, sequence, timestamp);
+            (*rit)->timestamp = timestamp;
+            break;
+        }
+    }
+}
+
+void MakerNote::acquireMakernoteData(uint64_t timestamp, Parameters *param) {
+    LOG1("@%s", __func__);
+    AutoMutex lock(mMknLock);
+    CheckError(mMknState != INIT, VOID_VALUE, "@%s, mkn isn't initialized", __func__);
+
+    for (auto rit = mMakernoteDataList.rbegin(); rit != mMakernoteDataList.rend(); ++rit) {
+        if ((*rit)->timestamp > 0 && timestamp >= (*rit)->timestamp) {
+            LOG2("@%s, found timestamp %ld for request timestamp %ld", __func__,
+                 (*rit)->timestamp, timestamp);
+            param->setMakernoteData((*rit)->section, (*rit)->size);
+            break;
+        }
+    }
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/3a/MakerNote.h b/camera/hal/intel/ipu6/src/3a/MakerNote.h
new file mode 100644
index 000000000000..201d696222aa
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/MakerNote.h
@@ -0,0 +1,116 @@
+/*
+ * Copyright (C) 2018-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <map>
+#include <list>
+#include <memory>
+
+#ifdef ENABLE_SANDBOXING
+#include "modules/sandboxing/client/IntelMkn.h"
+#else
+#include "modules/algowrapper/IntelMkn.h"
+#endif
+
+#include "iutils/Utils.h"
+#include "iutils/Thread.h"
+#include "Parameters.h"
+
+namespace icamera {
+
+struct MakernoteData {
+    int64_t sequence;
+    uint64_t timestamp;
+    unsigned int size;
+    char section[MAKERNOTE_SECTION1_SIZE + MAKERNOTE_SECTION2_SIZE];
+
+    MakernoteData() {
+        sequence = -1;
+        timestamp = 0;
+        size = 0;
+        CLEAR(section);
+    }
+};
+
+/**
+ * \class MakerNote
+ *
+ * This class encapsulates Intel Makernotes function, and provides interface
+ * for enabling and acquiring Makenotes which is called by AiqEngine, Ltm
+ * and AiqPlus.
+ *
+ */
+class MakerNote {
+ public:
+    MakerNote();
+    ~MakerNote();
+
+    /**
+     * \brief Save Makernote by ia_mkn_trg mode
+     *
+     * param[in] camera_makernote_mode_t: MAKERNOTE_MODE_JPEG is corresponding
+     *           to ia_mkn_trg_section_1 for Normal Jpeg capture;
+     *           MAKERNOTE_MODE_RAW is corresponding to ia_mkn_trg_section_2
+     *           for Raw image capture.
+     * param[in] int64_t sequence: the sequence in latest AiqResult
+     *
+     * return OK if get Makernote successfully, otherwise return ERROR.
+     */
+    int saveMakernoteData(camera_makernote_mode_t makernoteMode, int64_t sequence);
+
+    /**
+     * \brief Get ia_mkn (Makernote) handle.
+     */
+    ia_mkn *getMknHandle();
+
+    /**
+     * \brief Update timestamp of frame.
+     *
+     * param[in] sequence: the sequence in frame buffer;
+     * param[in] timestamp: the timestamp in frame buffer.
+     *
+     */
+    void updateTimestamp(int64_t sequence, uint64_t timestamp);
+
+    /**
+     * \brief Acquire MakerNote data.
+     *
+     * param[in] timestamp: the timestamp in frame buffer;
+     * param[out] param: Makernote data will be saved in parameters as metadata.
+     *
+     */
+    void acquireMakernoteData(uint64_t timestamp, Parameters *param);
+
+ private:
+    // Should > max request number in processing
+    static const int MAX_MAKER_NOTE_LIST_SIZE = 32;
+
+    enum MknState {
+        UNINIT,
+        INIT
+    } mMknState;
+
+    // Guard for MakerNote API
+    Mutex mMknLock;
+    ia_mkn *mMkn;
+
+    IntelMkn mIntelMkn;
+
+    std::list<std::shared_ptr<MakernoteData>> mMakernoteDataList;
+};
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/3a/SensorManager.cpp b/camera/hal/intel/ipu6/src/3a/SensorManager.cpp
new file mode 100644
index 000000000000..1abd08dcdab7
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/SensorManager.cpp
@@ -0,0 +1,327 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "SensorManager"
+
+#include "iutils/Errors.h"
+#include "iutils/CameraLog.h"
+
+#include "AiqUtils.h"
+#include "SensorManager.h"
+#include "PlatformData.h"
+
+using std::vector;
+
+namespace icamera {
+
+SensorManager::SensorManager(int cameraId, SensorHwCtrl *sensorHw) :
+    mCameraId(cameraId),
+    mSensorHwCtrl(sensorHw),
+    mModeSwitched(false),
+    mPerframeControl(false),
+    mLastSofSequence(-1),
+    mGainDelay(0),
+    mSensorExposureHistoryIndex(-1)
+{
+    LOG1("@%s mCameraId = %d", __func__, mCameraId);
+
+    CLEAR(mWdrModeSetting);
+
+    if (PlatformData::getGainLag(mCameraId) > 0) {
+        mGainDelay = PlatformData::getExposureLag(mCameraId) - PlatformData::getGainLag(mCameraId);
+    }
+}
+
+SensorManager::~SensorManager()
+{
+    LOG1("@%s mCameraId = %d", __func__, mCameraId);
+}
+
+int SensorManager::init()
+{
+    AutoMutex l(mLock);
+    LOG1("@%s mCameraId = %d", __func__, mCameraId);
+
+    mSensorExposures.reserve(kMaxSensorExposures);
+    reset();
+    return OK;
+}
+
+int SensorManager::deinit()
+{
+    AutoMutex l(mLock);
+    LOG1("@%s mCameraId = %d", __func__, mCameraId);
+
+    return OK;
+}
+
+void SensorManager::reset()
+{
+    mPerframeControl = false,
+    mLastSofSequence = -1;
+
+    mModeSwitched = false;
+
+    for (int i = 0; i < kMaxExposureHistory; i++) {
+        mSensorExposureHistory[i].clear();
+    }
+
+    mSensorExposureHistoryIndex = -1;
+
+    CLEAR(mWdrModeSetting);
+    mWdrModeSetting.tuningMode = TUNING_MODE_MAX;
+
+    mSofEventInfo.clear();
+}
+
+EventListener *SensorManager::getSofEventListener()
+{
+    AutoMutex l(mLock);
+    LOG1("@%s mCameraId = %d", __func__, mCameraId);
+
+    return this;
+}
+
+void SensorManager::handleEvent(EventData eventData)
+{
+    AutoMutex l(mLock);
+    LOG3A("@%s", __func__);
+
+    if (eventData.type == EVENT_ISYS_SOF) {
+        LOG3A("sequence = %ld, timestamp = %ld",
+                eventData.data.sync.sequence,
+                TIMEVAL2USECS(eventData.data.sync.timestamp));
+        handleSensorExposure();
+        mLastSofSequence = eventData.data.sync.sequence;
+
+        SofEventInfo info;
+        info.sequence = eventData.data.sync.sequence;
+        info.timestamp = ((long)eventData.data.sync.timestamp.tv_sec) * 1000000
+                         + eventData.data.sync.timestamp.tv_usec;
+        if (mSofEventInfo.size() >= kMaxSofEventInfo) {
+            mSofEventInfo.erase(mSofEventInfo.begin());
+        }
+        mSofEventInfo.push_back(info);
+    }
+}
+
+uint64_t SensorManager::getSofTimestamp(long sequence)
+{
+    AutoMutex l(mLock);
+
+    for (auto info : mSofEventInfo) {
+        if (info.sequence == sequence) {
+            return info.timestamp;
+        }
+    }
+    return 0;
+}
+
+void SensorManager::handleSensorExposure()
+{
+    if (!mSensorExposures.empty()) {
+        SensorExpGroup& exposures = mSensorExposures[0];
+        setFrameDuration(exposures[0].sensorParam.line_length_pixels, exposures[0].sensorParam.frame_length_lines);
+        setSensorExposureAndGains(exposures);
+        mSensorExposures.erase(mSensorExposures.begin());
+    } else {
+        if (mGainDelay > 0 && mSensorExposureHistoryIndex >= mGainDelay) {
+            // If gain setting is postponed, set last gain setting to sensor driver
+            int index = mSensorExposureHistoryIndex % kMaxExposureHistory;
+            setSensorGains(index);
+        }
+        mPerframeControl = false;
+    }
+}
+
+int SensorManager::getCurrentExposureAppliedDelay()
+{
+    AutoMutex l(mLock);
+
+    return mSensorExposures.size() + PlatformData::getExposureLag(mCameraId);
+}
+
+uint32_t SensorManager::updateSensorExposure(SensorExpGroup sensorExposures, bool useSof)
+{
+    AutoMutex l(mLock);
+
+    long appliedSeq = mLastSofSequence < 0 ? 0 : \
+                      mLastSofSequence + PlatformData::getExposureLag(mCameraId);
+
+    if (sensorExposures.empty()) {
+        LOGW("%s: No exposure parameter", __func__);
+        return ((uint32_t)appliedSeq);
+    }
+
+    if (useSof) {
+        mSensorExposures.push_back(sensorExposures);
+        mPerframeControl = true;
+        appliedSeq += mSensorExposures.size();
+    } else if (PlatformData::isIsysEnabled(mCameraId)
+               && !mPerframeControl) {
+        setFrameDuration(sensorExposures[0].sensorParam.line_length_pixels, sensorExposures[0].sensorParam.frame_length_lines);
+        setSensorExposureAndGains(sensorExposures);
+    }
+
+    LOG3A("@%s, useSof:%d, mLastSofSequence:%ld, appliedSeq %ld", __func__, useSof,
+           mLastSofSequence, appliedSeq);
+    return ((uint32_t)appliedSeq);
+}
+
+void SensorManager::setSensorExposureAndGains(SensorExpGroup sensorExposures)
+{
+    mSensorExposureHistoryIndex++;
+    mSensorExposureHistory[mSensorExposureHistoryIndex % kMaxExposureHistory] = sensorExposures;
+
+    vector<int> coarseExposures;
+    vector<int> fineExposures;
+    for (auto exp : sensorExposures) {
+        coarseExposures.push_back(exp.sensorParam.coarse_integration_time);
+        fineExposures.push_back(exp.sensorParam.fine_integration_time);
+    }
+    mSensorHwCtrl->setExposure(coarseExposures, fineExposures);
+
+    int64_t index = mSensorExposureHistoryIndex % kMaxExposureHistory;
+
+    // If exposure and gain lag are different, gain setting need to be postponed.
+    if (mGainDelay > 0 && mSensorExposureHistoryIndex >= mGainDelay) {
+        index = (mSensorExposureHistoryIndex - mGainDelay) % kMaxExposureHistory;
+    }
+
+    setSensorGains(index);
+}
+
+void SensorManager::setSensorGains(int index)
+{
+    vector<int> analogGains;
+    vector<int> digitalGains;
+    for (auto exp : mSensorExposureHistory[index]) {
+        analogGains.push_back(exp.sensorParam.analog_gain_code_global);
+
+        int digitalGain = exp.sensorParam.digital_gain_global;
+        if (PlatformData::isUsingIspDigitalGain(mCameraId)) {
+            digitalGain = PlatformData::getSensorDigitalGain(mCameraId, exp.realDigitalGain);
+        }
+        digitalGains.push_back(digitalGain);
+    }
+
+    mSensorHwCtrl->setGains(analogGains, digitalGains);
+}
+
+int SensorManager::setFrameDuration(int lineLengthPixels, int frameLengthLines)
+{
+    return mSensorHwCtrl->setFrameDuration(lineLengthPixels, frameLengthLines);
+}
+
+int SensorManager::setFrameRate(float fps)
+{
+    return mSensorHwCtrl->setFrameRate(fps);
+}
+
+int SensorManager::getSensorInfo(ia_aiq_frame_params &frameParams,
+                                 ia_aiq_exposure_sensor_descriptor &sensorDescriptor)
+{
+    LOG3A("@%s", __func__);
+    SensorFrameParams sensorFrameParams;
+    CLEAR(sensorFrameParams);
+
+    int ret = PlatformData::calculateFrameParams(mCameraId, sensorFrameParams);
+    if (ret == OK) {
+        AiqUtils::convertToAiqFrameParam(sensorFrameParams, frameParams);
+    }
+
+    if (!PlatformData::isIsysEnabled(mCameraId)) {
+        vector <camera_resolution_t> res;
+        PlatformData::getSupportedISysSizes(mCameraId, res);
+
+        CheckError(res.empty(), BAD_VALUE, "Supported ISYS resolutions are not configured.");
+        // In none-ISYS cases, only take 30 fps into account.
+        int fps = 30;
+        float freq = res[0].width * res[0].height * fps / 1000000;
+        sensorDescriptor = {freq, static_cast<unsigned short>(res[0].width),
+                            static_cast<unsigned short>(res[0].height), 24, 0,
+                            static_cast<unsigned short>(res[0].width), 6, 0};
+        LOG3A("freq %f, width %d, height %d", freq, res[0].width, res[0].height);
+        return OK;
+    }
+
+    ret |= getSensorModeData(sensorDescriptor);
+
+    LOG3A("ia_aiq_frame_params=[%d, %d, %d, %d, %d, %d, %d, %d]",
+        frameParams.horizontal_crop_offset,
+        frameParams.vertical_crop_offset,
+        frameParams.cropped_image_height,
+        frameParams.cropped_image_width,
+        frameParams.horizontal_scaling_numerator,
+        frameParams.horizontal_scaling_denominator,
+        frameParams.vertical_scaling_numerator,
+        frameParams.vertical_scaling_denominator);
+
+    LOG3A("ia_aiq_exposure_sensor_descriptor=[%f, %d, %d, %d, %d, %d, %d, %d]",
+        sensorDescriptor.pixel_clock_freq_mhz,
+        sensorDescriptor.pixel_periods_per_line,
+        sensorDescriptor.line_periods_per_field,
+        sensorDescriptor.line_periods_vertical_blanking,
+        sensorDescriptor.coarse_integration_time_min,
+        sensorDescriptor.coarse_integration_time_max_margin,
+        sensorDescriptor.fine_integration_time_min,
+        sensorDescriptor.fine_integration_time_max_margin);
+
+    return ret;
+}
+
+/**
+ * get sensor mode data (sensor descriptor) from sensor driver
+ *
+ * \return OK if successfully.
+ */
+int SensorManager::getSensorModeData(ia_aiq_exposure_sensor_descriptor& sensorData)
+{
+    int pixel = 0;
+    int status =  mSensorHwCtrl->getPixelRate(pixel);
+    CheckError(status != OK, status, "Failed to get pixel clock ret:%d", status);
+    sensorData.pixel_clock_freq_mhz = (float)pixel / 1000000;
+
+    int width = 0, height = 0, pixelCode = 0;
+    status = mSensorHwCtrl->getActivePixelArraySize(width, height, pixelCode);
+    CheckError(status != OK, status, "Failed to get active pixel array size ret:%d", status);
+
+    int pixel_periods_per_line, line_periods_per_field;
+    status = mSensorHwCtrl->getFrameDuration(pixel_periods_per_line, line_periods_per_field);
+    CheckError(status != OK, status, "Failed to get frame Durations ret:%d", status);
+
+    sensorData.pixel_periods_per_line = CLIP(pixel_periods_per_line, USHRT_MAX, 0);
+    sensorData.line_periods_per_field = CLIP(line_periods_per_field, USHRT_MAX, 0);
+
+    int coarse_int_time_min, integration_step = 0, integration_max = 0;
+    status = mSensorHwCtrl->getExposureRange(coarse_int_time_min, integration_max, integration_step);
+    CheckError(status != OK, status, "Failed to get Exposure Range ret:%d", status);
+
+    sensorData.coarse_integration_time_min = CLIP(coarse_int_time_min, USHRT_MAX, 0);
+    sensorData.coarse_integration_time_max_margin = PlatformData::getCITMaxMargin(mCameraId);
+
+    // fine integration is not supported by v4l2
+    sensorData.fine_integration_time_min = 0;
+    sensorData.fine_integration_time_max_margin = sensorData.pixel_periods_per_line;
+    int vblank;
+    status = mSensorHwCtrl->getVBlank(vblank);
+    CheckError(status != OK, status, "Failed to get vblank ret:%d", status);
+    sensorData.line_periods_vertical_blanking = CLIP(vblank, USHRT_MAX, 0);
+
+    return OK;
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/SensorManager.h b/camera/hal/intel/ipu6/src/3a/SensorManager.h
new file mode 100644
index 000000000000..bb42a306ffd3
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/SensorManager.h
@@ -0,0 +1,111 @@
+/*
+ * Copyright (C) 2015-2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "iutils/Thread.h"
+
+#include "ia_aiq.h"
+
+#include "CameraEvent.h"
+#include "SensorHwCtrl.h"
+
+namespace icamera {
+
+/*
+ * \struct WdrModeSetting
+ *
+ * This struct is used to control wdr mode switching.
+ */
+typedef struct {
+    long sequence;
+    TuningMode tuningMode;
+} WdrModeSetting;
+
+typedef struct {
+    unsigned short realDigitalGain;
+    ia_aiq_exposure_sensor_parameters sensorParam;
+} SensorExposure;
+
+typedef struct {
+    long sequence;
+    uint64_t timestamp;
+} SofEventInfo;
+
+typedef std::vector <SensorExposure> SensorExpGroup;
+/*
+ * \class SensorManager
+ *
+ * This class is used to control exposure and gain synchronization mechanism
+ * and get some sensor info.
+ */
+class SensorManager : public EventListener {
+
+public:
+    SensorManager(int cameraId, SensorHwCtrl *sensorHw);
+    ~SensorManager();
+
+    int init();
+    int deinit();
+    void reset();
+
+    // get EventListener
+    EventListener *getSofEventListener();
+
+    void handleEvent(EventData eventData);
+    uint32_t updateSensorExposure(SensorExpGroup sensorExposures, bool useSof = true);
+    int getSensorInfo(ia_aiq_frame_params &frameParams,
+                      ia_aiq_exposure_sensor_descriptor &sensorDescriptor);
+
+    int setFrameRate(float fps);
+    int getCurrentExposureAppliedDelay();
+    uint64_t getSofTimestamp(long sequence);
+private:
+    DISALLOW_COPY_AND_ASSIGN(SensorManager);
+
+    void handleSensorExposure();
+    int getSensorModeData(ia_aiq_exposure_sensor_descriptor& sensorData);
+    void setSensorExposureAndGains(SensorExpGroup sensorExposures);
+    int setFrameDuration(int lineLengthPixels, int frameLengthLines);
+    void setSensorGains(int index);
+
+private:
+    static const int kMaxSensorExposures = 10;
+    static const int kMaxExposureHistory = 5;
+    static const int kMaxSofEventInfo = 10;
+
+    int mCameraId;
+    SensorHwCtrl *mSensorHwCtrl;
+
+    bool    mModeSwitched;         // Whether the TuningMode get updated
+    WdrModeSetting mWdrModeSetting;
+
+    bool mPerframeControl;
+    std::vector<SensorExpGroup> mSensorExposures;
+
+    long mLastSofSequence;
+
+    // Guard for SensorManager public API.
+    Mutex mLock;
+
+    int mGainDelay;
+    SensorExpGroup mSensorExposureHistory[kMaxExposureHistory];
+    int64_t mSensorExposureHistoryIndex;
+
+    std::vector<SofEventInfo> mSofEventInfo;
+};
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/intel3a/Intel3AParameter.cpp b/camera/hal/intel/ipu6/src/3a/intel3a/Intel3AParameter.cpp
new file mode 100644
index 000000000000..b76c3b6c7f12
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/intel3a/Intel3AParameter.cpp
@@ -0,0 +1,714 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Intel3AParameter"
+
+#include <cmath>
+#include <climits>
+
+#include "iutils/Utils.h"
+#include "iutils/Errors.h"
+#include "iutils/CameraLog.h"
+
+#include "Intel3AParameter.h"
+#include "AiqUtils.h"
+
+namespace icamera {
+
+Intel3AParameter::Intel3AParameter(int cameraId) :
+    mCameraId(cameraId),
+    mUseManualAwbGain(false),
+    mWeightGridMode(WEIGHT_GRID_AUTO),
+    mAePerTicks(1),
+    mAwbPerTicks(1),
+    mAfForceLock(false),
+    mDuringAfTriggerScan(false)
+{
+    LOG3A("%s", __func__);
+
+    CLEAR(mAeParams);
+    CLEAR(mAfParams);
+    CLEAR(mAwbParams);
+
+    CLEAR(mManualGains);
+    CLEAR(mAwbGainShift);
+
+    CLEAR(mSensorDescriptor);
+    CLEAR(mExposureWindow);
+    CLEAR(mExposureCoordinate);
+    CLEAR(mAeFeatures);
+    CLEAR(mAeManualLimits);
+    CLEAR(mManualFocusParams);
+    CLEAR(mFocusRect);
+    CLEAR(mManualCctRange);
+    CLEAR(mManualWhiteCoordinate);
+
+    CLEAR(mManualExposureTimeUs);
+    CLEAR(mManualAnalogGain);
+    CLEAR(mManualIso);
+    mAfTrigger = AF_TRIGGER_IDLE;
+    mAfMode = AF_MODE_OFF;
+}
+
+Intel3AParameter::~Intel3AParameter()
+{
+    LOG3A("%s", __func__);
+}
+
+int Intel3AParameter::init()
+{
+    LOG3A("%s", __func__);
+
+    mAeParams.sensor_descriptor = &mSensorDescriptor;
+    mAeParams.exposure_window = &mExposureWindow;
+    mAeParams.exposure_coordinate = &mExposureCoordinate;
+    mAeParams.aec_features = &mAeFeatures;
+    mAeParams.manual_limits = &mAeManualLimits;
+
+    mAfParams.focus_rect = &mFocusRect;
+    mAfParams.manual_focus_parameters = &mManualFocusParams;
+
+    mAwbParams.manual_cct_range = &mManualCctRange;
+    mAwbParams.manual_white_coordinate = &mManualWhiteCoordinate;
+
+    // set default params
+    initAeParameter();
+    initAfParameter();
+    initAwbParameter();
+
+    mUseManualAwbGain = false;
+    mWeightGridMode = WEIGHT_GRID_AUTO;
+    mAePerTicks = 1;
+    mAwbPerTicks = 1;
+
+    mAfMode = AF_MODE_AUTO;
+    mAfForceLock = false;
+    mAfTrigger = AF_TRIGGER_IDLE;
+    mDuringAfTriggerScan = false;
+    return OK;
+}
+
+void Intel3AParameter::initAeParameter()
+{
+    mAeParams.num_exposures = 1;
+    mAeParams.frame_use = ia_aiq_frame_use_video;
+    mAeParams.flash_mode = ia_aiq_flash_mode_off;
+    mAeParams.operation_mode = ia_aiq_ae_operation_mode_automatic;
+    mAeParams.metering_mode = ia_aiq_ae_metering_mode_evaluative;
+    mAeParams.priority_mode = ia_aiq_ae_priority_mode_normal;
+    mAeParams.flicker_reduction_mode = ia_aiq_ae_flicker_reduction_auto;
+    mAeParams.ev_shift = 0.0f;
+    mAeParams.manual_exposure_time_us = nullptr;
+    mAeParams.manual_analog_gain = nullptr;
+    mAeParams.manual_iso = nullptr;
+
+    mAeParams.exposure_window = nullptr;
+    mAeParams.exposure_coordinate = nullptr;
+    mAeParams.aec_features = nullptr;
+    mAeParams.manual_convergence_time = -1;
+    mAeParams.exposure_distribution_priority = ia_aiq_ae_exposure_distribution_auto;
+}
+
+void Intel3AParameter::initAfParameter()
+{
+    mAfParams.frame_use = ia_aiq_frame_use_video;
+    mAfParams.lens_position = 0;
+    mAfParams.lens_movement_start_timestamp = 0;
+    mAfParams.focus_mode = ia_aiq_af_operation_mode_infinity;
+    mAfParams.focus_range = ia_aiq_af_range_extended;
+    mAfParams.focus_metering_mode = ia_aiq_af_metering_mode_auto;
+    mAfParams.flash_mode = ia_aiq_flash_mode_off;
+    mAfParams.focus_rect = &mFocusRect;
+    mAfParams.focus_rect->left = 0;
+    mAfParams.focus_rect->top = 0;
+    mAfParams.focus_rect->right = 0;
+    mAfParams.focus_rect->bottom = 0;
+    mAfParams.manual_focus_parameters = &mManualFocusParams;
+    mAfParams.manual_focus_parameters->manual_focus_action = ia_aiq_manual_focus_action_none;
+    mAfParams.manual_focus_parameters->manual_focus_distance = MAX_FOCUS_DISTANCE;
+    mAfParams.manual_focus_parameters->manual_lens_position = 0;
+    mAfParams.trigger_new_search = false;
+}
+
+void Intel3AParameter::initAwbParameter()
+{
+    mAwbParams.frame_use = ia_aiq_frame_use_video;
+    mAwbParams.scene_mode = ia_aiq_awb_operation_mode_auto;
+    mAwbParams.manual_convergence_time = -1;
+
+    mUseManualAwbGain = false;
+    CLEAR(mManualGains);
+    CLEAR(mAwbGainShift);
+}
+
+int Intel3AParameter::setSensorInfo(ia_aiq_exposure_sensor_descriptor descriptor)
+{
+    LOG3A("%s", __func__);
+
+    mSensorDescriptor.pixel_clock_freq_mhz = descriptor.pixel_clock_freq_mhz;
+    mSensorDescriptor.pixel_periods_per_line = descriptor.pixel_periods_per_line;
+    mSensorDescriptor.line_periods_per_field = descriptor.line_periods_per_field;
+    mSensorDescriptor.line_periods_vertical_blanking = descriptor.line_periods_vertical_blanking;
+    mSensorDescriptor.fine_integration_time_min = descriptor.fine_integration_time_min;
+    mSensorDescriptor.fine_integration_time_max_margin = descriptor.fine_integration_time_max_margin;
+    mSensorDescriptor.coarse_integration_time_min = descriptor.coarse_integration_time_min;
+    mSensorDescriptor.coarse_integration_time_max_margin = descriptor.coarse_integration_time_max_margin;
+
+    return OK;
+}
+
+int Intel3AParameter::updateParameter(aiq_parameter_t param)
+{
+    LOG3A("%s", __func__);
+
+    updateAeParameter(param);
+    updateAwbParameter(param);
+    updateAfParameter(param);
+
+    return OK;
+}
+
+/**
+ * Override ae result by those settings provided by application
+ */
+void Intel3AParameter::updateAeResult(ia_aiq_ae_results* aeResult)
+{
+    CheckError(!aeResult || !aeResult->weight_grid, VOID_VALUE, "Invalid aeResult");
+
+}
+
+float Intel3AParameter::convertdBGainToISO(float sensitivityGain, int baseIso)
+{
+    // Convert db Gain to ISO
+    float manualGain = pow(10, (sensitivityGain / 20));
+    manualGain *= baseIso;
+    return manualGain;
+}
+
+void Intel3AParameter::setAeManualLimits(const aiq_parameter_t& param)
+{
+    mAeParams.manual_limits = &mAeManualLimits;
+
+    if (param.aeFpsRange.min > 0.01 && param.aeFpsRange.max >= param.aeFpsRange.min) {
+        mAeManualLimits.manual_frame_time_us_max = 1000000 / param.aeFpsRange.min;
+        mAeManualLimits.manual_frame_time_us_min = 1000000 / param.aeFpsRange.max;
+    } else if (param.fps > 0.01) {
+        mAeManualLimits.manual_frame_time_us_max = 1000000 / param.fps;
+        mAeManualLimits.manual_frame_time_us_min = 1000000 / param.fps;
+    }
+
+    camera_range_t etRange = {-1, -1};
+    int ret = PlatformData::getSupportAeExposureTimeRange(mCameraId, param.sceneMode, etRange);
+    if (param.exposureTimeRange.min > 0 &&
+        param.exposureTimeRange.max >= param.exposureTimeRange.min) {
+        if (ret == OK) {
+            etRange.min = CLIP(param.exposureTimeRange.min, etRange.max, etRange.min);
+            etRange.max = CLIP(param.exposureTimeRange.max, etRange.max, etRange.min);
+        } else {
+            etRange.min = param.exposureTimeRange.min;
+            etRange.max = param.exposureTimeRange.max;
+        }
+    }
+    mAeManualLimits.manual_exposure_time_min = etRange.min;
+    mAeManualLimits.manual_exposure_time_max = etRange.max;
+
+    camera_range_t gainRange = {-1, -1};
+    ret = PlatformData::getSupportAeGainRange(mCameraId, param.sceneMode, gainRange);
+    if (param.sensitivityGainRange.min >= 0 &&
+        param.sensitivityGainRange.max >= param.sensitivityGainRange.min) {
+        if (ret == OK) {
+            gainRange.min = CLIP(param.sensitivityGainRange.min, gainRange.max, gainRange.min);
+            gainRange.max = CLIP(param.sensitivityGainRange.max, gainRange.max, gainRange.min);
+        } else {
+            gainRange.min = param.sensitivityGainRange.min;
+            gainRange.max = param.sensitivityGainRange.max;
+        }
+    }
+
+    mAeManualLimits.manual_iso_min = -1;
+    mAeManualLimits.manual_iso_max = -1;
+    if (gainRange.min >= 0 && gainRange.max >= gainRange.min) {
+        ia_binary_data aiqData;
+        ia_cmc_t *cmc = nullptr;
+
+        int ret = PlatformData::getCpfAndCmc(mCameraId, nullptr, &aiqData, nullptr,
+                                             nullptr, param.tuningMode, &cmc);
+        if (ret == OK) {
+            if (cmc != nullptr && cmc->cmc_sensitivity != nullptr) {
+                float isoMin = convertdBGainToISO(gainRange.min, cmc->cmc_sensitivity->base_iso);
+                float isoMax = convertdBGainToISO(gainRange.max, cmc->cmc_sensitivity->base_iso);
+                if (isoMin <= INT_MAX && isoMax <= INT_MAX) {
+                    mAeManualLimits.manual_iso_min = static_cast<int>(isoMin);
+                    mAeManualLimits.manual_iso_max = static_cast<int>(isoMax);
+                }
+            }
+        }
+    }
+
+    LOG3A("%s, manual limited ISO-[%d--%d], expo-[%d--%d], frame time-[%d--%d]", __func__,
+          mAeManualLimits.manual_iso_min, mAeManualLimits.manual_iso_max,
+          mAeManualLimits.manual_exposure_time_min, mAeManualLimits.manual_exposure_time_max,
+          mAeManualLimits.manual_frame_time_us_min, mAeManualLimits.manual_frame_time_us_max);
+}
+
+void Intel3AParameter::setManualExposure(const aiq_parameter_t& param)
+{
+    int64_t manualExpTimeUs = param.manualExpTimeUs;
+    if (manualExpTimeUs <= 0 || param.aeDistributionPriority == DISTRIBUTION_ISO) {
+        return;
+    }
+
+    camera_range_t etRange;
+    CLEAR(etRange);
+    if (PlatformData::getSupportAeExposureTimeRange(mCameraId, param.sceneMode, etRange) == OK) {
+         manualExpTimeUs = CLIP(manualExpTimeUs, etRange.max, etRange.min);
+    }
+
+    mAeParams.manual_exposure_time_us = mManualExposureTimeUs;
+    for (unsigned int i = 0; i < mAeParams.num_exposures - 1; i++) {
+        mAeParams.manual_exposure_time_us[i] = -1;
+    }
+    mAeParams.manual_exposure_time_us[mAeParams.num_exposures - 1] = manualExpTimeUs;
+    LOG3A("%s, manual exposure %ld", __func__, manualExpTimeUs);
+}
+
+void Intel3AParameter::setManualGain(const aiq_parameter_t& param)
+{
+    float manualGain = param.manualGain;
+    if (manualGain < 0 || param.aeDistributionPriority == DISTRIBUTION_SHUTTER) {
+        return;
+    }
+
+    camera_range_t gainRange;
+    CLEAR(gainRange);
+    if (PlatformData::getSupportAeGainRange(mCameraId, param.sceneMode, gainRange) == OK) {
+        manualGain = CLIP(manualGain, gainRange.max, gainRange.min);
+    }
+
+    mAeParams.manual_analog_gain = mManualAnalogGain;
+    // Convert db to sensor analog gain.
+    for (unsigned int i = 0; i < mAeParams.num_exposures; i++) {
+        mAeParams.manual_analog_gain[i] = pow(10, (manualGain / 20));
+    }
+
+    LOG3A("%s, manual gain %f, AG %f", __func__, manualGain, mManualAnalogGain[0]);
+}
+
+void Intel3AParameter::setManualIso(const aiq_parameter_t& param)
+{
+    int32_t manualIso = param.manualIso;
+    if (manualIso <= 0 || param.aeDistributionPriority == DISTRIBUTION_SHUTTER) {
+        return;
+    }
+
+    if (mAeManualLimits.manual_iso_min >= 0
+        && mAeManualLimits.manual_iso_max >= mAeManualLimits.manual_iso_min) {
+        manualIso = CLIP(manualIso, mAeManualLimits.manual_iso_max, mAeManualLimits.manual_iso_min);
+    }
+
+    // Will overwrite manual_analog_gain
+    mAeParams.manual_iso = mManualIso;
+    for (unsigned int i = 0; i < mAeParams.num_exposures; i++) {
+        mAeParams.manual_iso[i] = manualIso;
+    }
+
+    LOG3A("%s, manual iso  %d", __func__, manualIso);
+}
+
+void Intel3AParameter::updateAeParameter(const aiq_parameter_t& param)
+{
+    mAeParams.frame_use = AiqUtils::convertFrameUsageToIaFrameUsage(param.frameUsage);
+    mAeParams.num_exposures = PlatformData::getExposureNum(mCameraId,
+                                  CameraUtils::isMultiExposureCase(param.tuningMode));
+    setAeManualLimits(param);
+
+    switch(param.antibandingMode) {
+        case ANTIBANDING_MODE_AUTO:
+            mAeParams.flicker_reduction_mode = ia_aiq_ae_flicker_reduction_auto;
+            break;
+        case ANTIBANDING_MODE_50HZ:
+            mAeParams.flicker_reduction_mode = ia_aiq_ae_flicker_reduction_50hz;
+            break;
+        case ANTIBANDING_MODE_60HZ:
+            mAeParams.flicker_reduction_mode = ia_aiq_ae_flicker_reduction_60hz;
+            break;
+        case ANTIBANDING_MODE_OFF:
+            mAeParams.flicker_reduction_mode = ia_aiq_ae_flicker_reduction_off;
+            break;
+    }
+
+    switch (param.aeDistributionPriority) {
+        case DISTRIBUTION_AUTO:
+            mAeParams.exposure_distribution_priority = ia_aiq_ae_exposure_distribution_auto;
+            break;
+        case DISTRIBUTION_SHUTTER:
+            mAeParams.exposure_distribution_priority = ia_aiq_ae_exposure_distribution_shutter;
+            break;
+        case DISTRIBUTION_ISO:
+            mAeParams.exposure_distribution_priority = ia_aiq_ae_exposure_distribution_iso;
+            break;
+        case DISTRIBUTION_APERTURE:
+            mAeParams.exposure_distribution_priority = ia_aiq_ae_exposure_distribution_aperture;
+            break;
+        default:
+            mAeParams.exposure_distribution_priority = ia_aiq_ae_exposure_distribution_auto;
+            break;
+    }
+
+    mAeParams.manual_exposure_time_us = nullptr;
+    mAeParams.manual_analog_gain = nullptr;
+    mAeParams.manual_iso = nullptr;
+
+    if (param.aeMode == AE_MODE_MANUAL) {
+        setManualGain(param);
+        setManualIso(param);
+        setManualExposure(param);
+    } else {
+        mAeParams.ev_shift = param.evShift;
+    }
+
+    if (param.aeConvergeSpeedMode == CONVERGE_SPEED_MODE_AIQ) {
+        mAePerTicks = 1;
+
+        mAeParams.manual_convergence_time = AiqUtils::convertSpeedModeToTime(param.aeConvergeSpeed);
+    } else {
+        mAeParams.manual_convergence_time = -1;
+
+        /*
+         * The unit of mAePerTicks is frame count, 3 means 3 frames.
+         * The default value can be changed based on customer requirement.
+         */
+        switch (param.aeConvergeSpeed) {
+            case CONVERGE_MID:
+                mAePerTicks = 30;
+                break;
+            case CONVERGE_LOW:
+                mAePerTicks = 60;
+                break;
+            case CONVERGE_NORMAL:
+            default:
+                mAePerTicks = 1;
+                break;
+        }
+    }
+
+    mAeParams.exposure_coordinate = nullptr;
+    if (param.blcAreaMode == BLC_AREA_MODE_ON && !param.aeRegions.empty()) {
+        // Current only one AE metering window is supported, so use the latest one
+        camera_window_t window = param.aeRegions.back();
+
+        if (window.right > window.left && window.bottom > window.top) {
+            camera_coordinate_t coordinate;
+            CLEAR(coordinate);
+            coordinate.x = window.left + (window.right - window.left) / 2;
+            coordinate.y = window.top + (window.bottom - window.top) / 2;
+            camera_coordinate_system_t frameCoord = {0, 0, param.resolution.width, param.resolution.height};
+            LOG3A("%s: frame resolution %dx%d", __func__, param.resolution.width, param.resolution.height);
+
+            coordinate = AiqUtils::convertToIaCoordinate(frameCoord, coordinate);
+            mExposureCoordinate.x = coordinate.x;
+            mExposureCoordinate.y = coordinate.y;
+            mAeParams.exposure_coordinate = &mExposureCoordinate;
+            LOG3A("%s, exposure coordinate = [%d,%d], region = [%d,%d,%d,%d]", __func__,
+                    mAeParams.exposure_coordinate->x, mAeParams.exposure_coordinate->y,
+                    window.left, window.top, window.right, window.bottom);
+        }
+    }
+}
+
+/**
+ * Override awb result by AWB gains or gain shift provided by application
+ */
+void Intel3AParameter::updateAwbResult(ia_aiq_awb_results* awbResult)
+{
+    CheckError((awbResult == nullptr), VOID_VALUE, "No Awb result provided.");
+
+    camera_awb_gains_t& gains = mUseManualAwbGain ? mManualGains : mAwbGainShift;
+    float normalizedR = AiqUtils::normalizeAwbGain(gains.r_gain);
+    float normalizedG = AiqUtils::normalizeAwbGain(gains.g_gain);
+    float normalizedB = AiqUtils::normalizeAwbGain(gains.b_gain);
+
+    const float MAX_PER_G = AWB_GAIN_NORMALIZED_START / AWB_GAIN_NORMALIZED_END;
+    const float MIN_PER_G = 1.0 / MAX_PER_G;
+
+    if (mUseManualAwbGain) {
+        awbResult->accurate_b_per_g = CLIP((normalizedB / normalizedG), MAX_PER_G, MIN_PER_G);
+        awbResult->accurate_r_per_g = CLIP((normalizedR / normalizedG), MAX_PER_G, MIN_PER_G);
+    } else {
+        awbResult->accurate_b_per_g *= CLIP((normalizedB / normalizedG), MAX_PER_G, MIN_PER_G);
+        awbResult->accurate_r_per_g *= CLIP((normalizedR / normalizedG), MAX_PER_G, MIN_PER_G);
+    }
+
+    //Only override final results when manual gain or gain shift applied.
+    if (mUseManualAwbGain || gains.r_gain != 0 || gains.g_gain != 0
+        || gains.b_gain != 0) {
+        LOG3A("%s: override final awb results", __func__);
+        awbResult->final_b_per_g = awbResult->accurate_b_per_g;
+        awbResult->final_r_per_g = awbResult->accurate_r_per_g;
+    }
+
+    LOG3A("%s (r,g,b): (%d,%d,%d) -> (b/g, r/g): (%f,%f)",
+          mUseManualAwbGain ? "Manual gain" : "Gain shift",
+          gains.r_gain, gains.g_gain, gains.b_gain,
+          awbResult->accurate_b_per_g, awbResult->accurate_r_per_g);
+}
+
+void Intel3AParameter::updateAwbParameter(const aiq_parameter_t& param)
+{
+    mAwbParams.frame_use = AiqUtils::convertFrameUsageToIaFrameUsage(param.frameUsage);
+
+    switch (param.awbMode) {
+        case AWB_MODE_INCANDESCENT:
+            mAwbParams.scene_mode = ia_aiq_awb_operation_mode_incandescent;
+            break;
+
+        case AWB_MODE_FLUORESCENT:
+            mAwbParams.scene_mode = ia_aiq_awb_operation_mode_fluorescent;
+            break;
+
+        case AWB_MODE_DAYLIGHT:
+            mAwbParams.scene_mode = ia_aiq_awb_operation_mode_daylight;
+            break;
+
+        case AWB_MODE_FULL_OVERCAST:
+            mAwbParams.scene_mode = ia_aiq_awb_operation_mode_fully_overcast;
+            break;
+
+        case AWB_MODE_PARTLY_OVERCAST:
+            mAwbParams.scene_mode = ia_aiq_awb_operation_mode_partly_overcast;
+            break;
+
+        case AWB_MODE_SUNSET:
+            mAwbParams.scene_mode = ia_aiq_awb_operation_mode_sunset;
+            break;
+
+        case AWB_MODE_VIDEO_CONFERENCE:
+            mAwbParams.scene_mode = ia_aiq_awb_operation_mode_video_conference;
+            break;
+
+        case AWB_MODE_MANUAL_CCT_RANGE:
+            mAwbParams.scene_mode = ia_aiq_awb_operation_mode_manual_cct_range;
+            mManualCctRange.min_cct = std::min(param.cctRange.min, param.cctRange.max);
+            mManualCctRange.max_cct = std::max(param.cctRange.min, param.cctRange.max);
+            mAwbParams.manual_cct_range = &mManualCctRange;
+            break;
+
+        case AWB_MODE_MANUAL_WHITE_POINT:
+        {
+            mAwbParams.scene_mode = ia_aiq_awb_operation_mode_manual_white;
+            mAwbParams.manual_white_coordinate = &mManualWhiteCoordinate;
+            camera_coordinate_system_t frameCoord = {0, 0, param.resolution.width, param.resolution.height};
+            LOG3A("%s: frame resolution %dx%d", __func__, param.resolution.width, param.resolution.height);
+            camera_coordinate_t iaCoord = AiqUtils::convertToIaCoordinate(frameCoord, param.whitePoint);
+
+            mManualWhiteCoordinate.x = iaCoord.x;
+            mManualWhiteCoordinate.y = iaCoord.y;
+            break;
+        }
+        case AWB_MODE_MANUAL_GAIN:
+            mAwbParams.scene_mode = ia_aiq_awb_operation_mode_auto;
+            mManualGains = param.awbManualGain;
+            break;
+
+        default:
+            mAwbParams.scene_mode = ia_aiq_awb_operation_mode_auto;
+            break;
+    }
+
+    mUseManualAwbGain = (param.awbMode == AWB_MODE_MANUAL_GAIN);
+    mAwbGainShift = param.awbGainShift;
+
+    if (param.awbConvergeSpeedMode == CONVERGE_SPEED_MODE_AIQ) {
+        mAwbPerTicks = 1;
+
+        mAwbParams.manual_convergence_time = AiqUtils::convertSpeedModeToTime(param.awbConvergeSpeed);
+    } else {
+        mAwbParams.manual_convergence_time = -1;
+
+        /*
+         * The unit of mAePerTicks is frame count, 3 means 3 frames.
+         * The default value can be changed based on customer requirement.
+         */
+        switch (param.aeConvergeSpeed) {
+            case CONVERGE_MID:
+                mAwbPerTicks = 30;
+                break;
+            case CONVERGE_LOW:
+                mAwbPerTicks = 60;
+                break;
+            case CONVERGE_NORMAL:
+            default:
+                mAwbPerTicks = 1;
+                break;
+        }
+    }
+}
+
+void Intel3AParameter::updateAfParameter(const aiq_parameter_t& param)
+{
+    mAfParams.lens_position = param.lensPosition;
+    mAfParams.lens_movement_start_timestamp = param.lensMovementStartTimestamp;
+
+    LOG3A("%s, Focus position %d, timestamp %llu, afMode %d", __func__, param.lensPosition,
+            param.lensMovementStartTimestamp, param.afMode);
+
+    // Mode
+    if (mAfMode != param.afMode) {
+        // Reset af parameter
+        initAfParameter();
+
+        mAfMode = param.afMode;
+        if (mAfMode == AF_MODE_CONTINUOUS_PICTURE
+            || mAfMode == AF_MODE_CONTINUOUS_VIDEO) {
+            mAfParams.focus_mode = ia_aiq_af_operation_mode_auto;
+        }
+        mAfTrigger = AF_TRIGGER_IDLE;
+        mAfForceLock = false;
+        mDuringAfTriggerScan = false;
+    }
+    mAfParams.frame_use = AiqUtils::convertFrameUsageToIaFrameUsage(param.frameUsage);
+
+    // Trigger
+    mAfParams.trigger_new_search = false;
+    if (mAfTrigger != AF_TRIGGER_START && param.afTrigger == AF_TRIGGER_START) {
+        LOG3A("%s: Trigger AF scan, mode %d", __func__, mAfMode);
+        updateAfParameterForAfTriggerStart();
+    } else if (mAfTrigger != AF_TRIGGER_CANCEL && param.afTrigger == AF_TRIGGER_CANCEL) {
+        LOG3A("%s: Cancel AF scan, mode %d", __func__, mAfMode);
+        updateAfParameterForAfTriggerCancel();
+    }
+    mAfTrigger = param.afTrigger;
+
+    // Region
+    mAfParams.focus_rect = nullptr;
+    if (!param.afRegions.empty()) {
+        // Current only one AF metering window is supported, so use the latest one
+        camera_window_t window = param.afRegions.back();
+        if (window.right > window.left && window.bottom > window.top) {
+            camera_coordinate_system_t frameCoord = {0, 0, param.resolution.width, param.resolution.height};
+            window = AiqUtils::convertToIaWindow(frameCoord, window);
+            mFocusRect.left   = window.left;
+            mFocusRect.top    = window.top;
+            mFocusRect.right  = window.right;
+            mFocusRect.bottom = window.bottom;
+            mAfParams.focus_rect = &mFocusRect;
+            LOG3A("%s, af region = [%d,%d, %d, %d], window = [%d,%d,%d,%d]", __func__,
+                    mFocusRect.left, mFocusRect.top, mFocusRect.right, mFocusRect.bottom,
+                    window.left, window.top, window.right, window.bottom);
+        }
+    }
+
+    // Manual lens position
+    if (mAfMode == AF_MODE_OFF) {
+        mAfParams.manual_focus_parameters = &mManualFocusParams;
+        mAfParams.focus_mode = ia_aiq_af_operation_mode_manual;
+        mAfParams.focus_range = ia_aiq_af_range_extended;
+        mAfParams.focus_metering_mode = ia_aiq_af_metering_mode_auto;
+
+        // Set AIQ manual action to 'none' by default
+        mAfParams.manual_focus_parameters->manual_focus_action = ia_aiq_manual_focus_action_none;
+
+        // The focusDistance value from app is diopters, so the focusInMm = 1 / focusDistance
+        // Clamp focus distance between [0.0, minFocusDistance].
+        float focusDistance = param.focusDistance;
+        if (focusDistance > param.minFocusDistance) {
+            focusDistance = param.minFocusDistance;
+        } else if (focusDistance < 0.0f) {
+            focusDistance = 0.0f;
+        }
+
+        unsigned focusInMm = 0;
+        if (focusDistance != 0.0f) {
+            focusInMm = 1000 * (1.0f / focusDistance);
+            mAfParams.manual_focus_parameters->manual_focus_action =
+                ia_aiq_manual_focus_action_set_distance;
+        } else {
+            // 0.0f focus distance means infinity
+            mAfParams.focus_mode = ia_aiq_af_operation_mode_infinity;
+        }
+
+        mAfParams.manual_focus_parameters->manual_focus_distance = focusInMm;
+    } else {
+        mAfParams.manual_focus_parameters = nullptr;
+    }
+
+    LOG3A("%s, afForceLock %d, duringAfTriggerScan %d", __func__, mAfForceLock, mDuringAfTriggerScan);
+}
+
+void Intel3AParameter::updateAfParameterForAfTriggerStart()
+{
+    mDuringAfTriggerScan = true;
+    mAfForceLock = false;
+
+    switch (mAfMode) {
+        case AF_MODE_AUTO:
+        case AF_MODE_MACRO:
+            // Start user af scan in this frame.
+            mAfParams.frame_use = ia_aiq_frame_use_still;
+            mAfParams.focus_mode = ia_aiq_af_operation_mode_auto;
+            mAfParams.trigger_new_search = true;
+            break;
+        case AF_MODE_CONTINUOUS_VIDEO:
+            // Lock AF immediately
+            mAfForceLock = true;
+            break;
+        case AF_MODE_CONTINUOUS_PICTURE:
+            // Continue the current scan and check the af result later
+            break;
+        default:
+            break;
+    }
+}
+
+void Intel3AParameter::updateAfParameterForAfTriggerCancel()
+{
+    mDuringAfTriggerScan = false;
+    mAfForceLock = false;
+
+    switch (mAfMode) {
+        case AF_MODE_AUTO:
+        case AF_MODE_MACRO:
+            // Stop AF scan triggered by user.
+            mAfParams.focus_mode = ia_aiq_af_operation_mode_infinity;
+            break;
+        default:
+            break;
+    }
+}
+
+void Intel3AParameter::fillAfTriggerResult(ia_aiq_af_results *afResults)
+{
+    if (!afResults || !mAfForceLock) {
+        return;
+    }
+
+    // Check the result of autofocus triggered by user
+    switch (mAfMode) {
+        case AF_MODE_CONTINUOUS_PICTURE:
+        case AF_MODE_AUTO:
+        case AF_MODE_MACRO:
+            // Lock AF after current scan
+            mAfForceLock = (afResults->status != ia_aiq_af_status_local_search
+                          && afResults->status != ia_aiq_af_status_extended_search);
+            break;
+        default:
+            break;
+    }
+
+    LOG3A("%s, %d update afForceLock %d", __func__, afResults->status, mAfForceLock);
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/3a/intel3a/Intel3AParameter.h b/camera/hal/intel/ipu6/src/3a/intel3a/Intel3AParameter.h
new file mode 100644
index 000000000000..a3675a490502
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/3a/intel3a/Intel3AParameter.h
@@ -0,0 +1,104 @@
+/*
+ * Copyright (C) 2015-2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "ia_aiq.h"
+
+#include "AiqSetting.h"
+#include "AiqUtils.h"
+
+namespace icamera {
+
+/*
+ * \class Intel3AParameter
+ * This class is used to prepare those parameters for
+ * 3A running.
+ */
+class Intel3AParameter {
+
+public:
+    Intel3AParameter(int cameraId);
+    ~Intel3AParameter();
+
+    int init();
+    int setSensorInfo(ia_aiq_exposure_sensor_descriptor descriptor);
+    int updateParameter(aiq_parameter_t param);
+    void updateAeResult(ia_aiq_ae_results* aeResult);
+    void updateAwbResult(ia_aiq_awb_results* awbResult);
+
+    void fillAfTriggerResult(ia_aiq_af_results *afResults);
+
+private:
+    void initAeParameter();
+    void initAfParameter();
+    void initAwbParameter();
+
+    void updateAeParameter(const aiq_parameter_t& param);
+    void updateAwbParameter(const aiq_parameter_t& param);
+    void updateAfParameter(const aiq_parameter_t& param);
+    void updateAfParameterForAfTriggerStart();
+    void updateAfParameterForAfTriggerCancel();
+
+    float convertdBGainToISO(float sensitivityGain, int baseIso);
+    void setManualExposure(const aiq_parameter_t& param);
+    void setManualGain(const aiq_parameter_t& param);
+    void setManualIso(const aiq_parameter_t& param);
+    void setAeManualLimits(const aiq_parameter_t& param);
+public:
+    int mCameraId;
+    // aiq 3a parameters
+    ia_aiq_ae_input_params  mAeParams;
+    ia_aiq_af_input_params  mAfParams;
+    ia_aiq_awb_input_params mAwbParams;
+
+    bool mUseManualAwbGain;
+    camera_awb_gains_t mManualGains;
+    camera_awb_gains_t mAwbGainShift;
+    camera_weight_grid_mode_t mWeightGridMode;
+
+    int mAePerTicks;
+    int mAwbPerTicks;
+
+    bool mAfForceLock; // Lock AF to respond autofocus action triggered by user.
+private:
+    static const int MAX_FOCUS_DISTANCE = 5000; // unit is mm
+
+    /*!< ia_aiq_ae_input_params pointer contents */
+    ia_aiq_exposure_sensor_descriptor mSensorDescriptor;
+    ia_rectangle mExposureWindow;
+    ia_coordinate mExposureCoordinate;
+    ia_aiq_ae_features mAeFeatures;
+    ia_aiq_ae_manual_limits mAeManualLimits;
+
+    /*!< ia_aiq_af_input_params pointer contents */
+    ia_aiq_manual_focus_parameters mManualFocusParams;
+    ia_rectangle mFocusRect;
+
+    /*!< ia_aiq_awb_input_params pointer contents */
+    ia_aiq_awb_manual_cct_range mManualCctRange;
+    ia_coordinate mManualWhiteCoordinate;
+
+    long mManualExposureTimeUs[MAX_EXPOSURES_NUM];
+    float mManualAnalogGain[MAX_EXPOSURES_NUM];
+    short mManualIso[MAX_EXPOSURES_NUM];
+
+    camera_af_mode_t mAfMode;
+    camera_af_trigger_t mAfTrigger;
+    bool mDuringAfTriggerScan;
+};
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/core/BufferQueue.cpp b/camera/hal/intel/ipu6/src/core/BufferQueue.cpp
new file mode 100644
index 000000000000..32c160e46074
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/BufferQueue.cpp
@@ -0,0 +1,286 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "BufferQueue"
+
+#include "iutils/CameraLog.h"
+
+#include "BufferQueue.h"
+#include "PlatformData.h"
+
+namespace icamera {
+
+BufferProducer::BufferProducer(int memType) : mMemType(memType)
+{
+    LOG1("@%s BufferProducer created mMemType: %d", __func__, mMemType);
+}
+
+BufferQueue::BufferQueue() : mBufferProducer(nullptr),
+                             mProcessThread(nullptr),
+                             mThreadRunning(false)
+{
+    LOG1("@%s BufferQueue created", __func__);
+}
+
+BufferQueue::~BufferQueue()
+{
+    LOG1("@%s BufferQueue destroyed", __func__);
+}
+
+int BufferQueue::queueInputBuffer(Port port, const std::shared_ptr<CameraBuffer> &camBuffer)
+{
+    // If it's not in mInputQueue, then it's not for this processor.
+    if (mInputQueue.find(port) == mInputQueue.end()) {
+        return OK;
+    }
+
+    LOG2("%s CameraBuffer %p for port:%d", __func__, camBuffer.get(), port);
+
+    CameraBufQ &input = mInputQueue[port];
+    bool needSignal = input.empty();
+    input.push(camBuffer);
+    if (needSignal) {
+        mFrameAvailableSignal.signal();
+    }
+
+    LOG2("%s Exit", __func__);
+    return OK;
+}
+
+int BufferQueue::onFrameAvailable(Port port, const std::shared_ptr<CameraBuffer> &camBuffer)
+{
+    AutoMutex l(mBufferQueueLock);
+
+    return queueInputBuffer(port, camBuffer);
+}
+
+void BufferQueue::setBufferProducer(BufferProducer *producer)
+{
+    LOG1("%s producer %p", __func__, producer);
+
+    AutoMutex l(mBufferQueueLock);
+    mBufferProducer = producer;
+
+    if (producer == nullptr)
+        return;
+
+    mBufferProducer->addFrameAvailableListener(this);
+}
+
+void BufferQueue::addFrameAvailableListener(BufferConsumer *listener)
+{
+    LOG1("%s listener %p", __func__, listener);
+    AutoMutex   l(mBufferQueueLock);
+    bool isAlreadyAdded = false;
+    for (auto& consumer : mBufferConsumerList) {
+        if (consumer == listener) {
+            isAlreadyAdded = true;
+            break;
+        }
+    }
+
+    // If the listener has been already added, then we don't register it again.
+    if (isAlreadyAdded) {
+        return;
+    }
+    mBufferConsumerList.push_back(listener);
+}
+
+void BufferQueue::removeFrameAvailableListener(BufferConsumer *listener)
+{
+    LOG1("%s listener %p", __func__, listener);
+    AutoMutex   l(mBufferQueueLock);
+
+    for (auto it = mBufferConsumerList.begin(); it != mBufferConsumerList.end(); ++it) {
+        if ((*it) == listener) {
+            mBufferConsumerList.erase(it);
+            break;
+        }
+    }
+}
+
+int BufferQueue::qbuf(Port port, const std::shared_ptr<CameraBuffer> &camBuffer)
+{
+    LOG2("%s CameraBuffer %p for port:%d", __func__, camBuffer.get(), port);
+
+    //Enqueue buffer to internal pool
+    AutoMutex   l(mBufferQueueLock);
+    if (camBuffer != nullptr && camBuffer->getStreamType() == CAMERA_STREAM_INPUT) {
+        return queueInputBuffer(port, camBuffer);
+    }
+
+    if (mOutputQueue.find(port) == mOutputQueue.end()) {
+        LOGE("Not supported port:%d", port);
+        return BAD_VALUE;
+    }
+
+    CameraBufQ &output = mOutputQueue[port];
+    bool needSignal = output.empty();
+    output.push(camBuffer);
+    if (needSignal) {
+        mOutputAvailableSignal.signal();
+    }
+
+    return OK;
+}
+
+void BufferQueue::clearBufferQueues()
+{
+    AutoMutex l(mBufferQueueLock);
+
+    mInputQueue.clear();
+    for (const auto& input : mInputFrameInfo) {
+        mInputQueue[input.first] = CameraBufQ();
+    }
+
+    mOutputQueue.clear();
+    for (const auto& output : mOutputFrameInfo) {
+        mOutputQueue[output.first] = CameraBufQ();
+    }
+}
+
+void BufferQueue::setFrameInfo(const std::map<Port, stream_t>& inputInfo,
+                               const std::map<Port, stream_t>& outputInfo)
+{
+    mInputFrameInfo = inputInfo;
+    mOutputFrameInfo = outputInfo;
+
+    clearBufferQueues();
+}
+
+void BufferQueue::getFrameInfo(std::map<Port, stream_t>& inputInfo,
+                               std::map<Port, stream_t>& outputInfo) const
+{
+    inputInfo = mInputFrameInfo;
+    outputInfo = mOutputFrameInfo;
+}
+
+int BufferQueue::waitFreeBuffersInQueue(ConditionLock& lock,
+                                        std::map<Port, std::shared_ptr<CameraBuffer> > &cInBuffer,
+                                        std::map<Port, std::shared_ptr<CameraBuffer> > &cOutBuffer,
+                                        int64_t timeout)
+{
+    LOG2("@%s start waiting the input and output buffers", __func__);
+
+    if (!mThreadRunning) {
+        LOG1("@%s: Processor is not active.", __func__);
+        return OK;
+    }
+
+    int ret = OK;
+    timeout = (timeout ? timeout : kWaitDuration) * SLOWLY_MULTIPLIER;
+
+    for (auto& input: mInputQueue) {
+        Port port = input.first;
+        CameraBufQ &inputQueue = input.second;
+        while (inputQueue.empty()) {
+            LOG2("%s: wait input port %d", __func__, port);
+            ret = mFrameAvailableSignal.waitRelative(lock, timeout);
+
+            // Thread was stopped during wait
+            if (!mThreadRunning) {
+                LOG1("@%s: Processor is not active while waiting for input buffers", __func__);
+                return OK;
+            }
+
+            if (ret == TIMED_OUT) {
+                return ret;
+            }
+        }
+        // Wake up from the buffer available
+        cInBuffer[port] = inputQueue.front();
+    }
+
+    for (auto& output: mOutputQueue) {
+        Port port = output.first;
+        CameraBufQ &outputQueue = output.second;
+        while (outputQueue.empty()) {
+            LOG2("%s: wait output port %d", __func__, port);
+            ret = mOutputAvailableSignal.waitRelative(lock, timeout);
+
+            // Thread was stopped during wait
+            if (!mThreadRunning) {
+                LOG1("@%s: Processor is not active while waiting for output buffers.", __func__);
+                return OK;
+            }
+
+            if (ret == TIMED_OUT) {
+                return ret;
+            }
+        }
+
+        cOutBuffer[port] = outputQueue.front();
+    }
+
+    return ret;
+}
+
+int BufferQueue::allocProducerBuffers(int camId, int bufNum)
+{
+    LOG1("%s: buffer queue size %d", __func__, bufNum);
+
+    mInternalBuffers.clear();
+
+    CheckError(!mBufferProducer, BAD_VALUE ,"@%s: Buffer Producer is nullptr", __func__);
+
+    for (const auto& item : mInputFrameInfo) {
+        Port port = item.first;
+        int srcFmt = item.second.format;
+        int srcWidth = item.second.width;
+        int srcHeight = item.second.height;
+
+        LOG1("%s fmt:%s (%dx%d)", __func__,
+             CameraUtils::format2string(srcFmt).c_str(), srcWidth, srcHeight);
+
+        int32_t size = 0;
+        bool isISYSCompression = PlatformData::getISYSCompression(camId);
+        if (isISYSCompression)
+            size = CameraUtils::getFrameSize(srcFmt, srcWidth, srcHeight, false, true, true);
+        else
+            size = CameraUtils::getFrameSize(srcFmt, srcWidth, srcHeight);
+        int memType = mBufferProducer->getMemoryType();
+
+        for (int i = 0; i < bufNum; i++) {
+            std::shared_ptr<CameraBuffer> camBuffer;
+            switch (memType) {
+            case V4L2_MEMORY_USERPTR:
+                camBuffer = CameraBuffer::create(camId, BUFFER_USAGE_PSYS_INPUT, V4L2_MEMORY_USERPTR,
+                                                 size, i, srcFmt, srcWidth, srcHeight);
+                CheckError(!camBuffer, NO_MEMORY, "Allocate producer userptr buffer failed");
+                break;
+
+            case V4L2_MEMORY_MMAP:
+                camBuffer = std::make_shared<CameraBuffer>(camId, BUFFER_USAGE_PSYS_INPUT,
+                                                      V4L2_MEMORY_MMAP, size, i, srcFmt);
+                CheckError(!camBuffer, NO_MEMORY, "Allocate producer mmap buffer failed");
+                camBuffer->setUserBufferInfo(srcFmt, srcWidth, srcHeight);
+                mBufferProducer->allocateMemory(port, camBuffer);
+                break;
+
+            default:
+                LOGE("Not supported v4l2 memory type:%d", memType);
+                return BAD_VALUE;
+            }
+
+            mInternalBuffers[port].push_back(camBuffer);
+            mBufferProducer->qbuf(port, camBuffer);
+        }
+    }
+
+    return OK;
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/BufferQueue.h b/camera/hal/intel/ipu6/src/core/BufferQueue.h
new file mode 100644
index 000000000000..a8ab5d337688
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/BufferQueue.h
@@ -0,0 +1,206 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <map>
+#include <vector>
+#include "iutils/Thread.h"
+#include "iutils/Errors.h"
+
+#include "CameraEvent.h"
+#include "CameraBuffer.h"
+
+/**
+ * These are the abstract Classes for buffer communication between different class of HAL
+ */
+namespace icamera {
+
+class BufferProducer;
+
+/**
+ * BufferConsumer listens on the onFrameAvailable event from the producer by
+ * calling setBufferProducer
+ */
+class BufferConsumer {
+public:
+    virtual ~BufferConsumer() {};
+    virtual int onFrameAvailable(Port port, const std::shared_ptr<CameraBuffer> &camBuffer) = 0;
+    virtual void setBufferProducer(BufferProducer *producer) = 0;
+};
+
+/**
+ * BufferProcuder get the buffers from consumer by "qbuf".
+ * Notfiy the consumer by calling the onFramAvaible interface of consumer.
+ * The consumer must be registered by "addFrameAvailableListener" before getting
+ * any buffer done notification.
+ */
+class BufferProducer : public EventSource {
+public:
+    BufferProducer(int memType = V4L2_MEMORY_USERPTR);
+    virtual ~BufferProducer() {};
+    virtual int qbuf(Port port, const std::shared_ptr<CameraBuffer> &camBuffer) = 0;
+    virtual int allocateMemory(Port port, const std::shared_ptr<CameraBuffer> &camBuffer) = 0;
+    virtual void addFrameAvailableListener(BufferConsumer *listener) = 0;
+    virtual void removeFrameAvailableListener(BufferConsumer *listener) = 0;
+    int getMemoryType(void) const {return mMemType;}
+
+private:
+    int mMemType;
+};
+
+class BufferQueue: public BufferConsumer, public BufferProducer, public EventListener {
+public:
+    BufferQueue();
+    virtual ~BufferQueue();
+
+    /**
+     * \brief the notify when poll one frame buffer
+     *
+     * Push the CameraBuffer to InputQueue and send a signal if needed
+     */
+    virtual int onFrameAvailable(Port port, const std::shared_ptr<CameraBuffer> &camBuffer);
+
+    /**
+     * \brief Register the BufferProducer
+     *
+     * Register the BufferProducer: Psys, software, or captureUnit
+     */
+    virtual void setBufferProducer(BufferProducer *producer);
+
+    /**
+     * \brief Queue one buffer to producer
+     *
+     * Push this buffer to output queue
+     */
+    virtual int qbuf(Port port, const std::shared_ptr<CameraBuffer> &camBuffer);
+
+    /**
+     * \brief allocate memory
+     *
+     * Not support this function in Psys and SWProcessor
+     */
+    virtual int allocateMemory(Port port,
+                               const std::shared_ptr<CameraBuffer> &camBuffer) { return -1; }
+
+    /**
+     * \brief Add the get frame listener
+     */
+    virtual void addFrameAvailableListener(BufferConsumer *listener);
+
+    /**
+     * \brief Remove the get frame listener
+     */
+    virtual void removeFrameAvailableListener(BufferConsumer *listener);
+
+    /**
+     * \brief Set all frames configuration
+     *
+     * Must be called before configure which needs use frame configuration.
+     */
+    virtual void setFrameInfo(const std::map<Port, stream_t>& inputInfo,
+                              const std::map<Port, stream_t>& outputInfo);
+
+    /*
+     * \brief Get all frames configuration
+     */
+    virtual void getFrameInfo(std::map<Port, stream_t>& inputInfo,
+                              std::map<Port, stream_t>& outputInfo) const;
+
+    /**
+     * \brief Register user buffers to processor(PSys)
+     */
+    virtual int registerUserOutputBufs(Port port,
+            const std::shared_ptr<CameraBuffer> &camBuffer) { return OK; }
+
+    /**
+     * \brief Common Interface
+     */
+    virtual int start() = 0;
+    virtual void stop() = 0;
+    virtual int setParameters(const Parameters& param) { return OK; }
+    virtual int getParameters(Parameters& param) { return OK; }
+    virtual int configure(const std::vector<ConfigMode>& configModes) { return OK; }
+
+protected:
+    virtual int processNewFrame() = 0;
+
+    /**
+     * \brief Clear and initialize input and output buffer queues.
+     */
+    void clearBufferQueues();
+    /**
+     * \brief Wait for available input and output buffers.
+     *
+     * Only fetch buffer from the buffer queue, need pop buffer from
+     * the queue after the buffer is used, and need to be protected by mBufferQueueLock.
+     */
+    int waitFreeBuffersInQueue(ConditionLock& lock,
+                               std::map<Port, std::shared_ptr<CameraBuffer> > &cInBuffer,
+                               std::map<Port, std::shared_ptr<CameraBuffer> > &cOutBuffer,
+                               int64_t timeout = 0);
+    /**
+     * \brief Buffers allocation for producer
+     */
+    int allocProducerBuffers(int camId, int bufNum);
+
+protected:
+    /**
+     * \brief The process new frame buffer thread
+     *
+     * Use this thread listen to the input queue and output queue.
+     * And do process if these two queues are not empty
+     */
+    class ProcessThread: public Thread {
+        BufferQueue *mProcessor;
+        public:
+            ProcessThread(BufferQueue *p)
+                : mProcessor(p) { }
+
+            virtual bool threadLoop() {
+                int ret = mProcessor->processNewFrame();
+                return (ret == 0);
+            }
+    };
+    static const nsecs_t kWaitDuration = 10000000000; //10000ms
+
+    BufferProducer *mBufferProducer;
+    std::vector<BufferConsumer*> mBufferConsumerList;
+
+    std::map<Port, stream_t> mInputFrameInfo;
+    std::map<Port, stream_t> mOutputFrameInfo;
+
+    std::map<Port, CameraBufQ> mInputQueue;
+    std::map<Port, CameraBufQ> mOutputQueue;
+
+    // For internal buffers allocation for producer
+    std::map<Port, CameraBufVector> mInternalBuffers;
+
+    // Guard for BufferQueue public API
+    Mutex  mBufferQueueLock;
+    Condition mFrameAvailableSignal;
+    Condition mOutputAvailableSignal;
+
+    //for the thread loop
+    ProcessThread* mProcessThread;
+    bool mThreadRunning;   //state of the processor. true after start and false after stop
+
+private:
+    int queueInputBuffer(Port port, const std::shared_ptr<CameraBuffer> &camBuffer);
+
+};
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/CameraBuffer.cpp b/camera/hal/intel/ipu6/src/core/CameraBuffer.cpp
new file mode 100644
index 000000000000..596172263ee5
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/CameraBuffer.cpp
@@ -0,0 +1,407 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "CameraBuffer"
+
+#include <sys/mman.h>
+#include <stdlib.h>
+#include <errno.h>
+#include <string.h>
+#include <vector>
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+#include "PlatformData.h"
+#include "CameraBuffer.h"
+
+namespace icamera {
+CameraBuffer::CameraBuffer(int cameraId, int usage, int memory, uint32_t size, int index, int format) :
+    mNumPlanes(1),
+    mAllocatedMemory(false),
+    mU(nullptr),
+    mBufferUsage(usage),
+    mSettingSequence(-1)
+
+{
+    v4l2_buf_type type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;
+    int num_plane = 1;
+
+    LOG1("%s: construct CameraBuffer with cameraId:%d, usage:%d, memory:%d, size:%d, format:%d, index:%d",
+         __func__, cameraId, usage, memory, size, format, index);
+
+    mU = new camera_buffer_t;
+    CLEAR(*mU);
+    mU->flags = BUFFER_FLAG_INTERNAL;
+    mU->sequence = -1;
+
+    switch (usage) {
+        case BUFFER_USAGE_PSYS_INPUT:
+            //follow through
+        case BUFFER_USAGE_PSYS_INTERNAL:
+        case BUFFER_USAGE_GENERAL:
+            if (PlatformData::isIsysEnabled(cameraId)
+                && PlatformData::isCSIFrontEndCapture(cameraId)) {
+                type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;
+                num_plane = CameraUtils::getNumOfPlanes(format);
+            } else {
+                type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;
+            }
+            break;
+        case BUFFER_USAGE_PSYS_STATS:
+            type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;
+            break;
+        case BUFFER_USAGE_MIPI_CAPTURE:
+        case BUFFER_USAGE_METADATA:
+            type = V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE;
+            num_plane = CameraUtils::getNumOfPlanes(format);
+            break;
+        default:
+            LOGE("Not supported Usage");
+    }
+
+    CLEAR(mMmapAddrs);
+    CLEAR(mDmaFd);
+
+    initBuffer(memory, type, size, index, num_plane);
+}
+
+CameraBuffer::~CameraBuffer()
+{
+    LOG1("Free CameraBuffer");
+
+    freeMemory();
+
+    if (mU->flags & BUFFER_FLAG_INTERNAL) {
+        delete mU;
+    }
+}
+
+void CameraBuffer::initBuffer(int memType, v4l2_buf_type bufType, uint32_t size, int idx, int num_plane)
+{
+    mV.SetMemory(memType);
+    mV.SetIndex(idx);
+
+    mV.SetType(bufType);
+
+    if (!V4L2_TYPE_IS_MULTIPLANAR(bufType)) {
+        mV.SetOffset(0, 0);
+        mV.SetLength(size, 0);
+        LOGE("SINGLE PLANE!");
+    } else {
+        mV.SetLength(num_plane, 0);
+        mNumPlanes = num_plane;
+        for (int i = 0; i < mNumPlanes; ++i) {
+            mV.SetLength(size, i);
+        }
+    }
+
+    mV.SetFlags(mV.Flags() | V4L2_BUF_FLAG_NO_CACHE_INVALIDATE | V4L2_BUF_FLAG_NO_CACHE_CLEAN);
+}
+
+//Helper function to construct a Internal CameraBuffer
+std::shared_ptr<CameraBuffer> CameraBuffer::create(int cameraId, int usage, int memory, unsigned int size, int index,
+                                              int srcFmt, int srcWidth, int srcHeight)
+{
+    std::shared_ptr<CameraBuffer> camBuffer = std::make_shared<CameraBuffer>(cameraId, usage, memory, size, index, srcFmt);
+
+    CheckError(!camBuffer, nullptr, "@%s: fail to alloc CameraBuffer", __func__);
+
+    camBuffer->setUserBufferInfo(srcFmt, srcWidth, srcHeight);
+
+    int ret = camBuffer->allocateMemory();
+
+    CheckError(ret != OK, nullptr, "Allocate memory failed ret %d", ret);
+
+    return camBuffer;
+}
+
+//Internal frame Buffer
+void CameraBuffer::setUserBufferInfo(int format, int width, int height)
+{
+    LOG1("%s: format:%d, width:%d, height:%d", __func__, format, width, height);
+    mU->s.width = width;
+    mU->s.height = height;
+    mU->s.format = format;
+    if (format != -1) {
+        mU->s.stride = CameraUtils::getStride(format, width);
+    }
+}
+
+void CameraBuffer::setUserBufferInfo(int format, int width, int height, void *usrPtr)
+{
+    setUserBufferInfo(format, width, height);
+    setAddr(usrPtr, 0);
+}
+
+//Called when a buffer is from the application
+void CameraBuffer::setUserBufferInfo(camera_buffer_t *ubuffer)
+{
+    CheckError(ubuffer == nullptr, VOID_VALUE, "%s: ubuffer is nullptr", __func__);
+
+    if (mU->flags & BUFFER_FLAG_INTERNAL) delete mU;
+    mU = ubuffer;
+
+    LOG1("%s: ubuffer->s.MemType: %d, addr: %p, fd: %d", __func__, ubuffer->s.memType,
+         ubuffer->addr, ubuffer->dmafd);
+    //update the v4l2 buffer memory with user infro
+    switch (ubuffer->s.memType) {
+        case V4L2_MEMORY_USERPTR:
+            setAddr(ubuffer->addr, 0);
+            break;
+        case V4L2_MEMORY_DMABUF:
+            setFd(ubuffer->dmafd, 0);
+            break;
+        case V4L2_MEMORY_MMAP:
+            /* do nothing */
+            break;
+        default:
+            LOGE("iomode %d is not supported yet.", mV.Memory());
+            break;
+    }
+
+    if (mU->s.streamType == CAMERA_STREAM_INPUT || ubuffer->sequence >= 0) {
+        mV.SetSequence(ubuffer->sequence);
+        LOG2("%s, input buffer sequence %lld", __func__, ubuffer->sequence);
+    }
+}
+
+void CameraBuffer::updateV4l2Buffer(const v4l2_buffer_t& v4l2buf)
+{
+    mV.SetField(v4l2buf.field);
+    mV.SetTimestamp(v4l2buf.timestamp);
+    mV.SetSequence(v4l2buf.sequence);
+    mV.SetRequestFd(v4l2buf.request_fd);
+}
+
+/*export mmap buffer as dma_buf fd stored in mV and mU*/
+int CameraBuffer::exportMmapDmabuf(V4L2VideoNode *vDevice)
+{
+    std::vector<int> fds;
+
+    int ret = vDevice->ExportFrame(mV.Index(), &fds);
+
+    CheckError(ret != OK, -1, "exportMmapDmabuf failed, ret %d", ret);
+
+    for (size_t i = 0; i < fds.size(); ++i) {
+        setFd(fds[i], i);
+    }
+
+    if (mU->flags & BUFFER_FLAG_DMA_EXPORT) {
+        mU->dmafd = getFd(0);
+    }
+
+    return OK;
+}
+
+int CameraBuffer::allocateMemory(V4L2VideoNode* vDevice)
+{
+    int ret = BAD_VALUE;
+    LOG1("%s", __func__);
+    switch(mV.Memory()) {
+        case V4L2_MEMORY_USERPTR:
+            ret = allocateUserPtr();
+            mAllocatedMemory = true;
+            mU->addr = getAddr();
+            break;
+        case V4L2_MEMORY_MMAP:
+            exportMmapDmabuf(vDevice);
+            ret = allocateMmap(vDevice);
+            mU->addr = getAddr();
+            mAllocatedMemory = true;
+            break;
+        default:
+            LOGE("memory type %d is incorrect for allocateMemory.", mV.Memory());
+            return BAD_VALUE;
+    }
+
+    return ret;
+}
+
+int CameraBuffer::allocateUserPtr()
+{
+    void* buffer = nullptr;
+    for (int i = 0; i < mNumPlanes; ++i) {
+        int ret = posix_memalign(&buffer, getpagesize(), mV.Length(i));
+        CheckError(ret != 0, -1, "%s, posix_memalign fails, ret:%d", __func__, ret);
+        mV.SetUserptr(reinterpret_cast<uintptr_t>(buffer), i);
+        mMmapAddrs[i] = buffer;
+    }
+    return OK;
+}
+
+void CameraBuffer::freeUserPtr()
+{
+    for (int i = 0; i < mNumPlanes; ++i) {
+        void* ptr = reinterpret_cast<void*>(mV.Userptr(i));
+        mMmapAddrs[i] = nullptr;
+        ::free(ptr);
+        mV.SetUserptr(reinterpret_cast<uintptr_t>(nullptr), i);
+    }
+}
+
+int CameraBuffer::allocateMmap(V4L2VideoNode* dev)
+{
+    std::vector<void*> addrs;
+    int ret = dev->MapMemory(mV.Index(), PROT_READ | PROT_WRITE,
+                            MAP_SHARED, &addrs);
+
+    CheckError(ret != OK, -1, "allocateMmap failed, ret %d", ret);
+
+    for (unsigned int i = 0; i < addrs.size(); ++i) {
+        if (addrs[i] == MAP_FAILED) {
+            mMmapAddrs[i] = nullptr;
+            continue ;
+        }
+        mMmapAddrs[i] = addrs[i];
+    }
+
+    return OK;
+}
+
+void* CameraBuffer::getAddr(int plane)
+{
+    CheckError(plane < 0 || plane >= mNumPlanes, nullptr, "Wrong plane number %d", plane);
+
+    switch (mV.Memory()) {
+        case V4L2_MEMORY_MMAP:
+            return mMmapAddrs[plane];
+        case V4L2_MEMORY_USERPTR:
+            return reinterpret_cast<void*>(mV.Userptr(plane));
+        default:
+            LOGE("%s: Not supported memory type %u", __func__, mV.Memory());
+    }
+    return nullptr;
+}
+
+void CameraBuffer::setAddr(void *addr, int plane)
+{
+    CheckError(plane < 0 || plane >= mNumPlanes, VOID_VALUE,
+               "Wrong plane number %d", plane);
+
+    switch (mV.Memory()) {
+        case V4L2_MEMORY_MMAP:
+            mMmapAddrs[plane] = addr;
+            return;
+        case V4L2_MEMORY_USERPTR:
+            mV.SetUserptr(reinterpret_cast<uintptr_t>(addr), plane);
+            mMmapAddrs[plane] = addr;
+            return;
+        default:
+            LOGE("%s: Not supported memory type %u", __func__, mV.Memory());
+    }
+}
+
+void CameraBuffer::freeMmap()
+{
+    int ret = OK;
+
+    for (int i = 0; i < mNumPlanes; i++) {
+        if (getFd(i) != -1) {
+            ::close(getFd(i));
+            setFd(-1, i);
+        }
+        if (mMmapAddrs[i]) {
+            ret = ::munmap(mMmapAddrs[i], mV.Length(i));
+            CheckError(ret != 0, VOID_VALUE, "failed to munmap buffer %d", i);
+            mMmapAddrs[i] = nullptr;
+        }
+    }
+}
+
+void* CameraBuffer::mapDmaBufferAddr(int fd, unsigned int bufferSize)
+{
+    if(fd < 0 || !bufferSize) {
+        LOGE("%s, fd:0x%x, bufferSize:%u", __func__, fd, bufferSize);
+        return nullptr;
+    }
+    return ::mmap(nullptr, bufferSize, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
+}
+
+void CameraBuffer::unmapDmaBufferAddr(void* addr, unsigned int bufferSize)
+{
+    if(addr == nullptr || !bufferSize) {
+        LOGE("%s, addr:%p, bufferSize:%u", __func__, addr, bufferSize);
+        return;
+    }
+    munmap(addr, bufferSize);
+}
+
+void CameraBuffer::freeMemory()
+{
+    if (!mAllocatedMemory) {
+        LOG2("@%s Memory(in %p) is not allocated by CameraBuffer class. Don't free it.", __func__, this);
+        return ;
+    }
+
+    switch(mV.Memory()) {
+        case V4L2_MEMORY_USERPTR:
+            freeUserPtr();
+            break;
+        case V4L2_MEMORY_MMAP:
+            freeMmap();
+            break;
+        default:
+            LOGE("Free camera buffer failed, due to memory %d type is not implemented yet.", mV.Memory());
+    }
+}
+
+void CameraBuffer::updateUserBuffer(void)
+{
+    mU->timestamp = TIMEVAL2NSECS(getTimestamp());
+    mU->s.field = getField();
+
+    // Use valid setting sequence to align shutter/parameter with buffer
+    mU->sequence = (mSettingSequence < 0) ? getSequence() : mSettingSequence;
+}
+
+void CameraBuffer::updateFlags(void)
+{
+    int flag = V4L2_BUF_FLAG_NO_CACHE_INVALIDATE | V4L2_BUF_FLAG_NO_CACHE_CLEAN;
+    bool set = true;
+
+    //clear the flags if the buffers is accessed by the SW
+    if ((mU->flags & BUFFER_FLAG_SW_READ) || (mU->flags & BUFFER_FLAG_SW_WRITE)) {
+        set = false;
+    }
+
+    mV.SetFlags(set ? (mV.Flags() | flag): (mV.Flags() & (~flag)));
+}
+
+bool CameraBuffer::isFlagsSet(int flag)
+{
+    return ((mU->flags & flag) ? true : false);
+}
+
+void CameraBuffer::setFd(int val, int plane)
+{
+    if (mV.Memory() == V4L2_MEMORY_MMAP) {
+        mDmaFd[plane] = val;
+    } else {
+        mV.SetFd(val, plane);
+    }
+}
+
+int CameraBuffer::getFd(int plane)
+{
+    if (mV.Memory() == V4L2_MEMORY_MMAP) {
+        return mDmaFd[plane];
+    }
+
+    return mV.Fd(plane);
+}
+
+}//namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/CameraBuffer.h b/camera/hal/intel/ipu6/src/core/CameraBuffer.h
new file mode 100644
index 000000000000..c2361dfdd7cb
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/CameraBuffer.h
@@ -0,0 +1,170 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+#include <queue>
+#include <vector>
+
+#include <linux/videodev2.h>
+#include <v4l2_device.h>
+
+#include "api/Parameters.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+typedef struct v4l2_buffer v4l2_buffer_t;
+
+/* CameraBuffer is the core buffers for HAL. The buffer usage is described by the
+ * BufferUsage. CameraBuffer are constructed based on usage */
+enum BufferUsage {
+    BUFFER_USAGE_GENERAL = 0,
+    BUFFER_USAGE_PSYS_STATS,
+    BUFFER_USAGE_PSYS_INPUT,
+    BUFFER_USAGE_MIPI_CAPTURE,
+    BUFFER_USAGE_METADATA,
+    BUFFER_USAGE_PSYS_INTERNAL,
+};
+
+class CameraBuffer {
+public:
+    //assist function to create frame buffers
+    static std::shared_ptr<CameraBuffer>
+    create(int cameraId, int usage, int memory, unsigned int size, int index,
+           int srcFmt = -1, int srcWidth=-1, int srcHeight=-1);
+
+public:
+    CameraBuffer(int cameraId, int usage, int memory, uint32_t size, int index, int format = -1);
+    virtual ~CameraBuffer();
+
+public:
+    //user buffer information
+    int getWidth() const { return mU->s.width; }
+    int getHeight() const { return mU->s.height; }
+    int getStride() const { return mU->s.stride; }
+    int getFormat() const { return mU->s.format; }
+    int getStreamType() const { return mU->s.streamType; }
+    int getStreamUsage() const { return mU->s.usage; }
+    int getStreamId() const { return mU->s.id; }
+    int getFlags() const { return mU->flags; }
+
+    //v4l2 buffer information
+    uint32_t getIndex(void) const { return mV.Index(); }
+
+    uint32_t getSequence(void) const { return mV.Sequence(); }
+    void setSequence(uint32_t sequence) { mV.SetSequence(sequence); }
+
+    uint32_t getField() const { return mV.Field(); }
+    void setField(uint32_t field) { mV.SetField(field); }
+
+    struct timeval getTimestamp(void) const  { return mV.Timestamp(); }
+    void setTimestamp(struct timeval timestamp) { mV.SetTimestamp(timestamp); }
+
+    int getFd(int planeIndex = 0);
+
+    uint32_t getMemory(void) const { return mV.Memory(); }
+
+    int numPlanes() { return mNumPlanes; }
+
+     //For debug only v4l2 buffer information
+    int getCsi2Port(void) const { return (mV.RequestFd() >> 4) & 0xf; }
+    int getVirtualChannel(void) const { return mV.RequestFd() & 0xf; }
+
+    /* u buffer is used to attach user private structure pointer
+     * in CameraBuffer.
+     *
+     * Now, one of this usage is linking camera_buffer_t to CameraBuffer
+     * together, so that we can get each pointer by other.
+     * Notes: Please don't abuse this. It is only used in CameraDevice for user buffer
+     */
+    camera_buffer_t *getUserBuffer() { return mU; }
+    //update the user  buffer with latest v4l2 buffer info from driver
+    void    updateUserBuffer(void);
+    //Update the v4l2 flags according to user buffer flag
+    void    UpdateFlags(void);
+    void    updateFlags(void);
+
+    //Check if the specific flag in "mU->flags" is set or not
+    bool isFlagsSet(int flag);
+    //The ubuffer is from application
+    void setUserBufferInfo(camera_buffer_t *ubuffer);
+    void setUserBufferInfo(int format, int width, int height);
+    void setUserBufferInfo(int format, int width, int height, void *usrPtr);
+
+    uint32_t getBufferSize(int planeIndex = 0) { return mV.Length(planeIndex); }
+    void setBufferSize(unsigned int size, int planeIndex = 0) { mV.SetLength(size, planeIndex); }
+
+    unsigned int getBytesused(int planeIndex = 0) { return mV.BytesUsed(planeIndex); }
+    void setBytesused(unsigned int bytes, int planeIndex = 0) { mV.SetBytesUsed(bytes, planeIndex); }
+
+    void* getBufferAddr(int planeIndex = 0) { return getAddr(planeIndex); }
+    void  setBufferAddr(void *addr, int planeIndex = 0) { return setAddr(addr, planeIndex); }
+
+    void updateV4l2Buffer(const v4l2_buffer_t& v4l2buf);
+
+    V4L2Buffer& getV4L2Buffer() { return mV; }
+
+    int getUsage() const { return mBufferUsage; }
+
+    void setSettingSequence(long sequence) { mSettingSequence = sequence; }
+    long getSettingSequence() const { return mSettingSequence; }
+
+    //Buffers are allocated the buffers by Camera
+    int allocateMemory(V4L2VideoNode *vDevice = nullptr);
+
+public:
+    static void* mapDmaBufferAddr(int fd, unsigned int bufferSize);
+    static void unmapDmaBufferAddr(void* addr, unsigned int bufferSize);
+
+private:
+    CameraBuffer(const CameraBuffer&);
+    CameraBuffer& operator=(const CameraBuffer&);
+
+    void freeMemory();
+    int exportMmapDmabuf(V4L2VideoNode *vDevice);
+
+    int allocateMmap(V4L2VideoNode* dev);
+    int allocateUserPtr();
+    void freeUserPtr();
+    void freeMmap();
+    void* getAddr(int plane = 0);
+    void setAddr(void *userAddr, int plane = 0);
+    void initBuffer(int memType, v4l2_buf_type bufType, uint32_t size, int idx, int num_plane);
+
+    void setFd(int val, int plane);
+
+protected:
+    V4L2Buffer mV;
+    int mNumPlanes;
+
+private:
+    //To tag whether the memory is allocated by CameraBuffer class. We need to free them
+    bool mAllocatedMemory;
+
+    camera_buffer_t *mU;
+    int mBufferUsage;
+    long mSettingSequence;
+
+    void* mMmapAddrs[VIDEO_MAX_PLANES];
+    int mDmaFd[VIDEO_MAX_PLANES];
+};
+
+typedef std::vector<std::shared_ptr<CameraBuffer> > CameraBufVector;
+typedef std::queue<std::shared_ptr<CameraBuffer> > CameraBufQ;
+
+}
diff --git a/camera/hal/intel/ipu6/src/core/CameraDevice.cpp b/camera/hal/intel/ipu6/src/core/CameraDevice.cpp
new file mode 100644
index 000000000000..6133c2103f8e
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/CameraDevice.cpp
@@ -0,0 +1,1182 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "CameraDevice"
+
+#include <vector>
+
+#include "iutils/Utils.h"
+#include "iutils/CameraLog.h"
+
+#include "IGraphConfig.h"
+#include "ICamera.h"
+#include "PlatformData.h"
+#include "CameraDevice.h"
+#include "V4l2DeviceFactory.h"
+#include "I3AControlFactory.h"
+#include "CaptureUnit.h"
+
+using std::vector;
+
+namespace icamera {
+
+CameraDevice::CameraDevice(int cameraId) :
+    mState(DEVICE_UNINIT),
+    mCameraId(cameraId),
+    mStreamNum(0),
+    mCallback(nullptr)
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s, cameraId:%d", __func__, mCameraId);
+
+    CLEAR(mStreams);
+
+    V4l2DeviceFactory::createDeviceFactory(mCameraId);
+    CLEAR(mInputConfig);
+    mInputConfig.format = -1;
+
+    mProducer = createBufferProducer();
+
+    mSofSource = new SofSource(mCameraId);
+
+    mPerframeControlSupport = PlatformData::isFeatureSupported(mCameraId, PER_FRAME_CONTROL);
+    LOG2("%s: support perframe %d", __func__, mPerframeControlSupport);
+    mParamGenerator = new ParameterGenerator(mCameraId);
+
+    mLensCtrl = new LensHw(mCameraId);
+    mSensorCtrl = SensorHwCtrl::createSensorCtrl(mCameraId);
+
+    m3AControl = I3AControlFactory::createI3AControl(mCameraId, mSensorCtrl, mLensCtrl);
+    mRequestThread = new RequestThread(mCameraId, m3AControl, mParamGenerator);
+    mRequestThread->registerListener(EVENT_PROCESS_REQUEST, this);
+    mRequestThread->registerListener(EVENT_DEVICE_RECONFIGURE, this);
+
+    mProcessorManager = new ProcessorManager(mCameraId);
+
+    if (PlatformData::getGraphConfigNodes(mCameraId)) {
+        mGCM = IGraphConfigManager::getInstance(mCameraId);
+    } else {
+        mGCM = nullptr;
+    }
+}
+
+CameraDevice::~CameraDevice()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+    AutoMutex   m(mDeviceLock);
+
+    // Clear the media control when close the device.
+    MediaCtlConf *mc = PlatformData::getMediaCtlConf(mCameraId);
+    if (mc) {
+        MediaControl::getInstance()->mediaCtlClear(mCameraId, mc);
+    }
+
+    mRequestThread->removeListener(EVENT_PROCESS_REQUEST, this);
+    mRequestThread->removeListener(EVENT_DEVICE_RECONFIGURE, this);
+
+    delete mProcessorManager;
+
+    for (int i = 0; i < MAX_STREAM_NUMBER; i++)
+        delete mStreams[i];
+
+    delete mLensCtrl;
+    delete m3AControl;
+    delete mSensorCtrl;
+    delete mParamGenerator;
+    delete mSofSource;
+    delete mProducer;
+    delete mRequestThread;
+
+    V4l2DeviceFactory::releaseDeviceFactory(mCameraId);
+    IGraphConfigManager::releaseInstance(mCameraId);
+}
+
+int CameraDevice::init()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s, mCameraId:%d, mState:%d", __func__, mCameraId, mState);
+    AutoMutex   m(mDeviceLock);
+
+    int ret = mProducer->init();
+    CheckError(ret < 0, ret, "%s: Init capture unit failed", __func__);
+
+    ret = mSofSource->init();
+    CheckError(ret != OK, ret, "@%s: init sync manager failed", __func__);
+
+    initDefaultParameters();
+
+    ret = m3AControl->init();
+    CheckError((ret != OK), ret, "%s: Init 3A Unit falied", __func__);
+
+    ret = mLensCtrl->init();
+    CheckError((ret != OK), ret, "%s: Init Lens falied", __func__);
+
+    mRequestThread->run("RequestThread", PRIORITY_NORMAL);
+
+    mState = DEVICE_INIT;
+    return ret;
+}
+
+void CameraDevice::deinit()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s, mCameraId:%d, mState:%d", __func__, mCameraId, mState);
+    AutoMutex   m(mDeviceLock);
+
+    //deinit should not be call in UNINIT or START STATE
+    if (mState == DEVICE_UNINIT) {
+        //Do nothing
+        return;
+    }
+
+    m3AControl->stop();
+
+    if (mState == DEVICE_START) {
+        //stop first
+        stopLocked();
+    }
+
+    // stop request thread
+    mRequestThread->requestExit();
+    mRequestThread->join();
+
+    deleteStreams();
+
+    mProcessorManager->deleteProcessors();
+
+    m3AControl->deinit();
+
+    mSofSource->deinit();
+
+    mProducer->deinit();
+
+    mState = DEVICE_UNINIT;
+}
+
+void CameraDevice::callbackRegister(const camera_callback_ops_t* callback)
+{
+    mCallback = const_cast<camera_callback_ops_t*>(callback);
+}
+
+StreamSource* CameraDevice::createBufferProducer()
+{
+
+    return new CaptureUnit(mCameraId);
+}
+
+void CameraDevice::bindListeners()
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+
+    vector<EventListener*> statsListenerList = m3AControl->getStatsEventListener();
+    for (auto statsListener : statsListenerList) {
+
+        for (auto& item : mProcessors) {
+            // Subscribe PSys statistics.
+            item->registerListener(EVENT_PSYS_STATS_BUF_READY, statsListener);
+            item->registerListener(EVENT_PSYS_STATS_SIS_BUF_READY, statsListener);
+        }
+    }
+        for (auto& item : mProcessors) {
+            item->registerListener(EVENT_PSYS_STATS_BUF_READY, mRequestThread);
+        }
+
+    vector<EventListener*> sofListenerList = m3AControl->getSofEventListener();
+    for (auto sofListener : sofListenerList) {
+        mSofSource->registerListener(EVENT_ISYS_SOF, sofListener);
+    }
+
+    if (PlatformData::psysAlignWithSof(mCameraId)) {
+        for (auto& item : mProcessors) {
+            mSofSource->registerListener(EVENT_ISYS_SOF, item);
+        }
+    }
+
+    if (mPerframeControlSupport || !PlatformData::isIsysEnabled(mCameraId)) {
+        mProcessors.back()->registerListener(EVENT_PSYS_FRAME, mRequestThread);
+    } else {
+        mProducer->registerListener(EVENT_ISYS_FRAME, mRequestThread);
+    }
+
+    if (!mProcessors.empty()) {
+        mProcessors.front()->registerListener(EVENT_PSYS_REQUEST_BUF_READY, this);
+    }
+
+    mSofSource->registerListener(EVENT_ISYS_SOF, mRequestThread);
+}
+
+void CameraDevice::unbindListeners()
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+
+    vector<EventListener*> statsListenerList = m3AControl->getStatsEventListener();
+    for (auto statsListener : statsListenerList) {
+
+        for (auto& item : mProcessors) {
+            item->removeListener(EVENT_PSYS_STATS_BUF_READY, statsListener);
+            item->removeListener(EVENT_PSYS_STATS_SIS_BUF_READY, statsListener);
+        }
+    }
+        for (auto& item : mProcessors) {
+            item->removeListener(EVENT_PSYS_STATS_BUF_READY, mRequestThread);
+        }
+
+    vector<EventListener*> sofListenerList = m3AControl->getSofEventListener();
+    for (auto sofListener : sofListenerList) {
+        mSofSource->removeListener(EVENT_ISYS_SOF, sofListener);
+    }
+
+    if (PlatformData::psysAlignWithSof(mCameraId)) {
+        for (auto& item : mProcessors) {
+            mSofSource->removeListener(EVENT_ISYS_SOF, item);
+        }
+    }
+
+    if (!mProcessors.empty()) {
+        mProcessors.front()->removeListener(EVENT_PSYS_REQUEST_BUF_READY, this);
+    }
+
+    if (mPerframeControlSupport || !PlatformData::isIsysEnabled(mCameraId)) {
+        mProcessors.back()->removeListener(EVENT_PSYS_FRAME, mRequestThread);
+    } else {
+        mProducer->removeListener(EVENT_ISYS_FRAME, mRequestThread);
+    }
+
+    mSofSource->removeListener(EVENT_ISYS_SOF, mRequestThread);
+}
+
+int CameraDevice::configureInput(const stream_t *inputConfig)
+{
+    PERF_CAMERA_ATRACE();
+
+    AutoMutex lock(mDeviceLock);
+    mInputConfig = *inputConfig;
+
+    return OK;
+}
+
+int CameraDevice::configure(stream_config_t *streamList)
+{
+    PERF_CAMERA_ATRACE();
+
+    int numOfStreams = streamList->num_streams;
+    CheckError(!streamList->streams, BAD_VALUE, "%s: No valid stream config", __func__);
+    CheckError(numOfStreams > MAX_STREAM_NUMBER || numOfStreams <= 0, BAD_VALUE,
+          "%s: The requested stream number(%d) is invalid. Should be between [1-%d]",
+          __func__, numOfStreams, MAX_STREAM_NUMBER);
+
+    AutoMutex lock(mDeviceLock);
+
+    CheckError((mState != DEVICE_STOP) && (mState != DEVICE_INIT) && (mState != DEVICE_CONFIGURE),
+               INVALID_OPERATION, "%s: Add streams in wrong state %d", __func__, mState);
+
+    mRequestThread->configure(streamList);
+
+    // Use concrete ISP mode from request thread for full and auto switch
+    if (PlatformData::getAutoSwitchType(mCameraId) == AUTO_SWITCH_FULL &&
+        (ConfigMode)(streamList->operation_mode) == CAMERA_STREAM_CONFIGURATION_MODE_AUTO) {
+        stream_config_t requestStreamList = mRequestThread->getStreamConfig();
+        LOG2("%s: for full and auto switch, use concrete config mode %u from request thread.",
+             __func__, requestStreamList.operation_mode);
+        return configureL(&requestStreamList);
+    }
+
+    return configureL(streamList);
+}
+
+int CameraDevice::configureL(stream_config_t *streamList, bool clean)
+{
+    LOG1("@%s, mCameraId:%d, operation_mode %x", __func__, mCameraId, (ConfigMode)streamList->operation_mode);
+
+    int ret = analyzeStream(streamList);
+    CheckError(ret != OK, ret, "@%s, analyzeStream failed", __func__);
+
+    // If configured before, destroy current streams first.
+    if (mStreamNum > 0 && clean) {
+        deleteStreams();
+    }
+    mProcessorManager->deleteProcessors();
+
+    // Clear all previous added listeners.
+    mProducer->removeAllFrameAvailableListener();
+    if (clean) {
+        ret = createStreams(streamList);
+        CheckError(ret < 0, ret, "@%s create stream failed with %d", __func__, ret);
+    }
+
+    int mcId = -1;
+    if (mGCM != nullptr) {
+        ret = mGCM->configStreams(streamList);
+        CheckError(ret != OK, INVALID_OPERATION, "No matching graph config found");
+
+        mcId = mGCM->getSelectedMcId();
+    }
+
+    std::map<Port, stream_t> producerConfigs = selectProducerConfig(streamList, mcId);
+    CheckError(producerConfigs.empty(), BAD_VALUE, "The config for producer is invalid.");
+
+    bool needProcessor = isProcessorNeeded(streamList, producerConfigs[MAIN_PORT]);
+    for (auto& item : producerConfigs) {
+        LOG1("Producer config for port:%d, fmt:%s (%dx%d), needProcessor=%d", item.first,
+             CameraUtils::format2string(item.second.format).c_str(),
+             item.second.width, item.second.height, needProcessor);
+        // Only V4L2_MEMORY_MMAP is supported when using post processor
+        if (needProcessor) {
+            item.second.memType = V4L2_MEMORY_MMAP;
+        }
+    }
+
+    vector<ConfigMode> configModes;
+    PlatformData::getConfigModesByOperationMode(mCameraId, streamList->operation_mode, configModes);
+
+    ret = mProducer->configure(producerConfigs, configModes);
+    CheckError(ret < 0, BAD_VALUE, "@%s Device Configure failed", __func__);
+
+    ret = mSofSource->configure();
+    CheckError(ret != OK, ret, "@%s failed to configure SOF source device", __func__);
+
+    m3AControl->configure(streamList);
+
+    if (needProcessor) {
+        mProcessors = mProcessorManager->createProcessors(mInputConfig.format, producerConfigs,
+                                                          mStreamIdToPortMap,
+                                                          streamList, mParameter, mParamGenerator);
+        ret = mProcessorManager->configureProcessors(configModes,
+                                                     mProducer, mParameter);
+        CheckError(ret != OK, ret, "@%s configure post processor failed with:%d", __func__, ret);
+    }
+
+    ret = bindStreams(streamList);
+    CheckError(ret < 0, ret, "@%s bind stream failed with %d", __func__, ret);
+
+    mState = DEVICE_CONFIGURE;
+    return OK;
+}
+
+/**
+ * Select the producer's config from the supported list.
+ *
+ * How to decide the producer's config?
+ * 1. The producer's config can only be one of the combination from ISYS supported format and
+ *    resolution list.
+ * 2. Try to use the same config as user's required.
+ * 3. If the ISYS supported format and resolution cannot satisfy user's requirement, then use
+ *    the closest one, and let post processor do the conversion.
+ */
+std::map<Port, stream_t> CameraDevice::selectProducerConfig(const stream_config_t *streamList, int mcId)
+{
+    // Use the biggest stream to configure the producer.
+    stream_t biggestStream = streamList->streams[mSortedStreamIds[0]];
+    std::map<Port, stream_t> producerConfigs;
+
+    if (!PlatformData::isIsysEnabled(mCameraId)) {
+        // Input stream id is the last one of mSortedStreamIds
+        const stream_t& tmp = streamList->streams[mSortedStreamIds.back()];
+        if (tmp.streamType == CAMERA_STREAM_INPUT) {
+            producerConfigs[MAIN_PORT] = tmp;
+            return producerConfigs;
+        }
+    }
+
+    /*
+     * According to the stream info and operation mode to select MediaCtlConf.
+     * and isys output format. If inputFmt is given and supported, we use it as isys output format.
+     */
+    int inputFmt = mInputConfig.format;
+    int iSysFmt = biggestStream.format;
+    if (inputFmt != -1) {
+        if (!PlatformData::isISysSupportedFormat(mCameraId, inputFmt)) {
+            LOGE("The given ISYS format %s is unsupported.", CameraUtils::pixelCode2String(inputFmt));
+            return producerConfigs;
+        }
+        iSysFmt = inputFmt;
+    }
+
+    // Use CSI output to select MC config
+    vector <ConfigMode> configModes;
+    PlatformData::getConfigModesByOperationMode(mCameraId, streamList->operation_mode,
+                                             configModes);
+    stream_t matchedStream = biggestStream;
+    if (!configModes.empty() && mGCM != nullptr) {
+        std::shared_ptr<IGraphConfig> gc = mGCM->getGraphConfig(configModes[0]);
+        if (gc) {
+            camera_resolution_t csiOutput = {0, 0};
+            gc->getCSIOutputResolution(csiOutput);
+            if (csiOutput.width > 0 && csiOutput.height > 0) {
+                matchedStream.width = csiOutput.width;
+                matchedStream.height = csiOutput.height;
+            }
+        }
+    }
+
+    camera_crop_region_t cropRegion;
+    int ret = mParameter.getCropRegion(cropRegion);
+    if ((ret == OK) && (cropRegion.flag == 1)) {
+        PlatformData::selectMcConf(mCameraId, mInputConfig,
+                                  (ConfigMode)streamList->operation_mode, mcId);
+    } else {
+        PlatformData::selectMcConf(mCameraId, matchedStream,
+                                  (ConfigMode)streamList->operation_mode, mcId);
+    }
+
+    PlatformData::selectISysFormat(mCameraId, iSysFmt);
+
+    // Use the ISYS output if it's provided in media config section of config file.
+    stream_t mainConfig = PlatformData::getISysOutputByPort(mCameraId, MAIN_PORT);
+    mainConfig.memType = biggestStream.memType;
+    mainConfig.field = biggestStream.field;
+
+    if (mainConfig.width != 0 && mainConfig.height != 0) {
+        producerConfigs[MAIN_PORT] = mainConfig;
+
+        return producerConfigs;
+    }
+
+    int inputWidth = mInputConfig.width;
+    int inputHeight = mInputConfig.height;
+
+    camera_resolution_t producerRes = {inputWidth, inputHeight};
+    if (inputWidth == 0 && inputHeight == 0) {
+        // Only get the ISYS resolution when input config is not specified.
+        producerRes = PlatformData::getISysBestResolution(mCameraId, biggestStream.width,
+                                                          biggestStream.height, biggestStream.field);
+    } else if (!PlatformData::isISysSupportedResolution(mCameraId, producerRes)) {
+        LOGE("The stream config: (%dx%d) is not supported.", inputWidth, inputHeight);
+        return producerConfigs;
+    }
+
+    mainConfig.format = PlatformData::getISysFormat(mCameraId);
+    mainConfig.width = producerRes.width;
+    // Update the height according to the field.
+    mainConfig.height = CameraUtils::getInterlaceHeight(mainConfig.field, producerRes.height);
+
+    // configuration with main port
+    producerConfigs[MAIN_PORT] = mainConfig;
+
+    return producerConfigs;
+}
+
+/**
+ * Check if post processor is needed.
+ * The processor is needed when:
+ * 1. At least one of the given streams does not match with the producer's output.
+ * 2. To support specific features such as HW weaving or dewarping.
+ */
+bool CameraDevice::isProcessorNeeded(const stream_config_t *streamList,
+                                     const stream_t &producerConfig)
+{
+    camera_crop_region_t cropRegion;
+    int ret = mParameter.getCropRegion(cropRegion);
+    if ((ret == OK) && (cropRegion.flag == 1)) return true;
+
+    camera_fisheye_dewarping_mode_t dewarping_mode = FISHEYE_DEWARPING_OFF;
+    mParameter.getFisheyeDewarpingMode(dewarping_mode);
+    if (dewarping_mode > FISHEYE_DEWARPING_OFF) {
+        return true;
+    }
+
+    if (producerConfig.field != V4L2_FIELD_ANY) {
+        camera_deinterlace_mode_t mode = DEINTERLACE_OFF;
+        mParameter.getDeinterlaceMode(mode);
+        if (mode == DEINTERLACE_WEAVING) {
+            return true;
+        }
+    }
+
+    if (producerConfig.field != V4L2_FIELD_ALTERNATE) {
+        int streamCounts = streamList->num_streams;
+        for (int streamId = 0; streamId < streamCounts; streamId++) {
+            if (producerConfig.width != streamList->streams[streamId].width ||
+                producerConfig.height != streamList->streams[streamId].height ||
+                producerConfig.format != streamList->streams[streamId].format) {
+                return true;
+            }
+        }
+    }
+
+    camera_mono_downscale_mode_t monoDsMode = MONO_DS_MODE_OFF;
+    mParameter.getMonoDsMode(monoDsMode);
+    if (monoDsMode != MONO_DS_MODE_OFF) {
+        return true;
+    }
+
+    return false;
+}
+
+/**
+ * Return true only if there are both still and video stream configured.
+ */
+bool CameraDevice::isStillDuringVideo(const stream_config_t *streamList)
+{
+    bool containStill = false;
+    bool containVideo = false;
+    for (int streamId = 0; streamId < streamList->num_streams; streamId++) {
+        switch (streamList->streams[streamId].usage) {
+        case CAMERA_STREAM_PREVIEW:
+        case CAMERA_STREAM_VIDEO_CAPTURE:
+            containVideo = true;
+            break;
+        case CAMERA_STREAM_STILL_CAPTURE:
+            containStill = true;
+            break;
+        default:
+            break;
+        }
+    }
+
+    return (containStill && containVideo);
+}
+
+int CameraDevice::createStreams(stream_config_t *streamList)
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+
+    int streamCounts = streamList->num_streams;
+    for (int streamId = 0; streamId < streamCounts; streamId++) {
+        stream_t& streamConf = streamList->streams[streamId];
+        LOG1("@%s, stream_number:%d, stream configure: format:%s (%dx%d)", __func__, streamCounts,
+             CameraUtils::pixelCode2String(streamConf.format), streamConf.width, streamConf.height);
+
+        streamConf.id = streamId;
+        streamConf.max_buffers = PlatformData::getMaxRequestsInflight(mCameraId);
+        CameraStream *stream = new CameraStream(mCameraId, streamId, streamConf);
+        stream->registerListener(EVENT_FRAME_AVAILABLE, mRequestThread);
+        mStreams[streamId] = stream;
+        mStreamNum++;
+
+        LOG2("@%s: automation checkpoint: interlaced: %d", __func__, streamConf.field);
+    }
+
+    return OK;
+}
+
+/**
+ * According resolution to store the streamId in descending order.
+ * Use this order to bind stream to port, and set output Port mapping
+ */
+int CameraDevice::analyzeStream(stream_config_t *streamList)
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+
+    mSortedStreamIds.clear();
+    mStreamIdToPortMap.clear();
+
+    int inputStreamId = -1;
+    int opaqueRawStreamId = -1;
+    for (int i = 0; i < streamList->num_streams; i++) {
+        const stream_t& stream = streamList->streams[i];
+
+        if (stream.streamType == CAMERA_STREAM_INPUT) {
+            CheckError(inputStreamId >= 0, BAD_VALUE, "Don't support two input streams!");
+            inputStreamId = i;
+            continue;
+        }
+
+        if (stream.usage == CAMERA_STREAM_OPAQUE_RAW) {
+            CheckError(opaqueRawStreamId >= 0, BAD_VALUE, "Don't support two RAW streams!");
+            opaqueRawStreamId = i;
+            continue;
+        }
+
+        camera_crop_region_t cropRegion;
+        int ret = mParameter.getCropRegion(cropRegion);
+        if (ret != OK || cropRegion.flag == 0) {
+            bool valid = PlatformData::isSupportedStream(mCameraId, stream);
+            CheckError(!valid, BAD_VALUE, "Stream config is not supported. format:%s (%dx%d)",
+                       CameraUtils::pixelCode2String(stream.format), stream.width, stream.height);
+        }
+
+        bool saved = false;
+        // Store the streamId in descending order.
+        for (size_t j = 0; j < mSortedStreamIds.size(); j++) {
+            const stream_t& tmp = streamList->streams[mSortedStreamIds[j]];
+            if (stream.width * stream.height > tmp.width * tmp.height) {
+                mSortedStreamIds.insert((mSortedStreamIds.begin() + j), i);
+                saved = true;
+                break;
+            }
+        }
+        if (!saved)
+            mSortedStreamIds.push_back(i);
+    }
+
+    // Set opaque RAW stream as last one
+    if (opaqueRawStreamId >= 0) {
+        mSortedStreamIds.push_back(opaqueRawStreamId);
+        // Ignore input raw stream for ZSL case
+        inputStreamId = -1;
+    }
+
+    const Port kPorts[] = {MAIN_PORT, SECOND_PORT, THIRD_PORT, FORTH_PORT};
+    for (size_t i = 0; i < mSortedStreamIds.size(); i++) {
+        mStreamIdToPortMap[mSortedStreamIds[i]] = kPorts[i];
+
+        // Dump the stream info by descending order.
+        const stream_t& stream = streamList->streams[mSortedStreamIds[i]];
+        LOG1("%s  streamId: %d, %dx%d(%s)", __func__, mSortedStreamIds[i],
+                stream.width, stream.height, CameraUtils::format2string(stream.format).c_str());
+    }
+
+    bool checkInput = !PlatformData::isIsysEnabled(mCameraId);
+    if (checkInput) {
+        CheckError(inputStreamId < 0, BAD_VALUE, "Input stream was missing");
+    }
+    // Handle input stream
+    if (inputStreamId >= 0) {
+        CheckError(mSortedStreamIds.empty(), BAD_VALUE, "There is no output stream!");
+        // Check if input stream is supported or not
+        const stream_t& stream = streamList->streams[inputStreamId];
+        camera_resolution_t inputResolution = {stream.width, stream.height};
+        bool valid = PlatformData::isISysSupportedResolution(mCameraId, inputResolution);
+        CheckError(!valid, BAD_VALUE, "Stream config is not supported. format:%s (%dx%d)",
+                   CameraUtils::pixelCode2String(stream.format), stream.width, stream.height);
+        // Push input stream index to the end of vector mSortedStreamIds
+        mSortedStreamIds.push_back(inputStreamId);
+        // Use MAIN PORT for input stream
+        mStreamIdToPortMap[inputStreamId] = MAIN_PORT;
+    }
+
+    return OK;
+}
+
+/**
+ * Bind all streams to their producers and to the correct port.
+ *
+ * Bind the streams to Port in resolution descending order:
+ * Stream with max resolution            --> MAIN_PORT
+ * Stream with intermediate resolution   --> SECOND_PORT
+ * Stream with min resolution            --> THIRD_PORT
+ */
+int CameraDevice::bindStreams(stream_config_t *streamList)
+{
+    for (auto& iter : mStreamIdToPortMap) {
+        mStreams[iter.first]->setPort(iter.second);
+
+        // If no post processors, bind the stream to the producer.
+        if (mProcessors.empty()) {
+            mStreams[iter.first]->setBufferProducer(mProducer);
+        } else {
+            mStreams[iter.first]->setBufferProducer(mProcessors.back());
+        }
+    }
+
+    return OK;
+}
+
+int CameraDevice::start()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s, mCameraId:%d, mState:%d", __func__, mCameraId, mState);
+
+    // Not protected by mDeviceLock because it is required in qbufL()
+    mRequestThread->wait1stRequestDone();
+
+    AutoMutex   m(mDeviceLock);
+    CheckError(mState != DEVICE_BUFFER_READY, BAD_VALUE, "start camera in wrong status %d", mState);
+    CheckError(mStreamNum == 0, BAD_VALUE, "@%s: device doesn't add any stream yet.", __func__);
+
+    int ret = startLocked();
+    if (ret != OK) {
+        LOGE("Camera device starts failed.");
+        stopLocked();  // There is error happened, stop all related units.
+        return INVALID_OPERATION;
+    }
+
+    mState = DEVICE_START;
+    return OK;
+}
+
+int CameraDevice::stop()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s, mCameraId:%d, mState:%d", __func__, mCameraId, mState);
+    AutoMutex   m(mDeviceLock);
+
+    mRequestThread->clearRequests();
+
+    m3AControl->stop();
+
+    if (mState == DEVICE_START)
+        stopLocked();
+
+    mState = DEVICE_STOP;
+
+    return OK;
+}
+
+//No Lock for this fuction as it doesn't update any class member
+int CameraDevice::allocateMemory(camera_buffer_t *ubuffer)
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+    CheckError(mState < DEVICE_CONFIGURE, BAD_VALUE, "@%s: Wrong state id %d", __func__, mState);
+    CheckError(ubuffer->s.id < 0 || ubuffer->s.id >= mStreamNum, BAD_VALUE,
+          "@%s: Wrong stream id %d", __func__, ubuffer->s.id);
+
+    int ret = mStreams[ubuffer->s.id]->allocateMemory(ubuffer);
+    CheckError(ret < 0, ret, "@%s: failed, index: %d", __func__, ubuffer->index);
+
+    return ret;
+}
+
+/**
+ * Delegate it to RequestThread, make RequestThread manage all buffer related actions.
+ */
+int CameraDevice::dqbuf(int streamId, camera_buffer_t **ubuffer, Parameters* settings)
+{
+    CheckError(streamId < 0 || streamId > mStreamNum, BAD_VALUE,
+          "@%s: the given stream(%d) is invalid.", __func__, streamId);
+
+    LOG2("@%s, camera id:%d, stream id:%d", __func__, mCameraId, streamId);
+
+    int ret = mRequestThread->waitFrame(streamId, ubuffer);
+    while (ret == TIMED_OUT)
+        ret = mRequestThread->waitFrame(streamId, ubuffer);
+
+    CheckError(!*ubuffer || ret != OK, BAD_VALUE, "failed to get ubuffer from stream %d", streamId);
+
+    // Update and keep latest result, copy to settings when needed.
+    ret = mParamGenerator->getParameters((*ubuffer)->sequence, &mResultParameter);
+
+    if (settings) {
+        ret = mParamGenerator->getParameters((*ubuffer)->sequence, settings, false);
+    }
+
+    return ret;
+}
+
+int CameraDevice::handleQueueBuffer(int bufferNum, camera_buffer_t **ubuffer, long sequence)
+{
+    LOG2("@%s, mCameraId:%d, sequence = %ld", __func__, mCameraId, sequence);
+    CheckError(mState < DEVICE_CONFIGURE, BAD_VALUE,"@%s: Wrong state id %d", __func__, mState);
+
+    // All streams need to be queued with either a real buffer from user or an empty buffer.
+    for (int streamId = 0; streamId < mStreamNum; streamId++) {
+        bool isBufferQueued = false;
+        CheckError(mStreams[streamId] == nullptr, BAD_VALUE,
+              "@%s: stream %d is nullptr", __func__, streamId);
+
+        // Find if user has queued a buffer for mStreams[streamId].
+        for (int bufferId = 0; bufferId < bufferNum; bufferId++) {
+            camera_buffer_t* buffer = ubuffer[bufferId];
+            int streamIdInBuf = buffer->s.id;
+            CheckError(streamIdInBuf < 0 || streamIdInBuf > mStreamNum, BAD_VALUE,
+                "@%s: Wrong stream id %d", __func__, streamIdInBuf);
+
+            if (streamIdInBuf == streamId) {
+                int ret = mStreams[streamId]->qbuf(buffer, sequence);
+                CheckError(ret < 0, ret, "@%s: queue buffer:%p failed:%d", __func__, buffer, ret);
+                isBufferQueued = true;
+                break;
+            }
+        }
+
+        // If streamId is not found in buffers queued by user, then we need to queue
+        // an empty buffer to keep the BufferQueue run.
+        if (!isBufferQueued) {
+            int ret = mStreams[streamId]->qbuf(nullptr, sequence);
+            CheckError(ret < 0, ret, "@%s: queue empty buffer failed:%d", __func__, ret);
+        }
+    }
+
+    return OK;
+}
+
+int CameraDevice::registerBuffer(camera_buffer_t **ubuffer, int bufferNum)
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+    CheckError(mState < DEVICE_CONFIGURE, BAD_VALUE,"@%s: Wrong state id %d", __func__, mState);
+    if (mProcessors.empty()) return OK;
+
+    for (int bufferId = 0; bufferId < bufferNum; bufferId++) {
+        camera_buffer_t *buffer = ubuffer[bufferId];
+        CheckError(buffer == nullptr, BAD_VALUE, "@%s, the queue ubuffer %d is NULL", __func__, bufferId);
+        int streamIdInBuf = buffer->s.id;
+        CheckError(streamIdInBuf < 0 || streamIdInBuf > mStreamNum, BAD_VALUE,
+                "@%s: Wrong stream id %d", __func__, streamIdInBuf);
+        std::shared_ptr<CameraBuffer> camBuffer =
+            mStreams[streamIdInBuf]->userBufferToCameraBuffer(buffer);
+        for (auto& iter : mStreamIdToPortMap) {
+            // Register buffers to the last processor
+            if (iter.first == streamIdInBuf) {
+                BufferQueue *processor = mProcessors.back();
+                processor->registerUserOutputBufs(iter.second, camBuffer);
+                break;
+            }
+        }
+    }
+
+    return OK;
+}
+
+int CameraDevice::qbuf(camera_buffer_t **ubuffer,
+                       int bufferNum, const Parameters *settings)
+{
+    LOG2("@%s, mCameraId:%d", __func__, mCameraId);
+
+    {
+        AutoMutex   m(mDeviceLock);
+        if (mState == DEVICE_CONFIGURE || mState == DEVICE_STOP) {
+            // Start 3A here then the HAL can run 3A for request
+            int ret = m3AControl->start();
+            CheckError((ret != OK), BAD_VALUE, "Start 3a unit failed with ret:%d.", ret);
+
+            mState = DEVICE_BUFFER_READY;
+        }
+    }
+
+    if (mState != DEVICE_START && PlatformData::isNeedToPreRegisterBuffer(mCameraId)) {
+        registerBuffer(ubuffer, bufferNum);
+    }
+
+    // Make sure request's configure mode is updated by latest result param if no settings
+    if (!settings) {
+        mRequestThread->setConfigureModeByParam(mResultParameter);
+    }
+
+    return mRequestThread->processRequest(bufferNum, ubuffer, settings);
+}
+
+int CameraDevice::getParameters(Parameters& param, long sequence)
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s mCameraId:%d", __func__, mCameraId);
+    AutoMutex   m(mDeviceLock);
+
+    if (sequence >= 0) {
+        // fetch target parameter and results
+        return mParamGenerator->getParameters(sequence, &param, false);
+    }
+
+    param = mParameter;
+
+    for (auto& item : mProcessors) {
+        item->getParameters(param);
+    }
+
+    return OK;
+}
+
+int CameraDevice::setParameters(const Parameters& param)
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s mCameraId:%d", __func__, mCameraId);
+    AutoMutex   m(mDeviceLock);
+    return setParametersL(param);
+}
+
+int CameraDevice::setParametersL(const Parameters& param)
+{
+    // Merge given param into internal unique mParameter
+    mParameter.merge(param);
+
+    int ret = m3AControl->setParameters(param);
+    for (auto& item : mProcessors) {
+        item->setParameters(mParameter);
+    }
+
+    // Set test pattern mode
+    camera_test_pattern_mode_t testPatternMode = TEST_PATTERN_OFF;
+    if (PlatformData::isTestPatternSupported(mCameraId)
+            && param.getTestPatternMode(testPatternMode) == OK) {
+        int32_t sensorTestPattern = PlatformData::getSensorTestPattern(mCameraId, testPatternMode);
+        if (sensorTestPattern >= 0) {
+            ret |= mSensorCtrl->setTestPatternMode(sensorTestPattern);
+        }
+    }
+
+    return ret;
+}
+
+//Private Functions, these functions are called with device lock hold
+
+//Destroy all the streams
+void CameraDevice::deleteStreams()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s mCameraId:%d, streams:%d", __func__, mCameraId, mStreamNum);
+
+    for (int streamId = 0; streamId < mStreamNum; streamId++) {
+        mStreams[streamId]->stop();
+        delete mStreams[streamId];
+        mStreams[streamId] = nullptr;
+    }
+    mStreamNum = 0;
+}
+
+// Internal start without lock hold
+int CameraDevice::startLocked()
+{
+    int ret = OK;
+
+    bindListeners();
+
+    //Start all the streams
+    for(int i = 0; i < mStreamNum; i++) {
+        ret = mStreams[i]->start();
+        CheckError(ret < 0, BAD_VALUE, "Start stream %d failed with ret:%d.", i, ret);
+    }
+
+    for (auto& item : mProcessors) {
+        ret = item->start();
+        CheckError((ret < 0), BAD_VALUE, "Start image processor failed with ret:%d.", ret);
+    }
+
+    //Start the CaptureUnit for streamon
+    ret = mProducer->start();
+    CheckError((ret < 0), BAD_VALUE, "Start capture unit failed with ret:%d.", ret);
+
+    ret = mSofSource->start();
+    CheckError((ret != OK), BAD_VALUE, "Start SOF event source failed with ret:%d.", ret);
+
+    return OK;
+}
+
+// Internal stop without lock hold
+int CameraDevice::stopLocked()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+
+    mSofSource->stop();
+
+    //Stop the CaptureUnit for streamon
+    mProducer->stop();
+
+    for (auto& item : mProcessors) {
+        item->stop();
+    }
+
+    unbindListeners();
+    mParamGenerator->reset();
+
+    return OK;
+}
+
+int CameraDevice::reconfigure(stream_config_t *streamList)
+{
+    AutoMutex   m(mDeviceLock);
+
+    int ret = OK;
+
+    LOG2("%s: switch type: %d, new mode:%d", __func__,
+        PlatformData::getAutoSwitchType(mCameraId), streamList->operation_mode);
+
+    if (PlatformData::getAutoSwitchType(mCameraId) == AUTO_SWITCH_FULL) {
+        // Wait and return all user buffers in all streams firstly
+        for (int streamId = 0; streamId < mStreamNum; streamId++) {
+            mStreams[streamId]->waitToReturnAllUserBufffers();
+        }
+
+        LOG2("%s: all streams stopped", __func__);
+
+        // Stop and clean what needed.
+        m3AControl->stop();
+
+        if (mState == DEVICE_START)
+            stopLocked();
+
+        mState = DEVICE_STOP;
+
+        for (int streamId = 0; streamId < mStreamNum; streamId++) {
+            mStreams[streamId]->stop();
+        }
+
+        mProcessorManager->deleteProcessors();
+
+        m3AControl->deinit();
+
+        mSofSource->deinit();
+
+        mProducer->deinit();
+
+        /* TODO: Currently kernel have issue and need reopen subdevices
+         * when stream off and on. Remove below delete and recreate code
+         * when all kernel issues got fixed.
+         */
+        // Delete related components and v4l2 devices
+        delete mLensCtrl;
+        delete m3AControl;
+        delete mSensorCtrl;
+        delete mSofSource;
+        delete mProducer;
+
+        V4l2DeviceFactory::releaseDeviceFactory(mCameraId);
+
+        // Re-create related components and v4l2 devices
+        mProducer = createBufferProducer();
+        mSofSource = new SofSource(mCameraId);
+        mLensCtrl = new LensHw(mCameraId);
+        mSensorCtrl = SensorHwCtrl::createSensorCtrl(mCameraId);
+        m3AControl = I3AControlFactory::createI3AControl(mCameraId, mSensorCtrl, mLensCtrl);
+
+        // Init and config with new mode
+        int ret = mProducer->init();
+        CheckError(ret < 0, ret, "%s: Init capture unit failed", __func__);
+
+        ret = mSofSource->init();
+        CheckError(ret != OK, ret, "@%s: init sync manager failed", __func__);
+
+        initDefaultParameters();
+
+        ret = m3AControl->init();
+        CheckError((ret != OK), ret, "%s: Init 3A Unit falied", __func__);
+
+        ret = mLensCtrl->init();
+        CheckError((ret != OK), ret, "%s: Init Lens falied", __func__);
+
+        mState = DEVICE_INIT;
+
+        // Auto switch do not recreate streams.
+        configureL(streamList, false);
+
+        ret = m3AControl->setParameters(mParameter);
+        for (auto& item : mProcessors) {
+            item->setParameters(mParameter);
+        }
+        CheckError((ret != OK), ret, "%s: set parameters falied", __func__);
+
+        ret = m3AControl->start();
+        CheckError((ret != OK), BAD_VALUE, "Start 3a unit failed with ret:%d.", ret);
+
+        mState = DEVICE_BUFFER_READY;
+
+        ret = startLocked();
+        if (ret != OK) {
+            LOGE("Camera device starts failed.");
+            stopLocked();  // There is error happened, stop all related units.
+            return INVALID_OPERATION;
+        }
+
+        mState = DEVICE_START;
+
+        LOG2("%s: reconfigure CameraDevice done", __func__);
+    } else {
+
+        /* TODO: scene mode based psys-only auto switch here will be used to
+         * replace current auto-switch mechanism in AiqSetting:updateTuningMode,
+         * which is for non-DOL sensor auto-switch. The switch stabilization
+         * counting in AiqSetting:updateTuningMode will also be replaced by the
+         * same mechanism in RequestThread.
+         */
+        LOG2("%s: reconfigure CameraDevice to new mode %d with psys-only switch",
+             __func__, streamList->operation_mode);
+    }
+
+    return ret;
+}
+
+void CameraDevice::handleEvent(EventData eventData)
+{
+    LOG2("%s, event type:%d", __func__, eventData.type);
+
+    switch (eventData.type) {
+    case EVENT_PROCESS_REQUEST: {
+        const EventRequestData& request = eventData.data.request;
+        if (request.param) {
+            for (auto& item : mProcessors) {
+                 item->setParameters(*request.param);
+            }
+
+            // Set test pattern mode
+            camera_test_pattern_mode_t testPatternMode = TEST_PATTERN_OFF;
+            if (PlatformData::isTestPatternSupported(mCameraId)
+                    && request.param->getTestPatternMode(testPatternMode) == OK) {
+                int32_t sensorTestPattern =
+                            PlatformData::getSensorTestPattern(mCameraId, testPatternMode);
+                if (sensorTestPattern >= 0) {
+                    if (mSensorCtrl->setTestPatternMode(sensorTestPattern) < 0) {
+                        LOGE("%s, set testPatternMode failed", __func__);
+                    }
+                }
+            }
+        }
+
+        handleQueueBuffer(request.bufferNum, request.buffer, request.settingSeq);
+        break;
+    }
+
+    case EVENT_DEVICE_RECONFIGURE: {
+        const EventConfigData& config = eventData.data.config;
+        reconfigure(config.streamList);
+        break;
+    }
+
+    case EVENT_PSYS_REQUEST_BUF_READY: {
+        if (mCallback) {
+            camera_msg_data_t data;
+            CLEAR(data);
+            data.type = CAMERA_ISP_BUF_READY;
+            int32_t userRequestId = 0;
+            int ret = mParamGenerator->getUserRequestId(eventData.data.requestReady.sequence,
+                                                        userRequestId);
+            CheckError(ret != OK, VOID_VALUE, "failed to find request id,  seq %ld",
+                       eventData.data.requestReady.sequence);
+            data.data.buffer_ready.sequence = eventData.data.requestReady.sequence;
+            data.data.buffer_ready.timestamp = eventData.data.requestReady.timestamp;
+            data.data.buffer_ready.frameNumber = static_cast<uint32_t>(userRequestId);
+            mCallback->notify(mCallback, data);
+            PlatformData::updateMakernoteTimeStamp(mCameraId, eventData.data.requestReady.sequence,
+                                                   data.data.buffer_ready.timestamp);
+        }
+        break;
+    }
+
+    default:
+        LOGE("Not supported event type:%d", eventData.type);
+        break;
+    }
+}
+
+int CameraDevice::initDefaultParameters()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s mCameraId:%d", __func__, mCameraId);
+
+    camera_info_t info;
+    CLEAR(info);
+    PlatformData::getCameraInfo(mCameraId, info);
+
+    // Init mParameter to camera's capability first and then add some others default settings
+    mParameter = *info.capability;
+
+    // TODO: Figure out a better place to set default parameters since they may be platform specified.
+    camera_range_t fps = {10, 60};
+    mParameter.setFpsRange(fps);
+    mParameter.setFrameRate(30);
+
+    camera_image_enhancement_t enhancement;
+    CLEAR(enhancement); // All use 0 as default
+    mParameter.setImageEnhancement(enhancement);
+
+    mParameter.setWeightGridMode(WEIGHT_GRID_AUTO);
+
+    mParameter.setWdrLevel(100);
+
+    mParameter.setFlipMode(FLIP_MODE_NONE);
+
+    mParameter.setRun3ACadence(1);
+
+    mParameter.setYuvColorRangeMode(PlatformData::getYuvColorRangeMode(mCameraId));
+
+    mParameter.setFocusDistance(0.0f);
+    mParameter.setTonemapMode(TONEMAP_MODE_FAST);
+
+    return OK;
+}
+
+} //namespace icamera
+
diff --git a/camera/hal/intel/ipu6/src/core/CameraDevice.h b/camera/hal/intel/ipu6/src/core/CameraDevice.h
new file mode 100644
index 000000000000..43d28f31a5ff
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/CameraDevice.h
@@ -0,0 +1,284 @@
+/*
+ * Copyright (C) 2016-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "CameraStream.h"
+#include "RequestThread.h"
+#include "StreamSource.h"
+#include "AiqUnit.h"
+#include "Parameters.h"
+#include "ParameterGenerator.h"
+#include "SensorHwCtrl.h"
+#include "SofSource.h"
+#include "LensHw.h"
+
+#include "ProcessorManager.h"
+
+#include "gc/IGraphConfigManager.h"
+
+namespace icamera {
+
+class RequestThread;
+
+/**
+  * CameraDevice : Create elements to construct the streaming pipeline
+  * Each element must be a producer or a consumer or Both.
+  *
+  * These are the typical pipelines:
+  *
+  * For a single SOC YUV capture in by pass mode
+  * StreamSource -> CameraStream
+  *
+  * For a single NV12 capture of TPG/Sensor using SwImageProcess
+  * StreamSource -> SwImageProcessor -> CameraStream
+  *
+  * For a single NV12 capture of TPG/Sensor using PSYS processor
+  * StreamSource -> PsysProcessor -> CameraStream
+  *
+  * For a dual streams NV12 video capture of TPG/Sensor using PSYS processor
+  * StreamSource -> PsysProcessor | -> CameraStream (For video recording)
+  *                              | -> CameraStream (for Preview)
+  *
+  * For a SDV(Snapshot during video) capture of TPG/Sensor uinsg PSYS processor
+  * StreamSource | -> PsysProcessor | -> CameraStream (For video recording)
+  *             |                  | -> CameraStream (for Preview)
+  *             | -> PsysProcessor | -> CameraStream (For still YUV)
+  *
+  * The buffer notification between the Class is based on Interface defined
+  * in BufferQueue. The upstream element use "onFrameAvailable" message to notfiy
+  * downstream elements that the buffers are ready for further processing
+  *
+  * Following singleton instances are created and maintained by CameraDevice
+  * 1. IGraphConfigManager
+  * 2. AiqResultStorage
+  */
+class CameraDevice : public EventListener {
+
+public:
+    CameraDevice(int cameraId);
+    ~CameraDevice();
+
+    /**
+     * \brief Camera device class init
+     *
+     * 1.Related classes init: StreamSource, SofSource, 3AControl, lensCtrl
+     * 2.Register listener if enable AIQ
+     * 3.Set the defualt parameters
+     *
+     * \return OK if succeed, other value indicates failed
+     */
+    int init();
+
+    /**
+     * \brief Camera device class deinit
+     *
+     * 1.Change the state
+     * 2.Destory the listeners
+     * 3.Delete the streams
+     * 4.Deinit the Related classes.
+     */
+    void deinit();
+
+    /**
+     * \brief Camera device start
+     *
+     * 1. Start all streams
+     * 2. Related classes start
+     *
+     * \return OK if succeed, other value indicates failed
+     */
+    int start();
+
+    /**
+     * \brief Camera device stop
+     *
+     * 1. Stop all related class module.
+     * 2. Change the state
+     */
+    int stop();
+
+    /**
+     * \brief Allocate momery according to user buffer
+     *
+     * 1. Convert user buffer to CameraBuffer and push it into UserBufferQueue
+     * 2. Calling CameraStream class to allocateMemory.
+     *
+     * \return OK if succeed, other value indicates failed
+     */
+    int allocateMemory(camera_buffer_t *ubuffer);
+
+    /**
+     * \brief dequeue buffer from cameraStream.
+     *
+     * 1. Dequeue one CameraBuffer from CameraStream
+     * 2. Fill the user buffer base on CameraBuffer
+     * 3. Fill the settings was used for this buffer if settings is not nullptr
+     *
+     * \return 0 if succeed, other value indicates failed
+     */
+    int dqbuf(int streamId, camera_buffer_t **ubuffer, Parameters* settings = nullptr);
+
+    /**
+     * \brief Queue one buffer to CameraStream
+     *
+     * 1. Get the CameraBuffer base on the ubuffer
+     * 2. Calling CameraStream to queue one CameraBuffer
+     *
+     * \return OK if succeed and BAD_VALUE if failed
+     */
+    int qbuf(camera_buffer_t **ubuffer, int bufferNum = 1, const Parameters* settings = nullptr);
+
+    /**
+     * \brief Configure the device sensor input
+     *
+     *
+     * \param inputConfig: the Output format/resolution of the isys
+     *
+     * \return OK if succeed and BAD_VALUE if failed
+     */
+    int configureInput(const stream_t *inputConfig);
+
+    /**
+     * \brief Configure the streams, devices and post processor.
+     *
+     * Configure the streams according to the streamList info
+     * Extra processor is needed if the format isn't supported in Psys output
+     *
+     * \param streamList: all the streams info
+     *
+     * \return OK if succeed and BAD_VALUE if failed
+     */
+    int configure(stream_config_t *streamList);
+
+    /**
+     * \brief Set the parameters
+     *
+     * Merge the param to internal parameters and set them to 3A
+     * and processor.
+     *
+     * \param param: the parameters need to set
+     *
+     * \return OK if succeed, other value indicates failed
+     */
+    int setParameters(const Parameters& param);
+
+    /**
+     * \brief Get the parameters
+     *
+     * Get the internal parameters list
+     *
+     * \return OK if succeed, other value indicates failed
+     */
+    int getParameters(Parameters& param, long sequence);
+
+    void handleEvent(EventData eventData);
+
+    void callbackRegister(const camera_callback_ops_t* callback);
+private:
+    DISALLOW_COPY_AND_ASSIGN(CameraDevice);
+
+    /**
+     * \brief Reconfigure the streams, devices and post processor.
+     *
+     * \param streamList: all the streams info
+     *
+     * \return OK if succeed and BAD_VALUE if failed
+     */
+    int reconfigure(stream_config_t *streamList);
+
+    int startLocked();
+    int stopLocked();
+    int initDefaultParameters();
+    std::shared_ptr<CameraBuffer> userBufferToCameraBuffer(Port port,
+                                                                camera_buffer_t *ubuffer);
+
+    StreamSource* createBufferProducer();
+    std::map<Port, stream_t> selectProducerConfig(const stream_config_t *streamList, int mcId);
+    bool isProcessorNeeded(const stream_config_t *streamList, const stream_t &producerConfig);
+    bool isStillDuringVideo(const stream_config_t *streamList);
+    int analyzeStream(stream_config_t *streamList);
+    int createStreams(stream_config_t *streamList);
+    int bindStreams(stream_config_t *streamList);
+    void deleteStreams();
+
+    /**
+     * Bind all event listeners with their source.
+     */
+    void bindListeners();
+
+    /**
+     * Unbind all event listeners from their source.
+     */
+    void unbindListeners();
+
+    long fetchCaptureSettings(const Parameters * params);
+
+    // The second phase of qbuf(), done in RequestThread
+    int handleQueueBuffer(int bufferNum, camera_buffer_t **ubuffer, long sequence);
+
+    // No lock protect for internal usage, should be protected by mDeviceLock from outside
+    int configureL(stream_config_t *streamList, bool clean = true);
+    int setParametersL(const Parameters& param);
+
+    int registerBuffer(camera_buffer_t **ubuffer, int bufferNum);
+
+private:
+    enum {
+        DEVICE_UNINIT = 0,
+        DEVICE_INIT,
+        DEVICE_CONFIGURE, //means stream configured
+        DEVICE_START,
+        DEVICE_STOP,
+        DEVICE_BUFFER_READY, //At least one buffer is queued to ISP
+    } mState;
+
+    // Guard for CameraDevice public API
+    Mutex mDeviceLock;
+
+    static const nsecs_t kWaitDuration = 5000000000; //5000ms
+
+    // Pipeline elements
+    CameraStream* mStreams[MAX_STREAM_NUMBER];
+    std::map<int, Port> mStreamIdToPortMap;
+    std::vector<int> mSortedStreamIds; // Used to save sorted stream ids with descending order.
+    StreamSource *mProducer;
+
+    ProcessorManager* mProcessorManager;
+    std::vector<BufferQueue*> mProcessors;
+
+    ParameterGenerator *mParamGenerator;
+
+    LensHw *mLensCtrl;
+    SensorHwCtrl *mSensorCtrl;
+    SofSource* mSofSource;
+    AiqUnitBase *m3AControl;
+
+    //Internal used variable
+    int     mCameraId;
+    int     mStreamNum;
+    Parameters mParameter;
+    //The latest result parameters
+    Parameters mResultParameter;
+    bool mPerframeControlSupport;
+
+    RequestThread* mRequestThread;
+    IGraphConfigManager *mGCM;
+    stream_t mInputConfig;
+    camera_callback_ops_t *mCallback;
+};
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/CameraEvent.cpp b/camera/hal/intel/ipu6/src/core/CameraEvent.cpp
new file mode 100644
index 000000000000..18317091e1e7
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/CameraEvent.cpp
@@ -0,0 +1,84 @@
+/*
+ * Copyright (C) 2015-2017 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "CameraEvent"
+
+#include "iutils/CameraLog.h"
+
+#include "CameraEvent.h"
+
+namespace icamera {
+
+EventSource::EventSource()
+{
+    LOG1("@%s EventSource created", __func__);
+}
+
+EventSource::~EventSource()
+{
+    LOG1("@%s EventSource destructed", __func__);
+}
+
+void EventSource::registerListener(EventType eventType, EventListener* eventListener)
+{
+    LOG1("@%s eventType: %d, listener: %p", __func__, eventType, eventListener);
+
+    CheckError(eventListener == nullptr, VOID_VALUE,
+          "%s: event listener is nullptr, skip registration.", __func__);
+
+    AutoMutex l(mListenersLock);
+
+    std::set<EventListener*> listenersOfType;
+    if (mListeners.find(eventType) != mListeners.end()) {
+        listenersOfType = mListeners[eventType];
+    }
+
+    listenersOfType.insert(eventListener);
+    mListeners[eventType] = listenersOfType;
+}
+
+void EventSource::removeListener(EventType eventType, EventListener* eventListener)
+{
+    LOG1("@%s eventType: %d, listener: %p", __func__, eventType, eventListener);
+    AutoMutex l(mListenersLock);
+
+    if (mListeners.find(eventType) == mListeners.end()) {
+        LOG1("%s: no listener found for event type %d", __func__, eventType);
+        return;
+    }
+
+    std::set<EventListener*> listenersOfType = mListeners[eventType];
+    listenersOfType.erase(eventListener);
+    mListeners[eventType] = listenersOfType;
+}
+
+void EventSource::notifyListeners(EventData eventData)
+{
+    LOG2("@%s eventType: %d", __func__, eventData.type);
+    AutoMutex l(mListenersLock);
+
+    if (mListeners.find(eventData.type) == mListeners.end()){
+        LOG2("%s: no listener found for event type %d", __func__, eventData.type);
+        return;
+    }
+
+    for (auto listener : mListeners[eventData.type]) {
+        LOG2("%s: send event data to listener %p for event type %d", __func__, listener, eventData.type);
+        listener->handleEvent(eventData);
+    }
+}
+
+}
diff --git a/camera/hal/intel/ipu6/src/core/CameraEvent.h b/camera/hal/intel/ipu6/src/core/CameraEvent.h
new file mode 100644
index 000000000000..4a15167cd799
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/CameraEvent.h
@@ -0,0 +1,50 @@
+/*
+ * Copyright (C) 2015-2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <map>
+#include <set>
+
+#include "iutils/Thread.h"
+#include "CameraEventType.h"
+
+namespace icamera {
+
+class EventListener
+{
+public:
+    EventListener() {};
+    virtual ~EventListener() {};
+    virtual void handleEvent(EventData eventData) {};
+};
+
+class EventSource
+{
+private:
+    std::map<EventType, std::set<EventListener*>> mListeners;
+
+    // Guard for EventSource public API to protect mListeners.
+    Mutex mListenersLock;
+public:
+    EventSource();
+    virtual ~EventSource();
+    virtual void registerListener(EventType eventType, EventListener* eventListener);
+    virtual void removeListener(EventType eventType, EventListener* eventListener);
+    virtual void notifyListeners(EventData eventData);
+};
+
+}
diff --git a/camera/hal/intel/ipu6/src/core/CameraEventType.h b/camera/hal/intel/ipu6/src/core/CameraEventType.h
new file mode 100644
index 000000000000..bb6a017a5cda
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/CameraEventType.h
@@ -0,0 +1,111 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+
+#include "iutils/Utils.h"
+#include "Parameters.h"
+
+namespace icamera {
+
+class CameraBuffer;
+
+enum EventType
+{
+    EVENT_ISYS_SOF = 0,
+    EVENT_PSYS_STATS_BUF_READY,
+    EVENT_PSYS_STATS_SIS_BUF_READY,
+    EVENT_ISYS_FRAME,
+    EVENT_PSYS_FRAME,
+    EVENT_PROCESS_REQUEST,
+    EVENT_DEVICE_RECONFIGURE,
+    EVENT_FRAME_AVAILABLE,
+    EVENT_PSYS_REQUEST_BUF_READY,
+};
+
+struct EventDataStatsReady
+{
+    timeval timestamp;
+    long sequence;
+};
+
+struct EventDataSync
+{
+    timeval timestamp;
+    long sequence;
+};
+
+struct EventDataFrame
+{
+    timeval timestamp;
+    long sequence;
+};
+
+struct EventDataMeta
+{
+    timeval timestamp;
+    long sequence;
+};
+
+struct EventRequestData
+{
+    int bufferNum;
+    camera_buffer_t** buffer;
+    Parameters* param;
+
+    long settingSeq;
+};
+
+struct EventConfigData
+{
+    stream_config_t *streamList;
+};
+
+struct EventFrameAvailable
+{
+    int streamId;
+};
+
+struct EventRequestBufferReady
+{
+    int64_t timestamp;
+    long sequence;
+};
+
+struct EventData
+{
+    EventData() : type(EVENT_ISYS_SOF) {
+        CLEAR(data);
+    }
+
+    EventType type;
+    std::shared_ptr<CameraBuffer> buffer;
+    union
+    {
+        EventDataStatsReady statsReady;
+        EventDataSync sync;
+        EventDataFrame frame;
+        EventDataMeta meta;
+        EventRequestData request;
+        EventConfigData config;
+        EventFrameAvailable frameDone;
+        EventRequestBufferReady requestReady;
+    } data;
+};
+
+}
diff --git a/camera/hal/intel/ipu6/src/core/CameraStream.cpp b/camera/hal/intel/ipu6/src/core/CameraStream.cpp
new file mode 100644
index 000000000000..70da89fdd0dd
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/CameraStream.cpp
@@ -0,0 +1,239 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "CameraStream"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+#include "CameraStream.h"
+#include "PlatformData.h"
+
+using std::shared_ptr;
+
+namespace icamera {
+
+CameraStream::CameraStream(int cameraId, int streamId, const stream_t& stream)
+      : mCameraId(cameraId),
+        mStreamId(streamId),
+        mPort(MAIN_PORT),
+        mBufferProducer(nullptr),
+        mNumHoldingUserBuffers(0),
+        mIsWaitingBufferReturn(false)
+{
+    LOG1("@%s: mCameraId:%d, width:%d, height:%d, format:%s", __func__,
+          mCameraId, stream.width, CameraUtils::getInterlaceHeight(stream.field, stream.height),
+          CameraUtils::pixelCode2String(stream.format));
+    LOG2("@%s: automation checkpoint: WHF: %d,%d,%s", __func__,
+          stream.width, CameraUtils::getInterlaceHeight(stream.field, stream.height),
+          CameraUtils::pixelCode2String(stream.format));
+}
+
+CameraStream::~CameraStream()
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+}
+
+int CameraStream::start()
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+
+    return OK;
+}
+
+int CameraStream::stop()
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+
+    mIsWaitingBufferReturn = false;
+    mNumHoldingUserBuffers = 0;
+
+    if (mBufferProducer != nullptr)
+        mBufferProducer->removeFrameAvailableListener(this);
+
+    AutoMutex poolLock(mBufferPoolLock);
+    mUserBuffersPool.clear();
+
+    return OK;
+}
+
+/*
+ * Allocate memory to the stream processor which should be
+ * set by the CameraDevice
+ */
+int CameraStream::allocateMemory(camera_buffer_t *ubuffer)
+{
+    LOG1("@%s, mCameraId:%d, ubuffer %p", __func__, mCameraId, ubuffer);
+
+    int ret = BAD_VALUE;
+    shared_ptr<CameraBuffer> camBuffer = userBufferToCameraBuffer(ubuffer);
+    CheckError(!camBuffer, ret, "@%s: fail to alloc CameraBuffer", __func__);
+
+    // mBufferProducer will not change after start
+    if (mBufferProducer)
+        ret = mBufferProducer->allocateMemory(mPort, camBuffer);
+
+    return ret;
+}
+
+shared_ptr<CameraBuffer> CameraStream::userBufferToCameraBuffer(camera_buffer_t *ubuffer)
+{
+    if (ubuffer == nullptr) return nullptr;
+
+    shared_ptr<CameraBuffer> camBuffer = nullptr;
+
+    AutoMutex l(mBufferPoolLock);
+    for (auto buffer = mUserBuffersPool.begin(); buffer != mUserBuffersPool.end(); buffer++) {
+        /* check if the buffer is already in the pool, when the userBuffer is same as ubuffer
+         * will continue to check the buffer, because the data addr in ubuffer may change */
+        if ((*buffer)->getUserBuffer() == ubuffer) {
+            /* when memType matches, the dmafd or the addr should match */
+            if (((*buffer)->getMemory() == static_cast<uint32_t>(ubuffer->s.memType)) &&
+                ((ubuffer->addr != nullptr && (*buffer)->getUserBuffer()->addr == ubuffer->addr) ||
+                (ubuffer->dmafd >= 0 && (*buffer)->getUserBuffer()->dmafd == ubuffer->dmafd))) {
+                camBuffer = *buffer;
+            } else {
+                mUserBuffersPool.erase(buffer);
+            }
+            break;
+        }
+    }
+
+    if (!camBuffer) { // Not found in the pool, so create a new CameraBuffer for it.
+        ubuffer->index = mUserBuffersPool.size();
+        camBuffer = std::make_shared<CameraBuffer>(mCameraId, BUFFER_USAGE_GENERAL,
+                ubuffer->s.memType, ubuffer->s.size, ubuffer->index, ubuffer->s.format);
+        CheckError(!camBuffer, nullptr, "@%s: fail to alloc CameraBuffer", __func__);
+        mUserBuffersPool.push_back(camBuffer);
+    }
+    camBuffer->setUserBufferInfo(ubuffer);
+
+    // Update the v4l2 flags
+    camBuffer->updateFlags();
+
+    return camBuffer;
+}
+
+void CameraStream::waitToReturnAllUserBufffers()
+{
+    LOG1("%s: wait for all user buffers to be returned to user", __func__);
+
+    ConditionLock lock(mBufferPoolLock);
+
+    if (mNumHoldingUserBuffers > 0){
+        // mIsWaitingBufferReturn flag is used to prevent situation that signal goes before wait
+        mIsWaitingBufferReturn = true;
+        int ret = mAllBuffersReturnedSignal.waitRelative(lock,
+                                                         kWaitDuration * SLOWLY_MULTIPLIER);
+
+        if (ret == TIMED_OUT) {
+            LOGW("@%s, mCameraId:%d, time out happens when waiting return user buffers",
+                 __func__, mCameraId);
+            return;
+        }
+        mIsWaitingBufferReturn = false;
+    }
+
+    LOG1("%s: all buffers have been returned to user", __func__);
+
+    return;
+}
+
+// Q buffers to the stream processor which should be set by the CameraDevice
+int CameraStream::qbuf(camera_buffer_t *ubuffer, long sequence)
+{
+    shared_ptr<CameraBuffer> camBuffer = userBufferToCameraBuffer(ubuffer);
+    if (camBuffer) {
+        camBuffer->setSettingSequence(sequence);
+        LOG2("@%s, mCameraId:%d, mStreamId:%d, CameraBuffer:%p for port:%d, ubuffer:%p, addr:%p",
+             __func__, mCameraId, mStreamId, camBuffer.get(), mPort, ubuffer, ubuffer->addr);
+    }
+
+    int ret = BAD_VALUE;
+    // mBufferProducer will not change after start, no lock
+    if (mBufferProducer != nullptr) {
+        ret = mBufferProducer->qbuf(mPort, camBuffer);
+        if (ret == OK) {
+            mNumHoldingUserBuffers++;
+        }
+    }
+    return ret;
+}
+
+//This function is called in stop status, no lock
+void CameraStream::setBufferProducer(BufferProducer *producer)
+{
+    LOG1("@%s, mCameraId:%d, producer %p", __func__, mCameraId, producer);
+
+    mBufferProducer = producer;
+
+    if (producer != nullptr)
+        producer->addFrameAvailableListener(this);
+}
+
+int CameraStream::onFrameAvailable(Port port, const shared_ptr<CameraBuffer> &camBuffer)
+{
+    // Ignore if the buffer is not for this stream.
+    if (mPort != port) return OK;
+    if (camBuffer->getStreamId() != mStreamId) return OK;
+
+    LOG2("@%s: mCameraId:%d, mStreamId:%d, CameraBuffer:%p for port:%d",
+         __func__, mCameraId, mStreamId, camBuffer.get(), port);
+
+    // Update the user buffer info before return back
+    camBuffer->updateUserBuffer();
+
+    EventFrameAvailable frameData;
+    frameData.streamId = mStreamId;
+    EventData eventData;
+    eventData.type = EVENT_FRAME_AVAILABLE;
+    eventData.buffer = camBuffer;
+    eventData.data.frameDone = frameData;
+    notifyListeners(eventData);
+
+    camera_buffer_t* ubuffer = camBuffer->getUserBuffer();
+    LOG2("ubuffer:%p, addr:%p, timestamp:%lu, sequence:%ld",
+         ubuffer, ubuffer->addr, ubuffer->timestamp, ubuffer->sequence);
+
+    LOGVCSYNC("[onFrameDone], CPU-timestamp:%lu, sequence:%ld, vc:%d, kernel-timestamp:%luus, endl",
+        CameraUtils::systemTime(),
+        ubuffer->sequence,
+        camBuffer->getVirtualChannel(),
+        ubuffer->timestamp);
+
+    PERF_CAMERA_ATRACE_PARAM3("sequence", camBuffer->getSequence(),
+                              "csi2_port", camBuffer->getCsi2Port(),
+                              "virtual_channel", camBuffer->getVirtualChannel());
+
+    AutoMutex l(mBufferPoolLock);
+
+    if (mNumHoldingUserBuffers > 0) {
+        mNumHoldingUserBuffers--;
+    }
+
+    LOG2("mNumHoldingUserBuffers has already been counted down to %d", mNumHoldingUserBuffers);
+
+    // mIsWaitingBufferReturn is used to prevent signal before wait
+    if (mIsWaitingBufferReturn && mNumHoldingUserBuffers == 0) {
+        LOG2("%s: all user buffer returned, trigger signal", __func__);
+        mAllBuffersReturnedSignal.signal();
+    }
+
+    return OK;
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/CameraStream.h b/camera/hal/intel/ipu6/src/core/CameraStream.h
new file mode 100644
index 000000000000..5ac9472d4254
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/CameraStream.h
@@ -0,0 +1,118 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "Parameters.h"
+#include "CameraBuffer.h"
+#include "BufferQueue.h"
+
+#include "iutils/Thread.h"
+
+namespace icamera {
+
+/**
+  * CameraStream: The HAL represent of the application stream.
+  * CameraStream implement the BufferConsumer interface.
+  *
+  * CameraStream provide the buffer interface to application.
+  * It gets buffers from producers and returns to the app
+  *
+  * Application used the DQ buffer to get the buffers from Camera
+  * and Q buffer to return the buffers to camera.
+  */
+
+class CameraStream: public BufferConsumer, public EventSource {
+public:
+    CameraStream(int cameraId, int streamId, const stream_t& stream);
+    virtual ~CameraStream();
+
+    /**
+     * \brief Set which port this stream is linked to.
+     */
+    void setPort(Port port) { mPort = port; }
+
+    /**
+     * \brief get the port which the stream is linked to.
+     */
+    Port getPort() { return mPort; }
+
+    /**
+     * \brief Set the StreamActive state
+     */
+    int start();
+
+    /**
+     * \brief Clear streamActive state and clear up
+     * the buffer queue
+     */
+    int stop();
+
+    /**
+     * \brief Push one CameraBuffer to bufferProducer
+     */
+    int qbuf(camera_buffer_t *ubuffer, long sequence);
+
+    /**
+     * \brief Calling mBufferProducer to allocate memory
+     *
+     * \return OK if succeed and BAD_VALUE if failed
+     */
+    int allocateMemory(camera_buffer_t *buffer);
+
+    std::shared_ptr<CameraBuffer> userBufferToCameraBuffer(camera_buffer_t *ubuffer);
+
+    /**
+     * \brief Wait all user buffers to be returned to user
+     */
+    void waitToReturnAllUserBufffers();
+
+    /**
+     * \brief Register the mBufferProducer
+     */
+    virtual void setBufferProducer(BufferProducer *producer);
+
+    /**
+     * \brief The notify when polled or processed one frame buffer
+     */
+    virtual int onFrameAvailable(Port port, const std::shared_ptr<CameraBuffer> &camBuffer);
+
+private:
+    static const nsecs_t kWaitDuration = 10000000000; //10000ms
+
+    int mCameraId;
+    int mStreamId;
+    Port mPort;
+
+    BufferProducer   *mBufferProducer;
+
+    CameraBufVector    mUserBuffersPool;
+
+    // Guard for member mUserBuffersPool, used in the qbuf which is critical for FPS
+    // Prevent qbuf and dqbuf to share the same lock
+    Mutex mBufferPoolLock;
+
+    // Siginal for the situation that all buffers returned to user
+    Condition mAllBuffersReturnedSignal;
+
+    // How many user buffers are currently processing underhood.
+    int mNumHoldingUserBuffers;
+
+    // Flag to indicate that currently waiting for all user buffers to be returned
+    bool mIsWaitingBufferReturn;
+};
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/CaptureUnit.cpp b/camera/hal/intel/ipu6/src/core/CaptureUnit.cpp
new file mode 100644
index 000000000000..50301e6e0c05
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/CaptureUnit.cpp
@@ -0,0 +1,477 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "CaptureUnit"
+
+#include <poll.h>
+
+#include "iutils/CameraLog.h"
+#include "iutils/CameraDump.h"
+#include "iutils/Utils.h"
+
+#include "PlatformData.h"
+#include "MediaControl.h"
+#include "CaptureUnit.h"
+
+using std::vector;
+using std::map;
+
+namespace icamera {
+
+CaptureUnit::CaptureUnit(int cameraId, int memType) :
+    StreamSource(memType),
+    mCameraId(cameraId),
+    mDevice(nullptr),
+    mMaxBufferNum(PlatformData::getMaxRawDataNum(cameraId)),
+    mState(CAPTURE_UNINIT),
+    mExitPending(false)
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("%s, mCameraId:%d", __func__, mCameraId);
+
+    mPollThread = new PollThread(this);
+
+    mMaxBuffersInDevice = PlatformData::getExposureLag(mCameraId) + 1;
+    if (mMaxBuffersInDevice < 2) {
+        mMaxBuffersInDevice = 2;
+    }
+}
+
+CaptureUnit::~CaptureUnit()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("%s, mCameraId:%d", __func__, mCameraId);
+
+    delete mPollThread;
+}
+
+int CaptureUnit::init()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("%s, mCameraId:%d", __func__, mCameraId);
+
+    mState = CAPTURE_INIT;
+
+    return OK;
+}
+
+void CaptureUnit::deinit()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("%s, mCameraId:%d", __func__, mCameraId);
+
+    if (mState == CAPTURE_UNINIT) {
+        LOG1("%s: deinit without init", __func__);
+        return;
+    }
+
+    destroyDevices();
+    mPollThread->join();
+
+    mState = CAPTURE_UNINIT;
+}
+
+int CaptureUnit::createDevices()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("%s, mCameraId:%d", __func__, mCameraId);
+
+    destroyDevices();
+
+    // Default INVALID_PORT means the device isn't associated with any outside consumers.
+    const Port kDefaultPort = INVALID_PORT;
+    Port portOfMainDevice = findDefaultPort(mOutputFrameInfo);
+    // Use the config for main port as the default one.
+    const stream_t& kDefaultStream = mOutputFrameInfo.at(portOfMainDevice);
+
+    // Use VIDEO_GENERIC by default.
+    VideoNodeType nodeType = VIDEO_GENERIC;
+
+    mDevice = new MainDevice(mCameraId, nodeType, this);
+
+    // Open and configure the device. The stream and port that are used by the device is
+    // decided by whether consumer has provided such info, use the default one if not.
+    int ret = mDevice->openDevice();
+    CheckError(ret != OK, ret, "Open device(%s) failed:%d", mDevice->getName(), ret);
+
+    bool hasPort = mOutputFrameInfo.find(portOfMainDevice) != mOutputFrameInfo.end();
+    const stream_t& stream = hasPort ? mOutputFrameInfo.at(portOfMainDevice) : kDefaultStream;
+
+    ret = mDevice->configure(hasPort ? portOfMainDevice : kDefaultPort, stream, mMaxBufferNum);
+    CheckError(ret != OK, ret, "Configure device(%s) failed:%d", mDevice->getName(), ret);
+
+    return OK;
+}
+
+void CaptureUnit::destroyDevices()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("%s, mCameraId:%d", __func__, mCameraId);
+
+    if (mDevice) {
+        mDevice->closeDevice();
+        delete mDevice;
+        mDevice = nullptr;
+    }
+
+}
+
+/**
+ * Find the device that can handle the given port.
+ */
+DeviceBase* CaptureUnit::findDeviceByPort(Port port)
+{
+    if (mDevice && mDevice->getPort() == port) {
+        return mDevice;
+    }
+
+    return nullptr;
+}
+
+int CaptureUnit::streamOn()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("%s, mCameraId:%d", __func__, mCameraId);
+
+    if (mDevice) {
+        int ret = mDevice->streamOn();
+        CheckError(ret < 0, INVALID_OPERATION, "Device:%s stream on failed.", mDevice->getName());
+    }
+
+    return OK;
+}
+
+int CaptureUnit::start()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("%s, mCameraId:%d", __func__, mCameraId);
+
+    AutoMutex l(mLock);
+    CheckWarning(mState == CAPTURE_START, OK, "@%s: device already started", __func__);
+
+    int ret = streamOn();
+    if (ret != OK) {
+        streamOff();
+        LOGE("Devices stream on failed:%d", ret);
+        return ret;
+    }
+
+    mPollThread->run("CaptureUnit", PRIORITY_URGENT_AUDIO);
+    mState = CAPTURE_START;
+    mExitPending = false;
+    LOG2("@%s: automation checkpoint: flag: poll_started", __func__);
+
+    return OK;
+}
+
+void CaptureUnit::streamOff()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("%s, mCameraId:%d", __func__, mCameraId);
+
+    if (mDevice) {
+        mDevice->streamOff();
+    }
+}
+
+int CaptureUnit::stop()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("%s, mCameraId:%d", __func__, mCameraId);
+
+    if (mState != CAPTURE_START) {
+        LOGW("@%s: device not started", __func__);
+        return OK;
+    }
+
+    mExitPending = true;
+    mPollThread->requestExit();
+    streamOff();
+    mPollThread->requestExitAndWait();
+
+    AutoMutex   l(mLock);
+    mState = CAPTURE_STOP;
+
+    if (mDevice) {
+        mDevice->resetBuffers();
+    }
+    LOG2("@%s: automation checkpoint: flag: poll_stopped", __func__);
+
+    mExitPending = false; // It's already stopped.
+
+    return OK;
+}
+
+/**
+ * Check if the given outputFrames are different from the previous one.
+ * Only return false when the config for each port is exactly same.
+ */
+bool CaptureUnit::isNewConfiguration(const map<Port, stream_t>& outputFrames)
+{
+    for (const auto& item : outputFrames) {
+        if (mOutputFrameInfo.find(item.first) == mOutputFrameInfo.end()) {
+            return true;
+        }
+
+        const stream_t& oldStream = mOutputFrameInfo[item.first];
+        const stream_t& newStream = item.second;
+
+        bool isNewConfig = (oldStream.width != newStream.width || oldStream.height != newStream.height
+               || oldStream.format != newStream.format || oldStream.field != newStream.field
+               || oldStream.memType != newStream.memType);
+        if (isNewConfig) {
+            return true;
+        }
+    }
+
+    return false;
+}
+
+int CaptureUnit::configure(const map<Port, stream_t>& outputFrames,
+                           const vector<ConfigMode>& configModes)
+{
+    PERF_CAMERA_ATRACE();
+
+    CheckError(outputFrames.empty(), BAD_VALUE, "No frame info configured.");
+    CheckError(mState != CAPTURE_CONFIGURE && mState != CAPTURE_INIT && mState != CAPTURE_STOP,
+          INVALID_OPERATION, "@%s: Configure in wrong state %d", __func__, mState);
+
+    Port port = findDefaultPort(outputFrames);
+    const stream_t& mainStream = outputFrames.at(port);
+
+    for (const auto& item : outputFrames) {
+        LOG1("%s, mCameraId:%d, port:%d, w:%d, h:%d, f:%s", __func__, mCameraId, item.first,
+              item.second.width, item.second.height,
+              CameraUtils::format2string(item.second.format).c_str());
+    }
+
+    mConfigModes = configModes;
+    mOutputFrameInfo = outputFrames;
+
+    /* media ctl setup */
+    MediaCtlConf *mc = PlatformData::getMediaCtlConf(mCameraId);
+    CheckError(!mc, BAD_VALUE, "get format configuration failed for %s (%dx%d)",
+               CameraUtils::format2string(mainStream.format).c_str(),
+               mainStream.width, mainStream.height);
+
+    int status = MediaControl::getInstance()->mediaCtlSetup(mCameraId, mc,
+            mainStream.width, mainStream.height, mainStream.field);
+    CheckError(status != OK, status, "set up mediaCtl failed");
+
+    // Create, open, and configure all of needed devices.
+    status = createDevices();
+    CheckError(status != OK, status, "Create devices failed:%d", status);
+
+    mState = CAPTURE_CONFIGURE;
+
+    // mExitPending should also be set false in configure to make buffers queued before start
+    mExitPending = false;
+
+    return OK;
+}
+
+Port CaptureUnit::findDefaultPort(const map<Port, stream_t>& frames) const
+{
+    Port availablePorts[] = {MAIN_PORT, SECOND_PORT, THIRD_PORT, FORTH_PORT};
+    for (unsigned int i = 0; i < ARRAY_SIZE(availablePorts); i++) {
+        if (frames.find(availablePorts[i]) != frames.end()) {
+            return availablePorts[i];
+        }
+    }
+    return INVALID_PORT;
+}
+
+int CaptureUnit::allocateMemory(Port port, const std::shared_ptr<CameraBuffer> &camBuffer)
+{
+    const struct v4l2_buffer* v = camBuffer->getV4L2Buffer().Get();
+    CheckError(v->index >= mMaxBufferNum, -1
+        ,"index %d is larger than max count %d", v->index, mMaxBufferNum);
+    CheckError(v->memory != V4L2_MEMORY_MMAP, -1
+        ,"Allocating Memory Capture device only supports MMAP mode.");
+
+    DeviceBase* device = findDeviceByPort(port);
+    CheckError(!device, BAD_VALUE, "No device available for port:%d", port);
+
+    int ret = camBuffer->allocateMemory(device->getV4l2Device());
+    CheckError(ret < 0, ret, "Failed to allocate memory ret(%d) for port:%d", ret, port);
+
+    return OK;
+}
+
+int CaptureUnit::qbuf(Port port, const std::shared_ptr<CameraBuffer> &camBuffer)
+{
+    CheckError(camBuffer == nullptr, BAD_VALUE, "Camera buffer is null");
+    CheckError((mState == CAPTURE_INIT || mState == CAPTURE_UNINIT), INVALID_OPERATION,
+          "@%s: qbuf in wrong state %d", __func__, mState);
+
+    DeviceBase* device = findDeviceByPort(port);
+    CheckError(!device, BAD_VALUE, "No device available for port:%d", port);
+
+    LOG2("@%s, mCameraId:%d, queue CameraBuffer: %p to port:%d",
+         __func__, mCameraId, camBuffer.get(), port);
+
+    device->addPendingBuffer(camBuffer);
+
+    return processPendingBuffers();
+}
+
+int CaptureUnit::queueAllBuffers()
+{
+    PERF_CAMERA_ATRACE();
+
+    if (mExitPending) return OK;
+
+    if (mDevice) {
+        int ret = mDevice->queueBuffer(-1);
+        if (mExitPending) return OK;
+        CheckError(ret != OK, ret, "queueBuffer fails, dev:%s, ret:%d", mDevice->getName(), ret);
+        mDevice->getPredictSequence();
+    }
+
+    return OK;
+}
+
+void CaptureUnit::onDequeueBuffer()
+{
+    processPendingBuffers();
+}
+
+int CaptureUnit::processPendingBuffers()
+{
+    if (mDevice && mDevice->getBufferNumInDevice() < mMaxBuffersInDevice) {
+        LOG2("%s: buffers in device:%d", __func__, mDevice->getBufferNumInDevice());
+
+        if (!mDevice->hasPendingBuffer()) {
+            return OK;
+        }
+
+        int ret = queueAllBuffers();
+        if (mExitPending) return OK;
+        CheckError(ret != OK, ret, "Failed to queue buffers, ret=%d", ret);
+    }
+
+    return OK;
+}
+
+int CaptureUnit::poll()
+{
+    PERF_CAMERA_ATRACE();
+    int ret = 0;
+    const int poll_timeout_count = 10;
+    const int poll_timeout = gSlowlyRunRatio ? (gSlowlyRunRatio * 1000000) : 1000;
+
+    LOG2("@%s, mCameraId:%d", __func__, mCameraId);
+
+    CheckError((mState != CAPTURE_CONFIGURE && mState != CAPTURE_START), INVALID_OPERATION,
+          "@%s: poll buffer in wrong state %d", __func__, mState);
+
+    int timeOutCount = poll_timeout_count;
+
+    std::vector<V4L2Device*> pollDevs, readyDevices;
+    if (mDevice) {
+        pollDevs.push_back(mDevice->getV4l2Device());
+        LOG2("@%s: device:%s has %d buffers queued.", __func__,
+             mDevice->getName(), mDevice->getBufferNumInDevice());
+    }
+
+    while (timeOutCount-- && ret == 0) {
+        // If stream off, no poll needed.
+        if (mExitPending) {
+            LOG2("%s: mExitPending is true, exit", __func__);
+            //Exiting, no error
+            return -1;
+        }
+
+        V4L2DevicePoller poller {pollDevs, -1};
+        ret = poller.Poll(poll_timeout, POLLPRI | POLLIN | POLLOUT | POLLERR, &readyDevices);
+
+        LOG2("@%s: automation checkpoint: flag: poll_buffer, ret:%d", __func__, ret);
+    }
+
+    //In case poll error after stream off
+    if (mExitPending) {
+        LOG2("%s: mExitPending is true, exit", __func__);
+        //Exiting, no error
+        return -1;
+    }
+
+    CheckError(ret < 0, UNKNOWN_ERROR, "%s: Poll error, ret:%d", __func__, ret);
+
+    if (ret == 0) {
+        LOG1("%s, cameraId: %d: timeout happens, wait recovery", __func__, mCameraId);
+        return OK;
+    }
+
+    for (const auto& readyDevice : readyDevices) {
+        if (mDevice && mDevice->getV4l2Device() == readyDevice) {
+            int ret = mDevice->dequeueBuffer();
+            if (mExitPending) return -1;
+
+            if (ret != OK) {
+                LOGE("Device:%s grab frame failed:%d", mDevice->getName(), ret);
+            }
+            break;
+        }
+    }
+
+    return OK;
+}
+
+void CaptureUnit::addFrameAvailableListener(BufferConsumer *listener)
+{
+    LOG1("%s camera id:%d", __func__, mCameraId);
+
+    AutoMutex   l(mLock);
+    if (mDevice) {
+        mDevice->addFrameListener(listener);
+    }
+}
+
+void CaptureUnit::removeFrameAvailableListener(BufferConsumer *listener)
+{
+    LOG1("%s camera id:%d", __func__, mCameraId);
+
+    AutoMutex   l(mLock);
+    if (mDevice) {
+        mDevice->removeFrameListener(listener);
+    }
+}
+
+void CaptureUnit::removeAllFrameAvailableListener()
+{
+    LOG1("%s camera id:%d", __func__, mCameraId);
+
+    AutoMutex   l(mLock);
+    if (mDevice) {
+        mDevice->removeAllFrameListeners();
+    }
+}
+
+void CaptureUnit::registerListener(EventType eventType, EventListener* eventListener)
+{
+    if (mDevice) {
+        mDevice->registerListener(eventType, eventListener);
+    }
+}
+
+void CaptureUnit::removeListener(EventType eventType, EventListener* eventListener)
+{
+    if (mDevice) {
+        mDevice->removeListener(eventType, eventListener);
+    }
+}
+} // namespace icamera
+
diff --git a/camera/hal/intel/ipu6/src/core/CaptureUnit.h b/camera/hal/intel/ipu6/src/core/CaptureUnit.h
new file mode 100644
index 000000000000..08062ec8bc51
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/CaptureUnit.h
@@ -0,0 +1,194 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <map>
+#include <vector>
+
+#include "iutils/Thread.h"
+
+#include "StreamSource.h"
+#include "CameraBuffer.h"
+#include "DeviceBase.h"
+#include "IspParamAdaptor.h"
+
+namespace icamera {
+
+/**
+  * CaptureUnit abstract the ISYS function.
+  * It implements the BufferProducer Interface and it is the source of any pipeline
+  * It hides the v4l2 and media controller to the upper layer.
+  */
+class CaptureUnit : public StreamSource, public DeviceCallback {
+
+public:
+    CaptureUnit(int cameraId, int memType = V4L2_MEMORY_MMAP);
+    virtual ~CaptureUnit();
+
+public:
+    /**
+     * \brief Queue on buffer to driver
+     *
+     * 1. Get the v4l2 buffer form the CameraBuffer
+     * 2. Queue this v4l2 buffer to driver and save it to one queue.
+     *
+     * \param[in] port: Indicates the camBuffer belongs to which port
+     * \param[in] camBuffer: the cameraBuffer queue to driver
+     *
+     * \return 0 if succeed, other value indicates failed
+     */
+    virtual int qbuf(Port port, const std::shared_ptr<CameraBuffer> &camBuffer);
+
+    /**
+     * \brief allocate memory
+     *
+     * 1. Get the v4l2 buffer form the CameraBuffer
+     * 2. Query the v4l2 buffer to get the offset
+     * 3. Calling cameraBuffer class to allocate memory
+     *
+     * \return OK if succeed, other value indicates failed
+     */
+    virtual int allocateMemory(Port port, const std::shared_ptr<CameraBuffer> &camBuffer);
+
+    /**
+     * \brief Add the frame buffer listener
+     *
+     * \param listener: the listener need to add
+     */
+    virtual void addFrameAvailableListener(BufferConsumer *listener);
+
+    /**
+     * \brief Remove the frame buffer listener
+     *
+     * \param listener: the listener need to remove
+     */
+    virtual void removeFrameAvailableListener(BufferConsumer *listener);
+
+    /**
+     * \brief Remove all the listeners
+     */
+    virtual void removeAllFrameAvailableListener();
+
+    /**
+     * \brief CaptureUnit initialze
+     */
+    virtual int init();
+
+    /**
+     * \brief CaptureUnit deinit
+     *
+     * 1. Destory all the buffer pool
+     * 2. Deinit the v4l2 device
+     * 3. Destory the poll thread
+     */
+    virtual void deinit();
+
+    /**
+     * \brief CaptureUnit start
+     *
+     * 1. Stream on
+     * 2. Running the pool Thread
+     */
+    virtual int start();
+
+    /**
+     * \brief CaptureUnit stop
+     *
+     * 1. Stream off
+     * 3. Release all the buffer queue
+     * 3. Stop the pool thread.
+     */
+    virtual int stop();
+
+    /**
+     * \brief configure the streams
+     *
+     * 1. Setup and reset the MediaControl links
+     * 2. Set format to Capture Device
+     *
+     * \param outputFrames: The output frames' configuration for ISYS.
+     * \param configModes: ConfigMode types
+     *
+     * \return OK if succeed, other value indicates failed
+     */
+    virtual int configure(const std::map<Port, stream_t>& outputFrames,
+                          const std::vector<ConfigMode>& configModes);
+
+    // Override EventSource API to delegate the listeners to DeviceBase.
+    virtual void registerListener(EventType eventType, EventListener* eventListener);
+    virtual void removeListener(EventType eventType, EventListener* eventListener);
+
+    // Overwrite DeviceCallback API
+    void onDequeueBuffer();
+
+private:
+    DISALLOW_COPY_AND_ASSIGN(CaptureUnit);
+
+    int createDevices();
+    void destroyDevices();
+    DeviceBase* findDeviceByPort(Port port);
+    Port findDefaultPort(const std::map<Port, stream_t>& frames) const;
+
+    int streamOn();
+    void streamOff();
+
+    int poll();
+
+    bool isNewConfiguration(const std::map<Port, stream_t>& outputFrames);
+
+    int processPendingBuffers();
+    int queueAllBuffers();
+
+private:
+    /**
+     * \brief The pool frame buffer thread
+     */
+    class PollThread: public Thread {
+        CaptureUnit *mCaptureU;
+        public:
+            PollThread(CaptureUnit *hw) : mCaptureU(hw) { }
+
+            virtual bool threadLoop() {
+                return (mCaptureU->poll() == 0);
+            }
+    };
+
+    PollThread* mPollThread;
+
+    // Guard for mCaptureUnit public API except dqbuf and qbuf
+    Mutex mLock;
+
+    int mCameraId;
+    int mMaxBuffersInDevice;  // To control the number of buffers enqueued, for per-frame control.
+
+    std::vector<ConfigMode> mConfigModes;
+    std::map<Port, stream_t> mOutputFrameInfo;
+    DeviceBase* mDevice;
+    uint32_t mMaxBufferNum;
+
+    enum {
+        CAPTURE_UNINIT,
+        CAPTURE_INIT,
+        CAPTURE_CONFIGURE,
+        CAPTURE_START,
+        CAPTURE_STOP,
+    } mState;
+    bool mExitPending;
+};
+
+} // namespace icamera
+
diff --git a/camera/hal/intel/ipu6/src/core/DeviceBase.cpp b/camera/hal/intel/ipu6/src/core/DeviceBase.cpp
new file mode 100644
index 000000000000..df5958a4a20f
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/DeviceBase.cpp
@@ -0,0 +1,442 @@
+/*
+ * Copyright (C) 2018-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include <string>
+#include <vector>
+
+#define LOG_TAG "DeviceBase"
+
+#include "iutils/CameraLog.h"
+#include "iutils/CameraDump.h"
+#include "iutils/Utils.h"
+#include "linux/ipu-isys.h"
+
+#include "SyncManager.h"
+#include "PlatformData.h"
+#include "DeviceBase.h"
+#include "CameraEventType.h"
+#include "V4l2DeviceFactory.h"
+
+using std::shared_ptr;
+
+namespace icamera {
+
+DeviceBase::DeviceBase(int cameraId, VideoNodeType nodeType, VideoNodeDirection nodeDirection,
+                       DeviceCallback* deviceCB) :
+        mCameraId(cameraId),
+        mPort(INVALID_PORT),
+        mNodeType(nodeType),
+        mNodeDirection(nodeDirection),
+        mName(GetNodeName(nodeType)),
+        mDevice(nullptr),
+        mLatestSequence(-1),
+        mNeedSkipFrame(false),
+        mDeviceCB(deviceCB),
+        mMaxBufferNumber(MAX_BUFFER_COUNT)
+{
+    LOG1("%s, camera id:%d device:%s", __func__, mCameraId, mName);
+
+    mFrameSkipNum = PlatformData::getInitialSkipFrame(mCameraId);
+
+    std::string devName;
+    int ret = PlatformData::getDevNameByType(cameraId, nodeType, devName);
+    CheckError(ret != OK, VOID_VALUE,
+               "Failed to get video device name for cameraId: %d, node type: %d",
+               cameraId, nodeType);
+
+    mDevice = new V4L2VideoNode(devName);
+}
+
+DeviceBase::~DeviceBase()
+{
+    LOG1("%s, camera id:%d device:%s", __func__, mCameraId, mName);
+
+    delete mDevice;
+}
+
+int DeviceBase::openDevice()
+{
+    LOG1("%s, camera id:%d device:%s", __func__, mCameraId, mName);
+
+    if (PlatformData::isEnableFrameSyncCheck(mCameraId))
+        SyncManager::getInstance()->updateSyncCamNum();
+
+    return mDevice->Open(O_RDWR);
+}
+
+void DeviceBase::closeDevice()
+{
+    LOG1("%s, camera id:%d device:%s", __func__, mCameraId, mName);
+    {
+        AutoMutex l(mBufferLock);
+
+        std::vector<V4L2Buffer> bufs;
+        mDevice->SetupBuffers(0, true, mDevice->GetMemoryType(), &bufs);
+
+        mPendingBuffers.clear();
+        mBuffersInDevice.clear();
+        mAllocatedBuffers.clear();
+    }
+
+    mDevice->Close();
+}
+
+int DeviceBase::configure(Port port, const stream_t& config, uint32_t bufferNum)
+{
+    LOG1("%s, camera id:%d device:%s, port:%d", __func__, mCameraId, mName, port);
+
+    mPort = port;
+    mMaxBufferNumber = bufferNum;
+
+    int ret = createBufferPool(config);
+    CheckError(ret != OK, NO_MEMORY, "Failed to create buffer pool:%d", ret);
+
+    resetBuffers();
+
+    return OK;
+}
+
+int DeviceBase::streamOn()
+{
+    LOG1("%s, camera id:%d device:%s", __func__, mCameraId, mName);
+
+    mFrameSkipNum = PlatformData::getInitialSkipFrame(mCameraId);
+
+    return mDevice->Start();
+}
+
+int DeviceBase::streamOff()
+{
+    LOG1("%s, camera id:%d device:%s", __func__, mCameraId, mName);
+
+    mDevice->Stop(false);
+
+    return OK;
+}
+
+int DeviceBase::queueBuffer(long sequence)
+{
+    LOG2("%s, camera id:%d device:%s", __func__, mCameraId, mName);
+
+    shared_ptr<CameraBuffer> buffer;
+    AutoMutex l(mBufferLock);
+    if (mPendingBuffers.empty()) {
+        LOG2("Device:%s has no pending buffer to be queued.", mName);
+        return OK;
+    }
+    buffer = mPendingBuffers.front();
+
+    int ret = onQueueBuffer(sequence, buffer);
+    CheckError(ret != OK, ret, "Device:%s failed to preprocess the buffer with ret=%d", mName, ret);
+
+    ret = mDevice->PutFrame(&buffer->getV4L2Buffer());
+
+    if (ret >= 0) {
+        mPendingBuffers.pop_front();
+        mBuffersInDevice.push_back(buffer);
+    }
+
+    return ret;
+}
+
+int DeviceBase::dequeueBuffer()
+{
+    LOG2("%s, camera id:%d device:%s", __func__, mCameraId, mName);
+
+    shared_ptr<CameraBuffer> camBuffer = getFirstDeviceBuffer();
+    CheckError(!camBuffer, UNKNOWN_ERROR, "No buffer in device:%s.", mName);
+
+    int ret = OK;
+    int targetIndex = camBuffer->getIndex();
+
+    V4L2Buffer &vbuf = camBuffer->getV4L2Buffer();
+    int actualIndex = mDevice->GrabFrame(&vbuf);
+
+    CheckError(actualIndex < 0, BAD_VALUE, "Device grabFrame failed:%d", actualIndex);
+    if (actualIndex != targetIndex) {
+        LOGE("%s, CamBuf index isn't same with index used by kernel", __func__);
+        ret = BAD_VALUE;
+    }
+
+    mNeedSkipFrame = needQueueBack(camBuffer);
+    popBufferFromDevice();
+
+    // TODO: Will add device name info to distinguish different devices.
+    PERF_CAMERA_ATRACE_PARAM3("grabFrame SeqID", camBuffer->getSequence(),
+                              "csi2_port",       camBuffer->getCsi2Port(),
+                              "virtual_channel", camBuffer->getVirtualChannel());
+
+    ret |= onDequeueBuffer(camBuffer);
+
+    // Skip initial frames if needed.
+    if (mFrameSkipNum > 0) {
+        mFrameSkipNum--;
+    }
+    return ret;
+}
+
+int DeviceBase::getBufferNumInDevice()
+{
+    AutoMutex l(mBufferLock);
+
+    return mBuffersInDevice.size();
+}
+
+void DeviceBase::resetBuffers()
+{
+    AutoMutex l(mBufferLock);
+
+    mBuffersInDevice.clear();
+    mPendingBuffers.clear();
+
+    for (const auto& buffer : mAllocatedBuffers) {
+        mPendingBuffers.push_back(buffer);
+    }
+}
+
+bool DeviceBase::hasPendingBuffer()
+{
+    AutoMutex l(mBufferLock);
+
+    return !mPendingBuffers.empty();
+}
+
+void DeviceBase::addPendingBuffer(const shared_ptr<CameraBuffer>& buffer)
+{
+    AutoMutex l(mBufferLock);
+
+    mPendingBuffers.push_back(buffer);
+}
+
+long DeviceBase::getPredictSequence()
+{
+    AutoMutex l(mBufferLock);
+
+    return mLatestSequence + mFrameSkipNum + mBuffersInDevice.size();
+}
+
+shared_ptr<CameraBuffer> DeviceBase::getFirstDeviceBuffer()
+{
+    AutoMutex l(mBufferLock);
+
+    return mBuffersInDevice.empty() ? nullptr : mBuffersInDevice.front();
+}
+
+bool DeviceBase::skipFrameAfterSyncCheck(long sequence)
+{
+    //For multi-camera sensor, to check whether the frame synced or not
+    int count = 0;
+    const int timeoutDuration = gSlowlyRunRatio ? (gSlowlyRunRatio * 1000000) : 1000;
+    const int maxCheckTimes = 10;  //10 times
+    while (!SyncManager::getInstance()->isSynced(mCameraId, sequence)) {
+        usleep(timeoutDuration);
+        count++;
+        if (count > maxCheckTimes) {
+            return true;
+        }
+    }
+    return false;
+}
+
+void DeviceBase::popBufferFromDevice()
+{
+    AutoMutex l(mBufferLock);
+    if (mBuffersInDevice.empty()) {
+        return;
+    }
+
+    shared_ptr<CameraBuffer> camBuffer = mBuffersInDevice.front();
+    mBuffersInDevice.pop_front();
+    mLatestSequence = camBuffer->getSequence();
+
+    if (mNeedSkipFrame) {
+        mPendingBuffers.push_back(camBuffer);
+    }
+}
+
+void DeviceBase::dumpFrame(const shared_ptr<CameraBuffer>& buffer)
+{
+    if (!CameraDump::isDumpTypeEnable(DUMP_ISYS_BUFFER)) return;
+
+    LOGD("@%s, ISYS: fmt:%s(%dx%d), stride:%d, len:%d", __func__,
+         CameraUtils::format2string(buffer->getFormat()).c_str(),
+         buffer->getWidth(), buffer->getHeight(), buffer->getStride(), buffer->getBufferSize());
+
+    CameraDump::dumpImage(mCameraId, buffer, M_ISYS, mPort);
+}
+
+MainDevice::MainDevice(int cameraId, VideoNodeType nodeType, DeviceCallback* deviceCB) :
+        DeviceBase(cameraId, nodeType, INPUT_VIDEO_NODE, deviceCB)
+{
+    LOG1("%s, camera id:%d device:%s", __func__, mCameraId, mName);
+}
+
+MainDevice::~MainDevice()
+{
+    LOG1("%s, camera id:%d device:%s", __func__, mCameraId, mName);
+}
+
+int MainDevice::createBufferPool(const stream_t& config)
+{
+    LOG1("%s, camera id:%d, fmt:%s(%dx%d) field:%d", __func__, mCameraId,
+         CameraUtils::pixelCode2String(config.format), config.width, config.height, config.field);
+
+    // Pass down ISYS compression flag to driver, which is CSI-BE output compression
+    bool isISYSCompression = PlatformData::getISYSCompression(mCameraId);
+    if (PlatformData::isCSIBackEndCapture(mCameraId)) {
+        std::string csiBEDeviceNodeName;
+        int ret = PlatformData::getDevNameByType(mCameraId, VIDEO_GENERIC, csiBEDeviceNodeName);
+        CheckError(ret != OK, ret, "failed to get CSI-BE device node name, ret=%d", ret);
+        LOG1("csiBEDeviceNodeName is %s", csiBEDeviceNodeName.c_str());
+
+        V4L2Subdevice* csiBESubDev = V4l2DeviceFactory::getSubDev(mCameraId, csiBEDeviceNodeName);
+        ret = csiBESubDev->SetControl(V4L2_CID_IPU_ISYS_COMPRESSION, isISYSCompression);
+        LOG2("@%s, set control compression for BE capture, node name: %s, ret:%d",
+                __func__, csiBEDeviceNodeName.c_str(), ret);
+    }
+
+    bool setWithHeaderCtl = true;
+    std::string subDeviceNodeName;
+
+    if (PlatformData::getDevNameByType(mCameraId, VIDEO_ISYS_RECEIVER, subDeviceNodeName) == OK) {
+        LOG1("%s: found ISYS receiver subdevice %s", __func__, subDeviceNodeName.c_str());
+        if (PlatformData::isTPGReceiver(mCameraId)) {
+            LOG1("%s: no need to set csi header ctrl for tpg", __func__);
+            setWithHeaderCtl = false;
+        }
+    } else {
+        setWithHeaderCtl = false;
+    }
+
+    int withHeader = 1;
+    struct v4l2_format v4l2fmt;
+    v4l2fmt.fmt.pix_mp.field = config.field;
+
+    if (PlatformData::isCSIFrontEndCapture(mCameraId)) {
+        int planesNum = CameraUtils::getNumOfPlanes(config.format);
+        LOG1("@%s Num of planes: %d, mCameraId:%d", __func__, planesNum, mCameraId);
+
+        v4l2fmt.fmt.pix_mp.width = config.width;
+        v4l2fmt.fmt.pix_mp.height = config.height;
+        v4l2fmt.fmt.pix_mp.num_planes = planesNum;
+        v4l2fmt.fmt.pix_mp.pixelformat = config.format;
+        for (int i = 0; i < v4l2fmt.fmt.pix_mp.num_planes; i++) {
+            v4l2fmt.fmt.pix_mp.plane_fmt[i].bytesperline = config.width;
+            v4l2fmt.fmt.pix_mp.plane_fmt[i].sizeimage = 0;
+        }
+        // The frame data is without header(MIPI STORE MODE) when
+        // format is YUV/RGB and frame output from CSI-Front-End entity.
+        if (!CameraUtils::isRaw(config.format)) {
+            LOG2("@%s, set frame without header for format: %s",
+                    __func__, CameraUtils::pixelCode2String(config.format));
+            withHeader = 0;
+        }
+    } else {
+        v4l2fmt.fmt.pix.width = config.width;
+        v4l2fmt.fmt.pix.height = config.height;
+        v4l2fmt.fmt.pix.pixelformat = config.format;
+        v4l2fmt.fmt.pix.bytesperline = config.width;
+        v4l2fmt.fmt.pix.sizeimage = 0;
+    }
+
+    if (setWithHeaderCtl) {
+        V4L2Subdevice* receiverSubDev = V4l2DeviceFactory::getSubDev(mCameraId, subDeviceNodeName);
+        int ret = receiverSubDev->SetControl(V4L2_CID_IPU_STORE_CSI2_HEADER, withHeader);
+        CheckError(ret != OK, ret, "set v4l2 store csi2 header failed, ret=%d", ret);
+    }
+
+    v4l2fmt.type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
+    V4L2Format tmpbuf {v4l2fmt};
+    int ret = mDevice->SetFormat(tmpbuf);
+    CheckError(ret != OK, ret, "set v4l2 format failed ret=%d", ret);
+    v4l2fmt = *tmpbuf.Get();
+
+    int realBufferSize = v4l2fmt.fmt.pix.sizeimage;
+    int calcBufferSize = 0;
+    if (isISYSCompression) {
+        calcBufferSize = CameraUtils::getFrameSize(config.format, config.width, config.height, false, true, true);
+    } else {
+        calcBufferSize = CameraUtils::getFrameSize(config.format, config.width, config.height);
+    }
+
+    CheckError(calcBufferSize < realBufferSize, BAD_VALUE,
+        "realBufferSize %d is larger than calcBufferSize %d.", realBufferSize, calcBufferSize);
+
+    LOG2("@%s: compression:%d, realBufSize:%d, calcBufSize:%d",
+                __func__, isISYSCompression, realBufferSize, calcBufferSize);
+
+    std::vector<V4L2Buffer> bufs;
+    int bufNum = mDevice->SetupBuffers(mMaxBufferNumber, true,
+                                       static_cast<enum v4l2_memory>(config.memType), &bufs);
+
+    CheckError(bufNum < 0, BAD_VALUE, "request buffers failed return=%d", bufNum);
+
+    return OK;
+}
+
+int MainDevice::onDequeueBuffer(shared_ptr<CameraBuffer> buffer)
+{
+    mDeviceCB->onDequeueBuffer();
+
+    if (mNeedSkipFrame) return OK;
+
+    LOG2("@%s, sequence:%ld, field:%d, timestamp: sec=%ld, usec=%ld",
+          __func__, buffer->getSequence(), buffer->getField(),
+          buffer->getTimestamp().tv_sec, buffer->getTimestamp().tv_usec);
+
+    for (auto& consumer : mConsumers) {
+        consumer->onFrameAvailable(mPort, buffer);
+    }
+
+    EventData frameData;
+    frameData.type = EVENT_ISYS_FRAME;
+    frameData.buffer = nullptr;
+    frameData.data.frame.sequence = buffer->getSequence();
+    frameData.data.frame.timestamp.tv_sec = buffer->getTimestamp().tv_sec;
+    frameData.data.frame.timestamp.tv_usec = buffer->getTimestamp().tv_usec;
+    notifyListeners(frameData);
+
+    dumpFrame(buffer);
+
+    return OK;
+}
+
+bool MainDevice::needQueueBack(shared_ptr<CameraBuffer> buffer)
+{
+    bool needSkipFrame = (mFrameSkipNum > 0);
+
+    const V4L2Buffer& vbuf = buffer->getV4L2Buffer();
+    // Check for STR2MMIO Error from kernel space
+    if((vbuf.Flags() & V4L2_BUF_FLAG_ERROR) && PlatformData::isSkipFrameOnSTR2MMIOErr(mCameraId)) {
+        // On STR2MMIO error, enqueue this buffer back to V4L2 before notifying the
+        // listener/consumer and return
+        needSkipFrame = true;
+    }
+    if (PlatformData::isEnableFrameSyncCheck(mCameraId)) {
+        struct camera_buf_info sharedCamBufInfo;
+        sharedCamBufInfo.sequence = buffer->getSequence();
+        sharedCamBufInfo.sof_ts = buffer->getTimestamp();
+        SyncManager::getInstance()->updateCameraBufInfo(mCameraId, &sharedCamBufInfo);
+        if (skipFrameAfterSyncCheck(buffer->getSequence())) {
+            LOG1("@%s: CameraID:%d sequence %ld been dropped due to frame not sync",
+                  __func__, mCameraId, buffer->getSequence());
+            needSkipFrame = true;
+        }
+    }
+    return needSkipFrame;
+}
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/DeviceBase.h b/camera/hal/intel/ipu6/src/core/DeviceBase.h
new file mode 100644
index 000000000000..753604c7550d
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/DeviceBase.h
@@ -0,0 +1,171 @@
+/*
+ * Copyright (C) 2018-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <v4l2_device.h>
+
+#include <set>
+#include <list>
+#include <atomic>
+
+#include "iutils/Thread.h"
+#include "v4l2/NodeInfo.h"
+#include "BufferQueue.h"
+#include "CameraBuffer.h"
+#include "IspParamAdaptor.h"
+
+namespace icamera {
+
+enum VideoNodeDirection {
+    INPUT_VIDEO_NODE,   /*!< input video devices like cameras or capture cards */
+    OUTPUT_VIDEO_NODE  /*!< output video devices like displays */
+};
+
+class DeviceCallback {
+public:
+    DeviceCallback() {};
+    virtual ~DeviceCallback() {};
+    virtual void onDequeueBuffer() {};
+};
+
+/**
+ * DeviceBase is a base class of other devices which are for a particular purpose.
+ * It provides: general operation of V4l2 devices, and secured buffer management.
+ *
+ * There are several virtual functions for subclass to override. The subclass should
+ * base on what its implementation is to override one or several of them.
+ */
+class DeviceBase : public EventSource {
+public:
+    DeviceBase(int cameraId, VideoNodeType nodeType,
+               VideoNodeDirection nodeDirection, DeviceCallback* deviceCB = nullptr);
+    virtual ~DeviceBase();
+
+    int configure(Port port, const stream_t& config, uint32_t bufferNum);
+
+    int openDevice();
+    void closeDevice();
+
+    int streamOn();
+    int streamOff();
+
+    int queueBuffer(long sequence);
+    int dequeueBuffer();
+
+    void addFrameListener(BufferConsumer *listener) { mConsumers.insert(listener); }
+    void removeFrameListener(BufferConsumer *listener) { mConsumers.erase(listener); }
+    void removeAllFrameListeners() { mConsumers.clear(); }
+
+    bool hasPendingBuffer();
+    void addPendingBuffer(const std::shared_ptr<CameraBuffer>& buffer);
+    long getPredictSequence();
+    int getBufferNumInDevice();
+    void resetBuffers();
+    bool skipFrameAfterSyncCheck(long sequence);
+
+    V4L2VideoNode* getV4l2Device() { return mDevice; }
+    const char* getName() { return mName; }
+    Port getPort() { return mPort; }
+
+protected:
+    /**
+     * Configure the device and request or create(if needed) the buffer pool.
+     */
+    virtual int createBufferPool(const stream_t& config) { return OK; }
+
+    /**
+     * Pre-process the buffer which to be queued to the device.
+     */
+    virtual int onQueueBuffer(long sequence, std::shared_ptr<CameraBuffer>& buffer) { return OK; }
+
+    /**
+     * Post-process the buffer after it's dequeued from the device.
+     */
+    virtual int onDequeueBuffer(std::shared_ptr<CameraBuffer> buffer) { return OK; }
+
+    /**
+     * Return whether the buffer needs to be queued back to mPendingBuffers.
+     */
+    virtual bool needQueueBack(std::shared_ptr<CameraBuffer> buffer) { return false; }
+
+    void dumpFrame(const std::shared_ptr<CameraBuffer>& buffer);
+
+private:
+    DISALLOW_COPY_AND_ASSIGN(DeviceBase);
+
+    /**
+     * Get one available buffer from mBuffersInDevice
+     *
+     * Return the front buffer of mBuffersInDevice if available, otherwise return nullptr.
+     */
+    std::shared_ptr<CameraBuffer> getFirstDeviceBuffer();
+
+    /**
+     * Pop the first buffer in mBuffersInDevice.
+     * Add the buffer back to mPendingBuffers if needed.
+     */
+    void popBufferFromDevice();
+
+protected:
+    int mCameraId;
+    Port mPort;
+    VideoNodeType mNodeType;
+    VideoNodeDirection mNodeDirection;
+    const char* mName;
+    V4L2VideoNode* mDevice; // The device used to queue/dequeue buffers.
+    long mLatestSequence; // Track the latest bufffer sequence from driver.
+    bool mNeedSkipFrame; // True if the frame/buffer needs to be skipped.
+    int mFrameSkipNum; // How many frames need to be skipped after stream on.
+    DeviceCallback* mDeviceCB;
+    std::set<BufferConsumer*> mConsumers;
+
+    /**
+     * Each device has below three structures to manager its buffers.
+     * And please note that:
+     * 1. If the buffer is not allocated inside CaptureUnit, mAllocatedBuffers will be empty.
+     * 2. Buffer to be queued into drive comes from mPendingBuffers.
+     * 3. Buffer to be dequeued from driver comes from mBuffersInDevice.
+     * 4. To make code clean, no null CameraBuffer is allowed to be put into these structures.
+     * 5. The buffer cannot be in both mPendingBuffers and mBuffersInDevice.
+     *    We must make the data consistent.
+     */
+    std::vector<std::shared_ptr<CameraBuffer>> mAllocatedBuffers;
+            // Save all buffers allocated internally.
+    std::list<std::shared_ptr<CameraBuffer>> mPendingBuffers;
+            // The buffers that are going to be queued.
+    std::list<std::shared_ptr<CameraBuffer>> mBuffersInDevice; // The buffers that have been queued
+    Mutex mBufferLock; // The lock for protecting the internal buffers.
+
+    uint32_t mMaxBufferNumber;
+};
+
+/**
+ * MainDevice is a most commonly used device.
+ * It's usually for producing video frames.
+ */
+class MainDevice : public DeviceBase {
+public:
+    MainDevice(int cameraId, VideoNodeType nodeType, DeviceCallback* deviceCB);
+    ~MainDevice();
+
+private:
+    int createBufferPool(const stream_t& config);
+    int onDequeueBuffer(std::shared_ptr<CameraBuffer> buffer);
+    bool needQueueBack(std::shared_ptr<CameraBuffer> buffer);
+};
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/IspParamAdaptor.cpp b/camera/hal/intel/ipu6/src/core/IspParamAdaptor.cpp
new file mode 100644
index 000000000000..0a766200b8d8
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/IspParamAdaptor.cpp
@@ -0,0 +1,964 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "IspParamAdaptor"
+
+#include <stdio.h>
+
+#include "IspParamAdaptor.h"
+
+#include "3a/AiqResult.h"
+#include "3a/AiqResultStorage.h"
+#include "iutils/Utils.h"
+#include "iutils/CameraLog.h"
+#include "iutils/CameraDump.h"
+#include "iutils/Errors.h"
+#include "PlatformData.h"
+#include "IGraphConfig.h"
+
+#include "ia_pal_types_isp_ids_autogen.h"
+#include "ia_pal_types_isp.h"
+
+namespace icamera {
+
+IspParamAdaptor::IspParamAdaptor(int cameraId, PgParamType type) :
+        mIspAdaptorState(ISP_ADAPTOR_NOT_INIT),
+        mCameraId(cameraId),
+        mPgParamType(type),
+        mTuningMode(TUNING_MODE_VIDEO),
+        mIspAdaptHandle(nullptr),
+        mBCompResults(nullptr),
+        mCurIspParamIndex(-1),
+        mCallInfoOffset(-1),
+        mBNLM32Offset(-1),
+        mGCM(nullptr),
+        mAdaptor(nullptr)
+{
+    LOG1("IspParamAdaptor was created for id:%d type:%d", mCameraId, mPgParamType);
+    CLEAR(mFrameParam);
+    CLEAR(mLastPalDataForVideoPipe);
+
+    if (PlatformData::getGraphConfigNodes(cameraId)) {
+        mGCM = IGraphConfigManager::getInstance(cameraId);
+    }
+
+    mAdaptor = std::unique_ptr<IntelIspParamAdaptor>(new IntelIspParamAdaptor());
+}
+
+IspParamAdaptor::~IspParamAdaptor()
+{
+    LOG1("IspParamAdaptor was created for id:%d type:%d", mCameraId, mPgParamType);
+}
+
+int IspParamAdaptor::init()
+{
+    PERF_CAMERA_ATRACE();
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL1);
+    AutoMutex l(mIspAdaptorLock);
+
+    mIspAdaptorState = ISP_ADAPTOR_INIT;
+    return OK;
+}
+
+int IspParamAdaptor::deinit()
+{
+    LOG1("ISP HW param adaptor de-initialized for camera id:%d type:%d", mCameraId, mPgParamType);
+    AutoMutex l(mIspAdaptorLock);
+
+    deinitIspAdaptHandle();
+
+    {
+        AutoMutex l(mIpuParamLock);
+
+        // Release the memory and clear the mapping
+        for (auto& pgMap: mStreamIdToProgramGroupMap) {
+            delete[] pgMap.second.run_kernels;
+        }
+        mStreamIdToProgramGroupMap.clear();
+        mStreamIdToPGOutSizeMap.clear();
+        releaseIspParamBuffers();
+    }
+
+    CLEAR(mFrameParam);
+    CLEAR(mLastPalDataForVideoPipe);
+    mCallInfoOffset = -1;
+    mBNLM32Offset = -1;
+
+    mIspAdaptorState = ISP_ADAPTOR_NOT_INIT;
+    return OK;
+}
+
+int IspParamAdaptor::initIspAdaptHandle(ConfigMode configMode, TuningMode tuningMode)
+{
+    int ret = OK;
+
+    if (!PlatformData::isEnableAIQ(mCameraId)) {
+        return ret;
+    }
+
+    ia_binary_data ispData;
+    ia_cmc_t *cmcData = nullptr;
+    uintptr_t cmcHandle = reinterpret_cast<uintptr_t>(nullptr);
+
+    ret = PlatformData::getCpfAndCmc(mCameraId, &ispData, nullptr, nullptr,
+                                     &cmcHandle, tuningMode, &cmcData);
+    CheckError(ret != OK, NO_INIT, "get cpf and cmc data failed");
+
+    int statsNum = PlatformData::getExposureNum(mCameraId,
+                                                CameraUtils::isMultiExposureCase(tuningMode));
+    mIspAdaptHandle = mAdaptor->init(&ispData, reinterpret_cast<ia_cmc_t*>(cmcHandle),
+                                     MAX_STATISTICS_WIDTH, MAX_STATISTICS_HEIGHT,
+                                     statsNum, nullptr);
+    CheckError(!mIspAdaptHandle, NO_INIT, "ISP adaptor failed to initialize");
+
+    /*
+     * The number of streamId is identified in configure stream,
+     * fill the mStreamIdToProgramGroupMap and allocate the IspParameter memory
+     */
+    if (mGCM != nullptr && mGCM->isGcConfigured()) {
+        AutoMutex l(mIpuParamLock);
+
+        ret = initProgramGroupForAllStreams(configMode);
+        CheckError(ret != OK, ret, "%s, Failed to init programGroup for all streams", __func__);
+        ret = allocateIspParamBuffers();
+        CheckError(ret != OK, ret, "%s, Failed to allocate isp parameter buffers", __func__);
+    }
+
+    LOG1("ISP HW param adaptor initialized successfully camera id:%d", mCameraId);
+
+    return ret;
+}
+
+void IspParamAdaptor::deinitIspAdaptHandle()
+{
+    if (mIspAdaptHandle) {
+        mAdaptor->deInit(mIspAdaptHandle);
+        mIspAdaptHandle = nullptr;
+    }
+
+}
+
+int IspParamAdaptor::initProgramGroupForAllStreams(ConfigMode configMode)
+{
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL1);
+
+    std::vector<int32_t> streamIds;
+
+    //Release the memory and clear the mapping
+    for (auto& pgMap: mStreamIdToProgramGroupMap) {
+        delete[] pgMap.second.run_kernels;
+    }
+    mStreamIdToProgramGroupMap.clear();
+    mStreamIdToPGOutSizeMap.clear();
+    mStreamIdToMbrDataMap.clear();
+
+    std::shared_ptr<IGraphConfig> graphConfig = mGCM->getGraphConfig(configMode);
+    if(graphConfig == nullptr) {
+        LOGW("There isn't GraphConfig for camera configMode: %d", configMode);
+        return UNKNOWN_ERROR;
+    }
+
+    if (mPgParamType == PG_PARAM_ISYS) {
+        int streamId = 0; // 0 is for PG_PARAM_ISYS
+        streamIds.push_back(streamId);
+    } else {
+        status_t ret = graphConfig->graphGetStreamIds(streamIds);
+        CheckError(ret != OK, UNKNOWN_ERROR, "Failed to get the PG streamIds");
+    }
+
+    for (auto id : streamIds) {
+        ia_isp_bxt_program_group *pgPtr = graphConfig->getProgramGroup(id);
+        if (pgPtr != nullptr) {
+            ia_isp_bxt_program_group programGroup;
+            CLEAR(programGroup);
+            programGroup.run_kernels = new ia_isp_bxt_run_kernels_t[pgPtr->kernel_count];
+
+            // Skip those kernels with 0 uuid which isn't PAL uuid
+            for (unsigned int i = 0; i < pgPtr->kernel_count; i++) {
+                if (pgPtr->run_kernels[i].kernel_uuid != 0) {
+                    MEMCPY_S(&programGroup.run_kernels[programGroup.kernel_count],
+                             sizeof(ia_isp_bxt_run_kernels_t),
+                             &pgPtr->run_kernels[i],
+                             sizeof(ia_isp_bxt_run_kernels_t));
+                    programGroup.kernel_count++;
+                } else {
+                    LOG1("There is 0 uuid found, stream id %d", id);
+                }
+            }
+
+            // Override the stream id in kernel list with the one in sensor's config file.
+            // Remove this after the sensor's tuning file uses correct stream id.
+            int streamId = PlatformData::getStreamIdByConfigMode(mCameraId, configMode);
+            if (streamId != -1) {
+                programGroup.run_kernels->stream_id = streamId;
+            }
+
+            mStreamIdToProgramGroupMap[id] = programGroup;
+            mStreamIdToPGOutSizeMap[id] = mAdaptor->getPalDataSize(&programGroup);
+            ia_isp_bxt_gdc_limits mbrData;
+            status_t ret  = graphConfig->getMBRData(id, &mbrData);
+            if (ret == OK) {
+                mStreamIdToMbrDataMap[id] = mbrData;
+                LOG2("get mbr data for stream:%d:%f,%f,%f,%f",
+                     id, mbrData.rectilinear.zoom, mbrData.rectilinear.pitch,
+                     mbrData.rectilinear.yaw, mbrData.rectilinear.roll);
+            }
+        }
+    }
+
+    return OK;
+}
+
+void IspParamAdaptor::initInputParams(ia_isp_bxt_input_params_v2 *params, PgParamType type)
+{
+    CheckError(params == nullptr, VOID_VALUE, "NULL input parameter");
+
+    if (type == PG_PARAM_PSYS_ISA) {
+        params->ee_setting.feature_level = ia_isp_feature_level_low;
+        params->ee_setting.strength = 0;
+        LOG2("%s: set initial default edge enhancement setting: level: %d, strengh: %d",
+            __func__, params->ee_setting.feature_level, params->ee_setting.strength);
+
+        params->nr_setting.feature_level = ia_isp_feature_level_high;
+        params->nr_setting.strength = 0;
+        LOG2("%s: set initial default noise setting: level: %d, strengh: %d",
+            __func__, params->nr_setting.feature_level, params->nr_setting.strength);
+    }
+}
+
+int IspParamAdaptor::postConfigure(int width, int height, ia_binary_data *ipuParam)
+{
+    // The PG wrapper init is done by the imaging controller.
+    if(mPgParamType == PG_PARAM_PSYS_ISA) {
+        mIspAdaptorState = ISP_ADAPTOR_CONFIGURED;
+        return OK; //No need to do anything for P2P. It id done by libiacss
+    }
+
+    return OK;
+}
+
+/**
+ * configure
+ *
+ * (graph config version)
+ * This is the method used when the spatial parameters change, usually during
+ * stream configuration.
+ *
+ * We initialize the ISP adaptor to produce worst case scenario for memory
+ * allocation.
+ *
+ * At this state we initialize the wrapper code that helps encoding the PG
+ * descriptor and terminal payloads (i.e. the parameters for the PG).
+ *
+ * \param configMode[IN]: The real configure mode.
+ * \param tuningMode[IN]:  The tuning mode.
+ * \param stream[IN]: frame info.
+ * \return OK: everything went ok.
+ * \return UNKNOWN_ERROR: First run of ISP adaptation failed.
+ * \return NO_INIT: Initialization of P2P or PG_DIE wrapper failed.
+ */
+int IspParamAdaptor::configure(const stream_t &stream,
+        ConfigMode configMode, TuningMode tuningMode)
+{
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL1);
+    AutoMutex l(mIspAdaptorLock);
+
+    mTuningMode = tuningMode;
+    CLEAR(mLastPalDataForVideoPipe);
+    mCallInfoOffset = -1;
+    mBNLM32Offset = -1;
+
+    ia_isp_bxt_input_params_v2 inputParams;
+    CLEAR(inputParams);
+    ia_aiq_sa_results_v1 fakeSaResults;
+    CLEAR(fakeSaResults);
+
+    deinitIspAdaptHandle();
+    int ret = initIspAdaptHandle(configMode, tuningMode);
+    CheckError(ret != OK, ret, "%s, init Isp Adapt Handle failed %d", __func__, ret);
+
+    SensorFrameParams param;
+    int status = PlatformData::calculateFrameParams(mCameraId, param);
+    CheckError(status != OK, status, "%s: Failed to calculate frame params", __func__);
+    AiqUtils::convertToAiqFrameParam(param, mFrameParam);
+
+    LOG1("horizontal_crop_offset:%d", mFrameParam.horizontal_crop_offset);
+    LOG1("vertical_crop_offset:%d", mFrameParam.vertical_crop_offset);
+    LOG1("cropped_image_width:%d", mFrameParam.cropped_image_width);
+    LOG1("cropped_image_height:%d", mFrameParam.cropped_image_height);
+    LOG1("horizontal_scaling_numerator:%d", mFrameParam.horizontal_scaling_numerator);
+    LOG1("horizontal_scaling_denominator:%d", mFrameParam.horizontal_scaling_denominator);
+    LOG1("vertical_scaling_numerator:%d", mFrameParam.vertical_scaling_numerator);
+    LOG1("vertical_scaling_denominator:%d", mFrameParam.vertical_scaling_denominator);
+
+    /*
+     * Construct the dummy Shading Adaptor  results to force the creation of
+     * the LSC table.
+     * Assign them to the AIC input parameter structure.
+     */
+    unsigned short fakeLscTable[4] = {1,1,1,1};
+    for (int i = 0; i < MAX_BAYER_ORDER_NUM; i++) {
+        for (int j = 0; j < MAX_BAYER_ORDER_NUM; j++) {
+            fakeSaResults.lsc_grid[i][j] = fakeLscTable;
+        }
+    }
+    fakeSaResults.fraction_bits = 0;
+    fakeSaResults.color_order = cmc_bayer_order_grbg;
+    fakeSaResults.lsc_update = true;
+    fakeSaResults.width = 2;
+    fakeSaResults.height = 2;
+    inputParams.sa_results = &fakeSaResults;
+
+    initInputParams(&inputParams, mPgParamType);
+
+    /*
+     *  IA_ISP_BXT can run without 3A results to produce the defaults for a
+     *  given sensor configuration.
+     */
+    IspParameter *ipuParam = nullptr;
+    {
+        AutoMutex l(mIpuParamLock);
+        mCurIspParamIndex = 0;
+        ipuParam = &(mIspParameters[mCurIspParamIndex]);
+    }
+    CheckError(!ipuParam, UNKNOWN_ERROR, "%s, Failed to get memory for ipuParam", __func__);
+
+    ia_binary_data curIpuParam = {};
+    for (auto& binaryMap : ipuParam->streamIdToDataMap) {
+        inputParams.program_group = &(mStreamIdToProgramGroupMap[binaryMap.first]);
+        inputParams.sensor_frame_params = &mFrameParam;
+        curIpuParam = binaryMap.second;
+        curIpuParam.size = mStreamIdToPGOutSizeMap[binaryMap.first];
+
+        PERF_CAMERA_ATRACE_PARAM1_IMAGING("ia_isp_bxt_run", 1);
+
+        int ret = mAdaptor->runPal(mIspAdaptHandle, &inputParams, &curIpuParam);
+        CheckError(ret != OK, UNKNOWN_ERROR, "ISP parameter adaptation has failed %d", ret);
+
+        {
+            AutoMutex l(mIpuParamLock);
+            binaryMap.second.size = curIpuParam.size;
+            ipuParam->dataAvailableMap[binaryMap.first] = true;
+            ipuParam->sequence = -1;
+        }
+    }
+
+    dumpIspParameter(0, curIpuParam);
+
+    return postConfigure(stream.width, stream.height, &curIpuParam);
+}
+
+int IspParamAdaptor::getParameters(Parameters& param)
+{
+    AutoMutex l(mIspAdaptorLock);
+
+    return OK;
+}
+
+int IspParamAdaptor::decodeStatsData(TuningMode tuningMode,
+                                     std::shared_ptr<CameraBuffer> statsBuffer,
+                                     std::shared_ptr<IGraphConfig> graphConfig)
+{
+    CheckError(mIspAdaptorState != ISP_ADAPTOR_CONFIGURED,
+               INVALID_OPERATION, "%s, wrong state %d", __func__, mIspAdaptorState);
+
+    long sequence = statsBuffer->getSequence();
+    AiqResultStorage *aiqResultStorage = AiqResultStorage::getInstance(mCameraId);
+
+    const AiqResult *feedback = aiqResultStorage->getAiqResult(sequence);
+    if (feedback == nullptr) {
+        LOGW("No aiq result of sequence %ld! Use the latest instead", sequence);
+        feedback = aiqResultStorage->getAiqResult();
+    }
+
+    camera_resolution_t dvsInReso = {};
+    if (graphConfig) {
+        ia_isp_bxt_resolution_info_t resolution = {};
+        uint32_t gdcKernelId;
+        graphConfig->getGdcKernelSetting(&gdcKernelId, &resolution);
+        dvsInReso.width = resolution.input_width;
+        dvsInReso.height = resolution.input_height;
+    }
+
+    ia_binary_data *hwStatsData = (ia_binary_data *)(statsBuffer->getBufferAddr());
+    ConvertInputParam inputParams = {CameraUtils::isMultiExposureCase(tuningMode),
+                                     hwStatsData, &dvsInReso, &feedback->mAeResults, mBCompResults};
+
+    ConvertResult result;
+    ia_isp_bxt_statistics_query_results_t queryResults;
+    CLEAR(result);
+    CLEAR(queryResults);
+    result.queryResults = &queryResults;
+
+    int ret = mAdaptor->queryAndConvertStats(mIspAdaptHandle, &inputParams, &result);
+    CheckError(ret != OK, ret, "%s, Faield to query and convert statistics", __func__);
+
+    // Decode DVS statistics
+    if (queryResults.dvs_stats) {
+        if (CameraDump::isDumpTypeEnable(DUMP_PSYS_DECODED_STAT) && hwStatsData != nullptr) {
+            BinParam_t bParam;
+            bParam.bType = BIN_TYPE_GENERAL;
+            bParam.mType = M_PSYS;
+            bParam.sequence = statsBuffer->getSequence();
+            bParam.gParam.appendix = "dvs_p2p_decoded_stats";
+            CameraDump::dumpBinary(mCameraId, hwStatsData->data, hwStatsData->size, &bParam);
+        }
+
+        if (result.dvsStats) {
+            DvsStatistics dvsStatsStorage(result.dvsStats, statsBuffer->getSequence());
+            aiqResultStorage->updateDvsStatistics(dvsStatsStorage);
+        } else {
+            LOGW("Failed to get GDC kernel setting, DVS stats not decoded");
+        }
+    }
+
+    if (queryResults.rgbs_grid && queryResults.af_grid) {
+        int exposureNum = PlatformData::getExposureNum(mCameraId, false);
+        AiqStatistics *aiqStatistics = aiqResultStorage->acquireAiqStatistics();
+
+        if (*(result.rgbsGrid)) {
+            dumpRgbsStats(*(result.rgbsGrid),
+                          statsBuffer->getSequence(), feedback->mAeResults.num_exposures);
+        }
+        if (result.afGrid) {
+            dumpAfStats(result.afGrid, statsBuffer->getSequence());
+        }
+
+        aiqStatistics->saveRgbsGridData(result.rgbsGrid, exposureNum);
+        aiqStatistics->saveAfGridData(result.afGrid);
+        aiqStatistics->mSequence = sequence;
+        aiqStatistics->mTimestamp = TIMEVAL2USECS(statsBuffer->getTimestamp());
+        aiqStatistics->mTuningMode = tuningMode;
+        aiqResultStorage->updateAiqStatistics(sequence);
+    }
+
+    return OK;
+}
+
+void IspParamAdaptor::updateKernelToggles(ia_isp_bxt_program_group programGroup) {
+
+    if (!Log::isDebugLevelEnable(CAMERA_DEBUG_LOG_KERNEL_TOGGLE)) return;
+
+    const char* ENABLED_KERNELS = "/tmp/enabledKernels";
+    const char* DISABLED_KERNELS = "/tmp/disabledKernels";
+    const int FLIE_CONT_MAX_LENGTH = 1024;
+    ia_isp_bxt_run_kernels_t* curKernel = programGroup.run_kernels;
+    char enabledKernels[FLIE_CONT_MAX_LENGTH] = { 0 };
+    char disabledKernels[FLIE_CONT_MAX_LENGTH] = { 0 };
+
+    int enLen = CameraUtils::getFileContent(ENABLED_KERNELS, enabledKernels, FLIE_CONT_MAX_LENGTH - 1);
+    int disLen = CameraUtils::getFileContent(DISABLED_KERNELS, disabledKernels, FLIE_CONT_MAX_LENGTH - 1);
+
+    if (enLen == 0 && disLen == 0) {
+        LOG2("%s: no explicit kernel toggle.", __func__);
+        return;
+    }
+
+    LOG2("%s: enabled kernels: %s, disabled kernels %s", __func__,
+        enabledKernels, disabledKernels);
+
+    for (unsigned int i = 0; i < programGroup.kernel_count; i++) {
+
+        std::string curKernelUUID = std::to_string(curKernel->kernel_uuid);
+
+        LOG2("%s: checking kernel %s", __func__, curKernelUUID.c_str());
+
+        if (strstr(enabledKernels, curKernelUUID.c_str()) != nullptr) {
+            curKernel->enable = 1;
+            LOG2("%s: kernel %d is explicitly enabled", __func__,
+                curKernel->kernel_uuid);
+        }
+
+        if (strstr(disabledKernels, curKernelUUID.c_str()) != nullptr) {
+            curKernel->enable = 0;
+            LOG2("%s: kernel %d is explicitly disabled", __func__,
+                curKernel->kernel_uuid);
+        }
+
+        curKernel ++;
+    }
+}
+
+/*
+ * PAL output buffer is a reference data for next output buffer,
+ * but currently a ring buffer is used in HAL, which caused logic mismatching issue.
+ * So temporarily copy lastest call info and BNLM_3_2 into PAL output buffer.
+ */
+void IspParamAdaptor::updatePalDataForVideoPipe(ia_binary_data dest)
+{
+    if (mLastPalDataForVideoPipe.data == nullptr || mLastPalDataForVideoPipe.size == 0)
+        return;
+
+    char* src = static_cast<char*>(mLastPalDataForVideoPipe.data);
+    ia_pal_record_header *header_call_info = nullptr;
+    ia_pal_record_header *header_bnlm_3_2 = nullptr;
+    ia_pal_record_header *header = nullptr;
+    if (mBNLM32Offset >= 0 && mCallInfoOffset >= 0) {
+        header = reinterpret_cast<ia_pal_record_header*>(src + mCallInfoOffset);
+        if (header->uuid == ia_pal_uuid_isp_call_info) {
+            header_call_info = header;
+        }
+        header = reinterpret_cast<ia_pal_record_header*>(src + mBNLM32Offset);
+        if (header->uuid == ia_pal_uuid_isp_bnlm_3_2) {
+            header_bnlm_3_2 = header;
+        }
+    } else {
+        uint32_t offset = 0;
+        // find call_info and bnlm_3_2 data in saved PAL buffer
+        while (offset < mLastPalDataForVideoPipe.size) {
+            ia_pal_record_header *header = reinterpret_cast<ia_pal_record_header*>(src + offset);
+            // check if header is valid or not
+            if (header->uuid == 0 || header->size == 0) {
+                LOGW("%s, source header info isn't correct", __func__);
+                return;
+            }
+
+            if (header->uuid == ia_pal_uuid_isp_call_info) {
+                header_call_info = header;
+                mCallInfoOffset = offset;
+            } else if (header->uuid == ia_pal_uuid_isp_bnlm_3_2) {
+                header_bnlm_3_2 = header;
+                mBNLM32Offset = offset;
+            }
+            offset += header->size;
+        }
+        LOG2("call info offset %d, bnlm_3_2 offset %d", mCallInfoOffset, mBNLM32Offset);
+    }
+
+    // return if not all data is found
+    if (header_call_info == nullptr || header_bnlm_3_2 == nullptr) {
+        LOG2("%s, call info or bnlm_3_2 isn't found", __func__);
+        return;
+    }
+
+    char* destData = static_cast<char*>(dest.data);
+    // update call_info and bnlm_3_2 data
+    if (mBNLM32Offset >= 0 && mCallInfoOffset >= 0) {
+        header = reinterpret_cast<ia_pal_record_header*>(destData + mCallInfoOffset);
+        if (header->uuid == ia_pal_uuid_isp_call_info) {
+            MEMCPY_S(header, header->size, header_call_info, header_call_info->size);
+        }
+        header = reinterpret_cast<ia_pal_record_header*>(destData + mBNLM32Offset);
+        if (header->uuid == ia_pal_uuid_isp_bnlm_3_2) {
+            MEMCPY_S(header, header->size, header_bnlm_3_2, header_bnlm_3_2->size);
+        }
+    }
+
+    LOG2("%s, call info and bnlm_3_2 kernels have been updated", __func__);
+}
+
+/**
+ * runIspAdapt
+ * Convert the results of the 3A algorithms and parse with P2P.
+ */
+int IspParamAdaptor::runIspAdapt(const IspSettings* ispSettings, long settingSequence, int32_t streamId)
+{
+    PERF_CAMERA_ATRACE();
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL2);
+    AutoMutex l(mIspAdaptorLock);
+    CheckError(mIspAdaptorState != ISP_ADAPTOR_CONFIGURED, INVALID_OPERATION, "%s, wrong state %d",
+          __func__, mIspAdaptorState);
+
+    bool forceUpdate = false;
+    IspParameter *ipuParam = nullptr;
+    {
+        AutoMutex l(mIpuParamLock);
+
+        int updateIndex = -1;
+        // Check if the given sequence is already there, if so we need update it instead of
+        // updating mCurIspParamIndex and using next buffer.
+        for (int i = 0; i < ISP_PARAM_QUEUE_SIZE; i++) {
+            if (mIspParameters[i].sequence == settingSequence) {
+                updateIndex = i;
+                break;
+            }
+        }
+
+        if (updateIndex == -1) {
+            mCurIspParamIndex++;
+            mCurIspParamIndex = mCurIspParamIndex % ISP_PARAM_QUEUE_SIZE;
+            updateIndex = mCurIspParamIndex;
+            forceUpdate = true;
+            // Only Store the new sequence
+            LOG2("%s, the sequence list size: %zu", __func__, mSequenceList.size());
+            if (mSequenceList.size() >= PlatformData::getMaxRawDataNum(mCameraId)) {
+                mSequenceList.pop_front();
+            }
+            mSequenceList.push_back(settingSequence);
+            mIspParameters[updateIndex].dataAvailableMap.clear();
+        }
+
+        ipuParam = &(mIspParameters[updateIndex]);
+        ipuParam->sequence = -1;
+        LOG2("%s, current isp parameter index:%d, update index:%d, for sequence: %ld, stream %d",
+              __func__, mCurIspParamIndex, updateIndex, settingSequence, streamId);
+    }
+    CheckError(!ipuParam, UNKNOWN_ERROR, "%s, Failed to get memory for ipuParam", __func__);
+
+    ia_isp_bxt_gdc_limits* mbrData = nullptr;
+    ia_binary_data curIpuParam = {};
+    for (auto& binaryMap : ipuParam->streamIdToDataMap) {
+        if (!(streamId == -1 || binaryMap.first == streamId))
+            continue;
+        curIpuParam = binaryMap.second;
+        curIpuParam.size = mStreamIdToPGOutSizeMap[binaryMap.first];
+        if (mStreamIdToMbrDataMap.find(binaryMap.first) != mStreamIdToMbrDataMap.end())
+            mbrData = &(mStreamIdToMbrDataMap[binaryMap.first]);
+
+        // Update some PAL data to lastest PAL result
+        if (binaryMap.first == VIDEO_STREAM_ID) {
+            updatePalDataForVideoPipe(binaryMap.second);
+        }
+        int ret = runIspAdaptL(mStreamIdToProgramGroupMap[binaryMap.first], mbrData,
+                               ispSettings, settingSequence, &curIpuParam, forceUpdate);
+
+        CheckError(ret != OK, ret, "run isp adaptor error for streamId %d, sequence: %ld",
+                               binaryMap.first, settingSequence);
+        {
+            AutoMutex l(mIpuParamLock);
+            binaryMap.second.size = curIpuParam.size;
+            ipuParam->dataAvailableMap[binaryMap.first] = true;
+            ipuParam->sequence = settingSequence;
+
+            if (binaryMap.first == VIDEO_STREAM_ID) {
+                mLastPalDataForVideoPipe = binaryMap.second;
+            }
+        }
+    }
+
+    return OK;
+}
+
+ia_binary_data* IspParamAdaptor::getIpuParameter(long sequence, int streamId)
+{
+    AutoMutex l(mIpuParamLock);
+
+    /* For old version.
+     * We should get the ipu param according to streamId and
+     * sequenceId when there are multi-streams in one pipe.
+     *
+     * This is only for getting the default ipu parameter
+     */
+    if (sequence == -1 && streamId == -1) {
+        return &(mIspParameters[mCurIspParamIndex].streamIdToDataMap.begin()->second);
+    }
+
+    ia_binary_data* ipuParam = nullptr;
+    for (int i = 0; i < ISP_PARAM_QUEUE_SIZE; i++) {
+        IspParameter& param = mIspParameters[i];
+        if (param.sequence == sequence &&
+            param.streamIdToDataMap.find(streamId) != param.streamIdToDataMap.end() &&
+            param.dataAvailableMap.find(streamId) != param.dataAvailableMap.end()) {
+            ipuParam = &param.streamIdToDataMap[streamId];
+            break;
+        }
+    }
+
+    if (!ipuParam) {
+        LOG1("Failed to find ISP parameter for stream id %d, sequence: %ld", streamId, sequence);
+    }
+
+    return ipuParam;
+}
+
+int IspParamAdaptor::getPalOutputDataSize(const ia_isp_bxt_program_group* programGroup)
+{
+    CheckError(programGroup == nullptr, 0, "Request programGroup is nullptr");
+    return mAdaptor->getPalDataSize(const_cast<ia_isp_bxt_program_group*>(programGroup));
+}
+
+/*
+ * Allocate memory for mIspParameters
+ * TODO: Let PAL to expose the max ia_binary_data buffer size which
+ * come from mIspAdaptHandle->ia_pal.m_output_isp_parameters_size
+ */
+int IspParamAdaptor::allocateIspParamBuffers()
+{
+    releaseIspParamBuffers();
+
+    for (int i = 0; i < ISP_PARAM_QUEUE_SIZE; i++) {
+        for (auto & pgMap : mStreamIdToProgramGroupMap) {
+            ia_binary_data ispParam;
+            int size = mStreamIdToPGOutSizeMap[pgMap.first];
+            CLEAR(ispParam);
+            ispParam.size = size;
+            ispParam.data = mAdaptor->allocatePalBuffer(pgMap.first, i, size);
+            CheckError(ispParam.data == nullptr, NO_MEMORY, "Faile to calloc the memory for isp parameter");
+            mIspParameters[i].streamIdToDataMap[pgMap.first] = ispParam;
+        }
+        mIspParameters[i].sequence = -1;
+        mIspParameters[i].dataAvailableMap.clear();
+    }
+
+    return OK;
+}
+
+void IspParamAdaptor::releaseIspParamBuffers()
+{
+    for (int i = 0; i < ISP_PARAM_QUEUE_SIZE; i++) {
+        for (auto& binaryMap : mIspParameters[i].streamIdToDataMap)
+            mAdaptor->freePalBuffer(binaryMap.second.data);
+
+        mIspParameters[i].sequence = -1;
+        mIspParameters[i].streamIdToDataMap.clear();
+        mIspParameters[i].dataAvailableMap.clear();
+    }
+}
+
+int IspParamAdaptor::runIspAdaptL(ia_isp_bxt_program_group programGroup,
+                                  ia_isp_bxt_gdc_limits *mbrData,
+                                  const IspSettings* ispSettings, long settingSequence,
+                                  ia_binary_data *ipuParam, bool forceUpdate)
+{
+    PERF_CAMERA_ATRACE_IMAGING();
+    AiqResult* aiqResults = const_cast<AiqResult*>(AiqResultStorage::getInstance(mCameraId)->getAiqResult(settingSequence));
+    if (aiqResults == nullptr) {
+        LOGW("%s: no result for sequence %ld! use the latest instead", __func__, settingSequence);
+        aiqResults = const_cast<AiqResult*>(AiqResultStorage::getInstance(mCameraId)->getAiqResult());
+        CheckError((aiqResults == nullptr), INVALID_OPERATION, "Cannot find available aiq result.");
+    }
+    CheckError((aiqResults->mSaResults.width * aiqResults->mSaResults.height == 0),
+            INVALID_OPERATION, "No invalid aiq result needed to run Generic AIC");
+
+    LOG2("%s: device type: %d", __func__, mPgParamType);
+
+    ia_isp_bxt_input_params_v2 inputParams;
+    ia_view_config_t viewConfig;
+    CLEAR(inputParams);
+    CLEAR(viewConfig);
+
+    // LOCAL_TONEMAP_S
+    bool hasLtm = PlatformData::isLtmEnabled(mCameraId);
+
+    if (hasLtm) {
+        size_t ltmLag = PlatformData::getLtmGainLag(mCameraId);
+        long ltmSequence = settingSequence;
+
+        // Consider there may be skipped frames, so according to the gain lag and current
+        // sequence to find the actual ltm sequence in history list.
+        if (mSequenceList.size() > ltmLag) {
+            size_t index = 0;
+            for(auto iter = mSequenceList.begin(); iter != mSequenceList.end(); iter++) {
+                if (*iter == settingSequence && index >= ltmLag) {
+                    ltmSequence = *(std::prev(iter, ltmLag));
+                    break;
+                }
+                index++;
+            }
+        }
+        ltm_result_t* ltmResult = const_cast<ltm_result_t*>(AiqResultStorage::getInstance(mCameraId)->getLtmResult(ltmSequence));
+        if (ltmResult != nullptr) {
+            LOG2("%s: frame sequence %ld, ltm sequence %ld, actual sequence: %ld",
+                    __func__, settingSequence, ltmSequence, ltmResult->sequence);
+            inputParams.ltm_results = &ltmResult->ltmResults;
+            inputParams.ltm_drc_params = &ltmResult->ltmDrcParams;
+        }
+    }
+    // LOCAL_TONEMAP_E
+
+    // update metadata of runnning kernels
+    if (mPgParamType == PG_PARAM_PSYS_ISA) {
+        for (unsigned int i=0; i<programGroup.kernel_count; i++) {
+            switch (programGroup.run_kernels[i].kernel_uuid) {
+            case ia_pal_uuid_isp_tnr5_21:
+            case ia_pal_uuid_isp_tnr5_22:
+            case ia_pal_uuid_isp_tnr5_25:
+                programGroup.run_kernels[i].metadata[0] = aiqResults->mSequence;
+                LOG2("ia_pal_uuid_isp_tnr5_2x frame count = %d", programGroup.run_kernels[i].metadata[0]);
+                break;
+            case ia_pal_uuid_isp_bxt_ofa_dp:
+            case ia_pal_uuid_isp_bxt_ofa_mp:
+            case ia_pal_uuid_isp_bxt_ofa_ppp:
+                programGroup.run_kernels[i].metadata[2] = aiqResults->mAiqParam.flipMode;
+                LOG2("%s: flip mode set to %d", __func__, programGroup.run_kernels[i].metadata[2]);
+
+                programGroup.run_kernels[i].metadata[3] = aiqResults->mAiqParam.yuvColorRangeMode;
+                LOG2("ofa yuv color range mode %d", programGroup.run_kernels[i].metadata[3]);
+                break;
+            }
+        }
+    }
+
+    // Enable or disable kernels according to environment variables for debug purpose.
+    updateKernelToggles(programGroup);
+
+    inputParams.timestamp = aiqResults->mTimestamp;
+    inputParams.program_group = &programGroup;
+    inputParams.sensor_frame_params = &mFrameParam;
+
+    inputParams.ae_results = &aiqResults->mAeResults;
+    inputParams.gbce_results = &aiqResults->mGbceResults;
+    inputParams.awb_results = &aiqResults->mAwbResults;
+    inputParams.pa_results = &aiqResults->mPaResults;
+    inputParams.sa_results = &aiqResults->mSaResults;
+    inputParams.weight_grid = aiqResults->mAeResults.weight_grid;
+
+    if (aiqResults->mCustomControls.count > 0) {
+        inputParams.custom_controls = &aiqResults->mCustomControls;
+    }
+
+    if (ispSettings) {
+        inputParams.nr_setting = ispSettings->nrSetting;
+        inputParams.ee_setting = ispSettings->eeSetting;
+        LOG2("%s: ISP NR setting, level: %d, strength: %d",
+                __func__, (int)ispSettings->nrSetting.feature_level,
+                (int)ispSettings->nrSetting.strength);
+        inputParams.effects = ispSettings->effects;
+        inputParams.manual_brightness = ispSettings->manualSettings.manualBrightness;
+        inputParams.manual_contrast = ispSettings->manualSettings.manualContrast;
+        inputParams.manual_hue = ispSettings->manualSettings.manualHue;
+        inputParams.manual_saturation = ispSettings->manualSettings.manualSaturation;
+        LOG2("%s: ISP EE setting, level: %d, strength: %d",
+                __func__, ispSettings->eeSetting.feature_level,
+                ispSettings->eeSetting.strength);
+        // INTEL_DVS_S
+        if (ispSettings->videoStabilization) {
+            int dvsType = PlatformData::getDVSType(mCameraId);
+            LOG2("%s: ISP Video Stabilization Mode Enable, dvs type %d", __func__, dvsType);
+            DvsResult* dvsResult = const_cast<DvsResult*>(AiqResultStorage::getInstance(mCameraId)->getDvsResult());
+            if (dvsType == MORPH_TABLE) {
+                inputParams.dvs_morph_table = (dvsResult == nullptr) ? nullptr : &dvsResult->mMorphTable;
+            } else if (dvsType == IMG_TRANS) {
+                inputParams.gdc_transformation = (dvsResult == nullptr) ? nullptr : &dvsResult->mTransformation;
+            }
+        }
+        // INTEL_DVS_E
+
+        inputParams.pal_override = ispSettings->palOverride;
+    }
+
+    if (CameraUtils::isUllPsysPipe(mTuningMode)) {
+        CheckError((aiqResults->mAeResults.exposures[0].exposure == nullptr), BAD_VALUE, "Aiq exposure is NULL.");
+        // The situation that all DG passed to ISP, not sensor.
+        if (!PlatformData::isUsingSensorDigitalGain(mCameraId)) {
+            inputParams.manual_digital_gain = aiqResults->mAeResults.exposures[0].exposure->digital_gain;
+        }
+        // Fine-tune DG passed to ISP if partial ISP DG is needed.
+        if (PlatformData::isUsingIspDigitalGain(mCameraId)) {
+            inputParams.manual_digital_gain = PlatformData::getIspDigitalGain(mCameraId,
+                                aiqResults->mAeResults.exposures[0].exposure->digital_gain);
+        }
+
+        LOG3A("%s: set digital gain for ULL pipe: %f", __func__, inputParams.manual_digital_gain);
+    } else if (CameraUtils::isMultiExposureCase(mTuningMode) &&
+               PlatformData::getSensorGainType(mCameraId) == ISP_DG_AND_SENSOR_DIRECT_AG) {
+        CheckError((aiqResults->mAeResults.exposures[0].exposure == nullptr), BAD_VALUE, "Aiq exposure is NULL.");
+
+        LOG3A("%s: all digital gain is passed to ISP, DG(%ld): %f",
+              __func__, aiqResults->mSequence, aiqResults->mAeResults.exposures[0].exposure->digital_gain);
+        inputParams.manual_digital_gain = aiqResults->mAeResults.exposures[0].exposure->digital_gain;
+    }
+
+    if (forceUpdate) {
+        inputParams.sa_results->lsc_update = true;
+    }
+
+    int ret = OK;
+    {
+        PERF_CAMERA_ATRACE_PARAM1_IMAGING("ia_isp_bxt_run", 1);
+        ret = mAdaptor->runPal(mIspAdaptHandle, &inputParams, ipuParam);
+    }
+    CheckError(ret != OK, UNKNOWN_ERROR, "ISP parameter adaptation has failed %d", ret);
+
+    dumpIspParameter(aiqResults->mSequence, *ipuParam);
+
+    return OK;
+}
+
+void IspParamAdaptor::dumpAfStats(const ia_aiq_af_grid *afGrid, long sequence)
+{
+    if (!afGrid) return;
+
+    if (mPgParamType == PG_PARAM_PSYS_ISA && !CameraDump::isDumpTypeEnable(DUMP_PSYS_AIQ_STAT))
+        return;
+    if (mPgParamType == PG_PARAM_ISYS && !CameraDump::isDumpTypeEnable(DUMP_ISYS_AIQ_STAT))
+        return;
+
+    BinParam_t bParam;
+    bParam.bType = BIN_TYPE_STATISTIC;
+    bParam.mType = mPgParamType == PG_PARAM_PSYS_ISA ? M_PSYS : M_ISYS;
+    bParam.sequence = sequence;
+    bParam.sParam.gridWidth = afGrid->grid_width;
+    bParam.sParam.gridHeight = afGrid->grid_height;
+    bParam.sParam.appendix = "af_stats_filter_response_1";
+    CameraDump::dumpBinary(mCameraId, afGrid->filter_response_1,
+                           afGrid->grid_width * afGrid->grid_height * sizeof(int), &bParam);
+    bParam.sParam.appendix = "af_stats_filter_response_2";
+    CameraDump::dumpBinary(mCameraId, afGrid->filter_response_2,
+                           afGrid->grid_width * afGrid->grid_height * sizeof(int), &bParam);
+}
+
+void IspParamAdaptor::dumpRgbsStats(ia_aiq_rgbs_grid *rgbsGrid, long sequence, unsigned int num)
+{
+    if (rgbsGrid == nullptr) return;
+
+    if (Log::isDebugLevelEnable(CAMERA_DEBUG_LOG_AIQ)) {
+        for (unsigned int i = 0; i < num; i++ ) {
+            rgbs_grid_block *rgbsPtr = rgbsGrid[i].blocks_ptr;
+            int size = rgbsGrid[i].grid_width * rgbsGrid[i].grid_height;
+            // Print out some value to check if it's reasonable
+            for (int j = 100; j < 105 && j < size; j++) {
+                LOG3A("RGBS: [%d]:%d, %d, %d, %d, %d", j, rgbsPtr[j].avg_b, rgbsPtr[j].avg_gb,
+                            rgbsPtr[j].avg_gr, rgbsPtr[j].avg_r, rgbsPtr[j].sat);
+            }
+
+            // Only print last Rgbs Stats's y_mean for validation purpose
+            if (i < num - 1) continue;
+
+            int sumLuma = 0;
+            for (int j = 0; j < size; j++) {
+                sumLuma += (rgbsPtr[j].avg_b + rgbsPtr[j].avg_r + (rgbsPtr[j].avg_gb + rgbsPtr[j].avg_gr) / 2) / 3;
+            }
+            LOG3A("RGB stat grid[%d] %dx%d, y_mean %d", i, rgbsGrid[i].grid_width, rgbsGrid[i].grid_height, sumLuma/size);
+        }
+    }
+
+    if ((mPgParamType == PG_PARAM_PSYS_ISA && CameraDump::isDumpTypeEnable(DUMP_PSYS_AIQ_STAT)) ||
+        (mPgParamType == PG_PARAM_ISYS && CameraDump::isDumpTypeEnable(DUMP_ISYS_AIQ_STAT))) {
+        char name[30];
+        BinParam_t bParam;
+        bParam.bType    = BIN_TYPE_STATISTIC;
+        bParam.mType    = mPgParamType == PG_PARAM_PSYS_ISA ? M_PSYS : M_ISYS;
+        bParam.sequence = sequence;
+        for (unsigned int i = 0; i < num; i++ ) {
+            CLEAR(name);
+            snprintf(name, sizeof(name), "%s_stats_%u_%u",
+                    mPgParamType == PG_PARAM_PSYS_ISA ? "hdr_rgbs" : "rgbs", num, i);
+            bParam.sParam.gridWidth  = rgbsGrid[i].grid_width;
+            bParam.sParam.gridHeight = rgbsGrid[i].grid_height;
+            bParam.sParam.appendix   = name;
+            if (rgbsGrid[i].grid_width != 0 && rgbsGrid[i].grid_height != 0) {
+                CameraDump::dumpBinary(mCameraId, rgbsGrid[i].blocks_ptr,
+                                       rgbsGrid[i].grid_width * rgbsGrid[i].grid_height * sizeof(rgbs_grid_block),
+                                       &bParam);
+            }
+        }
+    }
+}
+
+void IspParamAdaptor::dumpIspParameter(long sequence, ia_binary_data ipuParam) {
+    if (mPgParamType == PG_PARAM_PSYS_ISA && !CameraDump::isDumpTypeEnable(DUMP_PSYS_PAL)) return;
+    if (mPgParamType == PG_PARAM_ISYS && !CameraDump::isDumpTypeEnable(DUMP_ISYS_PAL)) return;
+
+    BinParam_t bParam;
+    bParam.bType    = BIN_TYPE_GENERAL;
+    bParam.mType    = mPgParamType == PG_PARAM_PSYS_ISA ? M_PSYS : M_ISYS;
+    bParam.sequence = sequence;
+    bParam.gParam.appendix = "pal";
+    CameraDump::dumpBinary(mCameraId, ipuParam.data, ipuParam.size, &bParam);
+}
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/IspParamAdaptor.h b/camera/hal/intel/ipu6/src/core/IspParamAdaptor.h
new file mode 100644
index 000000000000..f621cea2d798
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/IspParamAdaptor.h
@@ -0,0 +1,153 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <v4l2_device.h>
+
+#include <memory>
+#include <vector>
+#include <list>
+#include <unordered_map>
+
+#include "iutils/Errors.h"
+#include "CameraBuffer.h"
+#include "CameraTypes.h"
+
+#include "NodeInfo.h"
+
+extern "C" {
+#ifndef CAL_BUILD
+#include "ia_camera/ipu_process_group_wrapper.h"
+#endif
+}
+
+#ifdef ENABLE_SANDBOXING
+#include "modules/sandboxing/client/IntelIspParamAdaptor.h"
+#else
+#include "modules/algowrapper/IntelIspParamAdaptor.h"
+#endif
+
+#include "ia_aiq_types.h"
+#include "ia_isp_bxt_types.h"
+#include "ia_isp_bxt_statistics_types.h"
+#include "ia_isp_bxt.h"
+#include "ia_bcomp_types.h"
+#include "gc/IGraphConfigManager.h"
+#include "IspSettings.h"
+
+namespace icamera {
+
+enum PgParamType {
+    PG_PARAM_VIDEO = 0,
+    PG_PARAM_PSYS_ISA,
+    PG_PARAM_ISYS,
+    PG_PARAM_STILL_4k,
+    PG_PARAM_STILL_8m
+};
+
+/**
+ * This class is for isp parameter converting including:
+ * 1. Convert hw statistics to aiq statistics
+ * 2. Convert aiq result to isa config
+ * 3. Run isp config
+ * 4. Provide p2p handle
+ */
+class IspParamAdaptor {
+public:
+    IspParamAdaptor(int cameraId, PgParamType type);
+    virtual ~IspParamAdaptor();
+
+    int init();
+    int deinit();
+    int configure(const stream_t &stream, ConfigMode configMode, TuningMode tuningMode);
+
+    int getParameters(Parameters& param);
+    int decodeStatsData(TuningMode tuningMode,
+                        std::shared_ptr<CameraBuffer> statsBuffer,
+                        std::shared_ptr<IGraphConfig> graphConfig = nullptr);
+
+    int runIspAdapt(const IspSettings* ispSettings, long settingSequence = -1, int32_t streamId = -1);
+    //Get ISP param from mult-stream ISP param adaptation
+    ia_binary_data* getIpuParameter(long sequence = -1, int streamId = -1);
+    int getPalOutputDataSize(const ia_isp_bxt_program_group* programGroup);
+
+private:
+    DISALLOW_COPY_AND_ASSIGN(IspParamAdaptor);
+
+    int initProgramGroupForAllStreams(ConfigMode configMode);
+    int postConfigure(int width, int height, ia_binary_data *ipuParam);
+    void initInputParams(ia_isp_bxt_input_params_v2 *params, PgParamType type);
+
+    int initIspAdaptHandle(ConfigMode configMode, TuningMode tuningMode);
+    void deinitIspAdaptHandle();
+
+    void updatePalDataForVideoPipe(ia_binary_data dest);
+    int runIspAdaptL(ia_isp_bxt_program_group programGroup, ia_isp_bxt_gdc_limits* mbrData,
+                     const IspSettings* ispSettings, long settingSequence,
+                     ia_binary_data *ipuParam, bool forceUpdate = false);
+
+    //Allocate memory for mIspParameters
+    int allocateIspParamBuffers();
+    //Release memory for mIspParameters
+    void releaseIspParamBuffers();
+
+    // Dumping methods for debugging purposes.
+    void dumpRgbsStats(ia_aiq_rgbs_grid *rgbs_grid, long sequence, unsigned int num = 1);
+    void dumpAfStats(const ia_aiq_af_grid *afGrid, long sequence);
+    void dumpIspParameter(long sequence, ia_binary_data ipuParam);
+    // Enable or disable kernels according to environment variables for debug purpose.
+    void updateKernelToggles(ia_isp_bxt_program_group programGroup);
+
+ private:
+    enum IspAdaptorState {
+        ISP_ADAPTOR_NOT_INIT,
+        ISP_ADAPTOR_INIT,
+        ISP_ADAPTOR_CONFIGURED
+    } mIspAdaptorState;
+
+    int mCameraId;
+    PgParamType mPgParamType;
+    TuningMode mTuningMode;
+
+    ia_isp_bxt   *mIspAdaptHandle;
+    ia_bcomp_results *mBCompResults;
+
+    //Guard for IspParamAdaptor public API
+    Mutex mIspAdaptorLock;
+    std::map<int, ia_isp_bxt_program_group> mStreamIdToProgramGroupMap;
+    std::map<int, int> mStreamIdToPGOutSizeMap;
+    std::map<int, ia_isp_bxt_gdc_limits> mStreamIdToMbrDataMap;
+    ia_aiq_frame_params mFrameParam;
+    static const int ISP_PARAM_QUEUE_SIZE = 10;
+    int mCurIspParamIndex;
+    struct IspParameter {
+        long sequence; // frame sequence id
+        std::map<int, ia_binary_data> streamIdToDataMap; // map from stream id to ia_binary_data
+        std::unordered_map<int32_t, bool> dataAvailableMap;
+    } mIspParameters[ISP_PARAM_QUEUE_SIZE];
+    ia_binary_data mLastPalDataForVideoPipe;
+    int mCallInfoOffset;
+    int mBNLM32Offset;
+    //Guard lock for ipu parameter
+    Mutex mIpuParamLock;
+
+    IGraphConfigManager *mGCM;
+    std::list<long> mSequenceList;  // Store the sequence history in IspParamAdaptor
+    std::unique_ptr<IntelIspParamAdaptor> mAdaptor;
+};
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/IspSettings.h b/camera/hal/intel/ipu6/src/core/IspSettings.h
new file mode 100644
index 000000000000..803c5bc17c86
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/IspSettings.h
@@ -0,0 +1,46 @@
+/*
+ * Copyright (C) 2018-2019 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "ia_isp_types.h"
+#include "iutils/Utils.h"
+#include "Parameters.h"
+
+namespace icamera {
+
+struct IspImageEnhancement{
+    char manualSharpness;
+    char manualBrightness;
+    char manualContrast;
+    char manualHue;
+    char manualSaturation;
+};
+
+struct IspSettings {
+    ia_isp_feature_setting nrSetting;
+    ia_isp_feature_setting eeSetting;
+    ia_isp_effect effects;
+    bool videoStabilization;
+    IspImageEnhancement manualSettings;
+    ia_binary_data* palOverride;
+    float zoom;
+    camera_mount_type_t sensorMountType;
+    IspSettings() { CLEAR(*this); zoom = 1.0f; }
+};
+
+} // namespace icamera
+
diff --git a/camera/hal/intel/ipu6/src/core/LensHw.cpp b/camera/hal/intel/ipu6/src/core/LensHw.cpp
new file mode 100644
index 000000000000..a2588e25a5f2
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/LensHw.cpp
@@ -0,0 +1,159 @@
+/*
+ * Copyright (C) 2016-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "LensHw"
+
+#include "LensHw.h"
+#include "iutils/CameraLog.h"
+#include "V4l2DeviceFactory.h"
+#include "PlatformData.h"
+
+namespace icamera {
+
+LensHw::LensHw(int cameraId):
+    mCameraId(cameraId),
+    mLensSubdev(nullptr),
+    mLastLensPosition(0),
+    mLensMovementStartTime(0)
+{
+    LOG1("@%s", __func__);
+}
+
+LensHw::~LensHw() {
+    LOG1("@%s", __func__);
+}
+
+int LensHw::init()
+{
+    LOG1("@%s", __func__);
+    std::string lensName = PlatformData::getLensName(mCameraId);
+    if (lensName.empty()) {
+        LOG1("%s No Lens for camera id:%d ", __func__, mCameraId);
+        return OK;
+    }
+
+    LOG1("%s camera id:%d lens name:%s", __func__, mCameraId, lensName.c_str());
+    std::string subDevName;
+    CameraUtils::getSubDeviceName(lensName.c_str(), subDevName);
+    if (!subDevName.empty()) {
+        mLensSubdev = V4l2DeviceFactory::getSubDev(mCameraId, subDevName);
+        mLensName=lensName;
+        return OK;
+    }
+
+    LOGW("%s Fail to init lens for camera id:%d lens name:%s", __func__, mCameraId, lensName.c_str());
+    return OK;
+}
+
+/**
+ * focus with absolute value
+ */
+int LensHw::setFocusPosition(int position)
+{
+    CheckError(!mLensSubdev, NO_INIT, "%s: No Lens device inited.", __func__);
+    mLastLensPosition = position;
+
+    struct timespec t = {};
+    clock_gettime(CLOCK_MONOTONIC, &t);
+
+    mLensMovementStartTime = ((long)t.tv_sec) * 1000000 + (long)t.tv_nsec / 1000;
+
+    LOG2("@%s: %d, time %lld", __func__, position, mLensMovementStartTime);
+    return mLensSubdev->SetControl(V4L2_CID_FOCUS_ABSOLUTE, position);
+}
+
+/**
+ * focus with  relative value
+ */
+int LensHw::setFocusStep(int steps)
+{
+    CheckError(!mLensSubdev, NO_INIT, "%s: No Lens device inited.", __func__);
+    LOG2("@%s", __func__);
+    return mLensSubdev->SetControl(V4L2_CID_FOCUS_RELATIVE, steps);
+}
+
+int LensHw::getFocusPosition(int &position)
+{
+    CheckError(!mLensSubdev, NO_INIT, "%s: No Lens device inited.", __func__);
+    LOG2("@%s", __func__);
+    return mLensSubdev->GetControl(V4L2_CID_FOCUS_ABSOLUTE, &position);
+}
+
+int LensHw::getFocusStatus(int & /*status*/)
+{
+    LOG2("@%s", __func__);
+    return OK;
+}
+
+int LensHw::startAutoFocus(void)
+{
+    CheckError(!mLensSubdev, NO_INIT, "%s: No Lens device inited.", __func__);
+    LOG2("@%s", __func__);
+    return mLensSubdev->SetControl(V4L2_CID_AUTO_FOCUS_START, 1);
+}
+
+int LensHw::stopAutoFocus(void)
+{
+    CheckError(!mLensSubdev, NO_INIT, "%s: No Lens device inited.", __func__);
+    LOG2("@%s", __func__);
+    return mLensSubdev->SetControl(V4L2_CID_AUTO_FOCUS_STOP, 0);
+}
+
+int LensHw::getAutoFocusStatus(int &status)
+{
+    CheckError(!mLensSubdev, NO_INIT, "%s: No Lens device inited.", __func__);
+    LOG2("@%s", __func__);
+    return mLensSubdev->GetControl(V4L2_CID_AUTO_FOCUS_STATUS,
+                                    reinterpret_cast<int*>(&status));
+}
+
+int LensHw::setAutoFocusRange(int value)
+{
+    CheckError(!mLensSubdev, NO_INIT, "%s: No Lens device inited.", __func__);
+    LOG2("@%s", __func__);
+    return mLensSubdev->SetControl(V4L2_CID_AUTO_FOCUS_RANGE, value);
+}
+
+int LensHw::getAutoFocusRange(int &value)
+{
+    CheckError(!mLensSubdev, NO_INIT, "%s: No Lens device inited.", __func__);
+    LOG2("@%s", __func__);
+    return mLensSubdev->GetControl(V4L2_CID_AUTO_FOCUS_RANGE, &value);
+}
+
+const char* LensHw::getLensName(void)
+{
+    return mLensName.c_str();
+}
+
+/**
+ * getLatestPosition
+ *
+ * returns the latest position commanded to the lens actuator and when this
+ * was issued.
+ * This method does not query the driver.
+ *
+ * \param: lensPosition[OUT]: lens position last applied
+ * \param: time[OUT]: time in micro seconds when the lens move command was sent.
+ */
+int LensHw::getLatestPosition(int& lensPosition, unsigned long long& time)
+{
+    lensPosition = mLastLensPosition;
+    time = mLensMovementStartTime;
+    return OK;
+}
+
+}   // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/LensHw.h b/camera/hal/intel/ipu6/src/core/LensHw.h
new file mode 100644
index 000000000000..997cc13dcf76
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/LensHw.h
@@ -0,0 +1,63 @@
+/*
+ * Copyright (C) 2016-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <v4l2_device.h>
+
+#include <string>
+
+namespace icamera {
+
+typedef ::cros::V4L2Subdevice V4L2Subdevice;
+
+/**
+ * \class LensHw
+ * This class adds the methods that are needed
+ * to drive the camera lens using v4l2 commands and custom ioctl.
+ *
+ */
+class LensHw {
+
+public:
+    LensHw(int cameraId);
+    ~LensHw();
+
+    int init();
+
+    const char* getLensName(void);
+
+    int setFocusPosition(int position);
+    int setFocusStep(int steps);
+    int getFocusPosition(int &position);
+    int getFocusStatus(int &status);
+    int startAutoFocus(void);
+    int stopAutoFocus(void);
+    int getAutoFocusStatus(int &status);
+    int setAutoFocusRange(int value);
+    int getAutoFocusRange(int &value);
+    int getLatestPosition(int& lensPosition, unsigned long long& time);
+    bool isLensSubdevAvailable() { return (mLensSubdev != nullptr); }
+
+private:
+    int mCameraId;
+    V4L2Subdevice* mLensSubdev;
+    std::string mLensName;
+    int mLastLensPosition;
+    unsigned long long mLensMovementStartTime; /*!< In microseconds */
+};  // class LensHW
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/PSysProcessor.cpp b/camera/hal/intel/ipu6/src/core/PSysProcessor.cpp
new file mode 100644
index 000000000000..7640f9a041ea
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/PSysProcessor.cpp
@@ -0,0 +1,903 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "PSysProcessor"
+
+#include <utility>
+
+#include "iutils/Utils.h"
+#include "iutils/CameraLog.h"
+#include "iutils/CameraDump.h"
+#include "iutils/SwImageConverter.h"
+
+#include "PlatformData.h"
+#include "3a/AiqResultStorage.h"
+#include "ParameterGenerator.h"
+
+#include "PSysProcessor.h"
+
+/*
+ * The sof event time margin is a tunning value
+ * it's based on sensor vblank, psys iterating time
+ * and thread scheduling
+ */
+#define SOF_EVENT_MARGIN (5000000)  // 5ms
+#define SOF_EVENT_MAX_MARGIN (60000000)  // 60ms
+
+using std::shared_ptr;
+using std::unique_ptr;
+
+namespace icamera {
+PSysProcessor::PSysProcessor(int cameraId, ParameterGenerator *pGenerator) :
+        mCameraId(cameraId),
+        mParameterGenerator(pGenerator),
+        mCurConfigMode(CAMERA_STREAM_CONFIGURATION_MODE_NORMAL),
+        mTuningMode(TUNING_MODE_MAX),
+        mRawPort(INVALID_PORT),
+        mSofSequence(-1),
+        mOpaqueRawPort(INVALID_PORT),
+        mStatus(PIPELINE_UNCREATED)
+{
+    LOG1("@%s camera id:%d", __func__, mCameraId);
+
+    mProcessThread = new ProcessThread(this);
+    CLEAR(mSofTimestamp);
+}
+
+PSysProcessor::~PSysProcessor()
+{
+    LOG1("@%s ", __func__);
+
+    mProcessThread->join();
+    delete mProcessThread;
+}
+
+int PSysProcessor::configure(const std::vector<ConfigMode>& configModes)
+{
+    //Create PSysDAGs actually
+    LOG1("@%s ", __func__);
+    CheckError(mStatus == PIPELINE_CREATED, -1, "@%s mStatus is in wrong status: PIPELINE_CREATED", __func__);
+    mConfigModes = configModes;
+    mSofSequence = -1;
+
+    std::map<Port, stream_t> outputFrameInfo;
+    // Check if it's required to output raw image from ISYS
+    for (auto &outFrameInfo : mOutputFrameInfo) {
+        if (outFrameInfo.second.format == V4L2_PIX_FMT_SGRBG12) {
+            mRawPort = outFrameInfo.first;
+        } else if (outFrameInfo.second.usage == CAMERA_STREAM_OPAQUE_RAW) {
+            mOpaqueRawPort = outFrameInfo.first;
+        } else {
+            outputFrameInfo[outFrameInfo.first] = outFrameInfo.second;
+        }
+    }
+
+    int ret = OK;
+    //Create PSysDAG according to real configure mode
+    for (auto &cfg : mConfigModes) {
+        if (mPSysDAGs.find(cfg) != mPSysDAGs.end()) {
+            continue;
+        }
+
+        TuningConfig tuningConfig;
+        ret = PlatformData::getTuningConfigByConfigMode(mCameraId, cfg, tuningConfig);
+        CheckError(ret != OK, ret, "%s: can't get config for mode %d", __func__, cfg);
+
+        LOG1("Create PSysDAG for ConfigMode %d", cfg);
+        unique_ptr<PSysDAG> pSysDAG = unique_ptr<PSysDAG>(new PSysDAG(mCameraId, this));
+
+        pSysDAG->setFrameInfo(mInputFrameInfo, outputFrameInfo);
+        ret = pSysDAG->configure(tuningConfig.configMode, tuningConfig.tuningMode);
+        CheckError(ret != OK, ret, "@%s configure psys dag failed:%d", __func__, ret);
+
+        mPSysDAGs[tuningConfig.configMode] = std::move(pSysDAG);
+
+        //Update default active config mode
+        mCurConfigMode = tuningConfig.configMode;
+        mTuningMode = tuningConfig.tuningMode;
+    }
+
+    if (ret == OK) mStatus = PIPELINE_CREATED;
+    return ret;
+
+}
+
+int PSysProcessor::registerUserOutputBufs(Port port, const shared_ptr<CameraBuffer> &camBuffer)
+{
+    for (auto &psysDAGPair : mPSysDAGs) {
+        if (!psysDAGPair.second) continue;
+        int ret = psysDAGPair.second->registerUserOutputBufs(port, camBuffer);
+        CheckError(ret != OK, BAD_VALUE, "%s, register user buffer failed, ret: %d", __func__, ret);
+    }
+
+    return OK;
+}
+
+int PSysProcessor::start()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("%s", __func__);
+    AutoMutex   l(mBufferQueueLock);
+    int rawBufferNum = mOpaqueRawPort != INVALID_PORT ? PlatformData::getMaxRawDataNum(mCameraId) :
+                       PlatformData::getPreferredBufQSize(mCameraId);
+
+    /* Should use MIN_BUFFER_COUNT to optimize frame latency when PSYS processing
+     * time is slower than ISYS
+     */
+    bool needProducerBuffer = PlatformData::isIsysEnabled(mCameraId);
+
+    if (needProducerBuffer) {
+        int ret = allocProducerBuffers(mCameraId, rawBufferNum);
+        CheckError(ret != OK, NO_MEMORY, "Allocating producer buffer failed:%d", ret);
+    }
+
+    mThreadRunning = true;
+    mProcessThread->run("PsysProcessor", PRIORITY_NORMAL);
+    for (auto &psysDAGPair : mPSysDAGs) {
+        if (!psysDAGPair.second) continue;
+        psysDAGPair.second->start();
+        if (needProducerBuffer && PlatformData::isNeedToPreRegisterBuffer(mCameraId)) {
+            psysDAGPair.second->registerInternalBufs(mInternalBuffers);
+        }
+    }
+
+    return OK;
+}
+
+void PSysProcessor::stop()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("%s", __func__);
+
+    for (auto &psysDAGPair : mPSysDAGs) {
+        if (!psysDAGPair.second) continue;
+        psysDAGPair.second->stop();
+    }
+
+    mProcessThread->requestExit();
+    {
+        AutoMutex l(mBufferQueueLock);
+        mThreadRunning = false;
+        //Wakeup the thread to exit
+        mFrameAvailableSignal.signal();
+        mOutputAvailableSignal.signal();
+        mFrameDoneSignal.signal();
+        AutoMutex lMeta(mMetaQueueLock);
+        mMetaAvailableSignal.signal();
+    }
+    {
+        AutoMutex l(mSofLock);
+        mSofCondition.signal();
+    }
+
+    mProcessThread->requestExitAndWait();
+
+    // Thread is not running. It is safe to clear the Queue
+    clearBufferQueues();
+}
+
+int PSysProcessor::setParameters(const Parameters& param)
+{
+    LOG1("%s camera id:%d", __func__, mCameraId);
+    // Process image enhancement related settings.
+    camera_image_enhancement_t enhancement;
+    int ret = param.getImageEnhancement(enhancement);
+    AutoWMutex wl(mIspSettingsLock);
+    if (ret == OK) {
+        mIspSettings.manualSettings.manualSharpness = (char)enhancement.sharpness;
+        mIspSettings.manualSettings.manualBrightness = (char)enhancement.brightness;
+        mIspSettings.manualSettings.manualContrast = (char)enhancement.contrast;
+        mIspSettings.manualSettings.manualHue = (char)enhancement.hue;
+        mIspSettings.manualSettings.manualSaturation = (char)enhancement.saturation;
+
+        // TODO: need to consider how to add feature level from user setting.
+        mIspSettings.eeSetting.feature_level = ia_isp_feature_level_low;
+        mIspSettings.eeSetting.strength = enhancement.sharpness;
+    } else {
+        mIspSettings.eeSetting.feature_level = ia_isp_feature_level_high;
+        mIspSettings.eeSetting.strength = 0;
+    }
+
+    camera_nr_mode_t manualNrMode;
+    camera_nr_level_t manualNrLevel;
+
+    int manualNrModeSet = param.getNrMode(manualNrMode);
+    int manualNrLevelSet = param.getNrLevel(manualNrLevel);
+
+    if (manualNrModeSet == OK) {
+        LOG2("%s: manual NR mode set: %d", __func__, manualNrMode);
+        switch (manualNrMode) {
+            case NR_MODE_OFF:
+                mIspSettings.nrSetting.feature_level = ia_isp_feature_level_off;
+                break;
+            case NR_MODE_AUTO:
+                mIspSettings.nrSetting.feature_level = ia_isp_feature_level_low;
+                break;
+            case NR_MODE_MANUAL_NORMAL:
+                mIspSettings.nrSetting.feature_level = ia_isp_feature_level_low;
+                break;
+            case NR_MODE_MANUAL_EXPERT:
+                mIspSettings.nrSetting.feature_level = ia_isp_feature_level_high;
+                break;
+            default:
+                mIspSettings.nrSetting.feature_level = ia_isp_feature_level_low;
+        }
+
+    } else {
+        LOG2("%s: manual NR mode not set, default enabled", __func__);
+        mIspSettings.nrSetting.feature_level = ia_isp_feature_level_high;
+    }
+
+    if (manualNrLevelSet == OK) {
+        LOG2("%s: manual NR level set: %d", __func__, manualNrLevel.overall);
+        mIspSettings.nrSetting.strength = (char)manualNrLevel.overall;
+    } else {
+        LOG2("%s: manual NR level not set, default used", __func__);
+        mIspSettings.nrSetting.strength = (char)0;
+    }
+
+    LOG2("%s: ISP NR setting, level: %d, strength: %d",
+            __func__, (int)mIspSettings.nrSetting.feature_level,
+            (int)mIspSettings.nrSetting.strength);
+
+    camera_video_stabilization_mode_t stabilizationMode;
+    ret = param.getVideoStabilizationMode(stabilizationMode);
+    if (ret == OK) {
+         mIspSettings.videoStabilization = (stabilizationMode == VIDEO_STABILIZATION_MODE_ON);
+    } else {
+         mIspSettings.videoStabilization = false;
+    }
+    LOG2("%s: Video stablilization enabled:%d", __func__, mIspSettings.videoStabilization);
+
+    return ret;
+}
+
+int PSysProcessor::getParameters(Parameters& param)
+{
+    LOG1("@%s ", __func__);
+    AutoRMutex rl(mIspSettingsLock);
+    camera_image_enhancement_t enhancement = { mIspSettings.manualSettings.manualSharpness,
+                                               mIspSettings.manualSettings.manualBrightness,
+                                               mIspSettings.manualSettings.manualContrast,
+                                               mIspSettings.manualSettings.manualHue,
+                                               mIspSettings.manualSettings.manualSaturation };
+    int ret = param.setImageEnhancement(enhancement);
+
+    ret |= mPSysDAGs[mCurConfigMode]->getParameters(param);
+
+    return ret;
+}
+
+/**
+ * Get available setting sequence from outBuf
+ */
+long PSysProcessor::getSettingSequence(const CameraBufferPortMap &outBuf)
+{
+    long settingSequence = -1;
+    for (auto& output: outBuf) {
+        if (output.second) {
+            settingSequence = output.second->getSettingSequence();
+            break;
+        }
+    }
+    return settingSequence;
+}
+
+/**
+ * Check if the input frame should be skipped
+ *
+ * If the corresponding mSkip of AiqResult gotten from sequence is true,
+ * return true; otherwise return false.
+ */
+bool PSysProcessor::needSkipOutputFrame(long sequence)
+{
+    // Check if need to skip output frame
+    const AiqResult* aiqResults = AiqResultStorage::getInstance(mCameraId)->getAiqResult(sequence);
+    if (aiqResults != nullptr && aiqResults->mSkip) {
+        LOG1("%s, sequence %ld", __func__, sequence);
+        return true;
+    }
+    return false;
+}
+
+/**
+ * Check if 'inBuffer' can be used for 'settingSequence' to run PSys pipe.
+ *
+ * If 'settingSequence' is -1, it means the output buffer doesn't require particular
+ * input buffer, so it can run the pipe.
+ * If 'inputSequence' larger than 'settingSequence', the pipeline needs to
+ * run as well, otherwise the pipe doesn't need to run and this input buffer needs to
+ * be skipped.
+ */
+bool PSysProcessor::needExecutePipe(long settingSequence, long inputSequence)
+{
+    if (settingSequence == -1 || inputSequence >= settingSequence) {
+        return true;
+    }
+
+    return false;
+}
+
+/**
+ * Check if the input buffer need to be reused
+ *
+ * If 'settingSequence' is -1, it means the output buffer doesn't require particular
+ * input buffer, so the input buffer doesn't need to be reused.
+ * If 'inputSequence' larger than 'settingSequence', means the input buffer
+ * may be required by following output buffer, so it may be reused later.
+ */
+bool PSysProcessor::needHoldOnInputFrame(long settingSequence, long inputSequence)
+{
+    if (settingSequence == -1 || inputSequence <= settingSequence) {
+        return false;
+    }
+
+    return true;
+}
+
+/**
+ * Check if pipe needs to be switched according to AIQ result.
+ */
+bool PSysProcessor::needSwitchPipe(long sequence)
+{
+    const AiqResult* aiqResults = AiqResultStorage::getInstance(mCameraId)->getAiqResult(sequence);
+    if (aiqResults == nullptr) {
+        LOG2("%s: not found sequence %ld in AiqResultStorage, no update for active modes",
+            __func__, sequence);
+        return false;
+    }
+
+    TuningMode curTuningMode = aiqResults->mTuningMode;
+    LOG2("%s: aiqResults->mTuningMode = %d", __func__, curTuningMode);
+
+    if (mTuningMode == curTuningMode) {
+        return false;
+    }
+
+    for (auto cfg : mConfigModes) {
+        TuningMode tMode;
+        int ret = PlatformData::getTuningModeByConfigMode(mCameraId, cfg, tMode);
+        if (ret == OK && tMode == curTuningMode) {
+            mCurConfigMode = cfg;
+            mTuningMode = curTuningMode;
+            return true;
+        }
+    }
+    return false;
+}
+
+void PSysProcessor::handleEvent(EventData eventData)
+{
+    LOG2("%s: got event type %d", __func__, eventData.type);
+    // Process registered events
+    switch (eventData.type) {
+        case EVENT_ISYS_SOF:
+            {
+                AutoMutex l(mSofLock);
+
+                mSofSequence = eventData.data.sync.sequence;
+                gettimeofday(&mSofTimestamp, nullptr);
+                LOG2("%s, received SOF event sequence: %ld, timestamp: %ld",
+                     __func__, eventData.data.sync.sequence, TIMEVAL2USECS(mSofTimestamp));
+                mSofCondition.signal();
+            }
+            break;
+        default:
+            LOGW("Unexpected event: %d", eventData.type);
+            break;
+    }
+}
+
+// PSysProcessor ThreadLoop
+int PSysProcessor::processNewFrame() {
+    PERF_CAMERA_ATRACE();
+    LOG2("@%s, mCameraId:%d", __func__, mCameraId);
+
+    CheckError(!mBufferProducer, INVALID_OPERATION, "No available producer");
+
+    int ret = OK;
+    CameraBufferPortMap srcBuffers, dstBuffers;
+
+    if (!PlatformData::psysAlignWithSof(mCameraId)) {
+        {
+            ConditionLock lock(mBufferQueueLock);
+            ret = waitFreeBuffersInQueue(lock, srcBuffers, dstBuffers);
+            // Already stopped
+            if (!mThreadRunning) return -1;
+
+            // Wait frame buffer time out should not involve thread exit.
+            if (ret != OK) {
+                LOG1("%s, cameraId: %d timeout happen, wait recovery", __func__, mCameraId);
+                return OK;
+            }
+        }
+
+        ret = prepareTask(&srcBuffers, &dstBuffers);
+        CheckError(ret != OK, UNKNOWN_ERROR, "%s, Failed to process frame", __func__);
+    } else {
+        {
+            ConditionLock lock(mSofLock);
+
+            timeval curTime;
+            gettimeofday(&curTime, nullptr);
+            int64_t sofInterval = TIMEVAL2NSECS(curTime) - TIMEVAL2NSECS(mSofTimestamp);
+
+            // Wait next sof event when missing last one for a long time
+            if (sofInterval > SOF_EVENT_MARGIN && sofInterval < SOF_EVENT_MAX_MARGIN) {
+                LOG2("%s, need to wait next sof event. sofInterval: %ld", __func__, sofInterval);
+                ret = mSofCondition.waitRelative(lock, kWaitDuration * SLOWLY_MULTIPLIER);
+
+                // Already stopped
+                if (!mThreadRunning) return -1;
+
+                // Wait sof event time out should not involve thread exit.
+                if (ret != OK) {
+                    LOG1("%s, cameraId: %d wait sof event timeout, recovery", __func__, mCameraId);
+                    return OK;
+                }
+            }
+        }
+
+        // push all the pending buffers to task
+        while (true) {
+            {
+                ConditionLock lock(mBufferQueueLock);
+                ret = waitFreeBuffersInQueue(lock, srcBuffers, dstBuffers, SOF_EVENT_MARGIN);
+
+                // Return to wait next sof event if there isn't pending buffer.
+                if (ret != OK) {
+                    LOG1("%s, cameraId: %d, there isn't pending buffer, recovery",
+                         __func__, mCameraId);
+                    return OK;
+                }
+            }
+
+            {
+                AutoMutex l(mSofLock);
+                if (srcBuffers.begin()->second->getSequence() >= mSofSequence) {
+                    LOG2("%s, run the frame in next sof: buffer sequence: %ld, sof sequence: %ld",
+                         __func__, srcBuffers.begin()->second->getSequence(), mSofSequence);
+                    return OK;
+                }
+            }
+
+            ret = prepareTask(&srcBuffers, &dstBuffers);
+            CheckError(ret != OK, UNKNOWN_ERROR, "%s, Failed to process frame", __func__);
+        }
+    }
+
+    return OK;
+}
+
+void PSysProcessor::handleRawReprocessing(CameraBufferPortMap *srcBuffers,
+                                          CameraBufferPortMap *dstBuffers, bool *hasYuv,
+                                          bool *hasRawOutput, bool *hasRawInput)
+{
+    std::shared_ptr<CameraBuffer> rawOutputBuffer = nullptr;
+    long settingSequence = -1;
+
+    for (const auto& item : *dstBuffers) {
+        if (item.second) {
+            if (item.second->getStreamUsage() == CAMERA_STREAM_OPAQUE_RAW) {
+                rawOutputBuffer = item.second;
+            } else {
+                *hasYuv = true;
+            }
+            if (item.second->getSettingSequence() >= 0) {
+                settingSequence = item.second->getSettingSequence();
+            }
+        }
+    }
+
+    Port defaultPort = srcBuffers->begin()->first;
+    shared_ptr<CameraBuffer> mainBuf = (*srcBuffers)[defaultPort];
+    long inputSequence = mainBuf->getSequence();
+
+    if (rawOutputBuffer) {
+        if (!needExecutePipe(settingSequence, inputSequence)) {
+            LOG2("%s, inputSequence %ld is smaller than settingSequence %ld, skip sensor frame.",
+                 __func__, inputSequence, settingSequence);
+            return;
+        }
+        // Return opaque RAW data
+        uint64_t timestamp = TIMEVAL2NSECS(mainBuf->getTimestamp());
+        sensor_raw_info_t opaqueRawInfo = { inputSequence, timestamp };
+
+        rawOutputBuffer->updateV4l2Buffer(*mainBuf->getV4L2Buffer().Get());
+
+        MEMCPY_S(rawOutputBuffer->getBufferAddr(), rawOutputBuffer->getBufferSize(),
+                 &opaqueRawInfo, sizeof(opaqueRawInfo));
+        LOG2("%s, timestamp %ld, inputSequence %ld, dstBufferSize %d, addr %p", __func__,
+              timestamp, inputSequence, rawOutputBuffer->getBufferSize(),
+              rawOutputBuffer->getBufferAddr());
+
+        // Return opaque RAW buffer
+        for (auto &it : mBufferConsumerList) {
+            it->onFrameAvailable(mOpaqueRawPort, rawOutputBuffer);
+        }
+        *hasRawOutput = true;
+
+        PlatformData::updateMakernoteTimeStamp(mCameraId, settingSequence, timestamp);
+
+        if (!(*hasYuv)) {
+            EventData frameData;
+            frameData.type = EVENT_PSYS_FRAME;
+            frameData.buffer = nullptr;
+            frameData.data.frame.sequence = rawOutputBuffer->getSequence();
+            frameData.data.frame.timestamp.tv_sec = rawOutputBuffer->getTimestamp().tv_sec;
+            frameData.data.frame.timestamp.tv_usec = rawOutputBuffer->getTimestamp().tv_usec;;
+            notifyListeners(frameData);
+            LOG2("%s, frame done for sequence: %ld", __func__, frameData.data.frame.sequence);
+        }
+
+        // Remove from dstBuffers map
+        dstBuffers->erase(mOpaqueRawPort);
+
+        // Save buffer into mRawQueue
+        CameraBufferPortMap mapBuf;
+        for (const auto& src : *srcBuffers) {
+            mapBuf[src.first] = src.second;
+        }
+
+        AutoMutex lock(mRawBufferMapLock);
+        mRawBufferMap[inputSequence] = mapBuf;
+    } else if (settingSequence != -1 && inputSequence > settingSequence) {
+        AutoMutex lock(mRawBufferMapLock);
+        // Find Raw buffer in mRawBufferMap
+        if (mRawBufferMap.find(settingSequence) != mRawBufferMap.end()) {
+            CameraBufferPortMap &mapBuf = mRawBufferMap[settingSequence];
+            // Update source buffers
+            for (const auto& bufPortMap : mapBuf) {
+                (*srcBuffers)[bufPortMap.first] = bufPortMap.second;
+            }
+            *hasRawInput = true;
+        }
+    }
+
+    LOG2("%s, hasRawInput %d, hasRawOutput %d, hasYuv %d, settingSequence %ld, inputSequence %ld",
+          __func__, *hasRawInput, *hasRawOutput, *hasYuv, settingSequence, inputSequence);
+}
+
+bool PSysProcessor::isBufferHoldForRawReprocess(long sequence)
+{
+    if (mOpaqueRawPort == INVALID_PORT) return false;
+
+    AutoMutex lock(mRawBufferMapLock);
+    if (mRawBufferMap.find(sequence) == mRawBufferMap.end()) return false;
+
+    // If too many buffers are holden in mRawQueue, return back to producer
+    if (mRawBufferMap.size() > (PlatformData::getMaxRawDataNum(mCameraId) -
+                                PlatformData::getMaxRequestsInflight(mCameraId))) {
+        std::map<long, CameraBufferPortMap>::iterator it = mRawBufferMap.begin();
+        CameraBufferPortMap &bufferPortMap = it->second;
+        for (auto &item : bufferPortMap) {
+            mBufferProducer->qbuf(item.first, item.second);
+        }
+        mRawBufferMap.erase(mRawBufferMap.begin());
+    }
+    return true;
+}
+
+status_t PSysProcessor::prepareTask(CameraBufferPortMap *srcBuffers,
+                                    CameraBufferPortMap *dstBuffers) {
+    CheckError(srcBuffers->empty() || dstBuffers->empty(),
+               UNKNOWN_ERROR, "%s, the input or output buffer is empty", __func__);
+
+    bool hasYuv = false;
+    bool hasRawOutput = false;
+    bool hasRawInput = false;
+    if (mOpaqueRawPort != INVALID_PORT) {
+        handleRawReprocessing(srcBuffers, dstBuffers, &hasYuv, &hasRawOutput, &hasRawInput);
+        if (hasRawOutput && !hasYuv) {
+            // If no YUV is needed, only RAW output requests
+            AutoMutex l(mBufferQueueLock);
+            for (auto& input: mInputQueue) {
+                input.second.pop();
+            }
+            for (auto& output: mOutputQueue) {
+                output.second.pop();
+            }
+            return OK;
+        }
+    }
+
+    Port defaultPort = srcBuffers->begin()->first;
+    shared_ptr<CameraBuffer> mainBuf = (*srcBuffers)[defaultPort];
+    long inputSequence = mainBuf->getSequence();
+    TRACE_LOG_POINT("PSysProcessor", "input output buffer ready", MAKE_COLOR(inputSequence),
+                    inputSequence);
+    uint64_t timestamp = TIMEVAL2NSECS(mainBuf->getTimestamp());
+    LOG2("%s: input buffer sequence %ld timestamp %ld", __func__, inputSequence, timestamp);
+
+    // Output raw image
+    if (mRawPort != INVALID_PORT) {
+        shared_ptr<CameraBuffer> dstBuf = nullptr;
+
+        // Get output buffer and remove it from dstBuffers
+        for (auto &buffer : *dstBuffers) {
+            if (buffer.first == mRawPort) {
+                dstBuf = buffer.second;
+                CheckError(!dstBuf, UNKNOWN_ERROR, "%s, dstBuf for output raw is null", __func__);
+                dstBuf->updateV4l2Buffer(*mainBuf->getV4L2Buffer().Get());
+                dstBuffers->erase(mRawPort);
+                break;
+            }
+        }
+        outputRawImage(mainBuf, dstBuf);
+    }
+
+    long settingSequence = getSettingSequence(*dstBuffers);
+    bool needRunPipe = needExecutePipe(settingSequence, inputSequence);
+    bool holdOnInput = needHoldOnInputFrame(settingSequence, inputSequence);
+    LOG2("%s: dst sequence = %ld, src sequence = %ld, needRunPipe = %d, needReuseInput = %d",
+         __func__, settingSequence, inputSequence, needRunPipe, holdOnInput);
+
+    {
+        AutoMutex l(mBufferQueueLock);
+        if (needRunPipe && !needSkipOutputFrame(inputSequence)) {
+            for (auto& output: mOutputQueue) {
+                output.second.pop();
+            }
+        }
+
+        // If input buffer will be used later, don't pop it from the queue.
+        if (!holdOnInput && !hasRawInput) {
+            for (auto& input: mInputQueue) {
+                input.second.pop();
+            }
+        }
+    }
+
+    if (needRunPipe) {
+        // Raw output already has been returned back, and don't need to handle again.
+        if (!hasRawOutput) {
+            for (const auto& output : *dstBuffers) {
+                if (output.second && output.second->getUsage() != BUFFER_USAGE_PSYS_INTERNAL) {
+                    EventData event;
+                    event.type = EVENT_PSYS_REQUEST_BUF_READY;
+                    event.buffer = nullptr;
+                    event.data.requestReady.timestamp = timestamp;
+                    event.data.requestReady.sequence = settingSequence;
+                    notifyListeners(event);
+                    break;
+                }
+            }
+        }
+        dispatchTask(*srcBuffers, *dstBuffers);
+    } else if (!holdOnInput && !hasRawOutput) {
+        for (const auto& src : *srcBuffers) {
+            mBufferProducer->qbuf(src.first, src.second);
+        }
+    }
+
+    return OK;
+}
+
+void PSysProcessor::dispatchTask(CameraBufferPortMap &inBuf, CameraBufferPortMap &outBuf)
+{
+    LOG2("@%s, mCameraId:%d", __func__, mCameraId);
+
+    long currentSequence = inBuf.begin()->second->getSequence();
+    TRACE_LOG_POINT("PSysProcessor", "start run PSYS", MAKE_COLOR(currentSequence),
+                    currentSequence);
+
+    {
+        ConditionLock lock(mBufferQueueLock);
+
+        ConfigMode previousMode = mCurConfigMode;
+        bool needSwitch = needSwitchPipe(currentSequence);
+
+        if (needSwitch) {
+            LOG1("Switch pipe for sequence:%ld, unprocessed buffer number:%zu",
+                  currentSequence, mSequenceInflight.size());
+
+            // Deactive the PSysDag which is no longer used.
+            mPSysDAGs[previousMode]->pause();
+
+            // Before switching, need to wait all buffers in current pipe being processed.
+            while (!mSequenceInflight.empty()) {
+                int ret = mFrameDoneSignal.waitRelative(lock, kWaitDuration * SLOWLY_MULTIPLIER);
+                if (!mThreadRunning) {
+                    LOG1("@%s: Processor is not active while waiting for frame done.", __func__);
+                    return;
+                }
+
+                if (ret == TIMED_OUT) {
+                    LOGE("Waiting for frame done event timeout");
+                    return;
+                }
+            }
+
+            // Activate the current used PSysDag.
+            mPSysDAGs[mCurConfigMode]->resume();
+        }
+        mSequenceInflight.push(currentSequence);
+    } // End of lock mBufferQueueLock
+
+    // Prepare the task input paramerters including input and output buffers, settings etc.
+    PSysTaskData taskParam;
+    taskParam.mTuningMode = mTuningMode;
+    taskParam.mInputBuffers = inBuf;
+    taskParam.mOutputBuffers = outBuf;
+
+    long settingSequence = getSettingSequence(outBuf);
+    // Handle per-frame settings if output buffer requires
+    if (settingSequence > -1 && mParameterGenerator) {
+        Parameters params;
+        if (mParameterGenerator->getParameters(currentSequence, &params) == OK) {
+            setParameters(params);
+        }
+    }
+    {
+        AutoRMutex rl(mIspSettingsLock);
+        mIspSettings.palOverride = nullptr;
+        taskParam.mIspSettings = mIspSettings;
+    }
+
+    if (!mThreadRunning) return;
+
+    mPSysDAGs[mCurConfigMode]->addTask(taskParam);
+}
+
+void PSysProcessor::registerListener(EventType eventType, EventListener* eventListener)
+{
+    // Only delegate stats event registration to deeper layer DAG and PipeExecutor
+    if ((eventType != EVENT_PSYS_STATS_BUF_READY) && (eventType != EVENT_PSYS_STATS_SIS_BUF_READY)) {
+        BufferQueue::registerListener(eventType, eventListener);
+        return;
+    }
+
+    for (auto const& realModeDAGPair: mPSysDAGs) {
+        realModeDAGPair.second->registerListener(eventType, eventListener);
+    }
+}
+
+void PSysProcessor::removeListener(EventType eventType, EventListener* eventListener)
+{
+    // Only delegate stats event unregistration to deeper layer DAG and PipeExecutor
+    if ((eventType != EVENT_PSYS_STATS_BUF_READY) && (eventType != EVENT_PSYS_STATS_SIS_BUF_READY)) {
+        BufferQueue::removeListener(eventType, eventListener);
+        return;
+    }
+
+    for (auto const& realModeDAGPair: mPSysDAGs) {
+        realModeDAGPair.second->removeListener(eventType, eventListener);
+    }
+}
+
+void PSysProcessor::onBufferDone(int64_t sequence, Port port,
+                                 const std::shared_ptr<CameraBuffer> &camBuffer)
+{
+    LOG2("@%s, mCameraId:%d, sequence %ld, port %d", __func__, mCameraId, sequence, port);
+
+    if (CameraDump::isDumpTypeEnable(DUMP_PSYS_OUTPUT_BUFFER)) {
+        CameraDump::dumpImage(mCameraId, camBuffer, M_PSYS, port);
+    }
+
+    if (!needSkipOutputFrame(sequence)) {
+        for (auto &it : mBufferConsumerList) {
+            it->onFrameAvailable(port, camBuffer);
+        }
+    }
+}
+
+void PSysProcessor::onFrameDone(const PSysTaskData& result)
+{
+    PERF_CAMERA_ATRACE();
+    LOG2("@%s, mCameraId:%d", __func__, mCameraId);
+
+    EventDataFrame eventDataFrame;
+    CLEAR(eventDataFrame);
+    eventDataFrame.sequence = -1;
+
+    long sequence = result.mInputBuffers.begin()->second->getSequence();
+    TRACE_LOG_POINT("PSysProcessor", __func__, MAKE_COLOR(sequence), sequence);
+
+    for (auto& dst : result.mOutputBuffers) {
+        shared_ptr<CameraBuffer> outBuf = dst.second;
+        // If the output buffer is nullptr, that means user doesn't request that buffer,
+        // so it doesn't need to be handled here.
+        if (!outBuf) {
+            continue;
+        }
+
+        if (!needSkipOutputFrame(sequence)) {
+            eventDataFrame.sequence = outBuf->getSequence();
+            eventDataFrame.timestamp.tv_sec = outBuf->getTimestamp().tv_sec;
+            eventDataFrame.timestamp.tv_usec = outBuf->getTimestamp().tv_usec;
+            break;
+        } else {
+            LOG1("Frame %ld is being skipped.", sequence);
+        }
+    }
+
+    if (eventDataFrame.sequence >= 0) {
+        EventData frameData;
+        frameData.type = EVENT_PSYS_FRAME;
+        frameData.buffer = nullptr;
+        frameData.data.frame.sequence = eventDataFrame.sequence;
+        frameData.data.frame.timestamp.tv_sec = eventDataFrame.timestamp.tv_sec;
+        frameData.data.frame.timestamp.tv_usec = eventDataFrame.timestamp.tv_usec;
+        notifyListeners(frameData);
+        LOG2("%s, frame done for sequence: %ld", __func__, sequence);
+    }
+
+    long settingSequence = getSettingSequence(result.mOutputBuffers);
+    bool holdOnInput = needHoldOnInputFrame(settingSequence, sequence);
+    bool hasRawOutput = isBufferHoldForRawReprocess(sequence);
+
+    // Return buffer only if the buffer is not used in the future.
+    if (!holdOnInput && mBufferProducer && !hasRawOutput) {
+        for (const auto& src : result.mInputBuffers) {
+            mBufferProducer->qbuf(src.first, src.second);
+
+            if (src.second->getStreamType() == CAMERA_STREAM_INPUT) {
+                for (auto &it : mBufferConsumerList) {
+                    it->onFrameAvailable(src.first, src.second);
+                }
+            }
+        }
+    }
+
+    AutoMutex l(mBufferQueueLock);
+    long oldest = mSequenceInflight.front();
+    if (sequence != oldest) {
+        // The output buffer should always be FIFO.
+        LOGW("The sequence should be %ld, but it's %ld", oldest, sequence);
+    }
+
+    mSequenceInflight.pop();
+    if (mSequenceInflight.empty()) {
+        mFrameDoneSignal.signal();
+    }
+}
+
+void PSysProcessor::outputRawImage(shared_ptr<CameraBuffer> &srcBuf, shared_ptr<CameraBuffer> &dstBuf)
+{
+    if ((srcBuf == nullptr) || (dstBuf == nullptr)) {
+        return;
+    }
+
+    // Copy from source buffer
+    int srcBufferSize = srcBuf->getBufferSize();
+    int srcMemoryType = srcBuf->getMemory();
+    void* pSrcBuf = (srcMemoryType == V4L2_MEMORY_DMABUF)
+                    ? CameraBuffer::mapDmaBufferAddr(srcBuf->getFd(), srcBufferSize)
+                    : srcBuf->getBufferAddr();
+
+    int dstBufferSize = dstBuf->getBufferSize();
+    int dstMemoryType = dstBuf->getMemory();
+    void* pDstBuf = (dstMemoryType == V4L2_MEMORY_DMABUF)
+                    ? CameraBuffer::mapDmaBufferAddr(dstBuf->getFd(), dstBufferSize)
+                    : dstBuf->getBufferAddr();
+
+    MEMCPY_S(pDstBuf, dstBufferSize, pSrcBuf, srcBufferSize);
+
+    if (srcMemoryType == V4L2_MEMORY_DMABUF) {
+        CameraBuffer::unmapDmaBufferAddr(pSrcBuf, srcBufferSize);
+    }
+
+    if (dstMemoryType == V4L2_MEMORY_DMABUF) {
+        CameraBuffer::unmapDmaBufferAddr(pDstBuf, dstBufferSize);
+    }
+
+    // Send output buffer to its consumer
+    for (auto &it : mBufferConsumerList) {
+        it->onFrameAvailable(mRawPort, dstBuf);
+    }
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/PSysProcessor.h b/camera/hal/intel/ipu6/src/core/PSysProcessor.h
new file mode 100644
index 000000000000..7f52b31284e0
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/PSysProcessor.h
@@ -0,0 +1,133 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <queue>
+
+#include "BufferQueue.h"
+#include "iutils/RWLock.h"
+
+#include "IspSettings.h"
+#include "psysprocessor/PSysDAG.h"
+
+namespace icamera {
+
+class ParameterGenerator;
+class PSysDAG;
+
+typedef std::map<Port, std::shared_ptr<CameraBuffer>> CameraBufferPortMap;
+typedef std::map<ConfigMode, std::unique_ptr<PSysDAG>> PSysDAGConfigModeMap;
+
+/**
+  * PSysProcessor runs the Image Process Algorithm in the PSYS.
+  * It implements the BufferConsumer and BufferProducer Interface
+  */
+class PSysProcessor: public BufferQueue, public PSysDagCallback {
+
+public:
+    PSysProcessor(int cameraId, ParameterGenerator *pGenerator);
+    virtual ~PSysProcessor();
+    virtual int configure(const std::vector<ConfigMode>& configModes);
+    virtual int setParameters(const Parameters& param);
+    virtual int getParameters(Parameters& param);
+
+    virtual int registerUserOutputBufs(Port port, const std::shared_ptr<CameraBuffer> &camBuffer);
+
+    //Overwrite event source API to delegate related functions
+    void registerListener(EventType eventType, EventListener* eventListener);
+    void removeListener(EventType eventType, EventListener* eventListener);
+
+    virtual int start();
+    virtual void stop();
+
+    // Overwrite PSysDagCallback API, used for returning back buffers from PSysDAG.
+    void onFrameDone(const PSysTaskData& result);
+    void onBufferDone(int64_t sequence, Port port,
+                      const std::shared_ptr<CameraBuffer> &camBuffer);
+
+private:
+    DISALLOW_COPY_AND_ASSIGN(PSysProcessor);
+
+private:
+    int processNewFrame();
+    std::shared_ptr<CameraBuffer> allocStatsBuffer(int index);
+
+    status_t prepareTask(CameraBufferPortMap *srcBuffers, CameraBufferPortMap *dstBuffers);
+    void dispatchTask(CameraBufferPortMap &inBuf, CameraBufferPortMap &outBuf);
+
+    void handleEvent(EventData eventData);
+
+    long getSettingSequence(const CameraBufferPortMap &outBuf);
+    bool needSkipOutputFrame(long sequence);
+    bool needExecutePipe(long settingSequence, long inputSequence);
+    bool needHoldOnInputFrame(long settingSequence, long inputSequence);
+    bool needSwitchPipe(long sequence);
+
+    void outputRawImage(std::shared_ptr<CameraBuffer> &srcBuf,
+                        std::shared_ptr<CameraBuffer> &dstBuf);
+
+    void handleRawReprocessing(CameraBufferPortMap *srcBuffers,
+                               CameraBufferPortMap *dstBuffers, bool *hasYuv,
+                               bool *hasRawOutput, bool *hasRawInput);
+    bool isBufferHoldForRawReprocess(long sequence);
+
+private:
+    int mCameraId;
+    static const nsecs_t kWaitDuration = 1000000000; //1000ms
+    ParameterGenerator *mParameterGenerator;
+
+    IspSettings mIspSettings;
+    RWLock mIspSettingsLock;
+
+    //Since the isp settings may be re-used in all modes, so the buffer size of
+    //isp settings should be equal to frame buffer size.
+    static const int IA_PAL_CONTROL_BUFFER_SIZE = 10;
+
+    Condition mFrameDoneSignal;
+    std::queue<long> mSequenceInflight; // Save the sequences which are being processed.
+
+    std::vector<ConfigMode> mConfigModes;
+    PSysDAGConfigModeMap mPSysDAGs;
+    // Active config mode and tuning mode
+    ConfigMode mCurConfigMode;
+    TuningMode mTuningMode;
+
+    std::queue<EventDataMeta> mMetaQueue;
+    //Guard for the metadata queue
+    Mutex  mMetaQueueLock;
+    Condition mMetaAvailableSignal;
+
+    Port mRawPort;
+
+    // variables for sof alignment
+    timeval mSofTimestamp;
+    Mutex mSofLock;
+    Condition mSofCondition;
+    long mSofSequence;
+
+    // variables for opaque raw
+    Port mOpaqueRawPort;
+    std::mutex mRawBufferMapLock;
+    std::map<long, CameraBufferPortMap> mRawBufferMap;
+
+    enum {
+        PIPELINE_UNCREATED = 0,
+        PIPELINE_CREATED
+    } mStatus;
+}; // End of class PSysProcessor
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/ProcessorManager.cpp b/camera/hal/intel/ipu6/src/core/ProcessorManager.cpp
new file mode 100644
index 000000000000..ccb204dbc8b9
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/ProcessorManager.cpp
@@ -0,0 +1,125 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "ProcessorManager"
+
+#include "ProcessorManager.h"
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+#include "SwImageProcessor.h"
+#include "PSysProcessor.h"
+
+namespace icamera {
+
+ProcessorManager::ProcessorManager(int cameraId) :
+        mCameraId(cameraId),
+        mPsysUsage(PSYS_NOT_USED)
+{
+    LOG1("@%s, cameraId:%d", __func__, mCameraId);
+}
+
+ProcessorManager::~ProcessorManager()
+{
+    LOG1("@%s, cameraId:%d", __func__, mCameraId);
+
+    deleteProcessors();
+}
+
+std::vector<BufferQueue*> ProcessorManager::createProcessors(int inputFmt,
+        const std::map<Port, stream_t>& producerConfigs,
+        const std::map<int, Port>& streamIdToPortMap,
+        stream_config_t *streamList, const Parameters& param, ParameterGenerator* paramGenerator)
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+
+    ProcessorConfig processorItem;
+    processorItem.mInputConfigs = producerConfigs;
+    for (const auto& item : streamIdToPortMap) {
+        if (streamList->streams[item.first].streamType == CAMERA_STREAM_INPUT) continue;
+        processorItem.mOutputConfigs[item.second] = streamList->streams[item.first];
+    }
+
+    // Check if PSysProcessor can be used.
+    mPsysUsage = PSYS_NORMAL;
+    for (int i = 0; i < streamList->num_streams; i++) {
+        if (streamList->streams[i].streamType == CAMERA_STREAM_INPUT ||
+            streamList->streams[i].usage == CAMERA_STREAM_OPAQUE_RAW) continue;
+
+        if (!PlatformData::usePsys(mCameraId, streamList->streams[i].format)) {
+            mPsysUsage = PSYS_NOT_USED;
+            break;
+        }
+    }
+
+    if (mPsysUsage == PSYS_NORMAL) {
+        LOG1("Using normal Psys to do image processing.");
+        processorItem.mProcessor = new PSysProcessor(mCameraId, paramGenerator);
+        mProcessors.push_back(processorItem);
+    }
+
+    if (mPsysUsage == PSYS_NOT_USED) {
+        LOG1("Using software to do color conversion.");
+        processorItem.mProcessor = new SwImageProcessor(mCameraId);
+        mProcessors.push_back(processorItem);
+    }
+
+    std::vector<BufferQueue*> processors;
+    for (auto& p : mProcessors) {
+        processors.push_back(p.mProcessor);
+    }
+
+    return processors;
+}
+
+int ProcessorManager::deleteProcessors()
+{
+    for (auto& item : mProcessors) {
+        delete item.mProcessor;
+    }
+    mProcessors.clear();
+
+    mPsysUsage = PSYS_NOT_USED;
+
+    return OK;
+}
+
+/**
+ * Configure processor with input and output streams
+ */
+int ProcessorManager::configureProcessors(const std::vector<ConfigMode>& configModes,
+                                          BufferProducer* producer,
+                                          const Parameters& param)
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+
+    BufferProducer* preProcess =  nullptr;
+    for (auto& item : mProcessors) {
+        BufferQueue* processor = item.mProcessor;
+        processor->setFrameInfo(item.mInputConfigs, item.mOutputConfigs);
+        processor->setParameters(param);
+        int ret = processor->configure(configModes);
+        CheckError(ret < 0, ret, "Configure processor failed with:%d", ret);
+
+        processor->setBufferProducer(preProcess ? preProcess : producer);
+        preProcess = processor;
+    }
+
+    return OK;
+}
+
+} // end of namespace icamera
+
diff --git a/camera/hal/intel/ipu6/src/core/ProcessorManager.h b/camera/hal/intel/ipu6/src/core/ProcessorManager.h
new file mode 100644
index 000000000000..d11cb14877a4
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/ProcessorManager.h
@@ -0,0 +1,71 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "BufferQueue.h"
+
+namespace icamera {
+
+class ParameterGenerator;
+
+/**
+ * \class ProcessorManager
+ *
+ * \brief ProcessorManager helps to create and maintain the post processors.
+ */
+class ProcessorManager {
+public:
+    ProcessorManager(int cameraId);
+    ~ProcessorManager();
+
+    std::vector<BufferQueue*> createProcessors(int inputFmt,
+                                               const std::map<Port, stream_t>& producerConfigs,
+                                               const std::map<int, Port>& streamIdToPortMap,
+                                               stream_config_t *streamList, const Parameters& param,
+                                               ParameterGenerator* paramGenerator);
+    int configureProcessors(const std::vector<ConfigMode>& configModes, BufferProducer* producer,
+                            const Parameters& param);
+    int deleteProcessors();
+
+private:
+    DISALLOW_COPY_AND_ASSIGN(ProcessorManager);
+private:
+    int mCameraId;
+
+    enum PSysUsage {
+        PSYS_NOT_USED = 0,
+        PSYS_NORMAL,
+        PSYS_WEAVING,
+        PSYS_SCALE,
+        PSYS_CSC,
+        PSYS_SCALE_CSC,
+        PSYS_WEAVING_SCALE,
+        PSYS_WEAVING_SCALE_CSC,
+        PSYS_FISHEYE,
+        PSYS_MONO_DS,
+    } mPsysUsage;
+
+    struct ProcessorConfig {
+        BufferQueue* mProcessor;
+        std::map<Port, stream_t> mInputConfigs;
+        std::map<Port, stream_t> mOutputConfigs;
+    };
+
+    std::vector<ProcessorConfig> mProcessors;
+};
+
+} // end of namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/RequestThread.cpp b/camera/hal/intel/ipu6/src/core/RequestThread.cpp
new file mode 100644
index 000000000000..fbbef98ba252
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/RequestThread.cpp
@@ -0,0 +1,542 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "RequestThread"
+
+#include "iutils/Errors.h"
+#include "iutils/CameraLog.h"
+
+#include "RequestThread.h"
+
+using std::vector;
+using std::shared_ptr;
+
+namespace icamera {
+
+RequestThread::RequestThread(int cameraId, AiqUnitBase *a3AControl, ParameterGenerator* aParamGen) :
+    mCameraId(cameraId),
+    m3AControl(a3AControl),
+    mParamGenerator(aParamGen),
+    mPerframeControlSupport(false),
+    mGet3AStatWithFakeRequest(false),
+    mRequestsInProcessing(0),
+    mFirstRequest(true),
+    mRequestConfigMode(CAMERA_STREAM_CONFIGURATION_MODE_END),
+    mUserConfigMode(CAMERA_STREAM_CONFIGURATION_MODE_END),
+    mNeedReconfigPipe(false),
+    mReconfigPipeScore(0),
+    mActive(false),
+    mLastRequestId(-1),
+    mLastPredictSeq(-1),
+    mLastEofSeq(-1),
+    mBlockRequest(true)
+{
+    CLEAR(mStreamConfig);
+    CLEAR(mConfiguredStreams);
+    CLEAR(mFakeReqBuf);
+
+    mStreamConfig.operation_mode = CAMERA_STREAM_CONFIGURATION_MODE_END;
+    mPerframeControlSupport = PlatformData::isFeatureSupported(mCameraId, PER_FRAME_CONTROL);
+}
+
+RequestThread::~RequestThread()
+{
+    while (!mReqParamsPool.empty()) {
+        mReqParamsPool.pop();
+    }
+}
+
+void RequestThread::requestExit()
+{
+    clearRequests();
+
+    Thread::requestExit();
+    AutoMutex l(mPendingReqLock);
+    mRequestSignal.signal();
+}
+
+void RequestThread::clearRequests()
+{
+    LOG1("%s", __func__);
+
+    mActive = false;
+    for (int streamId = 0; streamId < MAX_STREAM_NUMBER; streamId++) {
+        FrameQueue& frameQueue = mOutputFrames[streamId];
+        AutoMutex lock(frameQueue.mFrameMutex);
+        while (!frameQueue.mFrameQueue.empty()) {
+            frameQueue.mFrameQueue.pop();
+        }
+        frameQueue.mFrameAvailableSignal.broadcast();
+    }
+
+    AutoMutex l(mPendingReqLock);
+    mRequestsInProcessing = 0;
+    while (!mPendingRequests.empty()) {
+        mPendingRequests.pop_back();
+    }
+
+    mLastRequestId = -1;
+    mLastPredictSeq = -1;
+    mLastEofSeq = -1;
+    mFirstRequest = true;
+    mBlockRequest = true;
+}
+
+void RequestThread::setConfigureModeByParam(const Parameters& param)
+{
+    camera_scene_mode_t sceneMode = SCENE_MODE_MAX;
+    if (param.getSceneMode(sceneMode) != OK) {
+        return;
+    }
+
+    ConfigMode configMode = CameraUtils::getConfigModeBySceneMode(sceneMode);
+    LOG2("@%s, sceneMode %d, configMode %d", __func__, sceneMode, configMode);
+
+    if (configMode == CAMERA_STREAM_CONFIGURATION_MODE_END) {
+        LOG2("%s: no valid config mode, skip setting", __func__);
+        return;
+    }
+
+    /* Reset internal mode related settings if requested mode is same as
+     * the mode currently running for better stability.
+     */
+    if (mStreamConfig.operation_mode == configMode) {
+        LOG2("%s: config mode %d keep unchanged.", __func__, configMode);
+        mNeedReconfigPipe = false;
+        mReconfigPipeScore = 0;
+        mRequestConfigMode = configMode;
+        return;
+    }
+
+    if (mRequestConfigMode != configMode) {
+        if (mRequestConfigMode != CAMERA_STREAM_CONFIGURATION_MODE_END) {
+            mNeedReconfigPipe = true;
+            mReconfigPipeScore = 0;
+            LOG2("%s: request configure mode changed, reset score %d", __func__, mReconfigPipeScore);
+        }
+        LOG2("%s: mRequestConfigMode updated from %d to %d", __func__, mRequestConfigMode, configMode);
+        mRequestConfigMode = configMode;
+    } else if (mReconfigPipeScore < PlatformData::getPipeSwitchDelayFrame(mCameraId)) {
+        mReconfigPipeScore ++;
+        LOG2("%s: request configure mode unchanged, current score %d", __func__, mReconfigPipeScore);
+    }
+}
+
+int RequestThread::configure(const stream_config_t *streamList)
+{
+    mGet3AStatWithFakeRequest = mPerframeControlSupport ? PlatformData::isPsysContinueStats(mCameraId) : false;
+    CLEAR(mFakeReqBuf);
+    bool hasVideoStream = false;
+
+    mStreamConfig.num_streams = streamList->num_streams;
+    mStreamConfig.operation_mode = streamList->operation_mode;
+    mUserConfigMode = (ConfigMode)streamList->operation_mode;
+    int previewStreamIndex = -1;
+    for (int i = 0; i < streamList->num_streams; i++) {
+        mConfiguredStreams[i] = streamList->streams[i];
+        if (previewStreamIndex < 0 && mConfiguredStreams[i].usage == CAMERA_STREAM_PREVIEW) {
+            previewStreamIndex = i;
+        }
+        if (mConfiguredStreams[i].usage == CAMERA_STREAM_PREVIEW ||
+            mConfiguredStreams[i].usage == CAMERA_STREAM_VIDEO_CAPTURE) {
+            hasVideoStream = true;
+        }
+    }
+
+    LOG1("%s: user specified Configmode: %d, hasVideoStream %d", __func__, mUserConfigMode, hasVideoStream);
+    if (mGet3AStatWithFakeRequest) {
+        if (previewStreamIndex < 0) {
+            LOGW("Can't get 3a stats event due to no preview stream");
+            mGet3AStatWithFakeRequest = false;
+        } else {
+            stream_t &stream = mConfiguredStreams[previewStreamIndex];
+            LOG1("%s: create fake request with stream index %d", __func__, previewStreamIndex);
+            mFakeBuffer = CameraBuffer::create(mCameraId, BUFFER_USAGE_PSYS_INTERNAL, V4L2_MEMORY_USERPTR,
+                                               stream.size, 0, stream.format,
+                                               stream.width, stream.height);
+
+            mFakeReqBuf.s = stream;
+            mFakeReqBuf.s.memType = V4L2_MEMORY_USERPTR;
+            mFakeReqBuf.addr = mFakeBuffer->getUserBuffer()->addr;
+        }
+    }
+    mStreamConfig.streams = mConfiguredStreams;
+
+    // Use concrete mode in RequestThread
+    if ((ConfigMode)mStreamConfig.operation_mode == CAMERA_STREAM_CONFIGURATION_MODE_AUTO) {
+        vector <ConfigMode> configModes;
+        int ret = PlatformData::getConfigModesByOperationMode(mCameraId, mStreamConfig.operation_mode, configModes);
+        CheckError((ret != OK || configModes.empty()), ret, "%s, get real ConfigMode failed %d", __func__, ret);
+        mRequestConfigMode = configModes[0];
+        LOG2("%s: use concrete mode %d as default initial mode for auto op mode", __func__, mRequestConfigMode);
+        mStreamConfig.operation_mode = mRequestConfigMode;
+    }
+
+    // Don't block request handling if no 3A stats (from video pipe)
+    mBlockRequest = PlatformData::isEnableAIQ(mCameraId) && hasVideoStream;
+
+    LOG2("%s: mRequestConfigMode initial value: %d", __func__, mRequestConfigMode);
+    return OK;
+}
+
+bool RequestThread::blockRequest() {
+    /**
+     * Block request processing if:
+     * 1. mBlockRequest is true (except the 1st request), or
+     * 2. Too many requests in flight.
+     */
+    return ((mBlockRequest && (mLastRequestId >= 0)) ||
+            mRequestsInProcessing >= PlatformData::getMaxRequestsInflight(mCameraId));
+}
+
+int RequestThread::processRequest(int bufferNum, camera_buffer_t **ubuffer, const Parameters* params)
+{
+    AutoMutex l(mPendingReqLock);
+    CameraRequest request;
+    request.mBufferNum = bufferNum;
+    bool hasVideoBuffer = false;
+
+    for (int id = 0; id < bufferNum; id++) {
+        request.mBuffer[id] = ubuffer[id];
+        if (ubuffer[id]->s.usage == CAMERA_STREAM_PREVIEW ||
+            ubuffer[id]->s.usage == CAMERA_STREAM_VIDEO_CAPTURE) {
+            hasVideoBuffer = true;
+        }
+    }
+
+    if (mFirstRequest && !hasVideoBuffer) {
+        LOG2("there is no video buffer in first request, so don't block request processing.");
+        mBlockRequest = false;
+    }
+
+    request.mParams = copyRequestParams(params);
+    mPendingRequests.push_back(request);
+
+    if (!mActive) {
+        mActive = true;
+    }
+
+    mRequestSignal.signal();
+    return OK;
+}
+
+shared_ptr<Parameters>
+RequestThread::copyRequestParams(const Parameters *srcParams)
+{
+    if (srcParams == nullptr)
+        return nullptr;
+
+    if (mReqParamsPool.empty()) {
+        shared_ptr<Parameters> sParams = std::make_shared<Parameters>();
+        CheckError(!sParams, nullptr, "%s: no memory!", __func__);
+        mReqParamsPool.push(sParams);
+    }
+
+    shared_ptr<Parameters> sParams = mReqParamsPool.front();
+    mReqParamsPool.pop();
+    *sParams = *srcParams;
+    return sParams;
+}
+
+int RequestThread::waitFrame(int streamId, camera_buffer_t **ubuffer)
+{
+    FrameQueue& frameQueue = mOutputFrames[streamId];
+    ConditionLock lock(frameQueue.mFrameMutex);
+
+    while (frameQueue.mFrameQueue.empty()) {
+        int ret = frameQueue.mFrameAvailableSignal.waitRelative(
+                      lock,
+                      kWaitFrameDuration * SLOWLY_MULTIPLIER);
+        if (!mActive) return INVALID_OPERATION;
+
+        if (ret == TIMED_OUT) {
+            LOGW("@%s, mCameraId:%d, time out happens, wait recovery", __func__, mCameraId);
+            return ret;
+        }
+    }
+
+    shared_ptr<CameraBuffer> camBuffer = frameQueue.mFrameQueue.front();
+    frameQueue.mFrameQueue.pop();
+    *ubuffer = camBuffer->getUserBuffer();
+
+    LOG2("@%s, frame returned. camera id:%d, stream id:%d", __func__, mCameraId, streamId);
+
+    return OK;
+}
+
+int RequestThread::wait1stRequestDone()
+{
+    LOG1("%s", __func__);
+    int ret = OK;
+    ConditionLock lock(mFirstRequestLock);
+    if (mFirstRequest) {
+        LOG1("%s, waiting the first request done", __func__);
+        ret = mFirstRequestSignal.waitRelative(
+                  lock,
+                  kWaitFirstRequestDoneDuration * SLOWLY_MULTIPLIER);
+        if (ret == TIMED_OUT)
+            LOGE("@%s: Wait 1st request timed out", __func__);
+    }
+
+    return ret;
+}
+
+void RequestThread::handleEvent(EventData eventData)
+{
+    if (!mActive) return;
+
+    /* Notes:
+      * There should be only one of EVENT_ISYS_FRAME
+      * and EVENT_PSYS_FRAME registered.
+      * There should be only one of EVENT_xx_STATS_BUF_READY
+      * registered.
+      */
+    switch (eventData.type) {
+        case EVENT_ISYS_FRAME:
+        case EVENT_PSYS_FRAME:
+            {
+                AutoMutex l(mPendingReqLock);
+                if (mRequestsInProcessing > 0) {
+                    mRequestsInProcessing--;
+                }
+                // Continue process(that maight be blocked due to many requests in flight).
+                if (!mPendingRequests.empty()) {
+                    mRequestSignal.signal();
+                }
+            }
+            break;
+        case EVENT_PSYS_STATS_BUF_READY:
+            {
+                TRACE_LOG_POINT("RequestThread", "receive the stat event");
+                AutoMutex l(mPendingReqLock);
+                if (mBlockRequest) {
+                    mBlockRequest = false;
+                    mRequestSignal.signal();
+                }
+            }
+            break;
+        case EVENT_ISYS_SOF: {
+            AutoMutex l(mPendingReqLock);
+            mLastEofSeq = eventData.data.sync.sequence - 1;
+            break;
+        }
+        case EVENT_FRAME_AVAILABLE:
+            {
+                if (eventData.buffer->getUserBuffer() != &mFakeReqBuf) {
+                    int streamId = eventData.data.frameDone.streamId;
+                    FrameQueue& frameQueue = mOutputFrames[streamId];
+
+                    AutoMutex lock(frameQueue.mFrameMutex);
+                    bool needSignal = frameQueue.mFrameQueue.empty();
+                    frameQueue.mFrameQueue.push(eventData.buffer);
+                    if (needSignal) {
+                        frameQueue.mFrameAvailableSignal.signal();
+                    }
+                } else {
+                    LOG2("%s: fake request return %ld", __func__, eventData.buffer->getSequence());
+                }
+
+                AutoMutex l(mPendingReqLock);
+                // Insert fake request if no any request in the HAL to keep 3A running
+                if (mGet3AStatWithFakeRequest &&
+                    eventData.buffer->getSequence() >= mLastPredictSeq &&
+                    mPendingRequests.empty()) {
+                    LOGW("No request, insert fake req after req %d to keep 3A stats update",
+                         mLastRequestId);
+                    CameraRequest fakeRequest;
+                    fakeRequest.mBufferNum = 1;
+                    fakeRequest.mBuffer[0] = &mFakeReqBuf;
+                    mFakeReqBuf.sequence = -1;
+                    mPendingRequests.push_back(fakeRequest);
+                    mRequestSignal.signal();
+                }
+            }
+            break;
+        default:
+            {
+                LOGW("Unknown event type %d", eventData.type);
+            }
+            break;
+    }
+}
+
+/**
+ * Get the next request for processing.
+ * Return false if no pending requests or it is not ready for reconfiguration.
+ */
+bool RequestThread::fetchNextRequest(CameraRequest& request)
+{
+    ConditionLock lock(mPendingReqLock);
+    if (isReconfigurationNeeded() && !isReadyForReconfigure()) {
+        return false;
+    }
+
+    if (mPendingRequests.empty()) {
+        return false;
+    }
+
+    request = mPendingRequests.front();
+    mRequestsInProcessing++;
+    mPendingRequests.pop_front();
+    LOG2("@%s, mRequestsInProcessing %d", __func__, mRequestsInProcessing);
+    return true;
+}
+
+/**
+ * Check if ConfigMode is changed or not.
+ * If new ConfigMode is different with previous configured ConfigMode,
+ * return true.
+ */
+bool RequestThread::isReconfigurationNeeded()
+{
+    bool needReconfig = (mUserConfigMode == CAMERA_STREAM_CONFIGURATION_MODE_AUTO &&
+                         PlatformData::getAutoSwitchType(mCameraId) == AUTO_SWITCH_FULL &&
+                         mNeedReconfigPipe &&
+                         (mReconfigPipeScore >= PlatformData::getPipeSwitchDelayFrame(mCameraId)));
+    LOG2("%s: need reconfigure %d, score %d, decision %d",
+         __func__, mNeedReconfigPipe, mReconfigPipeScore, needReconfig);
+    return needReconfig;
+}
+
+/**
+ * If reconfiguration is needed, there are 2 extra conditions for reconfiguration:
+ * 1, there is no buffer in processing; 2, there is buffer in mPendingRequests.
+ * Return true if reconfiguration is ready.
+ */
+bool RequestThread::isReadyForReconfigure()
+{
+    return (!mPendingRequests.empty() && mRequestsInProcessing == 0);
+}
+
+bool RequestThread::threadLoop()
+{
+    bool restart = false;
+    {
+         ConditionLock lock(mPendingReqLock);
+
+         bool waitProcessing = blockRequest() || mPendingRequests.empty();
+         if (waitProcessing) {
+            int ret = mRequestSignal.waitRelative(lock, kWaitDuration * SLOWLY_MULTIPLIER);
+            waitProcessing = blockRequest() || mPendingRequests.empty();
+            if (ret == TIMED_OUT || waitProcessing) {
+                LOGW("%s: wait event time out", __func__);
+                return true;
+            }
+        }
+
+        restart = isReconfigurationNeeded();
+    }
+
+    if (!mActive) {
+        return false;
+    }
+
+    CameraRequest request;
+    if (fetchNextRequest(request)) {
+        // Update for reconfiguration
+        if (request.mParams.get()) {
+            setConfigureModeByParam(*(request.mParams.get()));
+        }
+        // Re-check
+        if (restart && isReconfigurationNeeded()) {
+            handleReconfig();
+        }
+
+        handleRequest(request);
+    }
+    return true;
+}
+
+void RequestThread::handleReconfig()
+{
+    LOG1("%s, ConfigMode change from %x to %x", __func__,
+            mStreamConfig.operation_mode, mRequestConfigMode);
+    mStreamConfig.operation_mode = mRequestConfigMode;
+    EventConfigData configData;
+    configData.streamList = &mStreamConfig;
+    EventData eventData;
+    eventData.type = EVENT_DEVICE_RECONFIGURE;
+    eventData.data.config = configData;
+    notifyListeners(eventData);
+    mNeedReconfigPipe = false;
+    mReconfigPipeScore = 0;
+    return;
+}
+
+void RequestThread::handleRequest(CameraRequest& request)
+{
+    long predictSequence = mLastPredictSeq + 1;
+    if (mLastPredictSeq < 0) {
+        predictSequence = PlatformData::getInitialSkipFrame(mCameraId);
+    } else if (predictSequence < mLastEofSeq) {
+        // Drop early frames
+        predictSequence = mLastEofSeq;
+    }
+
+    // Raw reprocessing case, don't run 3A.
+    if (request.mBuffer[0]->sequence >= 0 && request.mBuffer[0]->timestamp > 0) {
+        predictSequence = request.mBuffer[0]->sequence;
+        if (request.mParams.get()) {
+            mParamGenerator->updateParameters(predictSequence, request.mParams.get());
+        }
+    } else {
+        if (request.mParams.get()) {
+            m3AControl->setParameters(*request.mParams);
+        }
+        m3AControl->run3A(mPerframeControlSupport? &predictSequence : nullptr);
+
+        // Check the final prediction value
+        if (predictSequence <= mLastPredictSeq) {
+            LOGW("predict %ld error! should > %ld", predictSequence, mLastPredictSeq);
+            predictSequence = mLastPredictSeq + 1;
+        }
+
+        mLastPredictSeq = predictSequence;
+        mLastRequestId++;
+        mParamGenerator->saveParameters(mLastPredictSeq, mLastRequestId, request.mParams.get());
+    }
+    LOG2("%s: Process request: %ld:%ld, out buffer %d, param? %s", __func__,
+          mLastRequestId, predictSequence, request.mBufferNum,
+          request.mParams.get() ? "true" : "false");
+
+    // Sent event to handle request buffers
+    EventRequestData requestData;
+    requestData.bufferNum = request.mBufferNum;
+    requestData.buffer = request.mBuffer;
+    requestData.param = request.mParams.get();
+    requestData.settingSeq = predictSequence;
+    EventData eventData;
+    eventData.type = EVENT_PROCESS_REQUEST;
+    eventData.data.request = requestData;
+    notifyListeners(eventData);
+
+    // Recycle params ptr for re-using
+    if (request.mParams) {
+        AutoMutex l(mPendingReqLock);
+        mReqParamsPool.push(request.mParams);
+    }
+
+    {
+        AutoMutex l(mFirstRequestLock);
+        if (mFirstRequest) {
+            LOG1("%s: first request done", __func__);
+            mFirstRequest = false;
+            mFirstRequestSignal.signal();
+        }
+    }
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/RequestThread.h b/camera/hal/intel/ipu6/src/core/RequestThread.h
new file mode 100644
index 000000000000..dba2fd19c90a
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/RequestThread.h
@@ -0,0 +1,154 @@
+/*
+ * Copyright (C) 2016-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <deque>
+
+#include "iutils/Thread.h"
+#include "PlatformData.h"
+#include "Parameters.h"
+#include "AiqUnit.h"
+#include "Parameters.h"
+#include "ParameterGenerator.h"
+
+namespace icamera {
+
+/*
+ * The RequestThread is used to assist CameraDevice to handle request(qbuf/dqbuf).
+ */
+class RequestThread : public Thread, public EventSource, public EventListener {
+public:
+    RequestThread(int cameraId, AiqUnitBase *a3AControl, ParameterGenerator* aParamGen);
+    ~RequestThread();
+
+    bool threadLoop();
+    void requestExit();
+
+    void handleEvent(EventData eventData);
+
+    /**
+     * \Clear pending requests.
+     */
+    void clearRequests();
+
+    /**
+     * \Accept requests from user.
+     */
+    int processRequest(int bufferNum, camera_buffer_t **ubuffer, const Parameters * params);
+
+    int waitFrame(int streamId, camera_buffer_t **ubuffer);
+
+    /**
+     * \Block the caller until the first request is processed.
+     */
+    int wait1stRequestDone();
+
+    /**
+     * \brief configure the streams, devices and post processor.
+     *
+     * \param streamList: all the streams info
+     *
+     * \return OK if succeed and BAD_VALUE if failed
+     */
+    int configure(const stream_config_t *streamList);
+
+    /**
+     * \brief get stream config in request thread.
+     */
+    stream_config_t getStreamConfig() { return mStreamConfig; };
+
+    /**
+     * \brief set request configure mode by parameters.
+     */
+    void setConfigureModeByParam(const Parameters& param);
+
+private:
+    int mCameraId;
+    AiqUnitBase *m3AControl;
+    ParameterGenerator *mParamGenerator;
+    bool mPerframeControlSupport;
+    bool mGet3AStatWithFakeRequest;
+    camera_buffer_t mFakeReqBuf;
+    std::shared_ptr<CameraBuffer> mFakeBuffer;
+
+    struct CameraRequest {
+        CameraRequest() : mBufferNum(0), mParams(nullptr) {
+            CLEAR(mBuffer);
+        }
+
+        int mBufferNum;
+        camera_buffer_t *mBuffer[MAX_STREAM_NUMBER];
+        std::shared_ptr<Parameters> mParams;
+    };
+
+    std::shared_ptr<Parameters> copyRequestParams(const Parameters *params);
+
+    /**
+     * \Fetch one request from pending request Q for processing.
+     */
+    bool fetchNextRequest(CameraRequest& request);
+    bool isReadyForReconfigure();
+    bool isReconfigurationNeeded();
+    std::shared_ptr<Parameters> acquireParam();
+
+    void handleReconfig();
+    void handleRequest(CameraRequest& request);
+    bool blockRequest();
+
+    static const int kMaxRequests = MAX_BUFFER_COUNT;
+    static const nsecs_t kWaitFrameDuration = 5000000000; // 5s
+    static const nsecs_t kWaitDuration = 2000000000; // 2s
+    static const nsecs_t kWaitFirstRequestDoneDuration = 1000000000; // 1s
+
+    //Guard for all the pending requests
+    Mutex mPendingReqLock;
+    Condition mRequestSignal;
+    std::deque <CameraRequest> mPendingRequests;
+    std::queue <std::shared_ptr<Parameters> > mReqParamsPool;
+    int mRequestsInProcessing;
+
+    // Guard for the first request.
+    Mutex mFirstRequestLock;
+    Condition mFirstRequestSignal;
+    bool mFirstRequest;
+
+    // Internal used for restart function
+    ConfigMode mRequestConfigMode; // the ConfigMode is gotten from parameters set from user or AE result
+    ConfigMode mUserConfigMode; // user specified ConfigMode during initial configure
+    // Whether pipe need to reconfigure
+    bool mNeedReconfigPipe;
+    // Score indicate the num of consecutive configure mode settings, to make switch stable.
+    unsigned int  mReconfigPipeScore;
+    stream_config_t mStreamConfig;
+    stream_t mConfiguredStreams[MAX_STREAM_NUMBER];
+
+    struct FrameQueue {
+        Mutex mFrameMutex;
+        Condition mFrameAvailableSignal;
+        CameraBufQ mFrameQueue;
+    };
+    FrameQueue mOutputFrames[MAX_STREAM_NUMBER];
+    bool mActive;
+
+    long mLastRequestId;
+    long mLastPredictSeq;
+    long mLastEofSeq;
+    bool mBlockRequest;  // Process the 2nd or 3th request after the 1st 3A event
+                         // to avoid unstable AWB at the beginning of stream on
+};
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/SensorHwCtrl.cpp b/camera/hal/intel/ipu6/src/core/SensorHwCtrl.cpp
new file mode 100644
index 000000000000..7af50e24b747
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/SensorHwCtrl.cpp
@@ -0,0 +1,331 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "SensorHwCtrl"
+
+#include <limits.h>
+
+#include <linux/types.h>
+#include <linux/v4l2-controls.h>
+
+#include "iutils/CameraLog.h"
+
+#include "SensorHwCtrl.h"
+#include "V4l2DeviceFactory.h"
+#include "PlatformData.h"
+
+using std::vector;
+
+namespace icamera {
+
+SensorHwCtrl::SensorHwCtrl(int cameraId, V4L2Subdevice* pixelArraySubdev, V4L2Subdevice* sensorOutputSubdev):
+        mPixelArraySubdev(pixelArraySubdev),
+        mSensorOutputSubdev(sensorOutputSubdev),
+        mCameraId(cameraId),
+        mHorzBlank(0),
+        mVertBlank(0),
+        mCropWidth(0),
+        mCropHeight(0),
+        mCurFll(0),
+        mCalculatingFrameDuration(true)
+{
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL1);
+    LOG2("@%s, mCameraId:%d", __func__, mCameraId);
+}
+
+SensorHwCtrl::~SensorHwCtrl()
+{
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL1);
+}
+
+SensorHwCtrl* SensorHwCtrl::createSensorCtrl(int cameraId)
+{
+    if (!PlatformData::isIsysEnabled(cameraId)) {
+        return new DummySensor(cameraId);
+    }
+
+    std::string subDevName;
+    SensorHwCtrl* sensorCtrl = nullptr;
+    int ret = PlatformData::getDevNameByType(cameraId, VIDEO_PIXEL_ARRAY, subDevName);
+    if (ret == OK) {
+        LOG1("%s ArraySubdev camera id:%d dev name:%s", __func__, cameraId, subDevName.c_str());
+        V4L2Subdevice* pixelArraySubdev = V4l2DeviceFactory::getSubDev(cameraId, subDevName);
+
+        V4L2Subdevice* pixelOutputSubdev = nullptr;
+        // Binner and Scaler subdev only exits in CrlModule driver
+        if (PlatformData::isUsingCrlModule(cameraId)) {
+            subDevName.clear();
+            ret = PlatformData::getDevNameByType(cameraId, VIDEO_PIXEL_SCALER, subDevName);
+            if (ret == OK) {
+                LOG1("%s ScalerSubdev camera id:%d dev name:%s", __func__, cameraId, subDevName.c_str());
+                pixelOutputSubdev = V4l2DeviceFactory::getSubDev(cameraId, subDevName);
+            } else {
+                subDevName.clear();
+                ret = PlatformData::getDevNameByType(cameraId, VIDEO_PIXEL_BINNER, subDevName);
+                if (ret == OK) {
+                    LOG1("%s BinnerSubdev camera id:%d dev name:%s", __func__, cameraId, subDevName.c_str());
+                    pixelOutputSubdev = V4l2DeviceFactory::getSubDev(cameraId, subDevName);
+                }
+            }
+        }
+
+        sensorCtrl = new SensorHwCtrl(cameraId, pixelArraySubdev, pixelOutputSubdev);
+    } else {
+        LOG1("%s create a dummy sensor ctrl for camera id:%d", __func__, cameraId);
+        sensorCtrl = new DummySensor(cameraId);
+    }
+    return sensorCtrl;
+}
+
+int SensorHwCtrl::getActivePixelArraySize(int &width, int &height, int &pixelCode)
+{
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL2);
+    CheckError(!mPixelArraySubdev, NO_INIT, "pixel array sub device is not set");
+
+    int status = mPixelArraySubdev->GetPadFormat(0, &width, &height, &pixelCode);
+    mCropWidth = width;
+    mCropHeight = height;
+
+    LOG2("@%s, width:%d, height:%d, status:%d", __func__, width, height, status);
+    return status;
+}
+
+int SensorHwCtrl::getPixelRate(int &pixelRate)
+{
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL2);
+    CheckError(!mPixelArraySubdev, NO_INIT, "pixel array sub device is not set");
+
+    int ret = mPixelArraySubdev->GetControl(V4L2_CID_PIXEL_RATE, &pixelRate);
+
+    LOG2("@%s, pixelRate:%d, ret:%d", __func__, pixelRate, ret);
+
+    return ret;
+}
+
+int SensorHwCtrl::setTestPatternMode(int32_t testPatternMode)
+{
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL2);
+    CheckError(!mPixelArraySubdev, NO_INIT, "pixel array sub device is not set");
+
+    LOG2("@%s, testPatternMode: %d", __func__, testPatternMode);
+    return mPixelArraySubdev->SetControl(V4L2_CID_TEST_PATTERN, testPatternMode);
+}
+
+int SensorHwCtrl::setExposure(const vector<int>& coarseExposures, const vector<int>& fineExposures)
+{
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL2);
+    CheckError(!mPixelArraySubdev, NO_INIT, "pixel array sub device is not set");
+    CheckError((coarseExposures.empty() || fineExposures.empty()), BAD_VALUE, "No exposure data!");
+
+    LOG2("%s coarseExposure=%d fineExposure=%d", __func__, coarseExposures[0], fineExposures[0]);
+    LOG2("SENSORCTRLINFO: exposure_value=%d", coarseExposures[0]);
+    return mPixelArraySubdev->SetControl(V4L2_CID_EXPOSURE, coarseExposures[0]);
+}
+
+int SensorHwCtrl::setGains(const vector<int>& analogGains, const vector<int>& digitalGains)
+{
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL2);
+    CheckError(!mPixelArraySubdev, NO_INIT, "pixel array sub device is not set");
+    CheckError((analogGains.empty() || digitalGains.empty()), BAD_VALUE, "No gain data!");
+
+	int ret = mPixelArraySubdev->SetControl(V4L2_CID_ANALOGUE_GAIN, analogGains[0]);
+	ret |= mPixelArraySubdev->SetControl(V4L2_CID_DIGITAL_GAIN, digitalGains[0]);
+    return ret;
+}
+
+int SensorHwCtrl::setLineLengthPixels(int llp)
+{
+    int status = OK;
+    LOG2("@%s, llp:%d", __func__, llp);
+
+    if (mCalculatingFrameDuration) {
+        int horzBlank = llp - mCropWidth;
+        if (mHorzBlank != horzBlank) {
+            status = mPixelArraySubdev->SetControl(V4L2_CID_HBLANK, horzBlank);
+        }
+    }
+
+    CheckError(status != OK, status, "failed to set llp.");
+
+    mHorzBlank = llp - mCropWidth;
+    return status;
+}
+
+int SensorHwCtrl::setFrameLengthLines(int fll)
+{
+    int status = OK;
+    LOG2("@%s, fll:%d", __func__, fll);
+
+    if (mCalculatingFrameDuration) {
+        int vertBlank = fll - mCropHeight;
+        if (mVertBlank != vertBlank) {
+            status = mPixelArraySubdev->SetControl(V4L2_CID_VBLANK, vertBlank);
+        }
+    }
+
+    mCurFll = fll;
+
+    CheckError(status != OK, status, "failed to set fll.");
+
+    mVertBlank = fll - mCropHeight;
+    return status;
+}
+
+int SensorHwCtrl::setFrameDuration(int llp, int fll)
+{
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL2);
+    CheckError(!mPixelArraySubdev, NO_INIT, "pixel array sub device is not set");
+
+    int status = OK;
+    LOG2("@%s, llp:%d, fll:%d", __func__, llp, fll);
+
+    /* only set them to driver when llp or fll is not 0 */
+    if (llp) {
+        status = setLineLengthPixels(llp);
+    }
+
+    if (fll) {
+        status |= setFrameLengthLines(fll);
+    }
+
+    return status;
+}
+
+int SensorHwCtrl::getLineLengthPixels(int &llp)
+{
+    int status = OK;
+
+    if (mCalculatingFrameDuration) {
+        int horzBlank = 0;
+        status = mPixelArraySubdev->GetControl(V4L2_CID_HBLANK, &horzBlank);
+        if (status == OK) {
+            mHorzBlank = horzBlank;
+            llp = horzBlank + mCropWidth;
+        }
+    }
+
+    LOG2("@%s, llp:%d", __func__, llp);
+    CheckError(status != OK, status, "failed to get llp.");
+
+    return status;
+}
+
+int SensorHwCtrl::getFrameLengthLines(int &fll)
+{
+    int status = OK;
+
+    if (mCalculatingFrameDuration) {
+        int vertBlank = 0;
+        status = mPixelArraySubdev->GetControl(V4L2_CID_VBLANK, &vertBlank);
+        if (status == OK) {
+            mVertBlank = vertBlank;
+            fll = vertBlank + mCropHeight;
+        }
+    }
+
+    LOG2("@%s, fll:%d", __func__, fll);
+    CheckError(status != OK, status, "failed to get fll.");
+
+    return status;
+}
+
+int SensorHwCtrl::getFrameDuration(int &llp, int &fll)
+{
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL2);
+    CheckError(!mPixelArraySubdev, NO_INIT, "pixel array sub device is not set");
+
+    int status = getLineLengthPixels(llp);
+
+    status |= getFrameLengthLines(fll);
+    LOG2("@%s, llp:%d, fll:%d", __func__, llp, fll);
+
+    return status;
+}
+
+int SensorHwCtrl::getVBlank(int &vblank)
+{
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL2);
+    vblank = mVertBlank;
+    LOG2("@%s, vblank:%d", __func__, vblank);
+
+    return OK;
+}
+
+/**
+ * get exposure range value from sensor driver
+ *
+ * \param[OUT] coarse_exposure: exposure min value
+ * \param[OUT] fine_exposure: exposure max value
+ * \param[OUT] exposure_step: step of exposure
+ * V4L2 does not support FINE_EXPOSURE setting
+ *
+ * \return OK if successfully.
+ */
+int SensorHwCtrl::getExposureRange(int &exposureMin, int &exposureMax, int &exposureStep)
+{
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL2);
+    CheckError(!mPixelArraySubdev, NO_INIT, "pixel array sub device is not set");
+
+    v4l2_queryctrl exposure = {};
+    exposure.id = V4L2_CID_EXPOSURE;
+    int status = mPixelArraySubdev->QueryControl(&exposure);
+    CheckError(status != OK, status, "Couldn't get exposure Range status:%d", status);
+
+    exposureMin = exposure.minimum;
+    exposureMax = exposure.maximum;
+    exposureStep = exposure.step;
+    LOG2("@%s, exposureMin:%d, exposureMax:%d, exposureStep:%d",
+        __func__, exposureMin, exposureMax, exposureStep);
+
+    return status;
+}
+
+int SensorHwCtrl::setFrameRate(float fps)
+{
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL2);
+    CheckError(!mSensorOutputSubdev, NO_INIT, "sensor output sub device is not set");
+
+    LOG2("%s FPS is: %f", __func__, fps);
+
+    struct v4l2_queryctrl query;
+    CLEAR(query);
+    query.id = V4L2_CID_LINK_FREQ;
+    int status = mSensorOutputSubdev->QueryControl(&query);
+    CheckError(status != OK, status, "Couldn't get V4L2_CID_LINK_FREQ, status:%d", status);
+
+    LOG2("@%s, query V4L2_CID_LINK_FREQ:, default_value:%d, maximum:%d, minimum:%d, step:%d",
+        __func__, query.default_value, query.maximum, query.minimum, query.step);
+
+    int mode = 0;
+    if (query.maximum == query.minimum) {
+        mode = query.default_value;
+    } else {
+        /***********************************************************************************
+         * WA: This heavily depends on sensor driver implementation, need to find a graceful
+         * solution.
+         * imx185:
+         * When fps larger than 30, should switch to high speed mode, currently only
+         * 0, 1, 2 are available. 0 means 720p 30fps, 1 means 2M 30fps, and 2 means 2M 60fps.
+         * imx290:
+         * 0 and 1 available, for 30 and higher FPS.
+         ***********************************************************************************/
+        mode = (fps > 30) ? query.maximum : (query.maximum - 1);
+    }
+    LOG2("@%s, set V4L2_CID_LINK_FREQ to %d", __func__, mode);
+    return mSensorOutputSubdev->SetControl(V4L2_CID_LINK_FREQ, mode);
+}
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/SensorHwCtrl.h b/camera/hal/intel/ipu6/src/core/SensorHwCtrl.h
new file mode 100644
index 000000000000..c05f5329eff7
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/SensorHwCtrl.h
@@ -0,0 +1,99 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <v4l2_device.h>
+
+#include <vector>
+
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+/**
+ * Base class for sensor control might be inherited by ones have different sensor driver
+ */
+class SensorHwCtrl {
+
+public:
+    static SensorHwCtrl* createSensorCtrl(int cameraId);
+    SensorHwCtrl(int cameraId, V4L2Subdevice* pixelArraySubdev, V4L2Subdevice* sensorOutputSubdev);
+    virtual ~SensorHwCtrl();
+
+    virtual int setTestPatternMode(int32_t testPatternMode);
+    virtual int getPixelRate(int &pixelRate);
+    virtual int setExposure(const std::vector<int>& coarseExposures,
+                            const std::vector<int>& fineExposures);
+    virtual int setGains(const std::vector<int>& analogGains, const std::vector<int>& digitalGains);
+    virtual int setFrameDuration(int llp, int fll);
+    virtual int getFrameDuration(int &llp, int &fll);
+    virtual int getVBlank(int &vblank);
+    virtual int getActivePixelArraySize(int &width, int &height, int &pixelCode);
+    virtual int getExposureRange(int &exposureMin, int &exposureMax, int &exposureStep);
+
+    virtual int setFrameRate(float fps);
+private:
+    int setLineLengthPixels(int llp);
+    int getLineLengthPixels(int &llp);
+    int setFrameLengthLines(int fll);
+    int getFrameLengthLines(int &fll);
+
+private:
+
+    V4L2Subdevice* mPixelArraySubdev;
+    V4L2Subdevice* mSensorOutputSubdev;
+    int mCameraId;
+    int mHorzBlank;
+    int mVertBlank;
+    int mCropWidth;
+    int mCropHeight;
+
+    // Current frame length lines
+    int mCurFll;
+
+    /**
+     * if mCalculatingFrameDuration is true, it means sensor can't set/get llp/fll directly,
+     * use HBlank/VBlank to calculate it.
+     */
+    bool mCalculatingFrameDuration;
+}; //class SensorHwCtrl
+
+/**
+ * Dummy sensor hardware ctrl interface for those sensors cannot be controlled.
+ */
+class DummySensor : public SensorHwCtrl {
+public:
+    DummySensor(int cameraId) : SensorHwCtrl(cameraId, nullptr, nullptr) {}
+    ~DummySensor() {}
+
+    int setTestPatternMode(int32_t testPatternMode) { return OK; }
+    int setDevice(V4L2Subdevice* pixelArraySubdev) { return OK; }
+    int getPixelRate(int &pixelRate) { return OK; }
+    int setExposure(const std::vector<int>& coarseExposures,
+                    const std::vector<int>& fineExposures) { return OK; }
+    int setGains(const std::vector<int>& analogGains,
+                 const std::vector<int>& digitalGains) { return OK; }
+    int setFrameDuration(int llp, int fll) { return OK; }
+    int getFrameDuration(int &llp, int &fll) { return OK; }
+    int getVBlank(int &vblank) { return OK; }
+    int getActivePixelArraySize(int &width, int &height, int &code) { return OK; }
+    int getExposureRange(int &exposureMin, int &exposureMax, int &exposureStep) { return OK; }
+    int setFrameRate(float fps) { return OK; }
+};
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/SofSource.cpp b/camera/hal/intel/ipu6/src/core/SofSource.cpp
new file mode 100644
index 000000000000..58d1f42c2b88
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/SofSource.cpp
@@ -0,0 +1,201 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#define LOG_TAG "SofSource"
+
+#include <poll.h>
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+#include "V4l2DeviceFactory.h"
+#include "PlatformData.h"
+
+#include "SofSource.h"
+
+namespace icamera {
+
+SofSource::SofSource(int cameraId) :
+    mPollThread(nullptr),
+    mCameraId(cameraId),
+    mIsysReceiverSubDev(nullptr),
+    mExitPending(false)
+{
+    LOG1("%s: SofSource is constructed", __func__);
+
+    mSofDisabled = !PlatformData::isEnableAIQ(mCameraId);
+
+    mSofDisabled = mSofDisabled || !PlatformData::isIsysEnabled(cameraId);
+}
+
+SofSource::~SofSource()
+{
+    LOG1("%s: SofSource is distructed.", __func__);
+}
+
+int SofSource::init()
+{
+    if (mSofDisabled) {
+        return OK;
+    }
+
+    mPollThread = new PollThread(this);
+
+    return OK;
+}
+
+int SofSource::deinit()
+{
+    if (mSofDisabled) {
+        return OK;
+    }
+
+    int status = deinitDev();
+    mPollThread->join();
+    delete mPollThread;
+    return status;
+}
+
+int SofSource::initDev()
+{
+    //Create and open receiver subdevice.
+    std::string subDeviceNodeName;
+
+    if (PlatformData::getDevNameByType(mCameraId, VIDEO_ISYS_RECEIVER, subDeviceNodeName) == OK) {
+        LOG1("%s: found ISYS receiver subdevice %s", __func__, subDeviceNodeName.c_str());
+    }
+
+    deinitDev();
+
+    mIsysReceiverSubDev = V4l2DeviceFactory::getSubDev(mCameraId, subDeviceNodeName);
+
+    int status = mIsysReceiverSubDev->SubscribeEvent(V4L2_EVENT_FRAME_SYNC);
+    CheckError(status != OK, status, "%s: Failed to subscribe sync event 0", __func__);
+    LOG1("%s: Using SOF event id 0 for sync", __func__);
+
+    return OK;
+}
+
+int SofSource::deinitDev()
+{
+    if (mIsysReceiverSubDev == nullptr) return OK;
+
+    int status = 0;
+    status = mIsysReceiverSubDev->UnsubscribeEvent(V4L2_EVENT_FRAME_SYNC);
+    if (status == OK) {
+        LOG1("%s: Unsubscribe SOF event id 0 done", __func__);
+    } else {
+        LOGE("%s: Failed to unsubscribe SOF event 0, status: %d", __func__, status);
+    }
+
+    return status;
+}
+
+int SofSource::configure()
+{
+    if (mSofDisabled) {
+        return OK;
+    }
+
+    return initDev();
+}
+
+int SofSource::start()
+{
+    LOG1("%s", __func__);
+    if (mSofDisabled) {
+        return OK;
+    }
+
+    int status = mPollThread->run("SofSource", PRIORITY_URGENT_AUDIO);
+    mExitPending = false;
+    return status;
+
+}
+
+int SofSource::stop()
+{
+    LOG1("%s", __func__);
+    if (mSofDisabled) {
+        return OK;
+    }
+
+    mExitPending = true;
+    int status = mPollThread->requestExitAndWait();
+    return status;
+
+}
+
+int SofSource::poll()
+{
+    int ret = 0;
+    const int pollTimeoutCount = 10;
+    const int pollTimeout = 1000;
+
+    std::vector<V4L2Device*> pollDevs;
+    pollDevs.push_back(mIsysReceiverSubDev);
+    V4L2DevicePoller poller {pollDevs, -1};
+
+    vector<V4L2Device*> readyDevices;
+
+    LOG2("@%s", __func__);
+
+    int timeOutCount = pollTimeoutCount;
+
+    while (timeOutCount-- && ret == 0) {
+
+        ret = poller.Poll(pollTimeout, POLLPRI | POLLIN | POLLOUT | POLLERR, &readyDevices);
+
+        if (ret == 0 && mExitPending) {
+            //timed out
+            LOGD("@%s: Timedout or thread is not running, ret = %d", __func__, ret);
+            return BAD_VALUE;
+        }
+    }
+
+    //handle the poll error
+    if (ret < 0) {
+        if (mExitPending) {
+            //Exiting, no error
+            return 0;
+        }
+
+        LOGE("%s: Poll error", __func__);
+        return ret;
+    } else if (ret == 0) {
+        LOGD("@%s, Sof poll timeout.", __func__);
+        return 0;
+    }
+
+    struct v4l2_event event;
+    CLEAR(event);
+    mIsysReceiverSubDev->DequeueEvent(&event);
+
+    EventDataSync syncData;
+    syncData.sequence = event.u.frame_sync.frame_sequence;
+    syncData.timestamp.tv_sec = event.timestamp.tv_sec;
+    syncData.timestamp.tv_usec = (event.timestamp.tv_nsec / 1000);
+    LOG2("%s:sof event sequence %ld, event.id %u", __func__, syncData.sequence, event.id);
+    TRACE_LOG_POINT("SofSource", "receive sof event", MAKE_COLOR(syncData.sequence),
+                    syncData.sequence);
+    EventData eventData;
+    eventData.type = EVENT_ISYS_SOF;
+    eventData.buffer = nullptr;
+    eventData.data.sync = syncData;
+    notifyListeners(eventData);
+
+    return 0;
+}
+
+}
diff --git a/camera/hal/intel/ipu6/src/core/SofSource.h b/camera/hal/intel/ipu6/src/core/SofSource.h
new file mode 100644
index 000000000000..3b7ddf02556a
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/SofSource.h
@@ -0,0 +1,63 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <v4l2_device.h>
+
+#include <vector>
+
+#include "CameraEvent.h"
+
+#include "iutils/Thread.h"
+
+namespace icamera {
+
+//Event source for SOF event polled from subdevice.
+class SofSource : public EventSource {
+public:
+    SofSource(int cameraId);
+    ~SofSource();
+    int init();
+    int deinit();
+    int configure();
+    int start();
+    int stop();
+private:
+    int initDev();
+    int deinitDev();
+
+private:
+    class PollThread : public Thread {
+        SofSource *mSofSource;
+        public:
+            PollThread(SofSource *sofSource)
+                    : mSofSource(sofSource) { }
+
+            virtual bool threadLoop() {
+                int ret = mSofSource->poll();
+                return (ret == 0) ? true : false;
+            }
+    };
+    PollThread* mPollThread;
+    int mCameraId;
+    V4L2Subdevice* mIsysReceiverSubDev;
+    bool mExitPending;
+    bool mSofDisabled;
+
+    int poll();
+};
+}
diff --git a/camera/hal/intel/ipu6/src/core/StreamSource.h b/camera/hal/intel/ipu6/src/core/StreamSource.h
new file mode 100644
index 000000000000..1fff90dacdb7
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/StreamSource.h
@@ -0,0 +1,48 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "BufferQueue.h"
+#include "iutils/Errors.h"
+
+namespace icamera {
+
+/**
+ * \interface StreamSource
+ * It's an abstract interface for buffer producers, like CaptureUnit, FileSource
+ * or external source producer.
+ */
+class StreamSource : public BufferProducer {
+public:
+    StreamSource(int memType) : BufferProducer(memType) {}
+    virtual ~StreamSource() {}
+    /* Initialize stream source */
+    virtual int init() = 0;
+    /* Deinitialize stream source */
+    virtual void deinit() = 0;
+    /* Configure stream source */
+    virtual int configure(const std::map<Port, stream_t>& outputFrames,
+                          const std::vector<ConfigMode>& configModes) = 0;
+    /* Start stream source */
+    virtual int start() = 0;
+    /* Stop stream source */
+    virtual int stop() = 0;
+    /* Remove all liateners */
+    virtual void removeAllFrameAvailableListener() = 0;
+};
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/SwImageProcessor.cpp b/camera/hal/intel/ipu6/src/core/SwImageProcessor.cpp
new file mode 100644
index 000000000000..9652d6558405
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/SwImageProcessor.cpp
@@ -0,0 +1,158 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "SwImageProcessor"
+
+#include "iutils/Utils.h"
+#include "iutils/SwImageConverter.h"
+#include "iutils/CameraLog.h"
+#include "iutils/CameraDump.h"
+
+#include "PlatformData.h"
+#include "CameraBuffer.h"
+
+#include "SwImageProcessor.h"
+
+namespace icamera {
+
+SwImageProcessor::SwImageProcessor(int cameraId) : mCameraId(cameraId)
+{
+    LOGW("@%s: You are running the SwProcessor instead of PSYS pipeline. SwProcessor is for debug only", __func__);
+    LOG1("@%s camera id:%d", __func__, mCameraId);
+
+    mProcessThread = new ProcessThread(this);
+}
+
+SwImageProcessor::~SwImageProcessor()
+{
+    LOGW("@%s: You are running the SwProcessor instead of PSYS pipeline. SwProcessor is for debug only", __func__);
+
+    mProcessThread->join();
+    delete mProcessThread;
+}
+
+int SwImageProcessor::start()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("%s", __func__);
+    AutoMutex   l(mBufferQueueLock);
+
+    int memType = mOutputFrameInfo.begin()->second.memType;
+    CheckError(memType == V4L2_MEMORY_DMABUF, BAD_VALUE,
+          "@%s: DMABUF is not supported in SwProcessor as output", __func__);
+
+    int ret = allocProducerBuffers(mCameraId, MAX_BUFFER_COUNT);
+    CheckError(ret != OK, ret, "@%s: Allocate Buffer failed", __func__);
+    mThreadRunning = true;
+    mProcessThread->run("SwImageProcessor", PRIORITY_NORMAL);
+
+    return 0;
+}
+
+void SwImageProcessor::stop()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("%s", __func__);
+
+    mProcessThread->requestExit();
+    {
+        AutoMutex l(mBufferQueueLock);
+        mThreadRunning = false;
+        //Wakeup the thread to exit
+        mFrameAvailableSignal.signal();
+        mOutputAvailableSignal.signal();
+    }
+
+    mProcessThread->requestExitAndWait();
+
+    // Thread is not running. It is safe to clear the Queue
+    clearBufferQueues();
+}
+
+int SwImageProcessor::processNewFrame()
+{
+    PERF_CAMERA_ATRACE();
+    LOG2("%s", __func__);
+
+    int ret = OK;
+    std::map<Port, std::shared_ptr<CameraBuffer> > srcBuffers, dstBuffers;
+    std::shared_ptr<CameraBuffer> cInBuffer;
+    Port inputPort = INVALID_PORT;
+
+    { // Auto lock mBufferQueueLock scope
+    ConditionLock lock(mBufferQueueLock);
+    ret = waitFreeBuffersInQueue(lock, srcBuffers, dstBuffers);
+
+    if (!mThreadRunning) return -1;
+
+    CheckError((ret < 0), -1, "@%s: wake up from the wait abnomal such as stop", __func__);
+
+    inputPort = srcBuffers.begin()->first;
+    cInBuffer = srcBuffers[inputPort];
+
+    for (auto& output: mOutputQueue) {
+        output.second.pop();
+    }
+
+    for (auto& input: mInputQueue) {
+        input.second.pop();
+    }
+    } // End of auto lock mBufferQueueLock scope
+
+    CheckError(!cInBuffer, BAD_VALUE, "Invalid input buffer.");
+
+    for (auto& dst : dstBuffers) {
+        Port port = dst.first;
+        std::shared_ptr<CameraBuffer> cOutBuffer = dst.second;
+        // If the output buffer is nullptr, that means user doesn't request that buffer,
+        // so it doesn't need to be handled here.
+        if (!cOutBuffer) {
+            continue;
+        }
+
+        //No Lock for this function make sure buffers are not freed before the stop
+        ret = SwImageConverter::convertFormat(cInBuffer->getWidth(), cInBuffer->getHeight(),
+                (unsigned char *)cInBuffer->getBufferAddr(), cInBuffer->getBufferSize(), cInBuffer->getFormat(),
+                (unsigned char *)cOutBuffer->getBufferAddr(), cOutBuffer->getBufferSize(), cOutBuffer->getFormat());
+        CheckError((ret < 0), ret, "format convertion failed with %d", ret);
+
+        if (CameraDump::isDumpTypeEnable(DUMP_SW_IMG_PROC_OUTPUT)) {
+            CameraDump::dumpImage(mCameraId, cOutBuffer, M_SWIPOP);
+        }
+
+        //update the interlaced field, sequence, and timestamp  from the src buf to dst buf
+        cOutBuffer->updateV4l2Buffer(*cInBuffer->getV4L2Buffer().Get());
+
+        //Notify listener: No lock here: mBufferConsumerList will not updated in this state
+        for (auto &it : mBufferConsumerList) {
+            it->onFrameAvailable(port, cOutBuffer);
+        }
+    }
+
+    {
+        PERF_CAMERA_ATRACE_PARAM3("sof.sequence", cInBuffer->getSequence(), "csi2_port", cInBuffer->getCsi2Port(), \
+                                    "virtual_channel", cInBuffer->getVirtualChannel());
+    }
+
+    // Return the buffers to the producer
+    if (mBufferProducer) {
+        mBufferProducer->qbuf(inputPort, cInBuffer);
+    }
+
+    return OK;
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/SwImageProcessor.h b/camera/hal/intel/ipu6/src/core/SwImageProcessor.h
new file mode 100644
index 000000000000..caf832608a73
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/SwImageProcessor.h
@@ -0,0 +1,46 @@
+/*
+ * Copyright (C) 2015-2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "BufferQueue.h"
+
+namespace icamera {
+
+/**
+  * SwImageProcessor runs the Image Process Alogirhtm in the CPU.
+  * It implements the BufferConsumer and BufferProducer Interface
+  * This class is for debug purpose when the PsysProcess is not ready.
+  */
+class SwImageProcessor: public BufferQueue {
+public:
+    SwImageProcessor(int cameraId);
+    virtual ~SwImageProcessor();
+
+    /**
+     * \brief Buffer producer Interface
+     */
+    virtual int     start();
+    virtual void    stop();
+
+private:
+    int processNewFrame();
+
+private:
+    int mCameraId;
+};
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/SyncManager.cpp b/camera/hal/intel/ipu6/src/core/SyncManager.cpp
new file mode 100644
index 000000000000..0ebf7d52b471
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/SyncManager.cpp
@@ -0,0 +1,198 @@
+/*
+ * Copyright (C) 2018-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "SyncManager"
+
+#include <sys/sysinfo.h>
+#include <math.h>
+#include "iutils/CameraLog.h"
+#include "SyncManager.h"
+
+namespace icamera {
+SyncManager *SyncManager::sInstance = nullptr;
+Mutex  SyncManager::sLock;
+
+#define SEC_TO_MS(sec) ((sec) * (1000))
+#define USEC_TO_MS(usec) ((usec) / (1000))
+
+const int max_vc_sync_count = 128;
+
+SyncManager* SyncManager::getInstance()
+{
+    AutoMutex lock(sLock);
+    if (sInstance == nullptr) {
+        sInstance = new SyncManager();
+    }
+
+    return sInstance;
+}
+
+void SyncManager::releaseInstance()
+{
+    AutoMutex lock(sLock);
+    LOG1("@%s", __func__);
+
+    if (sInstance) {
+        delete sInstance;
+        sInstance = nullptr;
+    }
+}
+
+SyncManager::SyncManager()
+{
+    LOG1("@%s", __func__);
+    AutoMutex lock(mLock);
+    for (int i = 0; i < MAX_CAMERA_NUMBER; i++) {
+       for (int j = 0; j < MAX_BUFFER_COUNT; j++) {
+           mCameraBufInfo[i][j].sequence = -1;
+           CLEAR(mCameraBufInfo[i][j].sof_ts);
+       }
+    }
+
+    mTotalSyncCamNum = 0;
+    for (int i = 0; i < MAX_CAMERA_NUMBER; i++)
+        mVcSyncCount[i] = 0;
+}
+
+SyncManager::~SyncManager()
+{
+    LOG1("@%s", __func__);
+}
+
+bool SyncManager::isSynced(int cameraId, long sequence)
+{
+    LOG1("@%s", __func__);
+    const int TIME_DIFF_MS = 2;
+    bool sync = true;
+    int index = sequence % MAX_BUFFER_COUNT;
+
+    AutoMutex lock(mLock);
+    camera_buf_info bufInfo = mCameraBufInfo[cameraId][index];
+
+    int  syncNum = 0;
+    bool isSync[MAX_CAMERA_NUMBER];
+    long frameSyncedMs[MAX_CAMERA_NUMBER];
+    long curFrameMs = USEC_TO_MS(bufInfo.sof_ts.tv_usec) + SEC_TO_MS(bufInfo.sof_ts.tv_sec);
+
+    //first step: To check whether the current frame is synced with others camera channel
+    //            if timestamp difference <= 2ms, then think the frame is synced
+    for (int i = 0; i < MAX_CAMERA_NUMBER; i++) {
+        isSync[i] = false;
+        frameSyncedMs[i] = 0;
+        if (mCameraBufInfo[i][0].sequence == -1 || i == cameraId) {
+            continue;
+        }
+        for (int j = 0; j < MAX_BUFFER_COUNT; j++) {
+            if (mCameraBufInfo[i][j].sequence >= 0) {
+                camera_buf_info &temp = mCameraBufInfo[i][j];
+                long tempFrameMs = USEC_TO_MS(temp.sof_ts.tv_usec) + SEC_TO_MS(temp.sof_ts.tv_sec);
+                if (abs(tempFrameMs - curFrameMs ) <= TIME_DIFF_MS) {
+                    isSync[syncNum] = true;
+                    frameSyncedMs[syncNum] = tempFrameMs;
+                    syncNum++;
+                    break;
+                }
+            }
+        }
+    }
+    //second step: if current frame is synced with frames from other cameraID,
+    //             to check whether other 3 channel frames synced or not
+    if (syncNum >= mTotalSyncCamNum - 1) {
+        for (int i = 0; i < mTotalSyncCamNum - 1; i++) {
+            if (isSync[i]) {
+                if ((i + 1 < mTotalSyncCamNum - 1) &&
+                        abs(frameSyncedMs[i]-frameSyncedMs[i+1]) <= TIME_DIFF_MS) {
+                    sync &= true;
+                } else if ((i + 1 == mTotalSyncCamNum - 1) &&
+                        abs(frameSyncedMs[i]-frameSyncedMs[0]) <= TIME_DIFF_MS) {
+                    sync &= true;
+                } else {
+                    sync &= false;
+                }
+            }
+        }
+    } else {
+        sync = false;
+    }
+    LOG1("Id:%d, sof_ts:%ldms, sequence:%ld sync %d", cameraId, curFrameMs, sequence, sync);
+    return sync;
+}
+
+void SyncManager::updateCameraBufInfo(int cameraId, camera_buf_info* info)
+{
+    LOG1("@%s", __func__);
+    int index = info->sequence % MAX_BUFFER_COUNT;
+    AutoMutex lock(mLock);
+    mCameraBufInfo[cameraId][index] = *info;
+}
+
+void SyncManager::updateSyncCamNum()
+{
+    AutoMutex l(mLock);
+    CheckError(mTotalSyncCamNum >= MAX_CAMERA_NUMBER, VOID_VALUE,
+               "%s: sync cameras enough!", __func__);
+    mTotalSyncCamNum++;
+}
+
+bool SyncManager::vcSynced(int vc)
+{
+    CheckError(vc >= MAX_CAMERA_NUMBER, false, "%s: vc %d error!", __func__, vc);
+
+    AutoMutex l(mVcSyncLock);
+    int count = mVcSyncCount[vc];
+    int minCount = INT_MAX;
+    int maxCount = 0;
+
+    for (int i = 0; i < mTotalSyncCamNum; i++) {
+        minCount = std::min(minCount, mVcSyncCount[i]);
+        maxCount = std::max(maxCount, mVcSyncCount[i]);
+    }
+
+    // Check again if status is circling back to 0.
+    // Most of time handling code won't be executed because condition is false.
+    if (maxCount - minCount > max_vc_sync_count / 2) {
+        minCount = max_vc_sync_count;
+        maxCount = 0;
+        for (int i = 0; i < mTotalSyncCamNum; i++) {
+            count = (mVcSyncCount[i] + max_vc_sync_count) % (max_vc_sync_count + max_vc_sync_count / 4);
+            minCount = std::min(minCount, count);
+            maxCount = std::max(maxCount, count);
+        }
+        count = (mVcSyncCount[vc] + max_vc_sync_count) % (max_vc_sync_count + max_vc_sync_count / 4);
+    }
+
+    if (count > minCount) {
+        LOGVCSYNC("vc %d ready: false", vc);
+        return false;
+    } else
+        return true;
+};
+
+void SyncManager::updateVcSyncCount(int vc)
+{
+    CheckError(vc >= MAX_CAMERA_NUMBER, VOID_VALUE, "%s: vc %d error!", __func__, vc);
+    AutoMutex l(mVcSyncLock);
+    mVcSyncCount[vc] = (mVcSyncCount[vc] + 1) % (max_vc_sync_count + 1);
+};
+
+void SyncManager::printVcSyncCount(void)
+{
+    AutoMutex l(mVcSyncLock);
+    for (int i = 0; i < mTotalSyncCamNum; i++)
+        LOGVCSYNC("[%d]", mVcSyncCount[i]);
+}
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/SyncManager.h b/camera/hal/intel/ipu6/src/core/SyncManager.h
new file mode 100644
index 000000000000..c0a6af883817
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/SyncManager.h
@@ -0,0 +1,58 @@
+/*
+ * Copyright (C) 2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "PlatformData.h"
+
+namespace icamera {
+
+struct camera_buf_info {
+    long sequence;
+    struct timeval sof_ts;
+};
+class SyncManager {
+private:
+    //Prevent to create multiple instances
+    SyncManager();
+    ~SyncManager();
+public:
+     /**
+      * releaseInstance
+      * This function must be called when the hal is destroyed.
+      */
+    static void releaseInstance();
+    static SyncManager* getInstance();
+
+    bool isSynced(int cameraId, long sequence);
+    void updateCameraBufInfo(int cameraId, camera_buf_info* info);
+
+    void updateSyncCamNum();
+
+    bool vcSynced(int vc);
+    void updateVcSyncCount(int vc);
+    void printVcSyncCount();
+private:
+    static SyncManager* sInstance;
+    static Mutex sLock;
+    Mutex mLock;
+    struct camera_buf_info mCameraBufInfo[MAX_CAMERA_NUMBER][MAX_BUFFER_COUNT];
+
+    int mVcSyncCount[MAX_CAMERA_NUMBER];
+    Mutex mVcSyncLock;
+    int mTotalSyncCamNum;
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/core/TNRCommon.h b/camera/hal/intel/ipu6/src/core/TNRCommon.h
new file mode 100644
index 000000000000..f5d80ff96e85
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/TNRCommon.h
@@ -0,0 +1,45 @@
+/*
+ * Copyright (C) 2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "ia_pal_types_isp_parameters_autogen.h"
+#include "tnr7_ipu3_output_interface.h"
+
+#define TNR7US_REFERENCE_BUFFER_COUNT 2
+#define TNR7US_RESTART_THRESHOLD 3
+
+namespace icamera {
+
+typedef struct Tnr7Param {
+    tnr_scale_1_0_t scale;
+    tnr7_ims_1_0_t ims;
+    tnr7_bc_1_0_t bc;
+    tnr7_blend_1_0_t blend;
+} Tnr7Param;
+
+typedef struct TnrResolution {
+    int width;
+    int height;
+} TnrResolution;
+
+typedef struct TnrRunInfo {
+    int32_t inHandle;
+    int32_t outHandle;
+    int32_t paramHandle;
+    int gain;
+} TnrRunInfo;
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/psysprocessor/GPUExecutor.cpp b/camera/hal/intel/ipu6/src/core/psysprocessor/GPUExecutor.cpp
new file mode 100644
index 000000000000..a2ef2034514c
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/psysprocessor/GPUExecutor.cpp
@@ -0,0 +1,482 @@
+/*
+ * Copyright (C) 2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "GPUExecutor"
+
+#include "GPUExecutor.h"
+
+extern "C" {
+#include <ia_cipf_css/ia_cipf_css.h>
+#include <ia_pal_types_isp_ids_autogen.h>
+}
+
+#include <memory>
+#include <string>
+#include <vector>
+
+#include "3a/AiqResult.h"
+#include "3a/AiqResultStorage.h"
+#include "FormatUtils.h"
+#include "PSysDAG.h"
+#include "SyncManager.h"
+#include "ia_pal_output_data.h"
+#include "iutils/CameraDump.h"
+
+using std::map;
+using std::shared_ptr;
+using std::string;
+using std::vector;
+
+#define external_gpu_stage_uid(id) ia_fourcc(((id & 0xFF00) >> 8), id, 'E', 'X')
+
+namespace icamera {
+
+GPUExecutor::GPUExecutor(int cameraId, const ExecutorPolicy& policy, vector<string> exclusivePGs,
+                         PSysDAG* psysDag, shared_ptr<IGraphConfig> gc)
+        : PipeExecutor(cameraId, policy, exclusivePGs, psysDag, gc),
+          mTnr7usParam(nullptr),
+          mIntelTNR(nullptr),
+          mLastSequence(UINT32_MAX) {
+    LOG1("@%s %s", __func__, mName.c_str());
+}
+
+GPUExecutor::~GPUExecutor() {
+    LOG1("@%s %s", __func__, mName.c_str());
+    mPGExecutors.clear();
+}
+
+int GPUExecutor::initPipe() {
+    LOG1("@%s:%s", __func__, getName());
+    CheckError(mGraphConfig == nullptr, BAD_VALUE, "%s, the graph config is NULL, BUG!", __func__);
+
+    vector<IGraphType::PipelineConnection> connVector;
+
+    int ret = mGraphConfig->pipelineGetConnections(mPGNames, &connVector);
+    CheckError(ret != OK || connVector.empty(), ret, "Failed to get connections for executor:%s",
+               mName.c_str());
+
+    ret = createPGs();
+    CheckError(ret != OK, ret, "Failed to create PGs for executor: %s", mName.c_str());
+
+    ret = analyzeConnections(connVector);
+    CheckError(ret != OK, ret, "Failed to analyze connections for executor: %s", mName.c_str());
+
+    assignDefaultPortsForTerminals();
+    return OK;
+}
+
+// GPU executor doesn't have HW pg, create pg according to graph settings
+int GPUExecutor::createPGs() {
+    LOG1("@%s:%s", __func__, getName());
+
+    for (auto const& pgName : mPGNames) {
+        int pgId = mGraphConfig->getPgIdByPgName(pgName);
+        CheckError(pgId == -1, BAD_VALUE, "Cannot get PG ID for %s", pgName.c_str());
+        ExecutorUnit pgUnit;
+        pgUnit.pgId = pgId;
+        // GPU Executor doesn't have pg
+        pgUnit.pg = nullptr;
+        pgUnit.stageId = external_gpu_stage_uid(pgId);
+        mPGExecutors.push_back(pgUnit);
+    }
+    return OK;
+}
+
+int GPUExecutor::start() {
+    LOG1("%s executor:%s", __func__, mName.c_str());
+
+    mProcessThread = new ProcessThread(this);
+
+    CheckError(mPGExecutors.empty(), UNKNOWN_ERROR, "PGExecutors couldn't be empty");
+    ExecutorUnit* tnrUnit = &mPGExecutors[0];
+    int ret = OK;
+    if (!tnrUnit->inputTerminals.empty()) {
+        ia_uid term = tnrUnit->inputTerminals.at(0);
+        CheckError(mTerminalsDesc.find(term) == mTerminalsDesc.end(), NO_INIT,
+                   "Can't find TerminalDescriptor");
+
+        const FrameInfo& frameInfo = mTerminalsDesc[term].frameDesc;
+        mIntelTNR = std::unique_ptr<IntelTNR7US>(new IntelTNR7US());
+        ret = mIntelTNR->init(frameInfo.mWidth, frameInfo.mHeight);
+        CheckError(ret, UNKNOWN_ERROR, "@%s: init tnr failed", __func__);
+    }
+    AutoMutex l(mBufferQueueLock);
+    ret = allocBuffers();
+    CheckError(ret, UNKNOWN_ERROR, "@%s: allocBuffers failed", __func__);
+    dumpPGs();
+
+    mThreadRunning = true;
+    mProcessThread->run(mName.c_str(), PRIORITY_NORMAL);
+
+    return ret;
+}
+
+void GPUExecutor::stop() {
+    LOG1("%s executor:%s", __func__, mName.c_str());
+
+    mProcessThread->requestExitAndWait();
+
+    // Thread is not running. It is safe to clear the Queue
+    releaseBuffers();
+    clearBufferQueues();
+
+    delete mProcessThread;
+    mIntelTNR = nullptr;
+}
+
+int GPUExecutor::allocBuffers() {
+    LOG1("%s executor:%s", __func__, mName.c_str());
+
+    releaseBuffers();
+
+    // Allocate buffers for producer executor (external)
+    // don't need to check mPGExecutors and inputTerminals, already check before
+    ia_uid term = mPGExecutors[0].inputTerminals.at(0);
+    Port inputPort = mTerminalsDesc[term].assignedPort;
+
+    int srcFmt = mTerminalsDesc[term].frameDesc.mFormat;
+    int srcWidth = mTerminalsDesc[term].frameDesc.mWidth;
+    int srcHeight = mTerminalsDesc[term].frameDesc.mHeight;
+    int size = PGCommon::getFrameSize(srcFmt, srcWidth, srcHeight, true);
+
+    for (int i = 0; i < MAX_BUFFER_COUNT; i++) {
+        shared_ptr<CameraBuffer> buf = std::make_shared<CameraBuffer>(
+            mCameraId, BUFFER_USAGE_PSYS_INPUT, V4L2_MEMORY_USERPTR, size, i, srcFmt);
+        CheckError(!buf, NO_MEMORY, "@%s: fail to alloc CameraBuffer", __func__);
+
+        void* buffer = mIntelTNR->allocCamBuf(size, i);
+        CheckError(!buffer, NO_MEMORY, "@%s: fail to alloc shared memory", __func__);
+        buf->setUserBufferInfo(srcFmt, srcWidth, srcHeight, buffer);
+
+        mInternalBuffers[inputPort].push_back(buf);
+        mBufferProducer->qbuf(inputPort, buf);
+    }
+    int ret = mIntelTNR->allocRefBufs(size);
+    CheckError(ret, UNKNOWN_ERROR, "@%s: alloc tnr reference buffer failed", __func__);
+
+    mTnr7usParam = mIntelTNR->allocTnr7ParamBuf();
+    CheckError(!mTnr7usParam, NO_MEMORY, "@%s: Allocate Param buffer failed", __func__);
+    CLEAR(*mTnr7usParam);
+
+    // Allocate internal output buffers to support pipe execution without user output buffer
+    for (auto const& item : mOutputFrameInfo) {
+        int fmt = item.second.format;
+        int width = item.second.width;
+        int height = item.second.height;
+        int size = CameraUtils::getFrameSize(fmt, width, height, true);
+        shared_ptr<CameraBuffer> buf = CameraBuffer::create(
+            mCameraId, BUFFER_USAGE_PSYS_INPUT, V4L2_MEMORY_USERPTR, size, 0, fmt, width, height);
+        CheckError(!buf, NO_MEMORY, "@%s: Allocate internal output buffer failed", __func__);
+        mInternalOutputBuffers[item.first] = buf;
+    }
+
+    return OK;
+}
+
+void GPUExecutor::releaseBuffers() {
+    if (mIntelTNR) {
+        mIntelTNR->freeAllBufs();
+    }
+    // Release internel frame buffers
+    mInternalOutputBuffers.clear();
+    mInternalBuffers.clear();
+}
+
+int GPUExecutor::processNewFrame() {
+    PERF_CAMERA_ATRACE();
+
+    int ret = OK;
+    CameraBufferPortMap inBuffers, outBuffers;
+    // Wait frame buffers.
+    {
+        ConditionLock lock(mBufferQueueLock);
+        ret = waitFreeBuffersInQueue(lock, inBuffers, outBuffers);
+        // Already stopped
+        if (!mThreadRunning) return -1;
+
+        if (ret != OK) return OK;  // Wait frame buffer error should not involve thread exit.
+
+        CheckError(inBuffers.empty() || outBuffers.empty(), UNKNOWN_ERROR,
+                   "Failed to get input or output buffers.");
+
+        for (auto& output : mOutputQueue) {
+            output.second.pop();
+        }
+
+        for (auto& input : mInputQueue) {
+            input.second.pop();
+        }
+    }
+
+    // Check if the executor needs to run the actual pipeline.
+    // It only needs to run when there is at least one valid output buffer.
+    if (!hasValidBuffers(outBuffers)) {
+        // As an output edge, return the inBuffer when has no outBuffer,
+        for (const auto& item : inBuffers) {
+            mBufferProducer->qbuf(item.first, item.second);
+        }
+        return OK;
+    }
+
+    // Should find first not none input buffer instead of always use the first one.
+    shared_ptr<CameraBuffer> inBuf = inBuffers.begin()->second;
+    CheckError(!inBuf, UNKNOWN_ERROR, "@%s: no valid input buffer", __func__);
+    v4l2_buffer_t inV4l2Buf = *inBuf->getV4L2Buffer().Get();
+
+    // Fill real buffer to run pipe
+    for (auto& item : outBuffers) {
+        if (item.second.get() == nullptr) {
+            item.second = mInternalOutputBuffers[item.first];
+        }
+    }
+
+    std::shared_ptr<CameraBuffer> outBuf = outBuffers.begin()->second;
+    CheckError(!outBuf, UNKNOWN_ERROR, "@%s: no valid output buffer", __func__);
+
+    ret = runTnrFrame(inBuf, outBuf);
+    CheckError(ret != OK, ret, "@%s: run tnr failed", __func__);
+
+    // Remove internal output buffers
+    for (auto& item : outBuffers) {
+        if (item.second.get() == mInternalOutputBuffers[item.first].get()) {
+            item.second = nullptr;
+        }
+    }
+
+    notifyFrameDone(inV4l2Buf, outBuffers);
+
+    // Return buffers for the executor which is NOT an input edge
+    for (auto const& portBufferPair : inBuffers) {
+        // Queue buffer to producer
+        mBufferProducer->qbuf(portBufferPair.first, portBufferPair.second);
+    }
+
+    return OK;
+}
+
+/* get the tnr7 tuning data from ISP. PalOutput can help decode IPU tnr parameters from IPU result
+** The GPU tnr7 parameters struct is different from IPU tnr parameters struct, so here we copy
+** the useful parameter to tnr7 parameters object
+*/
+int GPUExecutor::updateTnrISPConfig(Tnr7Param* pbuffer, uint32_t sequence) {
+    int ret = ia_err_none;
+    if (mAdaptor) {
+        CLEAR(*pbuffer);
+        ia_isp_bxt_program_group* pg = mGraphConfig->getProgramGroup(mStreamId);
+        CheckError(pg == nullptr, UNKNOWN_ERROR, "Can't get IPU program group");
+
+        // Get ISP parameters
+        LOG1(" %s update tnr parameters for sequence %d", __func__, sequence);
+        ia_binary_data* ipuParameters = mAdaptor->getIpuParameter(sequence, mStreamId);
+        CheckError(ipuParameters == nullptr, UNKNOWN_ERROR, "Failed to find ISP parameter");
+
+        ia_isp_bxt_program_group tmpPg = *pg;
+        tmpPg.kernel_count = 0;
+        int blankKernelSize = mAdaptor->getPalOutputDataSize(&tmpPg);
+
+        PalOutputData PalOutput(pg);
+        ia_binary_data tmpTnr7BinData = {0};
+        tmpTnr7BinData.size = ipuParameters->size - blankKernelSize;
+        tmpTnr7BinData.data = reinterpret_cast<char*>(ipuParameters->data) + blankKernelSize;
+
+        PalOutput.setPublicOutput(&tmpTnr7BinData);
+        ia_pal_isp_tnr7_bc_1_0_t* pBc;
+        ia_pal_isp_tnr7_blend_1_0_t* pBlend;
+        ia_pal_isp_tnr7_ims_1_0_t* pIms;
+        ret |= PalOutput.getKernelPublicOutput(ia_pal_uuid_isp_tnr7_bc_1_0, (void*&)pBc);
+        ret |= PalOutput.getKernelPublicOutput(ia_pal_uuid_isp_tnr7_blend_1_0, (void*&)pBlend);
+        ret |= PalOutput.getKernelPublicOutput(ia_pal_uuid_isp_tnr7_ims_1_0, (void*&)pIms);
+        CheckError(ret != ia_err_none, UNKNOWN_ERROR, "Can't read isp tnr7 parameters");
+
+        tnr7_bc_1_0_t* tnr7_bc = &(pbuffer->bc);
+        tnr7_blend_1_0_t* tnr7_blend = &(pbuffer->blend);
+        tnr7_ims_1_0_t* tnr7_ims = &(pbuffer->ims);
+
+        // tnr7 ims params
+        tnr7_ims->enable = pIms->enable;
+        tnr7_ims->update_limit = pIms->update_limit;
+        tnr7_ims->update_coeff = pIms->update_coeff;
+        tnr7_ims->gpu_mode = pIms->gpu_mode;
+        for (int i = 0; i < sizeof(pIms->d_ml) / sizeof(int32_t); i++) {
+            tnr7_ims->d_ml[i] = pIms->d_ml[i];
+            tnr7_ims->d_slopes[i] = pIms->d_slopes[i];
+            tnr7_ims->d_top[i] = pIms->d_top[i];
+            tnr7_ims->outofbounds[i] = pIms->outofbounds[i];
+        }
+
+        // tnr7 bc params
+        tnr7_bc->enable = pBc->enable;
+        tnr7_bc->is_first_frame = pBc->is_first_frame;
+        tnr7_bc->do_update = pBc->do_update;
+        tnr7_bc->gpu_mode = pBc->gpu_mode;
+        tnr7_bc->tune_sensitivity = pBc->tune_sensitivity;
+        for (int i = 0; i < sizeof(pBc->coeffs) / sizeof(int32_t); i++) {
+            tnr7_bc->coeffs[i] = pBc->coeffs[i];
+        }
+
+        tnr7_bc->global_protection = pBc->global_protection;
+        tnr7_bc->global_protection_motion_level = pBc->global_protection_motion_level;
+        for (int i = 0; i < sizeof(pBc->global_protection_sensitivity_lut_values) / sizeof(int32_t);
+             i++) {
+            tnr7_bc->global_protection_sensitivity_lut_values[i] =
+                pBc->global_protection_sensitivity_lut_values[i];
+        }
+        for (int i = 0; i < sizeof(pBc->global_protection_sensitivity_lut_slopes) / sizeof(int32_t);
+             i++) {
+            tnr7_bc->global_protection_sensitivity_lut_slopes[i] =
+                pBc->global_protection_sensitivity_lut_slopes[i];
+        }
+
+        // tnr7 blend params
+        tnr7_blend->enable = pBlend->enable;
+        tnr7_blend->enable_main_output = pBlend->enable_main_output;
+        tnr7_blend->enable_vision_output = pBlend->enable_vision_output;
+        tnr7_blend->single_output_mode = pBlend->single_output_mode;
+        tnr7_blend->spatial_weight_coeff = pBlend->spatial_weight_coeff;
+        tnr7_blend->max_recursive_similarity = pBlend->max_recursive_similarity;
+        tnr7_blend->spatial_alpha = pBlend->spatial_alpha;
+        tnr7_blend->max_recursive_similarity_vsn = pBlend->max_recursive_similarity_vsn;
+        for (int i = 0; i < sizeof(pBlend->w_out_prev_LUT) / sizeof(int32_t); i++) {
+            tnr7_blend->w_out_prev_LUT[i] = pBlend->w_out_prev_LUT[i];
+            tnr7_blend->w_out_spl_LUT[i] = pBlend->w_out_spl_LUT[i];
+            tnr7_blend->w_vsn_out_prev_LUT[i] = pBlend->w_vsn_out_prev_LUT[i];
+            tnr7_blend->w_vsn_out_spl_LUT[i] = pBlend->w_vsn_out_spl_LUT[i];
+        }
+
+        for (int i = 0; i < sizeof(pBlend->output_cu_a) / sizeof(int32_t); i++) {
+            tnr7_blend->output_cu_a[i] = pBlend->output_cu_a[i];
+            tnr7_blend->output_cu_b[i] = pBlend->output_cu_b[i];
+        }
+
+        for (int i = 0; i < sizeof(pBlend->output_cu_x) / sizeof(int32_t); i++) {
+            tnr7_blend->output_cu_x[i] = pBlend->output_cu_x[i];
+        }
+    }
+
+    return ret;
+}
+
+int GPUExecutor::runTnrFrame(const std::shared_ptr<CameraBuffer>& inBuf,
+                             std::shared_ptr<CameraBuffer> outBuf) {
+    PERF_CAMERA_ATRACE();
+    CheckError(!inBuf->getBufferAddr(), UNKNOWN_ERROR, "%s invalid input buffer", __func__);
+
+    if (mPolicyManager) {
+        // Check if need to wait other executors.
+        mPolicyManager->wait(mName);
+    }
+    LOG2("Enter %s executor name:%s, sequence: %ld", __func__, mName.c_str(), inBuf->getSequence());
+    TRACE_LOG_PROCESS(mName.c_str(), __func__, MAKE_COLOR(inBuf->getSequence()),
+                      inBuf->getSequence());
+
+    int fd = outBuf->getFd();
+    int memoryType = outBuf->getMemory();
+    int bufferSize = outBuf->getBufferSize();
+    void* outPtr = (memoryType == V4L2_MEMORY_DMABUF)
+                       ? CameraBuffer::mapDmaBufferAddr(fd, bufferSize)
+                       : outBuf->getBufferAddr();
+    if (!outPtr) return UNKNOWN_ERROR;
+
+    uint32_t sequence = inBuf->getSequence();
+    int ret = updateTnrISPConfig(mTnr7usParam, sequence);
+
+    if (ret) {
+        // if no ISP parameter, copy input to user buffer directly
+        MEMCPY_S(outPtr, bufferSize, inBuf->getBufferAddr(), inBuf->getBufferSize());
+        if (memoryType == V4L2_MEMORY_DMABUF) {
+            CameraBuffer::unmapDmaBufferAddr(outPtr, bufferSize);
+        }
+        return OK;
+    }
+
+    if (mLastSequence == UINT32_MAX || sequence - mLastSequence >= TNR7US_RESTART_THRESHOLD) {
+        mTnr7usParam->bc.is_first_frame = 1;
+    } else {
+        mTnr7usParam->bc.is_first_frame = 0;
+    }
+    mLastSequence = sequence;
+
+    ret = mIntelTNR->runTnrFrame(inBuf->getBufferAddr(), outPtr, inBuf->getBufferSize(), bufferSize,
+                                 mTnr7usParam);
+
+    if (memoryType == V4L2_MEMORY_DMABUF) {
+        CameraBuffer::unmapDmaBufferAddr(outPtr, bufferSize);
+    }
+
+    AiqResult* aiqResults =
+        const_cast<AiqResult*>(AiqResultStorage::getInstance(mCameraId)->getAiqResult(sequence));
+    if (aiqResults == nullptr) {
+        LOGW("%s: no result for sequence %ld! use the latest instead", __func__, sequence);
+        aiqResults =
+            const_cast<AiqResult*>(AiqResultStorage::getInstance(mCameraId)->getAiqResult());
+        CheckError((aiqResults == nullptr), UNKNOWN_ERROR, "Cannot find available aiq result.");
+    }
+
+    ia_aiq_exposure_parameters* exposure = aiqResults->mAeResults.exposures[0].exposure;
+
+    /* update tnr param when gain changes. gain is float,
+     * multi by 1000 to avoid lose accuracy when cast to int */
+    ret = mIntelTNR->asyncParamUpdate(static_cast<int>(exposure->digital_gain * 1000));
+
+    LOG2("Exit %s executor name:%s, sequence: %ld", __func__, mName.c_str(), inBuf->getSequence());
+    return ret;
+}
+
+int GPUExecutor::dumpTnrParameters(uint32_t sequence) {
+    const int DUMP_FILE_SIZE = 0x1000;
+    std::string dumpFileName = std::string("/home/tnr7-") + to_string(sequence) + string(".txt");
+
+    LOG1("%s save tnr7 parameters to file %s", __func__, dumpFileName.c_str());
+    tnr7_bc_1_0_t* tnr7_bc = &(mTnr7usParam->bc);
+    tnr7_blend_1_0_t* tnr7_blend = &(mTnr7usParam->blend);
+    tnr7_ims_1_0_t* tnr7_ims = &(mTnr7usParam->ims);
+    char* dumpData = reinterpret_cast<char*>(malloc(DUMP_FILE_SIZE));
+    CheckError(dumpData == nullptr, NO_MEMORY, "failed to allocate memory for dump tnr7");
+
+    FILE* parFile = fopen(dumpFileName.c_str(), "wb");
+    if (parFile) {
+        int shift = 0;
+        size_t length = snprintf(dumpData + shift, DUMP_FILE_SIZE - shift, "%s\n", "tnr7_bc");
+        shift += length;
+        for (int i = 0; i < sizeof(tnr7_bc_1_0_t) / sizeof(int32_t); i++) {
+            length = snprintf(dumpData + shift, DUMP_FILE_SIZE - shift, "%u\n",
+                              *(reinterpret_cast<int32_t*>(tnr7_bc) + i));
+            shift += length;
+        }
+        length = snprintf(dumpData + shift, DUMP_FILE_SIZE - shift, "%s\n", "tnr7_blend");
+        shift += length;
+        for (int i = 0; i < sizeof(tnr7_blend_1_0_t) / sizeof(int32_t); i++) {
+            length = snprintf(dumpData + shift, DUMP_FILE_SIZE - shift, "%u\n",
+                              *(reinterpret_cast<int32_t*>(tnr7_blend) + i));
+            shift += length;
+        }
+        length = snprintf(dumpData + shift, DUMP_FILE_SIZE - shift, "%s\n", "tnr7_ims");
+        shift += length;
+        for (int i = 0; i < sizeof(tnr7_ims_1_0_t) / sizeof(int32_t); i++) {
+            length = snprintf(dumpData + shift, DUMP_FILE_SIZE - shift, "%u\n",
+                              *(reinterpret_cast<int32_t*>(tnr7_ims) + i));
+            shift += length;
+        }
+
+        fwrite(dumpData, shift, 1, parFile);
+        fclose(parFile);
+    } else {
+        LOGW("tnr7 failed to open dump file %s", (dumpFileName + to_string(sequence)).c_str());
+    }
+    free(dumpData);
+    return OK;
+}
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/psysprocessor/GPUExecutor.h b/camera/hal/intel/ipu6/src/core/psysprocessor/GPUExecutor.h
new file mode 100644
index 000000000000..e6851d26235d
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/psysprocessor/GPUExecutor.h
@@ -0,0 +1,60 @@
+/*
+ * Copyright (C) 2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+#include <string>
+#include <vector>
+
+#include "PipeLiteExecutor.h"
+#ifdef ENABLE_SANDBOXING
+#include "modules/sandboxing/client/IntelTNR7US.h"
+#else
+#include "modules/algowrapper/IntelTNR7US.h"
+#endif
+
+namespace icamera {
+
+class PSysDAG;
+
+class GPUExecutor : public PipeLiteExecutor {
+ public:
+    GPUExecutor(int cameraId, const ExecutorPolicy& policy, std::vector<std::string> exclusivePGs,
+                PSysDAG* psysDag, std::shared_ptr<IGraphConfig> gc);
+    virtual ~GPUExecutor();
+    virtual int start();
+    virtual int initPipe();
+    virtual void stop();
+
+ private:
+    int createPGs();
+    int allocBuffers();
+    void releaseBuffers();
+    int processNewFrame();
+    int updateTnrISPConfig(Tnr7Param* pbuffer, uint32_t sequence);
+    int dumpTnrParameters(uint32_t sequence);
+    int runTnrFrame(const std::shared_ptr<CameraBuffer>& inBuf,
+                    std::shared_ptr<CameraBuffer> outbuf);
+
+ private:
+    Tnr7Param* mTnr7usParam;
+    std::unique_ptr<IntelTNR7US> mIntelTNR;
+    uint32_t mLastSequence;
+
+    DISALLOW_COPY_AND_ASSIGN(GPUExecutor);
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/psysprocessor/PGCommon.cpp b/camera/hal/intel/ipu6/src/core/psysprocessor/PGCommon.cpp
new file mode 100644
index 000000000000..03b841e7d149
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/psysprocessor/PGCommon.cpp
@@ -0,0 +1,1235 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "PGCommon"
+
+#include "PGCommon.h"
+
+#include <stdint.h>
+#include <math.h>
+#include <utility>
+#include "iutils/Utils.h"
+#include "iutils/CameraLog.h"
+#include "iutils/CameraDump.h"
+
+namespace icamera {
+
+#define IS_VALID_TERMINAL(terminal) (terminal >=0 && terminal < mTerminalCount)
+
+int PGCommon::getFrameSize(int format, int width, int height,
+                           bool needAlignedHeight, bool needExtraSize, bool needCompression)
+{
+    int size = 0;
+    int cssFormat = PGUtils::getCssFmt(format);
+    int stride = PGUtils::getCssStride(format, width);
+    switch (cssFormat) {
+    case IA_CSS_DATA_FORMAT_BAYER_LINE_INTERLEAVED:  // CSL6
+        if (needAlignedHeight) {
+            height = ALIGN_64(height);
+        }
+        size = stride * height * 3 / 2;
+        break;
+    default:
+        break;
+    }
+
+    if (!size) {
+        size = CameraUtils::getFrameSize(format, width, height,
+                                         needAlignedHeight, needExtraSize, needCompression);
+    }
+    return size;
+}
+
+PGCommon::PGCommon(int pgId, const std::string& pgName, ia_uid terminalBaseUid):
+    mCtx(nullptr),
+    mManifestBuffer(nullptr),
+    mPGParamsBuffer(nullptr),
+    mPGParamAdapt(nullptr),
+    mPGId(pgId),
+    mName(pgName),
+    mTerminalBaseUid(terminalBaseUid),
+    mPGCount(0),
+    mPlatform(IA_P2P_PLATFORM_BXT_B0),
+    mProgramCount(0),
+    mTerminalCount(0),
+    mManifestSize(0),
+    mKernelBitmap(ia_css_kernel_bitmap_clear()),
+    mRoutingBitmap(nullptr),
+    mFragmentCount(1),
+    mPGBuffer(nullptr),
+    mProcessGroup(nullptr),
+    mCmdExtBuffer(nullptr),
+    mPPG(false),
+    mPPGStarted(false),
+    mPPGBuffer(nullptr),
+    mPPGProcessGroup(nullptr),
+    mToken(0),
+    mEvent(nullptr),
+    mTerminalBuffers(nullptr),
+    mInputMainTerminal(-1),
+    mOutputMainTerminal(-1)
+{
+    mTnrTerminalPair.inId = -1;
+    mTnrTerminalPair.outId = -1;
+    CLEAR(mTnrBuffers);
+    CLEAR(mParamPayload);
+}
+
+PGCommon::~PGCommon()
+{
+}
+
+int PGCommon::init()
+{
+    mDisableDataTermials.clear();
+    mPGParamAdapt = std::unique_ptr<IntelPGParam>(new IntelPGParam(mPGId));
+
+    mCtx = new CIPR::Context();
+    int ret = getCapability();
+    if (ret != OK) return ret;
+
+    // create mManifestBuffer
+    ret = getManifest(mPGId);
+    if (ret != OK) return ret;
+
+    mTerminalBuffers = (CIPR::Buffer**)CIPR::callocMemory(mTerminalCount, sizeof(CIPR::Buffer*));
+    CheckError(!mTerminalBuffers, NO_MEMORY, "Allocate terminal buffers fail");
+    memset(mTerminalBuffers, 0, (mTerminalCount * sizeof(CIPR::Buffer*)));
+
+    mFrameFormatType = std::unique_ptr<ia_css_frame_format_type[]>(new ia_css_frame_format_type[mTerminalCount]);
+    for (int i = 0; i < mTerminalCount; i++) {
+        mFrameFormatType[i] = IA_CSS_N_FRAME_FORMAT_TYPES;
+    }
+
+    mPgTerminals = std::unique_ptr<uint8_t[]>(new uint8_t[mTerminalCount]);
+    for (int i = 0; i < mTerminalCount; i++) {
+        mPgTerminals[i] = IPU_MAX_TERMINAL_COUNT;
+    }
+
+    std::vector<TerminalPair> tnrTerminalPairs;
+    if (PGUtils::getTerminalPairs(mPGId, PGUtils::TERMINAL_PAIR_TNR, &tnrTerminalPairs)) {
+        mTnrTerminalPair = tnrTerminalPairs[0];
+    }
+    PGUtils::getTerminalPairs(mPGId, PGUtils::TERMINAL_PAIR_DVS, &mDvsTerminalPairs);
+
+    return ret;
+}
+
+void PGCommon::deInit()
+{
+    if (mPPGStarted) {
+        stopPPG();
+        mPPGStarted = false;
+    }
+
+    destoryCommands();
+
+    for (int i = TNR_BUFFER_IN_INDEX; i <= TNR_BUFFER_OUT_INDEX; i++) {
+        if (mTnrBuffers[i]) {
+            free(mTnrBuffers[i]);
+            mTnrBuffers[i] = nullptr;
+        }
+    }
+
+    mDvsTerminalPairs.clear();
+
+    mDisableDataTermials.clear();
+    if (mTerminalBuffers) {
+        CIPR::freeMemory(mTerminalBuffers);
+    }
+
+    delete mManifestBuffer;
+    delete mPGParamsBuffer;
+    delete mPGBuffer;
+    if (mPPGBuffer) {
+        delete mPPGBuffer;
+    }
+    for (auto& item : mBuffers) {
+        delete item.ciprBuf;
+    }
+
+    delete mCtx;
+
+    mPGParamAdapt->deinit();
+    mRoutingBitmap.reset();
+}
+
+void PGCommon::setInputInfo(const TerminalFrameInfoMap& inputInfos)
+{
+    mInputMainTerminal = -1;
+    int maxFrameSize = 0;
+    for (const auto& item : inputInfos) {
+        int terminal = item.first - mTerminalBaseUid;
+        CheckError(!IS_VALID_TERMINAL(terminal), VOID_VALUE, "error input terminal %d", item.first);
+
+        FrameInfo frameInfo;
+        frameInfo.mWidth = item.second.mWidth;
+        frameInfo.mHeight = item.second.mHeight;
+        frameInfo.mFormat = item.second.mFormat;
+        frameInfo.mBpp = CameraUtils::getBpp(frameInfo.mFormat);
+        frameInfo.mStride = CameraUtils::getStride(frameInfo.mFormat, frameInfo.mWidth);
+        mTerminalFrameInfos[terminal] = frameInfo;
+        int size = frameInfo.mWidth * frameInfo.mHeight;
+        if (maxFrameSize < size) {
+            maxFrameSize = size;
+            mInputMainTerminal = terminal;
+        }
+    }
+
+    // Create frame info for tnr terminals (i.e. data terminals)
+    FrameInfo config = mTerminalFrameInfos[mInputMainTerminal];
+    if (config.mHeight % 32) {
+        LOG1("%s: height %d not multiple of 32, rounding up!", __func__, config.mHeight);
+        config.mHeight = ((config.mHeight / 32) + 1) * 32;
+    }
+
+    for (int i = TNR_BUFFER_IN_INDEX; i <= TNR_BUFFER_OUT_INDEX; i++) {
+        int tnrId = (i == TNR_BUFFER_IN_INDEX) ? mTnrTerminalPair.inId : mTnrTerminalPair.outId;
+        if (tnrId < 0) continue;
+
+        mFrameFormatType[tnrId] = IA_CSS_DATA_FORMAT_NV12; // for IPU6
+        mTerminalFrameInfos[tnrId] = config;
+    }
+
+    LOG1("%s:%d use input terminal %d as main", __func__, mPGId, mInputMainTerminal);
+}
+
+void PGCommon::setOutputInfo(const TerminalFrameInfoMap& outputInfos)
+{
+    mOutputMainTerminal = -1;
+    int maxFrameSize = 0;
+    for (const auto& item : outputInfos) {
+        int terminal = item.first - mTerminalBaseUid;
+        CheckError(!IS_VALID_TERMINAL(terminal), VOID_VALUE, "error output terminal %d", item.first);
+
+        FrameInfo frameInfo;
+        frameInfo.mWidth = item.second.mWidth;
+        frameInfo.mHeight = item.second.mHeight;
+        frameInfo.mFormat = item.second.mFormat;
+        frameInfo.mBpp = CameraUtils::getBpp(frameInfo.mFormat);
+        frameInfo.mStride = CameraUtils::getStride(frameInfo.mFormat, frameInfo.mWidth);
+        mTerminalFrameInfos[terminal] = frameInfo;
+        int size = frameInfo.mWidth * frameInfo.mHeight;
+        if (maxFrameSize < size) {
+            maxFrameSize = size;
+            mOutputMainTerminal = terminal;
+        }
+    }
+}
+
+void PGCommon::setDisabledTerminals(const std::vector<ia_uid>& disabledTerminals)
+{
+    for (auto const terminalUid : disabledTerminals) {
+        int terminal = terminalUid - mTerminalBaseUid;
+        CheckError(!IS_VALID_TERMINAL(terminal), VOID_VALUE, "error disabled terminal %d", terminalUid);
+        mDisableDataTermials.push_back(terminal);
+    }
+}
+
+void PGCommon::setRoutingBitmap(const void* rbm, uint32_t bytes)
+{
+    if (!rbm || !bytes) {
+        return;
+    }
+    const unsigned char* rbmData = (const unsigned char*)rbm;
+
+    if (mRoutingBitmap.get() == nullptr) {
+        mRoutingBitmap = std::unique_ptr<ia_css_rbm_t>(new ia_css_rbm_t);
+    }
+
+    ia_css_rbm_t* rbmPtr = mRoutingBitmap.get();
+    *rbmPtr = ia_css_rbm_clear();
+    for (uint32_t bit = 0; bit < bytes * 8; bit++) {
+        if (rbmData[bit / 8] & (1 << (bit %8))) {
+            *rbmPtr = ia_css_rbm_set(*rbmPtr, bit);
+        }
+    }
+}
+
+int PGCommon::prepare(IspParamAdaptor* adaptor, int streamId)
+{
+    // Set the data terminal frame format
+    int ret = configTerminalFormat();
+    CheckError((ret != OK), ret, "%s, call configTerminal fail", __func__);
+
+    // Init and config p2p handle
+    ret = initParamAdapt();
+    CheckError((ret != OK), ret, "%s, init p2p fail", __func__);
+
+    // Query and save the requirement for each terminal, get the final kernel bitmap
+    ret = mPGParamAdapt->prepare(adaptor->getIpuParameter(-1, streamId), mRoutingBitmap.get(), &mKernelBitmap);
+    CheckError((ret != OK), ret, "%s, prepare p2p fail", __func__);
+
+    // Init PG parameters
+    ret = handlePGParams(mFrameFormatType.get());
+    CheckError((ret != OK), ret, "%s, call handlePGParams fail", __func__);
+
+    ret = setKernelBitMap();
+    CheckError((ret != OK), ret, "%s, call setKernelBitMap fail", __func__);
+
+    ret = setTerminalParams(mFrameFormatType.get());
+    CheckError((ret != OK), ret, "%s, call setTerminalParams fail", __func__);
+
+   // Create process group
+    mProcessGroup = createPG(&mPGBuffer);
+    CheckError(!mProcessGroup, UNKNOWN_ERROR, "%s, create pg fail", __func__);
+    uint8_t pgTerminalCount = ia_css_process_group_get_terminal_count(mProcessGroup);
+    for (uint8_t termNum = 0 ; termNum < pgTerminalCount; termNum++) {
+        ia_css_terminal_t* terminal = ia_css_process_group_get_terminal(mProcessGroup, termNum);
+        CheckError(!terminal, UNKNOWN_ERROR, "failed to get terminal");
+        uint16_t termIdx = ia_css_terminal_get_terminal_manifest_index(terminal);
+        CheckError((termIdx >= IPU_MAX_TERMINAL_COUNT), UNKNOWN_ERROR, "wrong term index for terminal num %d", termNum);
+        mPgTerminals[termIdx] = termNum;
+    }
+
+    mPGParamAdapt->setPGAndPrepareProgram(mProcessGroup);
+    int count = mPGParamAdapt->allocatePayloads(mTerminalCount, mParamPayload);
+    CheckError(count != mTerminalCount, NO_MEMORY, "%s, allocatePayloads fails", __func__);
+    preparePayloadBuffers();
+
+    ret = configureFragmentDesc();
+    CheckError((ret != OK), ret, "%s, call configureFragmentDesc fail", __func__);
+
+    return OK;
+}
+
+ia_css_process_group_t* PGCommon::createPG(CIPR::Buffer** pgBuffer)
+{
+    CheckError(*pgBuffer, nullptr, "pg has already created");
+
+   // Create process group
+    ia_css_program_group_param_t* pgParamsBuf =
+        (ia_css_program_group_param_t*)getCiprBufferPtr(mPGParamsBuffer);
+    ia_css_program_group_manifest_t* manifestBuf =
+        (ia_css_program_group_manifest_t*)getCiprBufferPtr(mManifestBuffer);
+
+    size_t pgSize = ia_css_sizeof_process_group(manifestBuf, pgParamsBuf);
+    LOG1("%s process group size is %zu", __func__, pgSize);
+
+    void* pgMemory = mPGParamAdapt->allocatePGBuffer(pgSize);
+    CheckError(!pgMemory, nullptr, "allocate PG error");
+    *pgBuffer = createUserPtrCiprBuffer(pgSize, pgMemory);
+    CheckError(!*pgBuffer, nullptr, "%s, call createUserPtrCiprBuffer fail", __func__);
+
+    ia_css_process_group_t* pg = ia_css_process_group_create(getCiprBufferPtr(*pgBuffer),
+                   (ia_css_program_group_manifest_t*)getCiprBufferPtr(mManifestBuffer),
+                   (ia_css_program_group_param_t*)getCiprBufferPtr(mPGParamsBuffer));
+    CheckError(!pg, nullptr, "Create process group failed.");
+
+    if (mPPG) {
+        ia_css_process_group_set_num_queues(pg, 1);
+    }
+
+    if (mRoutingBitmap.get()) {
+        ia_css_process_group_set_routing_bitmap(pg, *mRoutingBitmap.get());
+    }
+    return pg;
+}
+
+int PGCommon::createCommands()
+{
+    int bufCount = ia_css_process_group_get_terminal_count(mProcessGroup);
+    int ret = createCommand(mPGBuffer, &mCmd, &mCmdExtBuffer, bufCount);
+    CheckError(ret, NO_MEMORY, "create cmd fail!");
+    if (mPPG) {
+        ret = createCommand(mPPGBuffer, &mPPGCmd[PPG_CMD_TYPE_START], &mPPGCmdExtBuffer[PPG_CMD_TYPE_START], bufCount);
+        CheckError(ret, NO_MEMORY, "create ppg start buffer %d fail");
+        ret = createCommand(mPPGBuffer, &mPPGCmd[PPG_CMD_TYPE_STOP], &mPPGCmdExtBuffer[PPG_CMD_TYPE_STOP], 0);
+        CheckError(ret, NO_MEMORY, "create ppg stop %d fail");
+    }
+
+    CIPR::PSysEventConfig eventCfg = {};
+    eventCfg.timeout = kEventTimeout;
+    mEvent = new CIPR::Event(eventCfg);
+
+    return OK;
+}
+
+int PGCommon::createCommand(CIPR::Buffer* pg, CIPR::Command** cmd, CIPR::Buffer** extBuffer, int bufCount)
+{
+    CIPR::PSysCommandConfig cmdCfg;
+    CIPR::ProcessGroupCommand *pgCommand;
+    CIPR::Result ret;
+
+    // Create command with basic setting
+    cmdCfg.buffers.resize(bufCount);
+    std::fill(cmdCfg.buffers.begin(), cmdCfg.buffers.end(), nullptr);
+
+    *cmd = new CIPR::Command(cmdCfg);
+    ret = (*cmd)->getConfig(&cmdCfg);
+    CheckError(ret != CIPR::Result::OK, UNKNOWN_ERROR, "%s, call get_command_config fail", __func__);
+
+    // Create ext buffer
+    *extBuffer = new CIPR::Buffer(sizeof(CIPR::ProcessGroupCommand),
+                                  CIPR::MemoryFlag::AllocateCpuPtr
+                                  | CIPR::MemoryFlag::PSysAPI,
+                                  nullptr);
+
+    ret = (*extBuffer)->attatchDevice(mCtx);
+    CheckError(ret != CIPR::Result::OK, NO_MEMORY, "unable to access extBuffer");
+
+    void* p = nullptr;
+    ret = (*extBuffer)->getMemoryCpuPtr(&p);
+    CheckError(ret != CIPR::Result::OK, NO_MEMORY, "unable to access extBuffer memory");
+    pgCommand = reinterpret_cast<CIPR::ProcessGroupCommand*>(p);
+    CheckError(!pgCommand, NO_MEMORY, "unable to access memory.cpu_ptr");
+
+    pgCommand->header.size = sizeof(CIPR::ProcessGroupCommand);
+    pgCommand->header.offset = sizeof(pgCommand->header);
+    pgCommand->header.version = psys_command_ext_ppg_1; // for ipu6
+    if (pgCommand->header.version == psys_command_ext_ppg_1) {
+        CIPR::memoryCopy(pgCommand->dynamicKernelBitmap, sizeof(ia_css_kernel_bitmap_t),
+                        &mKernelBitmap, sizeof(ia_css_kernel_bitmap_t));
+    }
+
+    // Update setting and set back to command
+    cmdCfg.id = mPGId;
+    cmdCfg.priority = 1;
+    cmdCfg.pgParamsBuf = mPPG ? nullptr : mPGParamsBuffer;
+    cmdCfg.pgManifestBuf = mManifestBuffer;
+    cmdCfg.pg = pg;
+    cmdCfg.extBuf = *extBuffer;
+    ret = (*cmd)->setConfig(cmdCfg);
+    CheckError(ret != CIPR::Result::OK, UNKNOWN_ERROR, "%s, call set_command_config fail", __func__);
+
+    return OK;
+}
+
+void PGCommon::destoryCommands()
+{
+    delete mCmd;
+    delete mCmdExtBuffer;
+
+    for (int i = 0; i < PPG_CMD_TYPE_COUNT; i++) {
+        delete mPPGCmd[i];
+        delete mPPGCmdExtBuffer[i];
+    }
+
+    if (mEvent) {
+        delete mEvent;
+    }
+}
+
+int PGCommon::configTerminalFormat()
+{
+    for (int i = 0; i < mTerminalCount; i++) {
+        if (mTerminalFrameInfos.find(i) != mTerminalFrameInfos.end()) {
+            mFrameFormatType[i] = PGUtils::getCssFmt(mTerminalFrameInfos[i].mFormat);
+        }
+    }
+    return OK;
+}
+
+int PGCommon::initParamAdapt()
+{
+    mFragmentCount = calcFragmentCount();
+
+    ia_css_program_group_manifest_t* manifestBuf =
+        (ia_css_program_group_manifest_t*)getCiprBufferPtr(mManifestBuffer);
+
+    PgConfiguration config;
+    config.pgManifest = manifestBuf;
+    config.pgManifestSize = getCiprBufferSize(mManifestBuffer);
+    config.disableDataTermials = mDisableDataTermials;
+    config.fragmentCount = mFragmentCount;
+
+    FrameInfo* pgInFrame = nullptr;
+    FrameInfo* pgOutFrame = nullptr;
+    if (mInputMainTerminal >= 0) {
+        pgInFrame = &mTerminalFrameInfos[mInputMainTerminal];
+    }
+    if (mOutputMainTerminal >= 0) {
+        pgOutFrame = &mTerminalFrameInfos[mOutputMainTerminal];
+    }
+    if (pgInFrame) {
+        config.inputMainFrame.width = pgInFrame->mWidth;
+        config.inputMainFrame.height = pgInFrame->mHeight;
+        config.inputMainFrame.bpe = pgInFrame->mBpp; //TODO: use bpe
+    }
+
+    if (pgOutFrame) {
+        config.outputMainFrame.width = pgOutFrame->mWidth;
+        config.outputMainFrame.height = pgOutFrame->mHeight;
+        config.outputMainFrame.bpe = pgOutFrame->mBpp; //TODO: use bpe
+    }
+
+    // init and config p2p handle
+    int ret = mPGParamAdapt->init(mPlatform, config);
+    return ret;
+}
+
+// Support horizontal fragment only now
+int PGCommon::calcFragmentCount(int overlap)
+{
+    int finalFragmentCount = 0;
+    ia_css_data_terminal_manifest_t * data_terminal_manifest = nullptr;
+
+    const ia_css_program_group_manifest_t *manifest =
+            (const ia_css_program_group_manifest_t*)getCiprBufferPtr(mManifestBuffer);
+    CheckError(!manifest, 1, "%s, can't get manifest ptr", __func__);
+
+    for (int termIdx = 0; termIdx < mTerminalCount; termIdx++) {
+        // Get max fragement size from manifest (align with 64)
+        ia_css_terminal_manifest_t *terminal_manifest = ia_css_program_group_manifest_get_term_mnfst(manifest, termIdx);
+        ia_css_terminal_type_t  terminal_type = ia_css_terminal_manifest_get_type(terminal_manifest);
+
+        if (!((terminal_type == IA_CSS_TERMINAL_TYPE_DATA_OUT) || (terminal_type == IA_CSS_TERMINAL_TYPE_DATA_IN))) {
+            continue;
+        }
+
+        data_terminal_manifest = ia_css_program_group_manifest_get_data_terminal_manifest(manifest, termIdx);
+        CheckError(!data_terminal_manifest, -1, "%s, can't get data terminal manifest for term %d", __func__, termIdx);
+
+        uint16_t size[IA_CSS_N_DATA_DIMENSION] = {0};
+        int ret = ia_css_data_terminal_manifest_get_max_size(data_terminal_manifest, size);
+        CheckError(ret < 0, 1, "%s: get max fragment size error for term %d", __func__, termIdx);
+
+        size[IA_CSS_COL_DIMENSION] = ALIGN_64(size[IA_CSS_COL_DIMENSION]);
+        // Overwrite the max value if need
+
+        // Calc fragment count for terminal (only for horizontal direction)
+        int maxFragmentWidth = size[IA_CSS_COL_DIMENSION];
+        FrameInfo config;
+        CLEAR(config);
+        if (mTerminalFrameInfos.find(termIdx) != mTerminalFrameInfos.end()) {
+            config = mTerminalFrameInfos[termIdx];
+        }
+        int fragmentCovered = maxFragmentWidth;
+        int fragmentCount = 1;
+        /*
+         * Calculate how many fragment frames can cover the whole frame.
+         * Consider overlap between two fragment frames.
+         * Example: frame width = 300, max fragment width = 100, overlap = 10
+         *       0|------------------------------|300
+         *  f1   0|----------|100
+         *  f2           90|----------|190
+         *  f3                   180|----------|280
+         *  f4                            270|---|300
+         */
+        while (fragmentCovered < config.mWidth) {
+            fragmentCovered += (maxFragmentWidth - overlap);
+            fragmentCount++;
+        }
+
+        if (finalFragmentCount < fragmentCount) {
+            finalFragmentCount = fragmentCount;
+        }
+    }
+
+    LOG2("%s: final fragment count %d for pg %d", __func__, finalFragmentCount, mPGId);
+    return finalFragmentCount;
+}
+
+int PGCommon::handlePGParams(const ia_css_frame_format_type* frameFormatTypes)
+{
+    int pgParamsSize = ia_css_sizeof_program_group_param(mProgramCount, mTerminalCount, mFragmentCount);
+
+    mPGParamsBuffer = createUserPtrCiprBuffer(pgParamsSize);
+    CheckError(!mPGParamsBuffer, NO_MEMORY, "%s, call createUserPtrCiprBuffer fail", __func__);
+
+    ia_css_program_group_param_t* pgParamsBuf = (ia_css_program_group_param_t*)getCiprBufferPtr(mPGParamsBuffer);
+    int ret = ia_css_program_group_param_init(pgParamsBuf, mProgramCount, mTerminalCount, mFragmentCount, frameFormatTypes);
+    CheckError((ret != OK), ret, "%s, call ia_css_program_group_param_init fail", __func__);
+
+    if (mPPG) {
+        ret = ia_css_program_group_param_set_protocol_version(
+                pgParamsBuf,
+                IA_CSS_PROCESS_GROUP_PROTOCOL_PPG);
+        CheckError((ret != OK), ret, "%s, call ia_css_program_group_param_set_protocol_version fail", __func__);
+    }
+    return ret;
+}
+
+int PGCommon::setKernelBitMap()
+{
+    ia_css_program_group_param_t* pgParamsBuf = (ia_css_program_group_param_t*)getCiprBufferPtr(mPGParamsBuffer);
+    LOG1("%s: mKernelBitmap: %#018lx", __func__, mKernelBitmap);
+    int ret = ia_css_program_group_param_set_kernel_enable_bitmap(pgParamsBuf, mKernelBitmap);
+    CheckError((ret != OK), ret, "%s, call ia_css_program_group_param_set_kernel_enable_bitmap fail", __func__);
+
+    return ret;
+}
+
+int PGCommon::setTerminalParams(const ia_css_frame_format_type* frameFormatTypes)
+{
+    ia_css_program_group_param_t* pgParamsBuf =
+        (ia_css_program_group_param_t*)getCiprBufferPtr(mPGParamsBuffer);
+    ia_css_program_group_manifest_t* pg_manifest =
+        (ia_css_program_group_manifest_t*)getCiprBufferPtr(mManifestBuffer);
+
+    for (int i = 0; i < mTerminalCount; i++) {
+        ia_css_terminal_param_t *terminalParam =
+            ia_css_program_group_param_get_terminal_param(pgParamsBuf, i);
+        CheckError(!terminalParam, UNKNOWN_ERROR, "%s, call ia_css_program_group_param_get_terminal_param fail", __func__);
+        ia_css_terminal_manifest_t *terminal_manifest = ia_css_program_group_manifest_get_term_mnfst(pg_manifest, i);
+        ia_css_terminal_type_t  terminal_type = ia_css_terminal_manifest_get_type(terminal_manifest);
+        if (!((terminal_type == IA_CSS_TERMINAL_TYPE_DATA_OUT) || (terminal_type == IA_CSS_TERMINAL_TYPE_DATA_IN))) {
+            continue;
+        }
+
+        FrameInfo config = mTerminalFrameInfos[i];
+        terminalParam->frame_format_type = frameFormatTypes[i];
+        terminalParam->dimensions[IA_CSS_COL_DIMENSION] = config.mWidth;
+        terminalParam->dimensions[IA_CSS_ROW_DIMENSION] = config.mHeight;
+        terminalParam->fragment_dimensions[IA_CSS_COL_DIMENSION] = config.mWidth;
+        terminalParam->fragment_dimensions[IA_CSS_ROW_DIMENSION] = config.mHeight;
+
+        terminalParam->bpp = PGUtils::getCssBpp(config.mFormat);
+        terminalParam->bpe = terminalParam->bpp;
+        terminalParam->stride = PGUtils::getCssStride(config.mFormat, config.mWidth);
+
+        terminalParam->offset = 0;
+        terminalParam->index[IA_CSS_COL_DIMENSION] = 0;
+        terminalParam->index[IA_CSS_ROW_DIMENSION] = 0;
+
+        LOG2("%s: setTerminalParams: index=%d, format=%d, w=%d, h=%d, fw=%d, fh=%d, bpp=%d, bpe=%d, stride=%d, offset=%d, col=%d, row=%d",
+             getName(), i,
+             terminalParam->frame_format_type,
+             terminalParam->dimensions[IA_CSS_COL_DIMENSION],
+             terminalParam->dimensions[IA_CSS_ROW_DIMENSION],
+             terminalParam->fragment_dimensions[IA_CSS_COL_DIMENSION],
+             terminalParam->fragment_dimensions[IA_CSS_ROW_DIMENSION],
+             terminalParam->bpp,
+             terminalParam->bpe,
+             terminalParam->stride,
+             terminalParam->offset,
+             terminalParam->index[IA_CSS_COL_DIMENSION],
+             terminalParam->index[IA_CSS_ROW_DIMENSION]);
+    }
+
+    return OK;
+}
+
+int PGCommon::configureFragmentDesc()
+{
+    int num = mTerminalCount * mFragmentCount;
+    std::unique_ptr<ia_p2p_fragment_desc[]> srcFragDesc =
+                    std::unique_ptr<ia_p2p_fragment_desc[]>(new ia_p2p_fragment_desc[num]);
+    int count = mPGParamAdapt->getFragmentDescriptors(num, srcFragDesc.get());
+    CheckError(!count, UNKNOWN_ERROR, "getFragmentDescriptors fails");
+
+    for (int termIdx = 0; termIdx < mTerminalCount; termIdx++) {
+        if (mPgTerminals[termIdx] >= IPU_MAX_TERMINAL_COUNT) {
+            continue;
+        }
+
+        ia_css_terminal_t* terminal = ia_css_process_group_get_terminal(mProcessGroup, mPgTerminals[termIdx]);
+        ia_css_terminal_type_t terminalType = ia_css_terminal_get_type(terminal);
+        if (!((terminalType == IA_CSS_TERMINAL_TYPE_DATA_OUT) || (terminalType == IA_CSS_TERMINAL_TYPE_DATA_IN))) {
+            continue;
+        }
+        configureTerminalFragmentDesc(termIdx, &srcFragDesc[termIdx]);
+    }
+    return OK;
+}
+
+int PGCommon::configureTerminalFragmentDesc(int termIdx, const ia_p2p_fragment_desc* srcDesc)
+{
+#define DDR_WORD_BYTES 64
+    ia_css_terminal_t* terminal = ia_css_process_group_get_terminal(mProcessGroup, mPgTerminals[termIdx]);
+    ia_css_terminal_type_t terminalType = ia_css_terminal_get_type(terminal);
+    if (!((terminalType == IA_CSS_TERMINAL_TYPE_DATA_OUT) || (terminalType == IA_CSS_TERMINAL_TYPE_DATA_IN))) {
+        return OK;
+    }
+
+    bool vectorized = false;
+    int packed_multiplier = 1;
+    int packed_divider = 1;
+    int dimension_bpp = PGUtils::getCssBpp(mTerminalFrameInfos[termIdx].mFormat);
+
+    switch(mFrameFormatType[termIdx]) {
+    case IA_CSS_DATA_FORMAT_BAYER_VECTORIZED:
+    case IA_CSS_DATA_FORMAT_BAYER_LINE_INTERLEAVED:
+        vectorized = true;
+        dimension_bpp = (uint8_t) ALIGN_8(PGUtils::getCssBpp(mTerminalFrameInfos[termIdx].mFormat));
+        break;
+    case IA_CSS_DATA_FORMAT_RAW:
+        dimension_bpp = (uint8_t) ALIGN_8(PGUtils::getCssBpp(mTerminalFrameInfos[termIdx].mFormat));
+        break;
+    case IA_CSS_DATA_FORMAT_BAYER_GRBG:
+    case IA_CSS_DATA_FORMAT_BAYER_RGGB:
+    case IA_CSS_DATA_FORMAT_BAYER_BGGR:
+    case IA_CSS_DATA_FORMAT_BAYER_GBRG:
+        dimension_bpp = (uint8_t) ALIGN_8(PGUtils::getCssBpp(mTerminalFrameInfos[termIdx].mFormat));
+        break;
+    case IA_CSS_DATA_FORMAT_YYUVYY_VECTORIZED:
+        dimension_bpp = (uint8_t) (PGUtils::getCssBpp(mTerminalFrameInfos[termIdx].mFormat) * 3 / 2);
+        packed_multiplier = 3;
+        packed_divider = 2;
+        vectorized = true;
+        break;
+    default:
+        break;
+    }
+
+    for (int fragIdx = 0; fragIdx < mFragmentCount; fragIdx++) {
+        ia_css_fragment_descriptor_t* dstFragDesc =
+                ia_css_data_terminal_get_fragment_descriptor((ia_css_data_terminal_t*)terminal, fragIdx);
+        CheckError(!dstFragDesc, -1, "%s: Can't get frag desc from terminal", __func__);
+
+        dstFragDesc->dimension[IA_CSS_COL_DIMENSION] = srcDesc[fragIdx].fragment_width;
+        dstFragDesc->dimension[IA_CSS_ROW_DIMENSION] = srcDesc[fragIdx].fragment_height;
+        dstFragDesc->index[IA_CSS_COL_DIMENSION] = (uint16_t)
+                (((srcDesc[fragIdx].fragment_start_x * packed_multiplier)
+                 / packed_divider) * (vectorized ? 2 : 1));
+        dstFragDesc->index[IA_CSS_ROW_DIMENSION] = (uint16_t)
+                (srcDesc[fragIdx].fragment_start_y / (vectorized ? 2 : 1));
+
+        int colOffset = 0;
+        int pixels_per_word = 0;
+        switch (mFrameFormatType[termIdx]) {
+        case IA_CSS_DATA_FORMAT_YUV420:
+        case IA_CSS_DATA_FORMAT_YYUVYY_VECTORIZED:
+        case IA_CSS_DATA_FORMAT_BAYER_VECTORIZED:
+            /** \todo Fragmentation with DMA packed formats is still open, need to
+             * check this again when it is more clear (see #H1804344344).
+             */
+            pixels_per_word = (uint16_t) floor(DDR_WORD_BYTES * 8 / dimension_bpp);
+            colOffset = (uint16_t) (floor(dstFragDesc->index[IA_CSS_COL_DIMENSION] / pixels_per_word) * DDR_WORD_BYTES);
+            colOffset = (uint16_t) (colOffset + (((dstFragDesc->index[IA_CSS_COL_DIMENSION] % pixels_per_word) * dimension_bpp) / 8));
+            break;
+        default:
+            colOffset = (uint16_t) (dstFragDesc->index[IA_CSS_COL_DIMENSION] * dimension_bpp / 8);
+            break;
+        }
+
+        dstFragDesc->offset[IA_CSS_COL_DIMENSION] = (uint16_t)colOffset;
+        dstFragDesc->offset[IA_CSS_ROW_DIMENSION] = dstFragDesc->index[IA_CSS_ROW_DIMENSION];
+
+        LOG2("%s: %d:%d: get frag desc %d (%d, %d, %d, %d)", __func__, mPGId, termIdx, fragIdx,
+             srcDesc[fragIdx].fragment_width, srcDesc[fragIdx].fragment_height,
+             srcDesc[fragIdx].fragment_start_x, srcDesc[fragIdx].fragment_start_y);
+        LOG2("%s: %d:%d:       frag %d: size(%d, %d) index(%d, %d), offset(%d, %d)", __func__, mPGId, termIdx,fragIdx,
+             dstFragDesc->dimension[IA_CSS_COL_DIMENSION],
+             dstFragDesc->dimension[IA_CSS_ROW_DIMENSION],
+             dstFragDesc->index[IA_CSS_COL_DIMENSION],
+             dstFragDesc->index[IA_CSS_ROW_DIMENSION],
+             dstFragDesc->offset[IA_CSS_COL_DIMENSION],
+             dstFragDesc->offset[IA_CSS_ROW_DIMENSION]);
+    }
+    return OK;
+}
+
+int PGCommon::iterate(CameraBufferMap &inBufs, CameraBufferMap &outBufs,
+                      ia_binary_data *statistics, const ia_binary_data *ipuParameters)
+{
+    LOG2("%s:%s ++", getName(), __func__);
+
+    int ret = prepareTerminalBuffers(ipuParameters, inBufs, outBufs);
+    CheckError((ret != OK), ret, "%s, prepareTerminalBuffers fail with %d", getName(), ret);
+
+    // Create PPG & PPG start/stop commands at the beginning
+    if (mPPG && !mPPGBuffer) {
+        ia_css_program_group_param_t* pgParamsBuf =
+            (ia_css_program_group_param_t*)getCiprBufferPtr(mPGParamsBuffer);
+        ia_css_program_group_manifest_t* manifestBuf =
+            (ia_css_program_group_manifest_t*)getCiprBufferPtr(mManifestBuffer);
+
+        size_t pgSize = ia_css_sizeof_process_group(manifestBuf, pgParamsBuf);
+
+        mPPGBuffer = createUserPtrCiprBuffer(pgSize);
+        CheckError(!mPPGBuffer, NO_MEMORY, "%s, call createUserPtrCiprBuffer fail", __func__);
+        mPPGProcessGroup = (ia_css_process_group_t*)getCiprBufferPtr(mPPGBuffer);
+        MEMCPY_S(mPPGProcessGroup, pgSize, mProcessGroup, ia_css_process_group_get_size(mProcessGroup));
+
+    }
+    if (!mCmd) {
+        ret = createCommands();
+       CheckError((ret != OK), ret, "%s, call createCommands fail", __func__);
+    }
+
+    if (mPPG && !mPPGStarted) {
+        ret = startPPG();
+        CheckError((ret != OK), ret, "%s, startPPG fail", getName());
+        mPPGStarted = true;
+    }
+
+    ret = executePG();
+    CheckError((ret != OK), ret, "%s, executePG fail", getName());
+
+    if (statistics) {
+        ret = mPGParamAdapt->decode(mTerminalCount, mParamPayload, statistics);
+        CheckError((ret != OK), ret, "%s, decode fail", getName());
+    }
+
+    LOG2("%s:%s -- ", getName(), __func__);
+    return ret;
+}
+
+int PGCommon::preparePayloadBuffers()
+{
+    CIPR::Buffer* ciprBuf = nullptr;
+    for (int termIdx = 0; termIdx < mTerminalCount; termIdx++) {
+        if (mParamPayload[termIdx].size && mParamPayload[termIdx].data) {
+            ciprBuf = registerUserBuffer(mParamPayload[termIdx].size, mParamPayload[termIdx].data);
+            CheckError(!ciprBuf, NO_MEMORY, "%s, register payload buffer %p for term %d fail",
+                       __func__, mParamPayload[termIdx].data, termIdx);
+            memset(mParamPayload[termIdx].data, 0, PAGE_ALIGN(mParamPayload[termIdx].size));
+            mTerminalBuffers[termIdx] = ciprBuf;
+        }
+    }
+
+    if (mTnrTerminalPair.inId >= 0 && !mTnrBuffers[TNR_BUFFER_IN_INDEX]) {
+        LOG1("%s:%s allocate the TNR input and output buffers", __func__, getName());
+        int size = CameraUtils::getFrameSize(mTerminalFrameInfos[mInputMainTerminal].mFormat,
+                                             mTerminalFrameInfos[mInputMainTerminal].mWidth,
+                                             mTerminalFrameInfos[mInputMainTerminal].mHeight);
+
+        for (int i = TNR_BUFFER_IN_INDEX; i <= TNR_BUFFER_OUT_INDEX; i++) {
+            int idx = (i == TNR_BUFFER_IN_INDEX) ? mTnrTerminalPair.inId : mTnrTerminalPair.outId;
+
+            int ret = posix_memalign((void**)&mTnrBuffers[i], PAGE_SIZE_U, PAGE_ALIGN(size));
+            CheckError(ret, NO_MEMORY, "%s, alloc tnr %d buf for term %d fails", __func__, i, idx);
+
+            ciprBuf = registerUserBuffer(size, mTnrBuffers[i]);
+            CheckError(!ciprBuf, NO_MEMORY, "%s, register tnr %d buf %p fails", __func__, i,
+                       mTnrBuffers[i]);
+            mTerminalBuffers[idx] = ciprBuf;
+        }
+    }
+    return OK;
+}
+
+int PGCommon::prepareTerminalBuffers(const ia_binary_data *ipuParameters,
+                                     const CameraBufferMap& inBufs, const CameraBufferMap& outBufs)
+{
+    CIPR::Buffer* ciprBuf = nullptr;
+    // Prepare payload
+    for (int termIdx = 0; termIdx < mTerminalCount; termIdx++) {
+        // Payload for data terminals
+        std::shared_ptr<CameraBuffer> buffer;
+        ia_uid terminalUid = mTerminalBaseUid + termIdx;
+        if (inBufs.find(terminalUid) != inBufs.end()) {
+             buffer = inBufs.at(terminalUid);
+        } else if (outBufs.find(terminalUid) != outBufs.end()) {
+             buffer = outBufs.at(terminalUid);
+        }
+
+        if (buffer) {
+            ciprBuf = (buffer->getMemory() == V4L2_MEMORY_DMABUF) \
+                     ? registerUserBuffer(buffer->getBufferSize(), buffer->getFd()) \
+                     : registerUserBuffer(buffer->getBufferSize(), buffer->getBufferAddr());
+            CheckError(!ciprBuf, NO_MEMORY, "%s, register buffer size %d for terminal %d fail",
+                       __func__, buffer->getBufferSize(), termIdx);
+            mTerminalBuffers[termIdx] = ciprBuf;
+        }
+    }
+
+    if (mTnrBuffers[TNR_BUFFER_IN_INDEX] && mTnrBuffers[TNR_BUFFER_OUT_INDEX]) {
+        std::swap(mTerminalBuffers[mTnrTerminalPair.inId],
+                  mTerminalBuffers[mTnrTerminalPair.outId]);
+    }
+
+    for (auto& pair : mDvsTerminalPairs) {
+        std::swap(mTerminalBuffers[pair.inId], mTerminalBuffers[pair.outId]);
+    }
+
+    return mPGParamAdapt->updatePALAndEncode(ipuParameters, mTerminalCount, mParamPayload);
+}
+
+int PGCommon::executePG()
+{
+    TRACE_LOG_PROCESS(mName.c_str(), __func__);
+    CheckError((!mCmd), INVALID_OPERATION, "%s, Command is invalid.", __func__);
+    CheckError((!mProcessGroup), INVALID_OPERATION, "%s, process group is invalid.", __func__);
+
+    mCmd->getConfig(&mCmdCfg);
+    int bufferCount = ia_css_process_group_get_terminal_count(mProcessGroup);
+    mCmdCfg.id = mPGId;
+    mCmdCfg.priority = 1;
+    mCmdCfg.pgParamsBuf = mPPG ? nullptr : mPGParamsBuffer;
+    mCmdCfg.pgManifestBuf = mManifestBuffer;
+    mCmdCfg.pg = mPGBuffer;
+    mCmdCfg.extBuf = mCmdExtBuffer;
+    mCmdCfg.buffers.resize(bufferCount);
+
+    for (int i = 0; i < bufferCount; i++) {
+        ia_css_terminal_t *terminal = ia_css_process_group_get_terminal(mProcessGroup, i);
+        CheckError(!terminal, UNKNOWN_ERROR, "failed to get terminal");
+        mCmdCfg.buffers[i] = mTerminalBuffers[terminal->tm_index];
+    }
+    if (mPPG) {
+         ia_css_process_group_set_token(mProcessGroup, mToken);
+    }
+
+    for (int fragIdx = 0; fragIdx < mFragmentCount; fragIdx++) {
+        int ret = ia_css_process_group_set_fragment_state(mProcessGroup, (uint16_t)fragIdx);
+        CheckError((ret != OK), ret, "%s, set fragment count %d fail %p", getName(), fragIdx, mProcessGroup);
+        ret = ia_css_process_group_set_fragment_limit(mProcessGroup, (uint16_t)(fragIdx + 1));
+        CheckError((ret != OK), ret, "%s, set fragment limit %d fail", getName(), fragIdx);
+
+        ret = handleCmd(&mCmd, &mCmdCfg);
+        CheckError((ret != OK), ret, "%s, call handleCmd fail", getName());
+    }
+
+    return OK;
+}
+
+int PGCommon::startPPG()
+{
+    // Get basic command config
+    CIPR::PSysCommandConfig cmdCfg;
+    mPPGCmd[PPG_CMD_TYPE_START]->getConfig(&cmdCfg);
+
+    // Update config
+    cmdCfg.id = mPGId;
+    cmdCfg.priority = 1;
+    cmdCfg.pgParamsBuf = mPPG ? nullptr : mPGParamsBuffer;
+    cmdCfg.pgManifestBuf = mManifestBuffer;
+    cmdCfg.pg = mPPGBuffer;
+    cmdCfg.extBuf = mPPGCmdExtBuffer[PPG_CMD_TYPE_START];
+    const int terminalCount = ia_css_process_group_get_terminal_count(mProcessGroup);
+    cmdCfg.buffers.resize(terminalCount);
+    std::fill(cmdCfg.buffers.begin(), cmdCfg.buffers.end(), nullptr);
+
+    ia_css_process_group_set_fragment_state(mPPGProcessGroup, 0);
+    ia_css_process_group_set_fragment_limit(mPPGProcessGroup, 1);
+
+    int ret = handleCmd(&mPPGCmd[PPG_CMD_TYPE_START], &cmdCfg);
+    mToken = ia_css_process_group_get_token(mPPGProcessGroup);
+    return ret;
+}
+
+int PGCommon::stopPPG()
+{
+    CIPR::PSysCommandConfig cmdCfg;
+
+    mPPGCmd[PPG_CMD_TYPE_STOP]->getConfig(&cmdCfg);
+
+    cmdCfg.id = mCmdCfg.id;
+    cmdCfg.priority = mCmdCfg.priority;
+    cmdCfg.pgParamsBuf = mCmdCfg.pgParamsBuf;
+    cmdCfg.pgManifestBuf = mCmdCfg.pgManifestBuf;
+    cmdCfg.pg = mPPGBuffer;
+    cmdCfg.extBuf = mPPGCmdExtBuffer[PPG_CMD_TYPE_STOP];
+    cmdCfg.buffers.resize(0);
+
+    int ret =  handleCmd(&mPPGCmd[PPG_CMD_TYPE_STOP], &cmdCfg);
+    return ret;
+}
+
+int PGCommon::handleCmd(CIPR::Command** cmd, CIPR::PSysCommandConfig* cmdCfg)
+{
+    CIPR::PSysEventConfig eventCfg = {};
+    mEvent->getConfig(&eventCfg);
+    cmdCfg->issueID = reinterpret_cast<uint64_t>(cmd);
+    eventCfg.commandIssueID = cmdCfg->issueID;
+
+    CIPR::Result ret = (*cmd)->setConfig(*cmdCfg);
+    CheckError((ret != CIPR::Result::OK), UNKNOWN_ERROR, "%s, call CIPR::Command::setConfig fail", __func__);
+
+    ret = (*cmd)->getConfig(cmdCfg);
+    CheckError((ret != CIPR::Result::OK), UNKNOWN_ERROR, "%s, call CIPR::Command::getConfig fail", __func__);
+
+    ret = (*cmd)->enqueue(mCtx);
+    CheckError((ret != CIPR::Result::OK), UNKNOWN_ERROR, "%s, call Context::enqueueCommand() fail %d", __func__, ret);
+
+    // Wait event
+    ret = mEvent->wait(mCtx);
+    CheckError((ret != CIPR::Result::OK), UNKNOWN_ERROR, "%s, call Context::waitForEvent fail, ret: %d", __func__, ret);
+
+    ret = mEvent->getConfig(&eventCfg);
+    CheckError((ret != CIPR::Result::OK), UNKNOWN_ERROR, "%s, call Event::getConfig() fail, ret: %d", __func__, ret);
+    // Ignore the error in event config since it's not a fatal error.
+    if (eventCfg.error) {
+        LOGW("%s, event config error: %d", __func__, eventCfg.error);
+    }
+
+    return (eventCfg.error == 0) ? OK : UNKNOWN_ERROR;
+}
+
+int PGCommon::getCapability()
+{
+    CIPR::PSYSCapability cap;
+    int ret = OK;
+    CIPR::Result err = mCtx->getCapabilities(&cap);
+    CheckError((err != CIPR::Result::OK), UNKNOWN_ERROR, "Call Context::getCapabilities() fail, ret:%d", ret);
+
+    LOG1("%s: capability.version:%d", __func__, cap.version);
+    LOG1("%s: capability.driver:%s", __func__, cap.driver);
+    LOG1("%s: capability.devModel:%s", __func__, cap.devModel);
+    LOG1("%s: capability.programGroupCount:%d", __func__, cap.programGroupCount);
+    mPGCount = cap.programGroupCount;
+
+    if (strncmp((char *)cap.devModel, "ipu4p", 5) == 0) {
+        mPlatform = IA_P2P_PLATFORM_CNL_B0;
+        LOG1("%s: cnl/icl/ksl shared the same p2p platform id", __func__);
+    } else if (strncmp((char *)cap.devModel, "ipu4", 4) == 0) {
+        switch (cap.devModel[13]) {
+            case 'B':
+                 mPlatform = IA_P2P_PLATFORM_BXT_B0;
+                 break;
+            default:
+                 LOGE("%s: unsupported psys device model :%s", __func__, cap.devModel);
+                 ret = BAD_VALUE;
+                 break;
+        }
+    } else if (strncmp((char *)cap.devModel, "ipu6", 4) == 0) {
+        mPlatform = IA_P2P_PLATFORM_IPU6;
+        mPPG = true;
+    } else {
+        LOGE("%s: unsupported psys device model : %s", __func__, cap.devModel);
+        ret = BAD_VALUE;
+    }
+
+    return ret;
+}
+
+int PGCommon::getManifest(int pgId)
+{
+    int i = 0;
+
+    for (; i < mPGCount; i++) {
+        CIPR::Buffer* manifestBuffer = nullptr;
+        int programCount = 0;
+        int terminalCount = 0;
+        int programGroupId = 0;
+        int manifestSize = 0;
+        ia_css_kernel_bitmap_t kernelBitmap = ia_css_kernel_bitmap_clear();
+        uint32_t size = 0;
+
+        CIPR::Result ret = mCtx->getManifest(i, &size, nullptr);
+        if (ret != CIPR::Result::OK) continue;
+        CheckError((size == 0), UNKNOWN_ERROR, "%s, the manifest size is 0", __func__);
+
+        manifestBuffer = createUserPtrCiprBuffer(size);
+        CheckError(!manifestBuffer, NO_MEMORY, "%s, call createUserPtrCiprBuffer fail", __func__);
+
+        void* manifest = getCiprBufferPtr(manifestBuffer);
+
+        ret = mCtx->getManifest(i, &size, manifest);
+        if (ret != CIPR::Result::OK) {
+            LOGE("%s, call Context::getManifest() fail", __func__);
+            delete manifestBuffer;
+            return UNKNOWN_ERROR;
+        }
+
+        LOG1("%s: pg index: %d, manifest size: %u", __func__, i, size);
+        const ia_css_program_group_manifest_t *mf = (const ia_css_program_group_manifest_t*)manifest;
+        programCount = ia_css_program_group_manifest_get_program_count(mf);
+        terminalCount = ia_css_program_group_manifest_get_terminal_count(mf);
+        programGroupId = ia_css_program_group_manifest_get_program_group_ID(mf);
+        manifestSize = ia_css_program_group_manifest_get_size(mf);
+        kernelBitmap = ia_css_program_group_manifest_get_kernel_bitmap(mf);
+
+        LOG1("%s: pgIndex: %d, programGroupId: %d, manifestSize: %d, programCount: %d, terminalCount: %d",
+             __func__, i, programGroupId, manifestSize, programCount, terminalCount);
+
+        if (pgId == programGroupId) {
+            mProgramCount = programCount;
+            mTerminalCount = terminalCount;
+            mManifestSize = manifestSize;
+            mKernelBitmap = kernelBitmap;
+            mManifestBuffer = manifestBuffer;
+            break;
+        }
+
+        delete manifestBuffer;
+    }
+
+    CheckError((i == mPGCount), BAD_VALUE, "%s, Can't found available pg: %d", __func__, pgId);
+
+    return OK;
+}
+
+CIPR::Buffer* PGCommon::createDMACiprBuffer(int size, int fd)
+{
+    CIPR::MemoryFlag deviceFlags = CIPR::MemoryFlag::MemoryHandle | CIPR::MemoryFlag::NoFlush;
+
+    CIPR::MemoryDesc mem;
+    mem.size = size;
+    mem.flags = CIPR::MemoryFlag::MemoryHandle | CIPR::MemoryFlag::HardwareOnly;
+    mem.handle = fd;
+    mem.cpuPtr = nullptr;
+    mem.anchor = nullptr;
+
+    CIPR::Buffer* buf = new CIPR::Buffer(size, mem.flags | deviceFlags, &mem);
+
+    CIPR::Result ret = buf->attatchDevice(mCtx);
+    if (ret != CIPR::Result::OK) {
+        LOGE("%s, call Buffer::attatchDevice() fail", __func__);
+        delete buf;
+        return nullptr;
+    }
+
+    return buf;
+}
+
+CIPR::Buffer* PGCommon::createUserPtrCiprBuffer(int size, void* ptr)
+{
+    CIPR::Buffer* buf = nullptr;
+    if (ptr == nullptr) {
+        buf = new CIPR::Buffer(size, CIPR::MemoryFlag::AllocateCpuPtr, nullptr);
+    } else {
+        CIPR::MemoryDesc mem;
+        mem.size = size;
+        mem.flags = CIPR::MemoryFlag::CpuPtr;
+        mem.handle = 0;
+        mem.cpuPtr = ptr;
+        mem.anchor = nullptr;
+        buf = new CIPR::Buffer(size, CIPR::MemoryFlag::CpuPtr, &mem);
+    }
+
+    CIPR::Result ret = buf->attatchDevice(mCtx);
+    if (ret != CIPR::Result::OK) {
+        LOGE("%s, call Buffer::attatchDevice() fail", __func__);
+        delete buf;
+        return nullptr;
+    }
+
+    return buf;
+}
+
+void* PGCommon::getCiprBufferPtr(CIPR::Buffer* buffer)
+{
+    CheckError(!buffer, nullptr, "%s, invalid cipr buffer", __func__);
+
+    void* ptr = nullptr;
+    CIPR::Result ret = buffer->getMemoryCpuPtr(&ptr);
+    CheckError((ret != CIPR::Result::OK), nullptr, "%s, call Buffer::getMemoryCpuPtr() fail", __func__);
+
+    return ptr;
+}
+
+int PGCommon::getCiprBufferSize(CIPR::Buffer* buffer)
+{
+    CheckError(!buffer, BAD_VALUE, "%s, invalid cipr buffer", __func__);
+
+    int size = 0;
+    CIPR::Result ret = buffer->getMemorySize(&size);
+    CheckError((ret != CIPR::Result::OK), NO_MEMORY, "%s, call Buffer::getMemorySize() fail", __func__);
+
+    return size;
+}
+
+CIPR::Buffer* PGCommon::registerUserBuffer(int size, void* ptr)
+{
+    CheckError((size <= 0 || ptr == nullptr), nullptr, "Invalid parameter: size=%d, ptr=%p", size, ptr);
+
+    for (auto it = mBuffers.begin(); it != mBuffers.end(); ++it) {
+        if (ptr == it->userPtr) {
+            if (size == getCiprBufferSize(it->ciprBuf)) {
+                return it->ciprBuf;
+            }
+
+            LOG2("%s, the buffer size is changed: old(%d), new(%d) addr(%p)",
+                 __func__, getCiprBufferSize(it->ciprBuf), size, it->userPtr);
+            delete it->ciprBuf;
+            it->ciprBuf = nullptr;
+            it->userPtr = nullptr;
+            mBuffers.erase(it);
+            break;
+        }
+    }
+
+    CIPR::Buffer* ciprBuf = createUserPtrCiprBuffer(size, ptr);
+    CheckError(!ciprBuf, nullptr, "Create cipr buffer for %p failed", ptr);
+
+    CiprBufferMapping bufMap;
+    bufMap.userPtr = ptr;
+    bufMap.ciprBuf = ciprBuf;
+    mBuffers.push_back(bufMap);
+
+    return ciprBuf;
+}
+
+CIPR::Buffer* PGCommon::registerUserBuffer(int size, int fd)
+{
+    CheckError((size <= 0 || fd < 0), nullptr, "Invalid parameter: size: %d, fd: %d", size, fd);
+
+    for (auto it = mBuffers.begin(); it != mBuffers.end(); ++it) {
+        if (fd == it->userFd) {
+            if (size == getCiprBufferSize(it->ciprBuf)) {
+                return it->ciprBuf;
+            }
+
+            LOG2("%s, the buffer size is changed: old(%d), new(%d) fd(%d)",
+                 __func__, getCiprBufferSize(it->ciprBuf), size, it->userFd);
+            delete it->ciprBuf;
+            it->ciprBuf = nullptr;
+            it->userFd = -1;
+            mBuffers.erase(it);
+            break;
+        }
+    }
+
+    CIPR::Buffer* ciprBuf = createDMACiprBuffer(size, fd);
+    CheckError(!ciprBuf, nullptr, "Create cipr buffer for fd %d failed", fd);
+
+    CiprBufferMapping bufMap;
+    bufMap.userFd = fd;
+    bufMap.ciprBuf = ciprBuf;
+    mBuffers.push_back(bufMap);
+
+    return ciprBuf;
+}
+
+void PGCommon::dumpTerminalPyldAndDesc(int pgId, long sequence, ia_css_process_group_t* pgGroup)
+{
+    if (!CameraDump::isDumpTypeEnable(DUMP_PSYS_PG)) return;
+
+    char fileName[MAX_NAME_LEN] = {'\0'};
+    uint32_t pgSize = ia_css_process_group_get_size(pgGroup);
+    snprintf(fileName, (MAX_NAME_LEN - 1), "hal_pg_%d_%ld.bin", pgId, sequence);
+
+    FILE *fp = fopen (fileName, "w+");
+    CheckError(fp == nullptr, VOID_VALUE, "open dump file %s failed", fileName);
+    const unsigned int* printPtr = (const unsigned int*)pgGroup;
+    fprintf(fp, "::pg dump size %d(0x%x)\n", pgSize, pgSize);
+    for (unsigned int i = 0; i < pgSize / sizeof(*printPtr); i++) {
+        fprintf(fp, "%08x\n", printPtr[i]);
+    }
+
+    int terminalCount = ia_css_process_group_get_terminal_count(pgGroup);
+    for (int i = 0; i < terminalCount; i++) {
+        ia_css_terminal_t *terminal = ia_css_process_group_get_terminal(pgGroup, i);
+        if (!terminal) {
+            LOGE("failed to get terminal");
+            fclose(fp);
+            return;
+        }
+        if (terminal->terminal_type == IA_CSS_TERMINAL_TYPE_DATA_IN
+            || terminal->terminal_type == IA_CSS_TERMINAL_TYPE_DATA_OUT) {
+            continue;
+        }
+
+        void* ptr = getCiprBufferPtr(mTerminalBuffers[terminal->tm_index]);
+        int size = getCiprBufferSize(mTerminalBuffers[terminal->tm_index]);
+        const char* typeStr = (terminal->terminal_type == IA_CSS_TERMINAL_TYPE_DATA_IN) ? "DATA_IN"
+                            : (terminal->terminal_type == IA_CSS_TERMINAL_TYPE_DATA_OUT) ? "DATA_OUT"
+                            : (terminal->terminal_type == IA_CSS_TERMINAL_TYPE_PARAM_STREAM) ? "PARAM_STREAM"
+                            : (terminal->terminal_type == IA_CSS_TERMINAL_TYPE_PARAM_CACHED_IN) ? "CACHED_IN"
+                            : (terminal->terminal_type == IA_CSS_TERMINAL_TYPE_PARAM_CACHED_OUT) ? "CACHED_OUT"
+                            : (terminal->terminal_type == IA_CSS_TERMINAL_TYPE_PARAM_SPATIAL_IN) ? "SPATIAL_IN"
+                            : (terminal->terminal_type == IA_CSS_TERMINAL_TYPE_PARAM_SPATIAL_OUT) ? "SPATIAL_OUT"
+                            : (terminal->terminal_type == IA_CSS_TERMINAL_TYPE_PARAM_SLICED_IN) ? "SLICED_IN"
+                            : (terminal->terminal_type == IA_CSS_TERMINAL_TYPE_PARAM_SLICED_OUT) ? "SLICED_OU"
+                            : (terminal->terminal_type == IA_CSS_TERMINAL_TYPE_STATE_IN) ? "STATE_IN"
+                            : (terminal->terminal_type == IA_CSS_TERMINAL_TYPE_STATE_OUT) ? "STATE_OUT"
+                            : (terminal->terminal_type == IA_CSS_TERMINAL_TYPE_PROGRAM) ? "PROGRAM"
+                            : (terminal->terminal_type == IA_CSS_TERMINAL_TYPE_PROGRAM_CONTROL_INIT) ? "PROGRAM_CONTROL_INIT"
+                            :                                                             "UNKNOWN";
+        printPtr = (const unsigned int*)ptr;
+        fprintf(fp, "::terminal %d dump size %d(0x%x), line %d, type %s\n", terminal->tm_index, size, size, PAGE_ALIGN(size)/4, typeStr);
+        for (unsigned int i = 0; i < PAGE_ALIGN(size) / sizeof(*printPtr); i++) {
+            fprintf(fp, "%08x\n", printPtr[i]);
+        }
+    }
+
+    fclose (fp);
+}
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/psysprocessor/PGCommon.h b/camera/hal/intel/ipu6/src/core/psysprocessor/PGCommon.h
new file mode 100644
index 000000000000..92f86012254f
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/psysprocessor/PGCommon.h
@@ -0,0 +1,240 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+extern "C" {
+#include <ia_css_psys_program_group_manifest.h>
+#include <ia_css_psys_terminal_manifest.h>
+#include <ia_css_program_group_data.h>
+#include <ia_css_program_group_param.h>
+#include <ia_css_psys_process_group.h>
+#include <ia_css_psys_terminal.h>
+#include <ia_css_terminal_types.h>
+#include <ia_css_terminal_manifest_types.h>
+#include <ia_css_psysapi_fw_version.h>
+}
+
+#include <vector>
+
+#ifdef ENABLE_SANDBOXING
+#include "modules/sandboxing/client/IntelPGParam.h"
+#else
+#include "modules/algowrapper/IntelPGParam.h"
+#endif
+#include "IspParamAdaptor.h"
+#include "BufferQueue.h"
+#include "PGUtils.h"
+
+#include "modules/ia_cipr/include/Buffer.h"
+#include "modules/ia_cipr/include/Context.h"
+#include "modules/ia_cipr/include/Event.h"
+#include "modules/ia_cipr/include/Command.h"
+
+namespace icamera {
+
+typedef std::map<ia_uid, FrameInfo> TerminalFrameInfoMap;
+typedef std::map<ia_uid, std::shared_ptr<CameraBuffer>> CameraBufferMap;
+
+#define FRAGMENT_OVERLAP 64
+
+/**
+ * \class PGCommon
+ *
+ * \brief This is a version PG implementation which is used to config and run PG.
+ *
+ * The call sequence as follows:
+ * 1. init();
+ * 2. setInputInfo();setOutputInfo();
+ * 3. setDisabledTerminals();
+ * 4. prepare():
+ *          configTerminalFormat();
+ *          calcFragmentCount();
+ *          handlePGParams();
+ *          setKernelBitMap();
+ *          setTerminalParams();
+ *          allocatePGBuffer();
+ *          setPGAndPrepareProgram();
+ *          configureFragmentDesc();
+ * 5. loop frame: iterate():
+ *          encodeTerminals();
+ *          handleCmd();
+ *          handleEvent();
+ *          decode();
+ * 6. deInit();
+ */
+class PGCommon {
+public:
+    static int getFrameSize(int format, int width, int height,
+                            bool needAlignedHeight = false, bool needExtraSize = true, bool needCompression = false);
+
+    PGCommon(int pgId, const std::string& pgName, ia_uid terminalBaseUid = 0);
+    virtual ~PGCommon();
+
+    /**
+     * allocate memory for some variables.
+     */
+    int init();
+
+    /**
+     * recycle memory.
+     */
+    void deInit();
+
+    /**
+     * set the input buffers info for terminals.
+     * use ia_fourcc
+     */
+    virtual void setInputInfo(const TerminalFrameInfoMap& inputInfos);
+
+    /**
+     * set the output buffers info for terminals.
+     * use ia_fourcc
+     */
+    virtual void setOutputInfo(const TerminalFrameInfoMap& outputInfos);
+
+    /**
+     * set the disabled terminals. Called before prepare()
+     */
+    virtual void setDisabledTerminals(const std::vector<ia_uid>& disabledTerminals);
+
+    /**
+     * set routing bitmap. Called before prepare()
+     */
+    virtual void setRoutingBitmap(const void* rbm, uint32_t bytes);
+
+    /**
+     * config the data terminals, init, config and prepare p2p, create process group.
+     */
+    virtual int prepare(IspParamAdaptor* adaptor, int streamId = -1);
+
+    /**
+     * run p2p to encode the params terminals, execute the PG and run p2p to decode the statistic terminals.
+     */
+    virtual int iterate(CameraBufferMap &inBufs, CameraBufferMap &outBufs,
+                        ia_binary_data *statistics, const ia_binary_data *ipuParameters);
+
+    const char* getName() { return mName.c_str(); }
+private:
+    DISALLOW_COPY_AND_ASSIGN(PGCommon);
+
+protected:
+    int getCapability();
+    int getManifest(int pgId);
+
+    // PG parameters intialization, for prepare()
+    virtual int configTerminalFormat();
+    int initParamAdapt();
+    virtual int calcFragmentCount(int overlap = FRAGMENT_OVERLAP);
+    int handlePGParams(const ia_css_frame_format_type* frameFormatTypes);
+    int setKernelBitMap();
+    virtual int setTerminalParams(const ia_css_frame_format_type* frameFormatTypes);
+    virtual int configureFragmentDesc();
+    int configureTerminalFragmentDesc(int termIdx, const ia_p2p_fragment_desc* srcDesc);
+    ia_css_process_group_t* createPG(CIPR::Buffer** pgBuffer);
+    int createCommands();
+    int createCommand(CIPR::Buffer* pg, CIPR::Command** cmd, CIPR::Buffer** extBuffer, int bufCount);
+    void destoryCommands();
+    int preparePayloadBuffers();
+
+    // For iteration
+    virtual int prepareTerminalBuffers(const ia_binary_data *ipuParameters,
+                                       const CameraBufferMap& inBufs,
+                                       const CameraBufferMap& outBufs);
+    int executePG();
+    int startPPG();
+    int stopPPG();
+    int handleCmd(CIPR::Command** cmd, CIPR::PSysCommandConfig* cmdCfg);
+
+    // Memory helper
+    CIPR::Buffer* createDMACiprBuffer(int size, int fd);
+    CIPR::Buffer* createUserPtrCiprBuffer(int size, void* ptr = nullptr);
+    void* getCiprBufferPtr(CIPR::Buffer* buffer);
+    CIPR::Buffer* registerUserBuffer(int size, void* ptr);
+    CIPR::Buffer* registerUserBuffer(int size, int fd);
+    int getCiprBufferSize(CIPR::Buffer* buffer);
+
+    void dumpTerminalPyldAndDesc(int pgId, long sequence, ia_css_process_group_t* pgGroup);
+
+protected:
+    enum PPGCommandType {
+        PPG_CMD_TYPE_START = 0,
+        PPG_CMD_TYPE_STOP,
+        PPG_CMD_TYPE_COUNT
+    };
+
+    struct CiprBufferMapping {
+        CiprBufferMapping() {}
+        void* userPtr = nullptr;
+        int userFd = -1;
+        CIPR::Buffer* baseCiprBuf = nullptr;
+        CIPR::Buffer* ciprBuf = nullptr;
+    };
+
+    static const int kEventTimeout = 8000;
+
+    CIPR::Context* mCtx = nullptr;
+    CIPR::Buffer* mManifestBuffer = nullptr;
+    CIPR::Buffer* mPGParamsBuffer = nullptr;
+    std::unique_ptr<IntelPGParam> mPGParamAdapt;
+
+    int mPGId;
+    std::string mName;  // For debug
+    ia_uid mTerminalBaseUid;
+    int mPGCount;
+    ia_p2p_platform_t mPlatform;
+    int mProgramCount;
+    int mTerminalCount;
+    int mManifestSize;
+    ia_css_kernel_bitmap_t mKernelBitmap;
+    std::unique_ptr<ia_css_rbm_t> mRoutingBitmap;
+    int mFragmentCount;
+    std::unique_ptr<uint8_t[]> mPgTerminals; // save terminal num in PG for each terminal
+    std::unique_ptr<ia_css_frame_format_type[]> mFrameFormatType;
+    std::vector<int> mDisableDataTermials;
+
+    ia_binary_data __attribute__ ((aligned (PG_PAGE_SIZE))) mParamPayload[IPU_MAX_TERMINAL_COUNT];
+
+    CIPR::Buffer* mPGBuffer = nullptr;
+    ia_css_process_group_t* mProcessGroup;
+    CIPR::Command* mCmd = nullptr;
+    CIPR::Buffer* mCmdExtBuffer = nullptr;
+
+    bool mPPG;
+    bool mPPGStarted;
+    CIPR::Buffer* mPPGBuffer = nullptr;
+    ia_css_process_group_t* mPPGProcessGroup;
+    CIPR::Command* mPPGCmd[PPG_CMD_TYPE_COUNT] = {nullptr, };
+    CIPR::Buffer* mPPGCmdExtBuffer[PPG_CMD_TYPE_COUNT] = {nullptr, };
+    uint64_t mToken;
+
+    CIPR::PSysCommandConfig mCmdCfg;
+    CIPR::Event* mEvent = nullptr;
+
+    CIPR::Buffer** mTerminalBuffers;;
+    std::map<int, FrameInfo> mTerminalFrameInfos; // valid for data terminals only
+    int mInputMainTerminal;
+    int mOutputMainTerminal;
+
+    std::vector<CiprBufferMapping> mBuffers;
+
+    TerminalPair mTnrTerminalPair;
+    uint8_t* mTnrBuffers[TNR_BUFFER_COUNT];
+
+    std::vector<TerminalPair> mDvsTerminalPairs;
+};
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/psysprocessor/PGUtils.cpp b/camera/hal/intel/ipu6/src/core/psysprocessor/PGUtils.cpp
new file mode 100644
index 000000000000..e79d01a9948d
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/psysprocessor/PGUtils.cpp
@@ -0,0 +1,194 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "PGUtils"
+
+#include <stdint.h>
+
+#include <vector>
+
+#include "iutils/Utils.h"
+#include "iutils/CameraLog.h"
+
+#include "PGUtils.h"
+
+namespace icamera {
+
+namespace PGUtils {
+
+/* ************************************************************
+ * Common definitions
+ * ***********************************************************/
+
+#define GET_FOURCC_FMT(a, b, c, d) ((uint32_t)(d) | ((uint32_t)(c) << 8) \
+                                 | ((uint32_t)(b) << 16) | ((uint32_t)(a) << 24))
+
+struct FormatMap {
+    int v4l2Fmt;
+    ia_css_frame_format_type cssFmt;
+
+    int cssBpp;
+};
+
+static const FormatMap sFormatMapping[] = {
+    { V4L2_PIX_FMT_YUYV,   IA_CSS_DATA_FORMAT_YUYV, 12 },
+    { V4L2_PIX_FMT_UYVY,   IA_CSS_DATA_FORMAT_UYVY, 12 },
+    { V4L2_PIX_FMT_YUV420, IA_CSS_DATA_FORMAT_YUV420, 16 },
+    { V4L2_PIX_FMT_NV12,   IA_CSS_DATA_FORMAT_NV12, 8 },
+    { V4L2_PIX_FMT_NV16,   IA_CSS_DATA_FORMAT_NV16, 12 },
+    { V4L2_PIX_FMT_RGB565, IA_CSS_DATA_FORMAT_RGB565, 16 },
+    { V4L2_PIX_FMT_RGB24,  IA_CSS_DATA_FORMAT_RGB888, 24 },
+    { V4L2_PIX_FMT_RGB32,  IA_CSS_DATA_FORMAT_RGBA888, 24 },
+    { V4L2_PIX_FMT_SGRBG12, IA_CSS_DATA_FORMAT_RAW, 16 },
+    { V4L2_PIX_FMT_SGRBG10, IA_CSS_DATA_FORMAT_RAW, 16 }, // IA_CSS_DATA_FORMAT_BAYER_GRBG or IA_CSS_DATA_FORMAT_RAW ?
+
+    { GET_FOURCC_FMT('Y', 'U', 'Y', 'V'), IA_CSS_DATA_FORMAT_YUYV, 12 },
+    { GET_FOURCC_FMT('U', 'Y', 'V', 'Y'), IA_CSS_DATA_FORMAT_UYVY, 12 },
+    { GET_FOURCC_FMT('Y', 'U', '1', '2'), IA_CSS_DATA_FORMAT_YUV420, 16 },
+    { GET_FOURCC_FMT('N', 'V', '1', '2'), IA_CSS_DATA_FORMAT_NV12, 8 },
+    { GET_FOURCC_FMT('N', 'V', '1', '6'), IA_CSS_DATA_FORMAT_NV16, 12 },
+    { GET_FOURCC_FMT('R', 'G', 'B', 'P'), IA_CSS_DATA_FORMAT_RGB565, 16 },
+    { GET_FOURCC_FMT('R', 'G', 'B', '3'), IA_CSS_DATA_FORMAT_RGB888, 24 },
+    { GET_FOURCC_FMT('R', 'G', 'B', '4'), IA_CSS_DATA_FORMAT_RGBA888, 24 },
+    { GET_FOURCC_FMT('B', 'A', '1', '2'), IA_CSS_DATA_FORMAT_RAW, 16 },
+    { GET_FOURCC_FMT('B', 'A', '1', '0'), IA_CSS_DATA_FORMAT_RAW, 16 }, // IA_CSS_DATA_FORMAT_BAYER_GRBG or IA_CSS_DATA_FORMAT_RAW ?
+    { GET_FOURCC_FMT('y', '0', '3', '2'), IA_CSS_DATA_FORMAT_YYUVYY_VECTORIZED, 16 },
+    { GET_FOURCC_FMT('V', '4', '2', '0'), IA_CSS_DATA_FORMAT_YUV420, 16 },
+    { GET_FOURCC_FMT('b','V','0','K'),    IA_CSS_DATA_FORMAT_BAYER_VECTORIZED, 16 },
+    { GET_FOURCC_FMT('C','S','L','6'),    IA_CSS_DATA_FORMAT_BAYER_LINE_INTERLEAVED, 16},
+    { GET_FOURCC_FMT('G','R','1','0'),    IA_CSS_DATA_FORMAT_BAYER_GRBG, 16 },
+    { GET_FOURCC_FMT('I','Y','U','V'),    IA_CSS_DATA_FORMAT_YUV420, 12 },
+};
+
+static int getStride(int cssFmt, int width);
+
+ia_css_frame_format_type getCssFmt(int v4l2Fmt) {
+    int size = ARRAY_SIZE(sFormatMapping);
+    for (int i = 0; i < size; i++) {
+        if (sFormatMapping[i].v4l2Fmt == v4l2Fmt) {
+            return sFormatMapping[i].cssFmt;
+        }
+    }
+
+    LOG2("%s: unsupported v4l2 pixel format: %s", __func__,
+         CameraUtils::format2string(v4l2Fmt).c_str());
+    return IA_CSS_N_FRAME_FORMAT_TYPES;
+}
+
+int getCssStride(int v4l2Fmt, int width) {
+    int stride = width;
+    ia_css_frame_format_type cssFmt = getCssFmt(v4l2Fmt);
+    switch (v4l2Fmt) {
+        case GET_FOURCC_FMT('I','Y','U','V'):
+            stride = width;
+            break;
+        default:
+            stride = getStride(cssFmt, width);
+            break;
+    }
+    return stride;
+}
+
+int getCssBpp(int v4l2Fmt) {
+    int size = ARRAY_SIZE(sFormatMapping);
+    for (int i = 0; i < size; i++) {
+        if (sFormatMapping[i].v4l2Fmt == v4l2Fmt) {
+            return sFormatMapping[i].cssBpp;
+        }
+    }
+
+    LOG2("%s: unsupported v4l2 pixel format: 0x%x", __func__, v4l2Fmt);
+    return 8;
+}
+
+int getStride(int cssFmt, int width) {
+    int stride = width;
+    switch (cssFmt) {
+        case IA_CSS_DATA_FORMAT_BAYER_GRBG: // GR10
+        case IA_CSS_DATA_FORMAT_RAW:        // BA10
+            stride = ALIGN_64(width * 2);
+            break;
+        case IA_CSS_DATA_FORMAT_YYUVYY_VECTORIZED: // y032
+            stride = width * 6;
+            break;
+        case IA_CSS_DATA_FORMAT_BAYER_VECTORIZED: // bv0k
+        case IA_CSS_DATA_FORMAT_BAYER_LINE_INTERLEAVED: // css_fourcc_grbg_12_li
+            stride = width * 4;
+            stride = ALIGN_64(stride);
+            break;
+        case IA_CSS_DATA_FORMAT_YUV420:
+            stride = width * 2;
+            stride = ALIGN_64(stride);
+            break;
+        case IA_CSS_DATA_FORMAT_NV12:
+            stride = width;
+            break;
+        default:
+            LOG2("TODO for format: %d", cssFmt);
+            break;
+    }
+    return stride;
+}
+
+/* ************************************************************
+ * Difference between PGs
+ * ***********************************************************/
+#define PG_PSYS_IPU6_ISA_LB 187
+#define PG_PSYS_IPU6_BB 189
+#define PG_PSYS_IPU6_ISL 198
+
+// the below terminals belong to PG_PSYS_IPU6_BB
+#define PG_BB_TERMINAL_ID_TNR_REF_IN 4 // data_terminal
+#define PG_BB_TERMINAL_ID_TNR_REF_OUT 6 // data_terminal
+
+// the below terminals belong to PG_PSYS_IPU6_ISA_LB
+#define ISA_LB_TERMINAL_ID_DVS_FE_IN_L0 21 // program_terminal
+#define ISA_LB_TERMINAL_ID_DVS_FE_IN_L1 22 // program_terminal
+#define ISA_LB_TERMINAL_ID_DVS_FE_IN_L2 23 // program_terminal
+#define ISA_LB_TERMINAL_ID_DVS_FE_OUT_L0 24 // param_terminal
+#define ISA_LB_TERMINAL_ID_DVS_FE_OUT_L1 25 // param_terminal
+#define ISA_LB_TERMINAL_ID_DVS_FE_OUT_L2 26 // param_terminal
+
+bool getTerminalPairs(int pgId, TERMINAL_PAIR_TYPE type, std::vector<TerminalPair>* pairs) {
+    LOG2("@%s, pgId:%d, type:%d, pairs:%p", __func__, pgId, type, pairs);
+    CheckError(!pairs, false, "@%s, pairs is nullptr", __func__);
+
+    struct TerminalPairs {
+        int pgId;
+        TERMINAL_PAIR_TYPE type;
+        std::vector<TerminalPair> pairs;
+    };
+    static const TerminalPairs tps[] = {
+        {PG_PSYS_IPU6_BB, TERMINAL_PAIR_TNR,
+         {{PG_BB_TERMINAL_ID_TNR_REF_IN, PG_BB_TERMINAL_ID_TNR_REF_OUT}}},
+        {PG_PSYS_IPU6_ISA_LB, TERMINAL_PAIR_DVS,
+         {{ISA_LB_TERMINAL_ID_DVS_FE_IN_L0, ISA_LB_TERMINAL_ID_DVS_FE_OUT_L0},
+          {ISA_LB_TERMINAL_ID_DVS_FE_IN_L1, ISA_LB_TERMINAL_ID_DVS_FE_OUT_L1},
+          {ISA_LB_TERMINAL_ID_DVS_FE_IN_L2, ISA_LB_TERMINAL_ID_DVS_FE_OUT_L2}}}
+    };
+
+    for (unsigned int i = 0; i < ARRAY_SIZE(tps); i++) {
+        if (tps[i].pgId == pgId && tps[i].type == type) {
+            *pairs = tps[i].pairs;
+            return true;
+        }
+    }
+
+    return false;
+}
+
+} // name space PGUtils
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/psysprocessor/PGUtils.h b/camera/hal/intel/ipu6/src/core/psysprocessor/PGUtils.h
new file mode 100644
index 000000000000..7e8ccfce6b99
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/psysprocessor/PGUtils.h
@@ -0,0 +1,86 @@
+/*
+ * Copyright (C) 2019 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+extern "C" {
+#include <ia_css_program_group_data.h>
+#include <ia_css_psys_program_group_manifest.h>
+#include <ia_css_psys_process_group.h>
+
+#include <ia_p2p.h>
+}
+
+#include <vector>
+
+namespace icamera {
+#define PG_PAGE_SIZE 4096
+#define PSYS_MAX_KERNELS_PER_PG IA_CSS_KERNEL_BITMAP_BITS
+#define IPU_MAX_TERMINAL_COUNT 40
+
+struct PgFrameDesc {
+    PgFrameDesc() {
+        width = 0;
+        height = 0;
+        bpe = 0;
+    }
+    int width;
+    int height;
+    int bpe;
+};
+
+struct PgConfiguration {
+    ia_css_program_group_manifest_t* pgManifest;
+    int pgManifestSize;
+    std::vector<int> disableDataTermials;
+    uint8_t fragmentCount;
+
+    // New API, for desc calculation by itself, instead of fragmentDesc
+    PgFrameDesc inputMainFrame;
+    PgFrameDesc outputMainFrame;
+};
+
+enum {
+    TNR_BUFFER_IN_INDEX = 0,
+    TNR_BUFFER_OUT_INDEX
+};
+#define TNR_BUFFER_COUNT (TNR_BUFFER_OUT_INDEX + 1)
+
+struct TerminalPair {
+    int inId;
+    int outId;
+};
+
+namespace PGUtils {
+/* ************************************************************
+ * Common definitions
+ * ***********************************************************/
+
+ia_css_frame_format_type getCssFmt(int v4l2Fmt);
+int getCssBpp(int v4l2Fmt);
+int getCssStride(int v4l2Fmt, int width);
+
+/* ************************************************************
+ * Difference between PGs
+ * ***********************************************************/
+enum TERMINAL_PAIR_TYPE {
+    TERMINAL_PAIR_TNR,
+    TERMINAL_PAIR_DVS
+};
+
+bool getTerminalPairs(int pgId, TERMINAL_PAIR_TYPE type, std::vector<TerminalPair>* pairs);
+} // name space PGUtils
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/core/psysprocessor/PSysDAG.cpp b/camera/hal/intel/ipu6/src/core/psysprocessor/PSysDAG.cpp
new file mode 100644
index 000000000000..9eca3f8b1924
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/psysprocessor/PSysDAG.cpp
@@ -0,0 +1,759 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "PSysDAG"
+
+#include <algorithm>
+#include "iutils/Utils.h"
+#include "iutils/CameraLog.h"
+#include "PSysDAG.h"
+#ifdef TNR7_CM
+#include "GPUExecutor.h"
+#endif
+
+namespace icamera {
+PSysDAG::PSysDAG(int cameraId, PSysDagCallback* psysDagCB) :
+    mCameraId(cameraId),
+    mPSysDagCB(psysDagCB),
+    mConfigMode(CAMERA_STREAM_CONFIGURATION_MODE_AUTO),
+    mTuningMode(TUNING_MODE_MAX),
+    mDefaultMainInputPort(MAIN_PORT),
+    mRunAicAfterQbuf(false),
+    mLastTaskSequence(-1)
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+
+    CLEAR(mOngoingSequence);
+    mPolicyManager = new PolicyManager(mCameraId);
+    mIspParamAdaptor = new IspParamAdaptor(mCameraId, PG_PARAM_PSYS_ISA);
+}
+
+PSysDAG::~PSysDAG()
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+
+    releasePipeExecutors();
+
+    mIspParamAdaptor->deinit();
+    delete mIspParamAdaptor;
+    delete mPolicyManager;
+}
+
+void PSysDAG::setFrameInfo(const std::map<Port, stream_t>& inputInfo,
+                           const std::map<Port, stream_t>& outputInfo) {
+    mInputFrameInfo = inputInfo;
+    mOutputFrameInfo = outputInfo;
+
+    mDefaultMainInputPort = inputInfo.begin()->first;
+    // Select default main input port in priority
+    Port availablePorts[] = {MAIN_PORT, SECOND_PORT, THIRD_PORT, FORTH_PORT, INVALID_PORT};
+    for (unsigned int i = 0; i < ARRAY_SIZE(availablePorts); i++) {
+        if (mInputFrameInfo.find(availablePorts[i]) != mInputFrameInfo.end()) {
+            mDefaultMainInputPort = availablePorts[i];
+            break;
+        }
+    }
+}
+
+void PSysDAG::releasePipeExecutors()
+{
+    for (auto &executor : mExecutorsPool) {
+        delete executor;
+    }
+    mExecutorsPool.clear();
+    mExecutorStreamId.clear();
+}
+
+/*
+ * According to the policy config to create the executors,
+ * and use the graph config data to configure the executors.
+ */
+int PSysDAG::createPipeExecutors()
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+
+    releasePipeExecutors();
+
+    // initialize the sequence list to -1
+    for (int i = 0; i < MAX_BUFFER_COUNT; i++) {
+        mOngoingSequence[i] = -1;
+    }
+
+    IGraphConfigManager *GCM = IGraphConfigManager::getInstance(mCameraId);
+    CheckError(!GCM, UNKNOWN_ERROR, "Failed to get GC manager in PSysDAG!");
+
+    std::shared_ptr<IGraphConfig> gc = GCM->getGraphConfig(mConfigMode);
+    CheckError(!gc, UNKNOWN_ERROR, "Failed to get GraphConfig in PSysDAG!");
+
+    int graphId = gc->getGraphId();
+    PolicyConfig* cfg = PlatformData::getExecutorPolicyConfig(graphId);
+    CheckError(!cfg, UNKNOWN_ERROR, "Failed to get PolicyConfig in PSysDAG!");
+
+    std::vector<std::string> pgNames;
+    gc->getPgNames(&pgNames);
+    bool hasVideoPipe = false, hasStillPipe = false;
+
+    for (auto &item : cfg->pipeExecutorVec) {
+        int streamId = -1;
+        bool pgFound = true;
+
+        // Not support multiple streamId in one executor,
+        // so need to the check the streamId of pgList.
+        for (auto &pgName : item.pgList) {
+            if (std::find(pgNames.begin(), pgNames.end(), pgName) == pgNames.end()) {
+                pgFound = false;
+                break;
+            }
+            int tmpId = gc->getStreamIdByPgName(pgName);
+            CheckError(tmpId == -1, BAD_VALUE, "Cannot get streamId for %s", pgName.c_str());
+            CheckError(((streamId != -1) && (tmpId != streamId)), BAD_VALUE,
+                    "the streamId: %d for pgName(%s) is different with previous: %d",
+                    tmpId, pgName.c_str(), streamId);
+            streamId = tmpId;
+            LOG1("%s executor:%s pg name:%s streamId: %d",
+                  __func__, item.exeName.c_str(), pgName.c_str(), streamId);
+        }
+        if (!pgFound)
+            continue;
+
+        if (!hasVideoPipe)
+            hasVideoPipe = (streamId == VIDEO_STREAM_ID);
+        if (!hasStillPipe)
+            hasStillPipe = (streamId == STILL_STREAM_ID);
+#ifdef TNR7_CM
+        PipeExecutor *executor;
+        if (strstr(item.exeName.c_str(), "gputnr") != nullptr) {
+            executor = new GPUExecutor(mCameraId, item, cfg->exclusivePgs, this, gc);
+        } else {
+            executor = new PipeExecutor(mCameraId, item, cfg->exclusivePgs, this, gc);
+        }
+#else
+        PipeExecutor *executor = new PipeExecutor(mCameraId, item, cfg->exclusivePgs, this, gc);
+#endif
+        executor->setIspParamAdaptor(mIspParamAdaptor);
+        executor->setStreamId(streamId);
+        executor->setPolicyManager(mPolicyManager);
+        executor->setNotifyPolicy(item.notifyPolicy);
+
+        int ret = executor->initPipe();
+        if (ret != OK) {
+            LOGE("Failed to create pipe for executor:%s", executor->getName());
+            delete executor;
+            return ret;
+        }
+
+        mExecutorsPool.push_back(executor);
+        mExecutorStreamId[executor] = streamId;
+    }
+    LOG2("%s, hasVideoPipe: %d, hasStillPipe: %d, enableBundleInSdv: %d",
+         __func__, hasVideoPipe, hasStillPipe, cfg->enableBundleInSdv);
+
+    // Only enable psys bundle with aic when has video pipe only
+    if (!hasStillPipe && PlatformData::psysBundleWithAic(mCameraId)) {
+        mRunAicAfterQbuf = true;
+    }
+
+    if (hasStillPipe && hasVideoPipe && !cfg->enableBundleInSdv) return OK;
+
+    for (auto &bundle : cfg->bundledExecutorDepths) {
+        bool foundExecutor = true;
+        for (auto &executor : bundle.bundledExecutors) {
+            std::vector<PipeExecutor*>::iterator it = std::find_if(mExecutorsPool.begin(),
+                    mExecutorsPool.end(), [executor](PipeExecutor* exec) {
+                        return exec->getName() == executor;
+                    });
+            if (it == mExecutorsPool.end()) {
+                foundExecutor = false;
+                break;
+            }
+        }
+
+        if (foundExecutor)
+            mPolicyManager->addExecutorBundle(bundle.bundledExecutors, bundle.depths);
+    }
+
+    return OK;
+}
+
+int PSysDAG::linkAndConfigExecutors()
+{
+    for (auto& consumer : mExecutorsPool) {
+        std::map<ia_uid, Port> input;
+
+        if (consumer->isInputEdge()) {
+            // Use its own input info due to no executor as producer
+            consumer->getInputTerminalPorts(input);
+        } else {
+            PipeExecutor* producer = findExecutorProducer(consumer);
+            CheckError(producer == nullptr, BAD_VALUE, "no producer for executor %s!", consumer->getName());
+            producer->getOutputTerminalPorts(input);
+
+            consumer->setBufferProducer(producer);
+            LOG1("%s: link consumer %s to %s", __func__, consumer->getName(), producer->getName());
+        }
+
+        // Link producer (output) to consumer (input) by terminal
+        consumer->setInputTerminals(input);
+
+        std::vector<ConfigMode> configModes;
+        configModes.push_back(mConfigMode);
+        consumer->configure(configModes);
+    }
+
+    return OK;
+}
+
+PipeExecutor* PSysDAG::findExecutorProducer(PipeExecutor* consumer)
+{
+    std::map<ia_uid, Port> inputTerminals;
+    consumer->getInputTerminalPorts(inputTerminals);
+
+    for (auto& executor : mExecutorsPool) {
+        if (executor == consumer) {
+            continue;
+        }
+
+        for (auto& inputTerminal : inputTerminals) {
+            // Return if one is matched, because only one producer is supported now.
+            if (executor->hasOutputTerminal(inputTerminal.first)) {
+                return executor;
+            }
+        }
+    }
+
+    return nullptr;
+}
+
+/*
+ * Search all the stream Ids in one pipe which provides frame buffer to the output port
+ * mOutputPortToStreamIds: store all the different stream Ids to output port mapping table
+ */
+status_t PSysDAG::searchStreamIdsForOutputPort(PipeExecutor *executor, Port port) {
+    LOG2("@%s, mCameraId:%d", __func__, mCameraId);
+
+    CheckError(!executor || !executor->isOutputEdge(), BAD_VALUE,
+               "%s, the executor is nullptr or is not output edge", __func__);
+
+    if (mOutputPortToStreamIds.find(port) != mOutputPortToStreamIds.end()) {
+        return OK;
+    }
+
+    std::vector<int32_t> streamIds;
+    PipeExecutor *tmpExecutor = executor;
+    // Loop to find the producer executor's stream id
+    do {
+        int32_t streamId = mExecutorStreamId[tmpExecutor];
+        if (std::find(streamIds.begin(), streamIds.end(), streamId) == streamIds.end()) {
+            streamIds.push_back(streamId);
+            LOG2("%s, store the streamId: %d for output port: %d", __func__, streamId, port);
+        }
+
+        tmpExecutor = findExecutorProducer(tmpExecutor);
+    } while (tmpExecutor);
+
+    mOutputPortToStreamIds[port] = streamIds;
+
+    return OK;
+}
+
+/**
+ * Bind the port between DAG and its edge executors.
+ * After the binding we'll know where the task buffer should be queued to.
+ */
+int PSysDAG::bindExternalPortsToExecutor()
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+
+    mInputMaps.clear();
+    mOutputMaps.clear();
+    mOutputPortToStreamIds.clear();
+
+    std::map<Port, stream_t> outputInfo;
+    std::map<Port, stream_t> inputInfo;
+
+    // Bind the input ports first.
+    LOG2("%s, start to bind the input port", __func__);
+    for (auto& executor : mExecutorsPool) {
+        if (!executor->isInputEdge()) {
+            continue;
+        }
+        executor->getFrameInfo(inputInfo, outputInfo);
+
+        for (auto& frameInfo : mInputFrameInfo) {
+            for (auto& portInfo : inputInfo) {
+                // Check if it has been cleared (bound already).
+                if (!portInfo.second.format) {
+                    continue;
+                }
+                if (executor->isSameStreamConfig(portInfo.second, frameInfo.second, mConfigMode, false)) {
+                    PortMapping portMap;
+                    portMap.mExecutor = executor;
+                    portMap.mDagPort = frameInfo.first;
+                    portMap.mExecutorPort = portInfo.first;
+                    mInputMaps.push_back(portMap);
+                    // Clear the stream of executor to avoid binding it again.
+                    CLEAR(portInfo.second);
+                    LOG2("%s, inputMap executor %p, dagPort %d, execPort %d", __func__,
+                        executor, frameInfo.first, portInfo.first);
+                    break;
+                }
+            }
+        }
+    }
+
+    // Then bind the output ports.
+    LOG2("%s, start to bind the output port", __func__);
+    for (auto& executor : mExecutorsPool) {
+        if (!executor->isOutputEdge()) {
+            continue;
+        }
+
+        executor->getFrameInfo(inputInfo, outputInfo);
+        for (auto& frameInfo : mOutputFrameInfo) {
+            for (auto& portInfo : outputInfo) {
+                // Check if it has been cleared (bound already).
+                if (!portInfo.second.format) {
+                    continue;
+                }
+                if (executor->isSameStreamConfig(portInfo.second, frameInfo.second, mConfigMode, true)) {
+                    PortMapping portMap;
+                    portMap.mExecutor = executor;
+                    portMap.mDagPort = frameInfo.first;
+                    portMap.mExecutorPort = portInfo.first;
+                    mOutputMaps.push_back(portMap);
+                    searchStreamIdsForOutputPort(executor, frameInfo.first);
+                    // Clear the stream of executor to avoid binding it again.
+                    CLEAR(portInfo.second);
+                    break;
+                }
+            }
+        }
+    }
+
+    // Each required port must be mapped to one of (edge) executor's port.
+    // One input port may be mapped to more of (edge) executor's ports.
+    CheckError(mInputMaps.size() < mInputFrameInfo.size(), BAD_VALUE, "Failed to bind input ports");
+    CheckError(mOutputMaps.size() != mOutputFrameInfo.size(), BAD_VALUE, "Failed to bind output ports");
+
+    return OK;
+}
+
+int PSysDAG::registerUserOutputBufs(Port port, const std::shared_ptr<CameraBuffer> &camBuffer)
+{
+    for (auto& outputMap : mOutputMaps) {
+        if (port == outputMap.mDagPort) {
+            outputMap.mExecutor->registerOutBuffers(outputMap.mExecutorPort, camBuffer);
+            break;
+        }
+    }
+
+    return OK;
+}
+
+int PSysDAG::registerInternalBufs(std::map<Port, CameraBufVector> &internalBufs)
+{
+    for (auto& portToBuffers : internalBufs) {
+        for (auto& inputMap : mInputMaps) {
+            if (inputMap.mDagPort == portToBuffers.first) {
+                for (auto& inputBuf : portToBuffers.second) {
+                    inputMap.mExecutor->registerInBuffers(inputMap.mExecutorPort, inputBuf);
+                }
+                break;
+            }
+        }
+    }
+
+    return OK;
+}
+
+/**
+ * Queue the buffers in PSysTaskData to the cooresponding executors.
+ */
+int PSysDAG::queueBuffers(const PSysTaskData& task)
+{
+    LOG2("@%s, mCameraId:%d", __func__, mCameraId);
+
+    // Provide the input buffers for the input edge executor.
+    for (auto& inputFrame : task.mInputBuffers) {
+        for (auto& inputMap : mInputMaps) {
+            if (inputMap.mDagPort == inputFrame.first) {
+                inputMap.mExecutor->onFrameAvailable(inputMap.mExecutorPort, inputFrame.second);
+                LOG2("%s, queue input buffer: dagPort: %d, executorPort: %d, name: %s", __func__,
+                     inputMap.mDagPort, inputMap.mExecutorPort, inputMap.mExecutor->getName());
+            }
+        }
+    }
+
+    // Provide the output buffers for the output edge executor.
+    for (auto& outputFrame : task.mOutputBuffers) {
+        for (auto& outputMap : mOutputMaps) {
+            if (outputMap.mDagPort == outputFrame.first) {
+                outputMap.mExecutor->qbuf(outputMap.mExecutorPort, outputFrame.second);
+                LOG2("%s, queue output buffer, dagPort: %d, executorPort: %d, name: %s", __func__,
+                     outputMap.mDagPort, outputMap.mExecutorPort, outputMap.mExecutor->getName());
+                break;
+            }
+        }
+    }
+
+    return OK;
+}
+
+int PSysDAG::configure(ConfigMode configMode, TuningMode tuningMode)
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+
+    mConfigMode = configMode;
+    mTuningMode = tuningMode;
+    mRunAicAfterQbuf = false;
+    mLastTaskSequence = -1;
+
+    // Configure IspParamAdaptor
+    int ret = mIspParamAdaptor->init();
+    CheckError(ret != OK, ret, "Init isp Adaptor failed, tuningMode %d", mTuningMode);
+
+    ret = mIspParamAdaptor->configure(mInputFrameInfo[mDefaultMainInputPort], mConfigMode, mTuningMode);
+    CheckError(ret != OK, ret, "Configure isp Adaptor failed, tuningMode %d", mTuningMode);
+
+    ret = createPipeExecutors();
+    CheckError(ret != OK, ret, "@%s, create psys executors failed", __func__);
+
+    ret = linkAndConfigExecutors();
+    CheckError(ret != OK, ret, "Link executors failed");
+
+    ret = bindExternalPortsToExecutor();
+    CheckError(ret != OK, ret, "Bind ports failed");
+
+    return OK;
+}
+
+int PSysDAG::start()
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+
+    mPolicyManager->setActive(true);
+
+    for (auto& executors : mExecutorsPool) {
+        executors->start();
+    }
+    return OK;
+}
+
+int PSysDAG::stop()
+{
+    LOG1("@%s, mCameraId:%d", __func__, mCameraId);
+
+    mPolicyManager->setActive(false);
+
+    for (auto& executors : mExecutorsPool) {
+        executors->notifyStop();
+    }
+
+    for (auto& executors : mExecutorsPool) {
+        executors->stop();
+    }
+    return OK;
+}
+
+int PSysDAG::resume()
+{
+    mPolicyManager->setActive(true);
+    return OK;
+}
+
+int PSysDAG::pause()
+{
+    mPolicyManager->setActive(false);
+    return OK;
+}
+
+void PSysDAG::addTask(PSysTaskData taskParam)
+{
+    LOG2("@%s, mCameraId:%d", __func__, mCameraId);
+
+    if (taskParam.mTuningMode != mTuningMode) {
+        tuningReconfig(taskParam.mTuningMode);
+    }
+
+    TaskInfo task = {};
+    {
+        // Save the task data into mOngoingTasks
+        task.mTaskData = taskParam;
+        // Count how many valid output buffers need to be returned.
+        for (auto& outBuf : taskParam.mOutputBuffers) {
+            if (outBuf.second) {
+                task.mNumOfValidBuffers++;
+            }
+        }
+        LOG2("%s:Id:%d push task with %d output buffers, sequence: %ld",
+                __func__, mCameraId, task.mNumOfValidBuffers,
+                taskParam.mInputBuffers.at(mDefaultMainInputPort)->getSequence());
+        AutoMutex taskLock(mTaskLock);
+        mOngoingTasks.push_back(task);
+    }
+
+    // It's too early to runIspAdapt here, and the ipu parameters
+    // may be incorrect when runPipe.
+    bool runIspAdaptor = true;
+
+    // Run aic before queue psys buffer:
+    //  1. disable psys bundle with aic or has still pipe configured
+    //  2. for the first task
+    //  3. need to run aic for current task if drop frame happens
+    long sequence = taskParam.mInputBuffers.at(mDefaultMainInputPort)->getSequence();
+    if (runIspAdaptor && (!mRunAicAfterQbuf ||
+        (mLastTaskSequence == -1) || (sequence > mLastTaskSequence + 1))) {
+        LOG2("%s, run AIC before execute psys for sequence: %ld", __func__, sequence);
+        prepareIpuParams(sequence, false, &task);
+    }
+
+    queueBuffers(taskParam);
+
+    if (runIspAdaptor && mRunAicAfterQbuf) {
+        LOG2("%s, run AIC bundle with execute psys for sequence: %ld", __func__, sequence);
+        // if running psys bundle with aic, current aic result is for next sequence
+        prepareIpuParams((sequence + 1), false, &task);
+    }
+
+    mLastTaskSequence = sequence;
+}
+
+int PSysDAG::getParameters(Parameters& param)
+{
+    return mIspParamAdaptor->getParameters(param);
+}
+
+TuningMode PSysDAG::getTuningMode(long sequence)
+{
+    AutoMutex taskLock(mTaskLock);
+
+    TuningMode taskTuningMode = mTuningMode;
+    bool taskTuningModeFound = false;
+
+    for (auto const& task : mOngoingTasks) {
+        if (sequence == task.mTaskData.mInputBuffers.at(mDefaultMainInputPort)->getSequence()) {
+            taskTuningMode = task.mTaskData.mTuningMode;
+            taskTuningModeFound = true;
+            break;
+        }
+    }
+
+    if (!taskTuningModeFound) {
+        LOGW("No task tuning mode found for sequence:%ld, use current DAG tuning mode.", sequence);
+    }
+
+    return taskTuningMode;
+}
+
+/**
+ * Use to handle the frame done event from the executors.
+ *
+ * This is for returning output buffers to PSysDAG. And it'll check if all the valid
+ * output buffer returned, if so, then it'll return the whole corresponding task data to
+ * PSysProcessor.
+ */
+int PSysDAG::onFrameDone(Port port, const std::shared_ptr<CameraBuffer>& buffer)
+{
+    LOG2("@%s, mCameraId:%d buffer=%p", __func__, mCameraId, buffer.get());
+
+    if (!buffer) return OK; // No need to handle if the buffer is nullptr.
+
+    long sequence = buffer->getSequence();
+    bool needReturn = false;
+    Port outputPort = INVALID_PORT;
+    PSysTaskData result;
+    {
+        AutoMutex taskLock(mTaskLock);
+        for (auto it = mOngoingTasks.begin(); it != mOngoingTasks.end(); it++) {
+            // Check if the returned buffer belong to the task.
+            if (sequence != it->mTaskData.mInputBuffers.at(mDefaultMainInputPort)->getSequence()) {
+                continue;
+            }
+
+            for (auto& buf : it->mTaskData.mOutputBuffers) {
+                if (buf.second && (buffer->getUserBuffer() == buf.second->getUserBuffer())) {
+                    outputPort = buf.first;
+                }
+            }
+            it->mNumOfReturnedBuffers++;
+            if (it->mNumOfReturnedBuffers >= it->mNumOfValidBuffers) {
+                result = it->mTaskData;
+                needReturn = true;
+                LOG2("%s:Id:%d finish task with %d returned output buffers, sequence: %ld",
+                     __func__, mCameraId, it->mNumOfReturnedBuffers, sequence);
+                // Remove the task data from mOngoingTasks since it's already processed.
+                mOngoingTasks.erase(it);
+
+                // Remove the sequence when finish to process the task
+                AutoMutex l(mSequenceLock);
+                for (int i = 0; i < MAX_BUFFER_COUNT; i++) {
+                    if (mOngoingSequence[i] == sequence) {
+                        mOngoingSequence[i] = -1;
+                        break;
+                    }
+                }
+            }
+            // No need check other if other tasks are matched with the returned buffer since
+            // we already found one.
+            break;
+        }
+    }
+
+    CheckError(outputPort == INVALID_PORT, INVALID_OPERATION, "outputPort is invalid");
+    // Return buffer
+    mPSysDagCB->onBufferDone(sequence, outputPort, buffer);
+
+    if (needReturn) {
+        returnBuffers(result);
+    }
+
+    return OK;
+}
+
+int PSysDAG::prepareIpuParams(long sequence, bool forceUpdate, TaskInfo *task)
+{
+    TRACE_LOG_PROCESS("PSysDAG", __func__, MAKE_COLOR(sequence), sequence);
+    // Make sure the AIC is executed once.
+    if (!forceUpdate) {
+        AutoMutex l(mSequenceLock);
+
+        for (int i = 0; i < MAX_BUFFER_COUNT; i++) {
+            // This means aic for the sequence has been executed.
+            if (mOngoingSequence[i] == sequence) {
+                return OK;
+            }
+        }
+
+        // Store the new sequence.
+        for (int i = 0; i < MAX_BUFFER_COUNT; i++) {
+            if (mOngoingSequence[i] == -1) {
+                mOngoingSequence[i] = sequence;
+                break;
+            }
+        }
+    }
+
+    if (task == nullptr) {
+        AutoMutex taskLock(mTaskLock);
+        for (size_t i = 0; i < mOngoingTasks.size(); i++) {
+            if (sequence ==
+                mOngoingTasks[i].mTaskData.mInputBuffers.at(mDefaultMainInputPort)->getSequence()) {
+                task = &mOngoingTasks[i];
+                break;
+            }
+        }
+    }
+    CheckError(!task, UNKNOWN_ERROR,
+               "%s, Failed to find the task for sequence: %ld", __func__, sequence);
+
+    // According to the output port to filter the valid executor stream Ids, and then run AIC
+    std::vector<int32_t> activeStreamIds;
+    for (auto& outputFrame : task->mTaskData.mOutputBuffers) {
+        if (outputFrame.second.get() == nullptr)
+            continue;
+
+        std::map<Port, std::vector<int32_t> >::iterator
+            it = mOutputPortToStreamIds.find(outputFrame.first);
+        CheckError(it == mOutputPortToStreamIds.end(), UNKNOWN_ERROR,
+                   "%s, failed to find streamIds for output port: %d",
+                   __func__, outputFrame.first);
+
+        for (auto& streamId : it->second) {
+            if (std::find(activeStreamIds.begin(), activeStreamIds.end(), streamId)
+                    == activeStreamIds.end()) {
+                activeStreamIds.push_back(streamId);
+            }
+        }
+    }
+    LOG2("%s, the active streamId size for aic(sequence: %ld) is %zu",
+         __func__, sequence, activeStreamIds.size());
+
+    int ret = OK;
+    for (auto& id : activeStreamIds) {
+        ret = mIspParamAdaptor->runIspAdapt(&task->mTaskData.mIspSettings, sequence, id);
+        CheckError(ret != OK, UNKNOWN_ERROR,
+                   "%s, Failed to run AIC: sequence: %ld streamId: %d", __func__, sequence, id);
+    }
+
+    return OK;
+}
+
+int PSysDAG::returnBuffers(PSysTaskData& result)
+{
+    LOG2("@%s, mCameraId:%d", __func__, mCameraId);
+
+    CheckError(!mPSysDagCB, INVALID_OPERATION, "Invalid PSysProcessor");
+
+    mPSysDagCB->onFrameDone(result);
+    return OK;
+}
+
+void PSysDAG::registerListener(EventType eventType, EventListener* eventListener)
+{
+    //Pass through event registration to PipeExecutor
+    for (auto const& executor : mExecutorsPool) {
+        executor->registerListener(eventType, eventListener);
+    }
+}
+
+void PSysDAG::removeListener(EventType eventType, EventListener* eventListener)
+{
+    //Pass through event unregistration to PipeExecutor
+    for (auto const& executor : mExecutorsPool) {
+        executor->removeListener(eventType, eventListener);
+    }
+}
+
+void PSysDAG::tuningReconfig(TuningMode newTuningMode)
+{
+    LOG1("@%s ", __func__);
+
+    if (mIspParamAdaptor) {
+        mIspParamAdaptor->deinit();
+    } else {
+        mIspParamAdaptor = new IspParamAdaptor(mCameraId, PG_PARAM_PSYS_ISA);
+    }
+
+    int ret = mIspParamAdaptor->init();
+    CheckError(ret != OK, VOID_VALUE, "Init isp Adaptor failed, tuningMode %d", newTuningMode);
+
+    ret = mIspParamAdaptor->configure(mInputFrameInfo[mDefaultMainInputPort], mConfigMode, newTuningMode);
+    CheckError(ret != OK, VOID_VALUE, "Failed to reconfig isp Adaptor.");
+
+    mTuningMode = newTuningMode;
+}
+
+void PSysDAG::dumpExternalPortMap()
+{
+    for (auto& inputMap : mInputMaps) {
+        if (inputMap.mExecutor) {
+            LOG2("@%s: Input port %d, executor: %s:%d", __func__, inputMap.mDagPort,
+                 inputMap.mExecutor->getName(), inputMap.mExecutorPort);
+        } else {
+            LOGE("%s: no executro for input port %d!", __func__, inputMap.mDagPort);
+        }
+    }
+    for (auto& outputMap : mOutputMaps) {
+        if (outputMap.mExecutor) {
+            LOG2("@%s: Output port %d, executor: %s:%d", __func__, outputMap.mDagPort,
+                 outputMap.mExecutor->getName(), outputMap.mExecutorPort);
+        } else {
+            LOGE("%s: no executro for output port %d!", __func__, outputMap.mDagPort);
+        }
+    }
+}
+
+}
diff --git a/camera/hal/intel/ipu6/src/core/psysprocessor/PSysDAG.h b/camera/hal/intel/ipu6/src/core/psysprocessor/PSysDAG.h
new file mode 100644
index 000000000000..399535159a5b
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/psysprocessor/PSysDAG.h
@@ -0,0 +1,155 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <unordered_map>
+
+#include "Parameters.h"
+#include "PlatformData.h"
+#include "CameraBuffer.h"
+#include "IspParamAdaptor.h"
+#ifdef USE_PG_LITE_PIPE
+#include "PipeLiteExecutor.h"
+#else
+#include "PipeExecutor.h"
+#endif
+#include "PolicyManager.h"
+
+/*************************************************
+ * TODO: currently only consider video stream,
+ *       will also consider still stream later.
+ *************************************************/
+
+namespace icamera {
+
+/**
+ * Encapsulation of all parameters needed by PSysExecutor to run PSYS pipeline.
+ */
+struct PSysTaskData {
+    IspSettings mIspSettings;
+    TuningMode mTuningMode;
+
+    CameraBufferPortMap mInputBuffers;
+    CameraBufferPortMap mOutputBuffers;
+    PSysTaskData() { mTuningMode = TUNING_MODE_MAX; }
+};
+
+// Used to save all on-processing tasks.
+struct TaskInfo {
+    TaskInfo() : mNumOfValidBuffers(0), mNumOfReturnedBuffers(0) {}
+    PSysTaskData mTaskData;
+    int mNumOfValidBuffers;
+    int mNumOfReturnedBuffers;
+};
+
+class PSysDagCallback {
+public:
+    PSysDagCallback() {}
+    virtual ~PSysDagCallback() {}
+    virtual void onFrameDone(const PSysTaskData& result) {}
+    virtual void onBufferDone(int64_t sequence, Port port,
+                              const std::shared_ptr<CameraBuffer> &camBuffer) {}
+};
+
+class PSysDAG {
+
+public:
+    PSysDAG(int cameraId, PSysDagCallback* psysDagCB);
+    virtual ~PSysDAG();
+    void setFrameInfo(const std::map<Port, stream_t>& inputInfo,
+                      const std::map<Port, stream_t>& outputInfo);
+    int configure(ConfigMode configMode, TuningMode tuningMode);
+    int start();
+    int stop();
+
+    int resume();
+    int pause();
+
+    int registerInternalBufs(std::map<Port, CameraBufVector> &internalBufs);
+    int registerUserOutputBufs(Port port, const std::shared_ptr<CameraBuffer> &camBuffer);
+
+    void addTask(PSysTaskData taskParam);
+    int getParameters(Parameters& param);
+
+    void registerListener(EventType eventType, EventListener* eventListener);
+    void removeListener(EventType eventType, EventListener* eventListener);
+
+    TuningMode getTuningMode(long sequence);
+    int prepareIpuParams(long sequence, bool forceUpdate = false, TaskInfo *task = nullptr);
+
+    /**
+     * Use to handle the frame done event from the executors.
+     */
+    int onFrameDone(Port port, const std::shared_ptr<CameraBuffer>& buffer);
+
+private:
+    DISALLOW_COPY_AND_ASSIGN(PSysDAG);
+
+    void tuningReconfig(TuningMode newTuningMode);
+
+    int createPipeExecutors();
+    int linkAndConfigExecutors();
+    int bindExternalPortsToExecutor();
+    void releasePipeExecutors();
+
+    PipeExecutor* findExecutorProducer(PipeExecutor* consumer);
+    status_t searchStreamIdsForOutputPort(PipeExecutor *executor, Port port);
+
+    int queueBuffers(const PSysTaskData& task);
+    int returnBuffers(PSysTaskData& result);
+
+    void dumpExternalPortMap();
+
+private:
+    int mCameraId;
+    PSysDagCallback* mPSysDagCB; //Used to callback notify frame done handling
+    PolicyManager* mPolicyManager;
+    ConfigMode mConfigMode; //It is actually real config mode.
+    TuningMode mTuningMode;
+    IspParamAdaptor* mIspParamAdaptor;
+
+    std::map<Port, stream_t> mInputFrameInfo;
+    std::map<Port, stream_t> mOutputFrameInfo;
+    Port mDefaultMainInputPort;
+
+    std::vector<PipeExecutor*> mExecutorsPool;
+    std::unordered_map<PipeExecutor*, int32_t> mExecutorStreamId;
+    std::map<Port, std::vector<int32_t> > mOutputPortToStreamIds;
+
+    // A lock for protecting task data from being accessed by different threads.
+    Mutex mTaskLock;
+    std::vector<TaskInfo> mOngoingTasks;
+
+    long mOngoingSequence[MAX_BUFFER_COUNT];
+    Mutex mSequenceLock;
+    bool mRunAicAfterQbuf;
+    long mLastTaskSequence;
+
+    /**
+     * The relationship mapping between DAG's port and executors port.
+     */
+    struct PortMapping {
+        PortMapping() : mExecutor(nullptr), mDagPort(INVALID_PORT), mExecutorPort(INVALID_PORT) {}
+        PipeExecutor* mExecutor;
+        Port mDagPort;
+        Port mExecutorPort;
+    };
+
+    std::vector<PortMapping> mInputMaps;
+    std::vector<PortMapping> mOutputMaps;
+};
+}
diff --git a/camera/hal/intel/ipu6/src/core/psysprocessor/PipeLiteExecutor.cpp b/camera/hal/intel/ipu6/src/core/psysprocessor/PipeLiteExecutor.cpp
new file mode 100644
index 000000000000..a4b7f3eb10de
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/psysprocessor/PipeLiteExecutor.cpp
@@ -0,0 +1,1127 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "PipeLiteExecutor"
+
+#include <algorithm>
+
+#include "PipeLiteExecutor.h"
+#include "PSysDAG.h"
+
+#include "FormatUtils.h"
+#include "iutils/CameraDump.h"
+#include "SyncManager.h"
+
+// CIPF backends
+extern "C" {
+#include <ia_cipf_css/ia_cipf_css.h>
+#include <ia_pal_types_isp_ids_autogen.h>
+}
+
+using std::vector;
+using std::string;
+using std::map;
+using std::shared_ptr;
+
+namespace icamera {
+
+static const int32_t sStatKernels[] = {
+    ia_pal_uuid_isp_bxt_awbstatistics,
+    ia_pal_uuid_isp_awbstatistics_2_0,
+    ia_pal_uuid_isp_bxt_dvsstatistics
+};
+
+static const int32_t sSisKernels[] = {
+    ia_pal_uuid_isp_sis_1_0_a
+};
+
+PipeLiteExecutor::PipeLiteExecutor(int cameraId, const ExecutorPolicy &policy,
+                                   vector<string> exclusivePGs, PSysDAG *psysDag,
+                                   shared_ptr<IGraphConfig> gc)
+      : mCameraId(cameraId),
+        mStreamId(-1),
+        mName(policy.exeName),
+        mPGNames(policy.pgList),
+        mOpModes(policy.opModeList),
+        mGraphConfig(gc),
+        mIsInputEdge(false),
+        mIsOutputEdge(false),
+        mNotifyPolicy(POLICY_FRAME_FIRST),
+        mAdaptor(nullptr),
+        mPolicyManager(nullptr),
+        mLastStatsSequence(-1),
+        mExclusivePGs(exclusivePGs),
+        mPSysDag(psysDag),
+        mkernelsCountWithStats(0)
+{
+}
+
+PipeLiteExecutor::~PipeLiteExecutor()
+{
+    while (!mPGExecutors.empty()) {
+        ExecutorUnit& unit = mPGExecutors.back();
+        if (unit.pg.get()) {
+            unit.pg->deInit();
+        }
+        mPGExecutors.pop_back();
+    }
+
+    releaseBuffers();
+}
+
+int PipeLiteExecutor::initPipe()
+{
+    LOG1("@%s:%s", __func__, getName());
+    CheckError(mGraphConfig == nullptr, BAD_VALUE, "%s, the graph config is NULL, BUG!", __func__);
+
+    NodesPtrVector programGroups;
+    vector<IGraphType::PipelineConnection> connVector;
+
+    int ret = mGraphConfig->pipelineGetConnections(mPGNames, &connVector);
+    CheckError(connVector.empty(), ret, "Failed to get connections for executor:%s", mName.c_str());
+
+    ret = createPGs();
+    CheckError(ret != OK, ret, "Failed to create PGs for executor: %s", ret, mName.c_str());
+
+    ret = analyzeConnections(connVector);
+    CheckError(ret != OK, ret, "Failed to analyze connections for executor: %s", ret, mName.c_str());
+
+    ret = configurePGs();
+    CheckError(ret != OK, ret, "Failed to configure connections for executor: %s", ret, mName.c_str());
+
+    assignDefaultPortsForTerminals();
+    return OK;
+}
+
+int PipeLiteExecutor::analyzeConnections(const vector<IGraphType::PipelineConnection>& connVector)
+{
+    ia_uid firstStageId = mPGExecutors.front().stageId;
+    ia_uid lastStageId = mPGExecutors.back().stageId;
+
+    for (auto const& connection : connVector) {
+        LOG2("%s: terminal %d (%d): %dx%d, 0x%x", getName(),
+             connection.portFormatSettings.terminalId, connection.portFormatSettings.enabled,
+             connection.portFormatSettings.width, connection.portFormatSettings.height,
+             connection.portFormatSettings.fourcc);
+        LOG2("%s:     connection source %d, %d, %d, has edge %d", getName(),
+             connection.connectionConfig.mSourceStage, connection.connectionConfig.mSourceTerminal,
+             connection.connectionConfig.mSourceIteration, connection.hasEdgePort);
+        LOG2("%s:     connection sink %d, %d, %d, type %d", getName(),
+             connection.connectionConfig.mSinkStage, connection.connectionConfig.mSinkTerminal,
+             connection.connectionConfig.mSinkIteration, connection.connectionConfig.mConnectionType);
+
+        storeTerminalInfo(connection);
+
+        if (connection.portFormatSettings.enabled == 0) {
+            // No actions are needed for the disabled connections.
+            continue;
+        }
+
+        // If the connection's sink stage is same as the first stage/pg id in this executor,
+        // then it means the connection belongs to input terminal pairs.
+        if (connection.connectionConfig.mSinkStage == firstStageId && connection.hasEdgePort) {
+            mIsInputEdge = true;
+        }
+
+        // If the connection's source stage is same as the last stage/pg id in this executor,
+        // then it means the connection belongs to output terminal pairs.
+        // SIS is output terminal but it doesn't belong to any stream, so it is not real edge output.
+        if (connection.connectionConfig.mSourceStage == lastStageId
+            && connection.hasEdgePort
+            && connection.connectionConfig.mSourceTerminal != connection.connectionConfig.mSinkTerminal) {
+            mIsOutputEdge = true;
+        }
+    }
+
+    return OK;
+}
+
+int PipeLiteExecutor::storeTerminalInfo(const IGraphType::PipelineConnection& connection)
+{
+    FrameInfo info;
+    info.mWidth = connection.portFormatSettings.width;
+    info.mHeight = connection.portFormatSettings.height;
+    info.mFormat = connection.portFormatSettings.fourcc;
+
+    ia_uid curTerminal    = connection.portFormatSettings.terminalId;
+    ia_uid sinkTerminal   = connection.connectionConfig.mSinkTerminal;
+    ia_uid sourceTerminal = connection.connectionConfig.mSourceTerminal;
+    ia_uid sinkStage      = connection.connectionConfig.mSinkStage;
+    ia_uid sourceStage    = connection.connectionConfig.mSourceStage;
+
+    TerminalDescriptor desc;
+    desc.terminal       = 0;
+    desc.stageId        = 0;
+    desc.sinkTerminal   = sinkTerminal;
+    desc.sourceTerminal = sourceTerminal;
+    desc.sinkStage      = sinkStage;
+    desc.sourceStage    = sourceStage;
+    desc.frameDesc      = info;
+    desc.enabled        = true;
+    desc.hasConnection  = true;
+    desc.assignedPort   = INVALID_PORT;
+    desc.usrStreamId   = connection.stream ? connection.stream->streamId() : -1;
+
+    if (connection.portFormatSettings.enabled) {
+        mConnectionMap[sinkTerminal]= sourceTerminal;
+    }
+
+    // Check if there is new input terminal
+    if (sinkStage && mTerminalsDesc.find(sinkTerminal) == mTerminalsDesc.end()) {
+        ExecutorUnit* unit = findPGExecutor(sinkStage);
+        if (unit) {
+            desc.terminal = sinkTerminal;
+            desc.stageId = sinkStage;
+            mTerminalsDesc[desc.terminal] = desc;
+            unit->inputTerminals.push_back(desc.terminal);
+        }
+    }
+    // Check if there is new output terminal
+    if (sourceStage && mTerminalsDesc.find(sourceTerminal) == mTerminalsDesc.end()) {
+        ExecutorUnit* unit = findPGExecutor(sourceStage);
+        if (unit) {
+            desc.terminal = sourceTerminal;
+            desc.stageId = sourceStage;
+            desc.hasConnection = (sinkTerminal != sourceTerminal);
+            mTerminalsDesc[desc.terminal] = desc;
+            unit->outputTerminals.push_back(desc.terminal);
+        }
+    }
+
+    if (mTerminalsDesc.find(curTerminal) != mTerminalsDesc.end()) {
+        mTerminalsDesc[curTerminal].enabled = connection.portFormatSettings.enabled;
+    }
+
+    return OK;
+}
+
+int PipeLiteExecutor::createPGs()
+{
+    for (auto const& pgName : mPGNames) {
+        int pgId = mGraphConfig->getPgIdByPgName(pgName);
+        CheckError(pgId == -1, BAD_VALUE, "Cannot get PG ID for %s", pgName.c_str());
+
+        ExecutorUnit pgUnit;
+        pgUnit.pgId = pgId;
+        pgUnit.stageId = psys_2600_pg_uid(pgId);
+        pgUnit.pg = std::shared_ptr<PGCommon>(new PGCommon(pgId, pgName, pgUnit.stageId + 1));
+        // Please refer to ia_cipf_css.h for terminalBaseUid
+        mPGExecutors.push_back(pgUnit);
+        int ret = pgUnit.pg->init();
+        CheckError(ret != OK, UNKNOWN_ERROR, "create PG %d error", pgId);
+    }
+    return OK;
+}
+
+int PipeLiteExecutor::configurePGs()
+{
+    mkernelsCountWithStats = 0;
+    for (auto &unit : mPGExecutors) {
+        map<ia_uid, FrameInfo> inputInfos;
+        map<ia_uid, FrameInfo> outputInfos;
+        vector<ia_uid> disabledTerminals;
+
+        getTerminalFrameInfos(unit.inputTerminals, inputInfos);
+        getTerminalFrameInfos(unit.outputTerminals, outputInfos);
+        getDisabledTerminalsForPG(unit.stageId, disabledTerminals);
+
+        unit.pg->setInputInfo(inputInfos);
+        unit.pg->setOutputInfo(outputInfos);
+        unit.pg->setDisabledTerminals(disabledTerminals);
+
+        IGraphType::StageAttr stageAttr;
+        if (mGraphConfig->getPgRbmValue(unit.pg->getName(), &stageAttr) == OK) {
+            LOG1("%s: Set rbm for pgId %d, pgName: %s bytes %d",
+                 __func__, unit.pgId, unit.pg->getName(), stageAttr.rbm_bytes);
+            unit.pg->setRoutingBitmap(stageAttr.rbm, stageAttr.rbm_bytes);
+        }
+        unit.pg->prepare(mAdaptor, mStreamId);
+
+        int statsCount = getStatKernels(unit.pgId, unit.statKernelUids);
+        mkernelsCountWithStats += statsCount;
+
+        statsCount = getSisKernels(unit.pgId, unit.sisKernelUids);
+        mkernelsCountWithStats += statsCount;
+    }
+
+    return OK;
+}
+
+/**
+ * Assign ports for terminals as internal default value
+ * Input ports may be overwritten with output ports of producer in setInputTerminals()
+ */
+int PipeLiteExecutor::assignDefaultPortsForTerminals()
+{
+    Port portTable[] = {MAIN_PORT, SECOND_PORT, THIRD_PORT, FORTH_PORT, INVALID_PORT};
+    for (auto &unit : mPGExecutors) {
+        int outPortIndex = 0;
+        for (auto terminal : unit.outputTerminals) {
+            TerminalDescriptor& termDesc = mTerminalsDesc[terminal];
+            if (termDesc.enabled && termDesc.hasConnection) {
+                CheckError(portTable[outPortIndex] == INVALID_PORT, BAD_VALUE,
+                    "Port unavailable for output term %d:%d", unit.pgId, terminal);
+                termDesc.assignedPort = portTable[outPortIndex];
+                outPortIndex++;
+            }
+        }
+
+        int inPortIndex = 0;
+        for (auto terminal : unit.inputTerminals) {
+            TerminalDescriptor& termDesc = mTerminalsDesc[terminal];
+            if (termDesc.enabled && termDesc.hasConnection) {
+                CheckError(portTable[inPortIndex] == INVALID_PORT, BAD_VALUE,
+                    "Port unavailable for input term %d", terminal);
+                termDesc.assignedPort = portTable[inPortIndex];
+                inPortIndex++;
+            }
+        }
+    }
+
+    return OK;
+}
+
+void PipeLiteExecutor::getOutputTerminalPorts(std::map<ia_uid, Port>& terminals) const
+{
+    getTerminalPorts(mPGExecutors.back().outputTerminals, terminals);
+}
+
+void PipeLiteExecutor::getInputTerminalPorts(std::map<ia_uid, Port>& terminals) const
+{
+    getTerminalPorts(mPGExecutors.front().inputTerminals, terminals);
+}
+
+int PipeLiteExecutor::setInputTerminals(const std::map<ia_uid, Port>& sourceTerminals)
+{
+    // In edge PGs accepts input ports arrangement from external
+    ExecutorUnit& inUnit = mPGExecutors.front();
+    for (auto sinkTerminal : inUnit.inputTerminals) {
+        if (mConnectionMap.find(sinkTerminal) == mConnectionMap.end()) {
+            continue;
+        }
+
+        ia_uid sourceTerminal = mConnectionMap[sinkTerminal];
+        if (sourceTerminals.find(sourceTerminal) != sourceTerminals.end()) {
+            mTerminalsDesc[sinkTerminal].assignedPort = sourceTerminals.at(sourceTerminal);
+            LOG2("pg %s get external %d -> input %d, port %d", getName(),
+                 sourceTerminal, sinkTerminal, mTerminalsDesc[sinkTerminal].assignedPort);
+        }
+    }
+
+    // Link internal PGs (sink PG accepts input ports arrangement from source PG (output ports)
+    // source PG(output ports) -> (input ports)sink PG
+    for (unsigned int i = 1; i < mPGExecutors.size(); i++) {
+        for (auto sinkTerminal : mPGExecutors[i].inputTerminals) {
+            if (!mTerminalsDesc[sinkTerminal].enabled) {
+                continue;
+            }
+            if (mConnectionMap.find(sinkTerminal) != mConnectionMap.end()) {
+                ia_uid sourceTerminal = mConnectionMap[sinkTerminal];
+                mTerminalsDesc[sinkTerminal].assignedPort = mTerminalsDesc[sourceTerminal].assignedPort;
+            }
+        }
+    }
+
+    // Set frame info to BufferQueue
+    map<Port, stream_t> inputInfo;
+    map<Port, stream_t> outputInfo;
+    ExecutorUnit& outUnit = mPGExecutors.back();
+    for (auto terminal : inUnit.inputTerminals) {
+        if (mTerminalsDesc[terminal].assignedPort == INVALID_PORT) {
+            continue;
+        }
+
+        stream_t inputConfig;
+        CLEAR(inputConfig);
+        inputConfig.width = mTerminalsDesc[terminal].frameDesc.mWidth;
+        inputConfig.height = mTerminalsDesc[terminal].frameDesc.mHeight;
+        inputConfig.format = mTerminalsDesc[terminal].frameDesc.mFormat;
+        inputConfig.id = mTerminalsDesc[terminal].usrStreamId;
+        inputInfo[mTerminalsDesc[terminal].assignedPort] = inputConfig;
+    }
+    for (auto terminal : outUnit.outputTerminals) {
+        if (mTerminalsDesc[terminal].assignedPort == INVALID_PORT) {
+            continue;
+        }
+
+        stream_t outputConfig;
+        CLEAR(outputConfig);
+        outputConfig.width = mTerminalsDesc[terminal].frameDesc.mWidth;
+        outputConfig.height = mTerminalsDesc[terminal].frameDesc.mHeight;
+        outputConfig.format = mTerminalsDesc[terminal].frameDesc.mFormat;
+        outputConfig.id = mTerminalsDesc[terminal].usrStreamId;
+        outputInfo[mTerminalsDesc[terminal].assignedPort] = outputConfig;
+    }
+    BufferQueue::setFrameInfo(inputInfo, outputInfo);
+
+    return OK;
+}
+
+int PipeLiteExecutor::start()
+{
+    LOG1("%s executor:%s", __func__, mName.c_str());
+    mProcessThread = new ProcessThread(this);
+    AutoMutex   l(mBufferQueueLock);
+
+    allocBuffers();
+    dumpPGs();
+
+    mLastStatsSequence = -1;
+
+    mThreadRunning = true;
+    mProcessThread->run(mName.c_str(), PRIORITY_NORMAL);
+
+    return OK;
+}
+
+void PipeLiteExecutor::stop()
+{
+    LOG1("%s executor:%s", __func__, mName.c_str());
+
+    mProcessThread->requestExitAndWait();
+
+    // Thread is not running. It is safe to clear the Queue
+    clearBufferQueues();
+    delete mProcessThread;
+}
+
+void PipeLiteExecutor::notifyStop()
+{
+    LOG1("%s executor:%s", __func__, mName.c_str());
+
+    mProcessThread->requestExit();
+    {
+        AutoMutex l(mBufferQueueLock);
+        mThreadRunning = false;
+        // Wakeup the thread to exit
+        mFrameAvailableSignal.signal();
+        mOutputAvailableSignal.signal();
+    }
+}
+
+int PipeLiteExecutor::releaseStatsBuffer(const shared_ptr<CameraBuffer> &statsBuf)
+{
+    LOG3A("%s executor:%s", __func__, mName.c_str());
+    AutoMutex lock(mStatsBuffersLock);
+
+    mStatsBuffers.push(statsBuf);
+
+    return OK;
+}
+
+bool PipeLiteExecutor::hasOutputTerminal(ia_uid sinkTerminal)
+{
+    if (mConnectionMap.find(sinkTerminal) == mConnectionMap.end()) {
+        return false;
+    }
+
+    ExecutorUnit& unit = mPGExecutors.back();
+    for (auto sourceTerminal : unit.outputTerminals) {
+        if (mConnectionMap[sinkTerminal] == sourceTerminal) {
+            return true;
+        }
+    }
+    return false;
+}
+
+int PipeLiteExecutor::getStatKernels(int pgId, vector<ia_uid>& kernels)
+{
+    kernels.clear();
+    for (unsigned int i = 0; i < ARRAY_SIZE(sStatKernels); i++) {
+        int pgIdOfKernel = -1;
+        int status = mGraphConfig->getPgIdForKernel(mStreamId, sStatKernels[i], &pgIdOfKernel);
+        if (status == OK && pgIdOfKernel == pgId) {
+             kernels.push_back(sStatKernels[i]);
+        }
+    }
+
+    LOG1("pg %d has %d stat kernels", pgId, kernels.size());
+    return kernels.size();
+}
+
+int PipeLiteExecutor::getSisKernels(int pgId, vector<ia_uid>& kernels)
+{
+    kernels.clear();
+    for (unsigned int i = 0; i < ARRAY_SIZE(sSisKernels); i++) {
+        int pgIdOfKernel = -1;
+        int status = mGraphConfig->getPgIdForKernel(mStreamId, sSisKernels[i], &pgIdOfKernel);
+        if (status == OK && pgIdOfKernel == pgId) {
+             kernels.push_back(sSisKernels[i]);
+        }
+    }
+
+    LOG1("pg %d has %d sis kernels", pgId, kernels.size());
+    return kernels.size();
+}
+
+bool PipeLiteExecutor::isSameStreamConfig(const stream_t& internal, const stream_t& external,
+                                          ConfigMode configMode, bool checkStreamId) const
+{
+    // The internal format is ia_fourcc based format, so need to convert it to V4L2 format.
+    int internalFormat = graphconfig::utils::getV4L2Format(internal.format);
+    int internalStride = CameraUtils::getStride(internalFormat, internal.width);
+    int externalStride = CameraUtils::getStride(external.format, external.width);
+
+    LOG1("%s: %s, id:%d, internal: %s(%dx%d: %d)(id %d), external: %s(%dx%d: %d) (id %d) usage:%d",
+          __func__, mName.c_str(), mStreamId,
+          CameraUtils::format2string(internalFormat).c_str(),
+          internal.width, internal.height, internalStride, internal.id,
+          CameraUtils::format2string(external.format).c_str(),
+          external.width, external.height, externalStride, external.id, external.usage);
+
+    if (checkStreamId && internal.id >= 0) {
+        return internal.id == external.id;
+    }
+
+    /*
+     * WA: PG accept GRBG format but actual input data is of RGGB format,
+     *     PG use its kernel to crop to GRBG
+     */
+    if ((internalFormat == V4L2_PIX_FMT_SGRBG10 || internalFormat == V4L2_PIX_FMT_SGRBG12)
+         && (external.format == V4L2_PIX_FMT_SRGGB10 || external.format == V4L2_PIX_FMT_SRGGB12)) {
+         return true;
+    }
+
+    bool sameHeight = internal.height == external.height ||
+                      internal.height == ALIGN_32(external.height);
+    if (internalFormat == external.format && sameHeight &&
+        (internal.width == external.width || internalStride == externalStride)) {
+        return true;
+    }
+
+    return false;
+}
+
+/**
+ * Check if there is any valid buffer(not null) in the given port/buffer pairs.
+ *
+ * return true if there is at least one not null buffer.
+ */
+bool PipeLiteExecutor::hasValidBuffers(const CameraBufferPortMap& buffers)
+{
+    for (const auto& item : buffers) {
+        if (item.second) return true;
+    }
+
+    return false;
+}
+
+int PipeLiteExecutor::processNewFrame()
+{
+    PERF_CAMERA_ATRACE();
+
+    int ret = OK;
+    CameraBufferPortMap inBuffers, outBuffers;
+    // Wait frame buffers.
+    {
+        ConditionLock lock(mBufferQueueLock);
+        ret = waitFreeBuffersInQueue(lock, inBuffers, outBuffers);
+        // Already stopped
+        if (!mThreadRunning) return -1;
+
+        if (ret != OK) return OK; // Wait frame buffer error should not involve thread exit.
+
+        CheckError(inBuffers.empty() || outBuffers.empty(),
+              UNKNOWN_ERROR, "Failed to get input or output buffers.");
+
+        for (auto& output: mOutputQueue) {
+            output.second.pop();
+        }
+
+        for (auto& input: mInputQueue) {
+            input.second.pop();
+        }
+    }
+
+    // Check if the executor needs to run the actual pipeline.
+    // It only needs to run when there is at least one valid output buffer.
+    if (!hasValidBuffers(outBuffers)) {
+        // Return buffers if the executor is NOT an input edge.
+        if (!mIsInputEdge) {
+            for (const auto& item : inBuffers) {
+                mBufferProducer->qbuf(item.first, item.second);
+            }
+        }
+        return OK;
+    }
+
+    // Fill real buffer to run pipe
+    for (auto &item : outBuffers) {
+        if (item.second.get() == nullptr) {
+            item.second = mInternalOutputBuffers[item.first];
+        }
+    }
+
+    vector<shared_ptr<CameraBuffer>> outStatsBuffers;
+    vector<EventType> eventType;
+    // Should find first not none input buffer instead of always use the first one.
+    shared_ptr<CameraBuffer> inBuf = inBuffers.begin()->second;
+    CheckError(!inBuf, UNKNOWN_ERROR, "@%s: no valid input buffer", __func__);
+    long inBufSequence = inBuf->getSequence();
+    v4l2_buffer_t inV4l2Buf = *inBuf->getV4L2Buffer().Get();
+    TuningMode tuningMode = mPSysDag->getTuningMode(inBufSequence);
+
+    // Enable RAW DUMP to get the MakerNote from jpeg
+    if (CameraDump::isDumpTypeEnable(DUMP_JPEG_BUFFER)) {
+        if (mName.find("still") != std::string::npos) {
+            CameraDump::dumpImage(mCameraId, inBuffers[MAIN_PORT], M_PSYS, MAIN_PORT);
+        }
+    }
+
+    LOG2("%s:Id:%d run pipe start for buffer:%ld", mName.c_str(), mCameraId, inBufSequence);
+
+    if (PlatformData::isEnableFrameSyncCheck(mCameraId)) {
+        shared_ptr<CameraBuffer> cInBuffer = inBuffers[MAIN_PORT];
+        int vc = cInBuffer->getVirtualChannel();
+
+        while ((!SyncManager::getInstance()->vcSynced(vc)) && mThreadRunning)
+            usleep(1);
+
+        if (gLogLevel & CAMERA_DEBUG_LOG_VC_SYNC) {
+            int seq = cInBuffer->getSequence();
+            SyncManager::getInstance()->printVcSyncCount();
+            LOGVCSYNC("[start runPipe], CPU-timestamp:%lu, sequence:%d, vc:%d, kernel-timestamp:%.3lfms, endl",
+                      CameraUtils::systemTime(),
+                      seq,
+                      cInBuffer->getVirtualChannel(),
+                      cInBuffer->getTimestamp().tv_sec*1000.0 + cInBuffer->getTimestamp().tv_usec/1000.0);
+        }
+
+        SyncManager::getInstance()->updateVcSyncCount(vc);
+
+        // Run pipe with buffers
+        ret = runPipe(inBuffers, outBuffers, outStatsBuffers, eventType);
+        LOGVCSYNC("[done runPipe], CPU-timestamp:%lu, sequence:%ld, vc:%d, kernel-timestamp:%.3lfms, endl",
+                  CameraUtils::systemTime(),
+                  cInBuffer->getSequence(),
+                  cInBuffer->getVirtualChannel(),
+                  cInBuffer->getTimestamp().tv_sec*1000.0 + cInBuffer->getTimestamp().tv_usec/1000.0);
+    } else {
+        // Run pipe with buffers
+        ret = runPipe(inBuffers, outBuffers, outStatsBuffers, eventType);
+    }
+    CheckError((ret != OK), UNKNOWN_ERROR, "@%s: failed to run pipe", __func__);
+    LOG2("%s:Id:%d run pipe end for buffer:%ld", mName.c_str(), mCameraId, inBufSequence);
+
+    // Remove internal output buffers
+    for (auto &item : outBuffers) {
+        if (item.second.get() == mInternalOutputBuffers[item.first].get()) {
+            item.second = nullptr;
+        }
+    }
+
+    if (mNotifyPolicy == POLICY_FRAME_FIRST) {
+        // For general case, notify frame prior to stats to make sure its consumers can get
+        // the frame buffers as early as possible.
+        notifyFrameDone(inV4l2Buf, outBuffers);
+        notifyStatsDone(tuningMode, inV4l2Buf, outStatsBuffers, eventType);
+    } else if (mNotifyPolicy == POLICY_STATS_FIRST) {
+        // Notify stats first and then handle frame buffers to make sure the next executor
+        // can get this executor's IQ result.
+        notifyStatsDone(tuningMode, inV4l2Buf, outStatsBuffers, eventType);
+
+        // After the stats notified, we need to update the IPU parameters as well to get the
+        // latest AIQ result.
+        mPSysDag->prepareIpuParams(inBufSequence, true);
+
+        notifyFrameDone(inV4l2Buf, outBuffers);
+    } else {
+        LOGW("Invalid notify policy:%d, should never happen.", mNotifyPolicy);
+    }
+
+    // Return buffers for the executor which is NOT an input edge
+    if (!mIsInputEdge) {
+        for (auto const& portBufferPair : inBuffers) {
+            // Queue buffer to producer
+            mBufferProducer->qbuf(portBufferPair.first, portBufferPair.second);
+        }
+    }
+
+    return OK;
+}
+
+int PipeLiteExecutor::registerInBuffers(Port port, const shared_ptr<CameraBuffer> &inBuf)
+{
+    return OK;
+}
+
+int PipeLiteExecutor::registerOutBuffers(Port port, const shared_ptr<CameraBuffer> &camBuffer)
+{
+    return OK;
+}
+
+int PipeLiteExecutor::runPipe(map<Port, shared_ptr<CameraBuffer> > &inBuffers,
+                              map<Port, shared_ptr<CameraBuffer> > &outBuffers,
+                              vector<shared_ptr<CameraBuffer> > &outStatsBuffers,
+                              vector<EventType> &eventType)
+{
+    PERF_CAMERA_ATRACE();
+
+    CheckError((inBuffers.empty() || outBuffers.empty()), BAD_VALUE,
+        "Error in pipe iteration input/output bufs");
+
+    int ret = OK;
+    if (mPolicyManager) {
+        // Check if need to wait other executors.
+        ret = mPolicyManager->wait(mName);
+    }
+
+    // Accept external buffers for in/out edge PGs
+    getTerminalBuffersFromExternal(mPGExecutors.front().inputTerminals, inBuffers,
+                                   mPGExecutors.front().inputBuffers);
+    getTerminalBuffersFromExternal(mPGExecutors.back().outputTerminals, outBuffers,
+                                   mPGExecutors.back().outputBuffers);
+
+    // Get ISP parameters
+    const ia_binary_data *ipuParameters = nullptr;
+    long sequence = inBuffers.begin()->second ? inBuffers.begin()->second->getSequence() : -1;
+    TRACE_LOG_PROCESS(mName.c_str(), "runPipe", MAKE_COLOR(sequence), sequence);
+    if (mAdaptor) {
+        ipuParameters = mAdaptor->getIpuParameter(sequence, mStreamId);
+        if (!ipuParameters) {
+            LOG1("%s: executor %s doesn't run for sequence %ld due to no pal",
+                 __func__, mName.c_str(), sequence);
+            return OK;
+        }
+    }
+
+    LOG2("%s: Executor %s run with input: %zu, output: %zu, sequence: %ld",
+         __func__, mName.c_str(), inBuffers.size(), outBuffers.size(), sequence);
+
+    outStatsBuffers.clear();
+    eventType.clear();
+    int statTotalNum = 0;
+    for (unsigned int pgIndex = 0; pgIndex < mPGExecutors.size(); pgIndex++) {
+        ExecutorUnit& unit = mPGExecutors[pgIndex];
+
+        // Prepare stats buffers for 3A/sis
+        vector<ia_binary_data*> pgStatsDatas;
+        // For 3A stats
+        unsigned int statsCount = unit.statKernelUids.size();
+        for (unsigned int counter = 0; counter < statsCount; counter++) {
+            if (mStatsBuffers.empty()) {
+                LOGW("No available stats buffer.");
+                break;
+            }
+            outStatsBuffers.push_back(mStatsBuffers.front());
+            eventType.push_back(EVENT_PSYS_STATS_BUF_READY);
+            ia_binary_data* buffer = (ia_binary_data*)mStatsBuffers.front()->getBufferAddr();
+            CheckError(buffer == nullptr, BAD_VALUE, "buffer is null pointer.");
+            buffer->size = 0; // Clear it, then the stats memory is from p2p
+            buffer->data = 0;
+            pgStatsDatas.push_back(buffer);
+            mStatsBuffers.pop();
+        }
+        unsigned int sisCount = unit.sisKernelUids.size();
+        for (unsigned int counter = 0; counter < sisCount; counter++) {
+            if (mStatsBuffers.empty()) {
+                LOGW("No available stats buffer.");
+                break;
+            }
+            outStatsBuffers.push_back(mStatsBuffers.front());
+            eventType.push_back(EVENT_PSYS_STATS_SIS_BUF_READY);
+            ia_binary_data* buffer = (ia_binary_data*)mStatsBuffers.front()->getBufferAddr();
+            pgStatsDatas.push_back(buffer);
+            mStatsBuffers.pop();
+        }
+
+        // Run PGs
+        ret = unit.pg->iterate(unit.inputBuffers,
+                               unit.outputBuffers,
+                               (statsCount > 0) ? pgStatsDatas[0] : nullptr, // Currently PG handles one stats buffer only
+                               ipuParameters);
+        CheckError((ret != OK), ret, "%s: error in pipe iteration with %d", mName.c_str(), ret);
+
+        statTotalNum += statsCount;
+        if (sisCount > 0) {
+            handleSisStats(unit.outputBuffers, outStatsBuffers[statTotalNum]); // Currently handle one sis output only
+        }
+        statTotalNum += sisCount;
+    }
+
+    return OK;
+}
+
+int PipeLiteExecutor::handleSisStats(map<ia_uid, shared_ptr<CameraBuffer>>& frameBuffers, const shared_ptr<CameraBuffer> &outStatsBuffers)
+{
+    LOG2("%s:", __func__);
+    ia_binary_data* statBuf = (ia_binary_data*)outStatsBuffers->getBufferAddr();
+    CheckError((statBuf == nullptr), BAD_VALUE, "Error getting buffer for sis a stats");
+    statBuf->data = nullptr;
+    statBuf->size = 0;
+
+    for (auto iterm : frameBuffers) {
+        ia_uid uid = iterm.first;
+        if (uid == psys_ipu6_isa_lb_output_sis_a_uid) {
+            statBuf->data = iterm.second->getBufferAddr();
+            statBuf->size = iterm.second->getBufferSize();
+            outStatsBuffers->setUserBufferInfo(-1, iterm.second->getWidth(), iterm.second->getHeight());
+            return OK;
+        }
+    }
+
+    return UNKNOWN_ERROR;
+}
+
+int PipeLiteExecutor::notifyFrameDone(const v4l2_buffer_t& inV4l2Buf, const CameraBufferPortMap& outBuf)
+{
+    PERF_CAMERA_ATRACE();
+    for (auto const& portBufferPair : outBuf) {
+        shared_ptr<CameraBuffer> outBuf = portBufferPair.second;
+        Port port = portBufferPair.first;
+        // If the output buffer is nullptr, that means user doesn't request that buffer,
+        // so it doesn't need to be handled here.
+        if (!outBuf) continue;
+
+        outBuf->updateV4l2Buffer(inV4l2Buf);
+
+        // If it's output edge, the buffer should be returned to PSysDag,
+        // otherwise they should be returned to its consumer.
+        if (mIsOutputEdge) {
+            mPSysDag->onFrameDone(port, outBuf);
+        } else {
+            for (auto &it : mBufferConsumerList) {
+                it->onFrameAvailable(port, outBuf);
+            }
+        }
+    }
+
+    return OK;
+}
+
+int PipeLiteExecutor::notifyStatsDone(TuningMode tuningMode,
+                                      const v4l2_buffer_t& inV4l2Buf,
+                                      const vector<shared_ptr<CameraBuffer>> &outStatsBuffers,
+                                      const vector<EventType> &eventType)
+{
+    PERF_CAMERA_ATRACE();
+
+    // The executor does not produce stats, so no need to notify.
+    if (outStatsBuffers.empty()) return OK;
+
+    /**
+     * Notice for EVENT_PSYS_STATS_BUF_READY:
+     * dvs stat & 3a stat come from different PG, and they are decoded separately
+     * in decodeStatsData().
+     * Fortunately stats data are stored in aiqResultStorage separately,
+     * and user will get them from storage instead of EventData.
+     * So here we can send one event after all stat buffers are decoded/stored/released.
+     */
+    int psysStatBufferCount = 0;
+    for (auto type : eventType) {
+        if (type == EVENT_PSYS_STATS_BUF_READY) {
+            psysStatBufferCount++;
+        }
+    }
+
+    int statsIndex = 0;
+    for (auto statsBuf : outStatsBuffers) {
+        if (!statsBuf) continue;
+
+        if (mStreamId == STILL_STREAM_ID) {
+            LOG2("%s: No statistics data for still pipe in buffer", __func__);
+            releaseStatsBuffer(statsBuf);
+            continue;
+        } else if (inV4l2Buf.sequence <= mLastStatsSequence) {
+            // Ignore old statistics for Raw reprocessing
+            LOG2("%s: new sequence %d is less than last sequence %ld", __func__,
+                 inV4l2Buf.sequence, mLastStatsSequence);
+            releaseStatsBuffer(statsBuf);
+            continue;
+        }
+
+        ia_binary_data *hwStatsData = (ia_binary_data *)(statsBuf->getBufferAddr());
+        if (hwStatsData->data == nullptr || hwStatsData->size == 0) {
+            LOGW("%s: No statistics data in buffer", __func__);
+            releaseStatsBuffer(statsBuf);
+            continue;
+        }
+
+        statsBuf->updateV4l2Buffer(inV4l2Buf);
+
+        // Decode the statistics data
+        if (eventType[statsIndex] == EVENT_PSYS_STATS_BUF_READY) {
+            mAdaptor->decodeStatsData(tuningMode, statsBuf, mGraphConfig);
+            psysStatBufferCount--;
+        }
+
+        // Notify listeners after all buffers done for type STATS_BUF_READY
+        // Notify immediately for other types
+        if (eventType[statsIndex] != EVENT_PSYS_STATS_BUF_READY
+            || !psysStatBufferCount) {
+            EventDataStatsReady statsReadyData;
+            statsReadyData.sequence = statsBuf->getSequence();
+            statsReadyData.timestamp.tv_sec = statsBuf->getTimestamp().tv_sec;
+            statsReadyData.timestamp.tv_usec = statsBuf->getTimestamp().tv_usec;
+            EventData eventData;
+            eventData.type = eventType[statsIndex];
+            eventData.buffer = statsBuf;
+            eventData.data.statsReady = statsReadyData;
+            notifyListeners(eventData);
+        }
+
+        releaseStatsBuffer(statsBuf);
+        statsIndex++;
+    }
+
+    if (mStreamId == VIDEO_STREAM_ID && inV4l2Buf.sequence > mLastStatsSequence) {
+        mLastStatsSequence = inV4l2Buf.sequence;
+    }
+
+    return OK;
+}
+
+int PipeLiteExecutor::allocBuffers()
+{
+    LOG1("%s executor:%s", __func__, mName.c_str());
+
+    releaseBuffers();
+
+    // Allocate buffer between PGs (internal)
+    for (auto const& item : mTerminalsDesc) {
+        const TerminalDescriptor& termDesc = item.second;
+        if (!termDesc.enabled) {
+            continue;
+        }
+
+        if (termDesc.assignedPort != INVALID_PORT
+            && !(findPGExecutor(termDesc.sinkStage) && findPGExecutor(termDesc.sourceStage))) {
+            // Don't allocate buffer here for external connection (has valid port)
+            continue;
+        }
+
+        // Allocated already
+        if (mPGBuffers.find(termDesc.terminal) != mPGBuffers.end()) {
+            continue;
+        }
+
+        int srcFmt = termDesc.frameDesc.mFormat;
+        int srcWidth = termDesc.frameDesc.mWidth;
+        int srcHeight = termDesc.frameDesc.mHeight;
+        int size = PGCommon::getFrameSize(srcFmt, srcWidth, srcHeight, true);
+        shared_ptr<CameraBuffer> buf = CameraBuffer::create(mCameraId,
+                     BUFFER_USAGE_PSYS_INPUT, V4L2_MEMORY_USERPTR, size, 0, srcFmt, srcWidth, srcHeight);
+        CheckError(!buf, NO_MEMORY, "@%s: Allocate producer buffer failed", __func__);
+        mPGBuffers[termDesc.sinkTerminal] = buf;
+        mPGBuffers[termDesc.sourceTerminal] = buf;
+    }
+
+    for (auto &unit : mPGExecutors) {
+        // Assign internal buffers for terminals of PGs according to connection
+        for (auto &terminal : unit.inputTerminals) {
+            if (mPGBuffers.find(terminal) != mPGBuffers.end()) {
+                unit.inputBuffers[terminal] = mPGBuffers[terminal];
+            }
+        }
+        for (auto &terminal : unit.outputTerminals) {
+            if (mPGBuffers.find(terminal) != mPGBuffers.end()) {
+                unit.outputBuffers[terminal] = mPGBuffers[terminal];
+            }
+        }
+
+        // Allocate stats buffers if needed.
+        unsigned int statsBufferCount = unit.statKernelUids.size();
+        if (!statsBufferCount) {
+            continue;
+        }
+        for (unsigned int i = 0; i < MAX_BUFFER_COUNT * statsBufferCount; i++) {
+            shared_ptr<CameraBuffer> statsBuf = CameraBuffer::create(mCameraId,
+                         BUFFER_USAGE_PSYS_STATS, V4L2_MEMORY_USERPTR, sizeof(ia_binary_data), i);
+            CheckError(!statsBuf, NO_MEMORY, "Executor %s: Allocate stats buffer failed", mName.c_str());
+
+            AutoMutex lock(mStatsBuffersLock);
+            mStatsBuffers.push(statsBuf);
+        }
+    }
+
+    // Allocate buffers for producer executor (external)
+    // Ignore input edge due to no producer
+    if (!mIsInputEdge) {
+        for (auto const& terminal : mPGExecutors.front().inputTerminals) {
+            Port inputPort = mTerminalsDesc[terminal].assignedPort;
+
+            int srcFmt = mTerminalsDesc[terminal].frameDesc.mFormat;
+            int srcWidth = mTerminalsDesc[terminal].frameDesc.mWidth;
+            int srcHeight = mTerminalsDesc[terminal].frameDesc.mHeight;
+            // Get frame size with aligned height taking in count for internal buffers.
+            // To garantee PSYS kernel like GDC always get enough buffer size to process.
+            int size = PGCommon::getFrameSize(srcFmt, srcWidth, srcHeight, true);
+            for (int i = 0; i < MAX_BUFFER_COUNT; i++) {
+                // Prepare internal frame buffer for its producer.
+                shared_ptr<CameraBuffer> buf = CameraBuffer::create(mCameraId,
+                             BUFFER_USAGE_PSYS_INPUT, V4L2_MEMORY_USERPTR, size, i, srcFmt, srcWidth, srcHeight);
+                CheckError(!buf, NO_MEMORY, "@%s: Allocate producer buffer failed", __func__);
+                mInternalBuffers[inputPort].push_back(buf);
+
+                mBufferProducer->qbuf(inputPort, buf);
+            }
+        }
+    }
+
+    // Allocate internal output buffers to support pipe execution without user output buffer
+    for (auto const &item : mOutputFrameInfo) {
+        int fmt = item.second.format;
+        int width = item.second.width;
+        int height = item.second.height;
+        int size = CameraUtils::getFrameSize(fmt, width, height, true);
+        shared_ptr<CameraBuffer> buf = CameraBuffer::create(mCameraId,
+                     BUFFER_USAGE_PSYS_INPUT, V4L2_MEMORY_USERPTR, size, 0, fmt, width, height);
+        CheckError(!buf, NO_MEMORY, "@%s: Allocate internal output buffer failed", __func__);
+        mInternalOutputBuffers[item.first]= buf;
+    }
+
+    return OK;
+}
+
+void PipeLiteExecutor::releaseBuffers()
+{
+    LOG1("%s executor:%s", __func__, mName.c_str());
+
+    // Release internel frame buffers
+    mInternalOutputBuffers.clear();
+    mInternalBuffers.clear();
+    mPGBuffers.clear();
+
+    // Release stats buffers
+    {
+        AutoMutex lock(mStatsBuffersLock);
+        while (!mStatsBuffers.empty()) mStatsBuffers.pop();
+    }
+}
+
+PipeLiteExecutor::ExecutorUnit* PipeLiteExecutor::findPGExecutor(ia_uid stageId)
+{
+    for (unsigned int i = 0; i < mPGExecutors.size(); i++) {
+        if (mPGExecutors[i].stageId == stageId) {
+            return &mPGExecutors[i];
+        }
+    }
+    return nullptr;
+}
+
+void PipeLiteExecutor::getTerminalPorts(const vector<ia_uid>& terminals,
+                                        map<ia_uid, Port>& terminalPortMap) const
+{
+    terminalPortMap.clear();
+    for (auto terminal : terminals) {
+        const TerminalDescriptor& termDesc = mTerminalsDesc.at(terminal);
+        if (termDesc.enabled && termDesc.assignedPort != INVALID_PORT) {
+            terminalPortMap[terminal] = termDesc.assignedPort;
+        }
+    }
+}
+
+void PipeLiteExecutor::getTerminalFrameInfos(const vector<ia_uid>& terminals,
+                                             map<ia_uid, FrameInfo>& infoMap) const
+{
+    infoMap.clear();
+    for (auto terminal : terminals) {
+        const TerminalDescriptor& termDesc = mTerminalsDesc.at(terminal);
+        if (termDesc.enabled) {
+            infoMap[terminal] = termDesc.frameDesc;
+        }
+    }
+}
+
+void PipeLiteExecutor::getDisabledTerminalsForPG(ia_uid stageId, vector<ia_uid>& terminals) const
+{
+    terminals.clear();
+    for (auto const item : mTerminalsDesc) {
+        const TerminalDescriptor& termDesc = item.second;
+        if (termDesc.stageId == stageId && !termDesc.enabled) {
+            terminals.push_back(termDesc.terminal);
+        }
+    }
+}
+
+void PipeLiteExecutor::getTerminalBuffersFromExternal(
+        const vector<ia_uid>& terminals,
+        const map<Port, shared_ptr<CameraBuffer> >& externals,
+        map<ia_uid, shared_ptr<CameraBuffer> >& internals) const
+{
+    for (auto &terminal : terminals) {
+        Port port = mTerminalsDesc.at(terminal).assignedPort;
+        if (externals.find(port) != externals.end()) {
+            internals[terminal] = externals.at(port);
+        }
+    }
+}
+
+void PipeLiteExecutor::dumpPGs() const
+{
+    if (!Log::isDebugLevelEnable(CAMERA_DEBUG_LOG_LEVEL2)) return;
+
+    LOG2("============= dump PGs for executor %s =================", getName());
+    if (mIsInputEdge) {
+        LOG2("This is input edge");
+    }
+    if (mIsOutputEdge) {
+        LOG2("This is output edge");
+    }
+    for (auto const &unit : mPGExecutors) {
+        ia_uid stageId = unit.stageId;
+        LOG2("    PG: %d: %s, stageId %d",
+             unit.pgId, unit.pg ? unit.pg->getName() : "GPU-TNR", stageId);
+
+        LOG2("        InTerms: %zu", unit.inputTerminals.size());
+        for (auto const &term : unit.inputTerminals) {
+            shared_ptr<CameraBuffer> buffer= nullptr;
+            if (mPGBuffers.find(term) != mPGBuffers.end()) {
+                buffer = mPGBuffers.at(term);
+            }
+
+            const TerminalDescriptor& termDesc = mTerminalsDesc.at(term);
+            if (termDesc.enabled) {
+                LOG2("            %d: %dx%d, 0x%x, port %d, buf %p",
+                     termDesc.terminal - termDesc.stageId - 1,
+                     termDesc.frameDesc.mWidth, termDesc.frameDesc.mHeight,
+                     termDesc.frameDesc.mFormat,
+                     termDesc.assignedPort, buffer.get());
+            } else {
+                LOG2("            %d: %dx%d, 0x%x, disabled",
+                     termDesc.terminal - termDesc.stageId - 1,
+                     termDesc.frameDesc.mWidth, termDesc.frameDesc.mHeight,
+                     termDesc.frameDesc.mFormat);
+            }
+        }
+
+        LOG2("        OutTerms: %zu", unit.outputTerminals.size());
+        for (auto const &term : unit.outputTerminals) {
+            shared_ptr<CameraBuffer> buffer= nullptr;
+            if (mPGBuffers.find(term) != mPGBuffers.end()) {
+                buffer = mPGBuffers.at(term);
+            }
+
+            const TerminalDescriptor& termDesc = mTerminalsDesc.at(term);
+            if (termDesc.enabled) {
+                LOG2("            %d: %dx%d, 0x%x, port %d, buf %p",
+                     termDesc.terminal - termDesc.stageId - 1,
+                     termDesc.frameDesc.mWidth, termDesc.frameDesc.mHeight,
+                     termDesc.frameDesc.mFormat,
+                     termDesc.assignedPort, buffer.get());
+            } else {
+                LOG2("            %d: %dx%d, 0x%x, disabled",
+                     termDesc.terminal - termDesc.stageId - 1,
+                     termDesc.frameDesc.mWidth, termDesc.frameDesc.mHeight,
+                     termDesc.frameDesc.mFormat);
+            }
+        }
+    }
+    LOG2("============= dump done for %s =================", getName());
+}
+
+}
+
diff --git a/camera/hal/intel/ipu6/src/core/psysprocessor/PipeLiteExecutor.h b/camera/hal/intel/ipu6/src/core/psysprocessor/PipeLiteExecutor.h
new file mode 100644
index 000000000000..3fbce9a9607b
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/psysprocessor/PipeLiteExecutor.h
@@ -0,0 +1,191 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <map>
+#include <vector>
+#include <memory>
+#include <string>
+#include <utility> // For std::pair, std::make_pair
+
+#include "Parameters.h"
+#include "CameraBuffer.h"
+#include "BufferQueue.h"
+#include "psysprocessor/PGCommon.h"
+#include "PolicyManager.h"
+#include "IspParamAdaptor.h"
+#include "GraphConfig.h"
+
+namespace icamera {
+
+class PSysDAG;
+
+typedef std::map<Port, std::shared_ptr<CameraBuffer>> CameraBufferPortMap;
+
+class PipeLiteExecutor : public BufferQueue {
+public:
+    PipeLiteExecutor(int cameraId, const ExecutorPolicy &policy,
+                     std::vector<std::string> exclusivePGs,
+                     PSysDAG *psysDag, std::shared_ptr<IGraphConfig> gc);
+    virtual ~PipeLiteExecutor();
+    virtual int start();
+    virtual void stop();
+    virtual int initPipe();
+    void notifyStop();
+
+    int releaseStatsBuffer(const std::shared_ptr<CameraBuffer> &statsBuf);
+
+    void setStreamId(int streamId) { mStreamId = streamId; }
+    void setIspParamAdaptor(IspParamAdaptor* adaptor) { mAdaptor = adaptor; }
+    void setPolicyManager(PolicyManager* policyManager) { mPolicyManager = policyManager; }
+    void setNotifyPolicy(ExecutorNotifyPolicy notifyPolicy) { mNotifyPolicy = notifyPolicy; }
+
+    void getOutputTerminalPorts(std::map<ia_uid, Port>& outputTerminals) const;
+    void getInputTerminalPorts(std::map<ia_uid, Port>& terminals) const;
+    bool hasOutputTerminal(ia_uid sinkTerminal);
+
+    // Link output terminals of producer to its input terminals
+    int setInputTerminals(const std::map<ia_uid, Port>& sourceTerminals);
+    int registerOutBuffers(Port port, const std::shared_ptr<CameraBuffer> &camBuffer);
+    int registerInBuffers(Port port, const std::shared_ptr<CameraBuffer> &inBuf);
+
+    /**
+     * Check if the two given stream configs are the same.
+     */
+    bool isSameStreamConfig(const stream_t& internal, const stream_t& external,
+                            ConfigMode configMode, bool checkStreamId) const;
+
+    bool isInputEdge() { return mIsInputEdge; }
+    bool isOutputEdge() { return mIsOutputEdge; }
+
+    const char* getName() const { return mName.c_str(); }
+
+ private:
+    DISALLOW_COPY_AND_ASSIGN(PipeLiteExecutor);
+
+ protected:
+    struct TerminalDescriptor{
+        ia_uid terminal;
+        ia_uid stageId;
+
+        ia_uid sourceTerminal;
+        ia_uid sinkTerminal;
+        ia_uid sourceStage;
+        ia_uid sinkStage;
+
+        FrameInfo frameDesc;
+
+        bool enabled;
+        bool hasConnection; // has related sink or source
+                            // expection: sis output, sink = source
+        Port assignedPort;  // INVALID_PORT for terminal without connection
+        int usrStreamId;
+    };
+
+    struct ExecutorUnit {
+        // Initialized during creation/configuration
+        int pgId;
+        ia_uid stageId;
+        std::shared_ptr<PGCommon> pg;
+        std::vector<ia_uid> statKernelUids;
+        std::vector<ia_uid> sisKernelUids;
+
+        // Initialized during connection analysis
+        std::vector<ia_uid> inputTerminals; // including disabled terminals
+        std::vector<ia_uid> outputTerminals;
+
+        // Initialized during buffer allocation
+        std::map<ia_uid, std::shared_ptr<CameraBuffer>> inputBuffers;
+        std::map<ia_uid, std::shared_ptr<CameraBuffer>> outputBuffers;
+    };
+
+ protected:
+    int analyzeConnections(const std::vector<IGraphType::PipelineConnection>& connVector);
+    int assignDefaultPortsForTerminals();
+    int notifyFrameDone(const v4l2_buffer_t& inV4l2Buf, const CameraBufferPortMap& outBuf);
+
+    /**
+     * Check if there is any valid buffer(not null) in the given port/buffer pairs.
+     */
+    bool hasValidBuffers(const CameraBufferPortMap& buffers);
+    void dumpPGs() const;
+
+ private:
+    int processNewFrame();
+    int runPipe(std::map<Port, std::shared_ptr<CameraBuffer>> &inBuffers,
+                std::map<Port, std::shared_ptr<CameraBuffer>> &outBuffers,
+                std::vector<std::shared_ptr<CameraBuffer>> &outStatsBuffers,
+                std::vector<EventType> &eventType);
+
+    int notifyStatsDone(TuningMode tuningMode, const v4l2_buffer_t& inV4l2Buf,
+                        const std::vector<std::shared_ptr<CameraBuffer>> &outStatsBuffers,
+                        const std::vector<EventType> &eventType);
+
+    int createPGs();
+    int configurePGs();
+    int allocBuffers();
+    void releaseBuffers();
+    int storeTerminalInfo(const IGraphType::PipelineConnection& connection);
+    int getStatKernels(int pgId, std::vector<ia_uid>& kernels);
+    int getSisKernels(int pgId, std::vector<ia_uid>& kernels);
+    ExecutorUnit* findPGExecutor(ia_uid stageId);
+    void getDisabledTerminalsForPG(ia_uid stageId, std::vector<ia_uid>& terminals) const;
+    void getTerminalFrameInfos(const std::vector<ia_uid>& terminals,
+                               std::map<ia_uid, FrameInfo>& infos) const;
+    void getTerminalPorts(const std::vector<ia_uid>& terminals,
+                          std:: map<ia_uid, Port>& terminalPortMap) const;
+    void getTerminalBuffersFromExternal(
+                        const std::vector<ia_uid>& terminals,
+                        const std::map<Port, std::shared_ptr<CameraBuffer> >& externals,
+                        std::map<ia_uid, std::shared_ptr<CameraBuffer> >& internals) const;
+
+    int handleSisStats(std::map<ia_uid, std::shared_ptr<CameraBuffer>>& frameBuffers,
+                       const std::shared_ptr<CameraBuffer> &outStatsBuffers);
+
+ protected:
+    int mCameraId;
+    int mStreamId;
+    std::string mName;
+    std::vector<std::string> mPGNames;
+    std::vector<int> mOpModes;
+    std::shared_ptr<IGraphConfig> mGraphConfig;
+    bool mIsInputEdge;
+    bool mIsOutputEdge;
+    ExecutorNotifyPolicy mNotifyPolicy;
+
+    std::vector<ExecutorUnit> mPGExecutors;
+    IspParamAdaptor* mAdaptor;
+
+    PolicyManager* mPolicyManager;
+
+    // For internal connections (between PGs)
+    std::map<ia_uid, std::shared_ptr<CameraBuffer> > mPGBuffers; // Buffers between PGs
+    std::map<ia_uid, ia_uid> mConnectionMap; // <sink, source>
+    std::map<ia_uid, TerminalDescriptor> mTerminalsDesc;
+
+    int64_t mLastStatsSequence;
+    CameraBufQ mStatsBuffers;
+    Mutex mStatsBuffersLock;
+    std::vector<std::string> mExclusivePGs;
+    PSysDAG *mPSysDag;
+
+    CameraBufferPortMap mInternalOutputBuffers;
+    int mkernelsCountWithStats;
+};
+
+typedef PipeLiteExecutor PipeExecutor;
+}
diff --git a/camera/hal/intel/ipu6/src/core/psysprocessor/PolicyManager.cpp b/camera/hal/intel/ipu6/src/core/psysprocessor/PolicyManager.cpp
new file mode 100644
index 000000000000..e51d012daa76
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/psysprocessor/PolicyManager.cpp
@@ -0,0 +1,168 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#define LOG_TAG "Camera_PolicyManager"
+
+#include "PolicyManager.h"
+
+#include "iutils/Errors.h"
+#include "iutils/CameraLog.h"
+
+using namespace std;
+
+namespace icamera {
+
+PolicyManager::PolicyManager(int cameraId) : mCameraId(cameraId), mIsActive(false)
+{
+    LOG1("@%s: camera id:%d", __func__, mCameraId);
+}
+
+PolicyManager::~PolicyManager()
+{
+    LOG1("@%s: camera id:%d", __func__, mCameraId);
+
+    releaseBundles();
+}
+
+void PolicyManager::releaseBundles()
+{
+    LOG1("@%s: camera id:%d", __func__, mCameraId);
+
+    for (const auto& bundle : mBundles) {
+        delete bundle;
+    }
+
+    mBundles.clear();
+}
+
+void PolicyManager::setActive(bool isActive)
+{
+    AutoMutex lock(mPolicyLock);
+
+    LOG1("@%s: camera id:%d update active mode from %d to %d",
+          __func__, mCameraId, mIsActive, isActive);
+
+    if (mIsActive == isActive) return; // No action is needed if the mode unchanged.
+
+    for (auto& bundle : mBundles) {
+        AutoMutex lock(bundle->mLock);
+
+        bundle->mWaitingCount = 0;
+        bundle->mIsActive = isActive;
+        for (auto& executorData : bundle->mExecutorData) {
+            executorData.second.mRunCount = 0;
+        }
+
+        // Wake up the executors who are waiting for other executors.
+        if (!bundle->mIsActive) {
+            bundle->mCondition.broadcast();
+        }
+    }
+
+    mIsActive = isActive;
+}
+
+int PolicyManager::addExecutorBundle(const vector<string>& executors, const vector<int>& depths)
+{
+    LOG1("@%s: camera id:%d", __func__, mCameraId);
+
+    AutoMutex lock(mPolicyLock);
+
+    uint8_t size = executors.size();
+    CheckError(size != depths.size(),
+          BAD_VALUE, "The size for executor and its depth not match");
+
+    int maxDepth = 0;
+    map<string, ExecutorData> executorData;
+
+    for (uint8_t i = 0; i < size; i++) {
+        executorData[executors[i]] = ExecutorData(depths[i]);
+        if (depths[i] > maxDepth) {
+            maxDepth = depths[i];
+        }
+        LOG1("%s, bundled executor name:%s, depth:%d)", __func__, executors[i].c_str(), depths[i]);
+    }
+
+    ExecutorBundle* bundle = new ExecutorBundle();
+    bundle->mExecutorData = executorData;
+    bundle->mExecutorNum = size;
+    bundle->mMaxDepth = maxDepth;
+    bundle->mWaitingCount = 0;
+    bundle->mIsActive = true;
+
+    mBundles.push_back(bundle);
+
+    return OK;
+}
+
+int PolicyManager::wait(string executorName)
+{
+    ExecutorBundle* bundle = nullptr;
+    {
+        AutoMutex lock(mPolicyLock);
+
+        // No need to wait when it's already inactive.
+        if (!mIsActive) return OK;
+
+        for (const auto& item : mBundles) {
+            if (item->mExecutorData.find(executorName) != item->mExecutorData.end()) {
+                bundle = item;
+                break;
+            }
+        }
+        // If the executor not in mBundles, it means it doesn't need to wait for others.
+        if (bundle == nullptr) return OK;
+    }
+
+    ConditionLock lock(bundle->mLock);
+
+    // If it's already inactive, there is no need to align the executors anymore.
+    if (!bundle->mIsActive) return OK;
+
+    ExecutorData& executorData = bundle->mExecutorData[executorName];
+    executorData.mRunCount++;
+
+    /**
+     * If an executor's run count plus its depth less than the max depth of all executors,
+     * it means the executor can run without checking other executors' status, since other
+     * may wait on this executor's output to reach the precondition of running together.
+     */
+    if (executorData.mRunCount + executorData.mDepth <= bundle->mMaxDepth) {
+        return OK;
+    }
+
+    bundle->mWaitingCount++;
+
+    /**
+     * If waiting count less than total executor number in the bundle, it means
+     * we need to wait for other executors to run with them together.
+     */
+    if (bundle->mWaitingCount < bundle->mExecutorNum) {
+        LOG2("%s: need wait for other executors.", executorName.c_str());
+        // the timeout value is 100ms * executor count
+        int64_t kWaitDuration = 100000000;
+        kWaitDuration *= bundle->mExecutorNum;
+        int ret = bundle->mCondition.waitRelative(lock, kWaitDuration * SLOWLY_MULTIPLIER);
+        CheckWarning(ret == TIMED_OUT, ret, "%s: wait executors timeout", executorName.c_str());
+    } else {
+        bundle->mWaitingCount = 0;
+        bundle->mCondition.broadcast();
+    }
+
+    return OK;
+}
+
+} // end of namespace icamera
+
diff --git a/camera/hal/intel/ipu6/src/core/psysprocessor/PolicyManager.h b/camera/hal/intel/ipu6/src/core/psysprocessor/PolicyManager.h
new file mode 100644
index 000000000000..3bef4a8421c3
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/core/psysprocessor/PolicyManager.h
@@ -0,0 +1,79 @@
+/*
+ * Copyright (C) 2017-2018 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <map>
+#include <vector>
+#include <string>
+
+#include "iutils/Utils.h"
+#include "iutils/Thread.h"
+
+namespace icamera {
+
+class PolicyManager {
+public:
+    PolicyManager(int cameraId);
+    ~PolicyManager();
+
+    /**
+     * Create a bundle for the given set of executors, and add the bundle into mBundles.
+     * These executors are guaranteed running at the same time.
+     */
+    int addExecutorBundle(const std::vector<std::string>& executors, const std::vector<int>& depths);
+
+    void setActive(bool isActive);
+
+    /**
+     * Check whether the given executor can run or not.
+     * If the executor cannot run then it'll wait for other executors in the same bundle.
+     * Once all executors are ready to run, then a broadcast will be sent out to wake all
+     * executors up and then run together.
+     */
+    int wait(std::string executorName);
+
+private:
+    DISALLOW_COPY_AND_ASSIGN(PolicyManager);
+
+    void releaseBundles();
+
+private:
+    struct ExecutorData {
+        ExecutorData(int depth = 0) : mRunCount(0), mDepth(depth) {}
+        long mRunCount; // How many times the executor has run.
+        int mDepth;     // Indicates how many direct dependencies the executor has.
+    };
+
+    struct ExecutorBundle {
+        std::map<std::string, ExecutorData> mExecutorData; // The index of the map is executor name.
+        int mMaxDepth;     // The max depth among all executors.
+        int mExecutorNum;  // Indicates how many executors the bundle has.
+        int mWaitingCount; // How many executors have already waited.
+        bool mIsActive;
+        //Guard for the Bundle data
+        Mutex mLock;
+        Condition mCondition;
+    };
+
+    int mCameraId;
+    //Guard for the PolicyManager public API
+    Mutex mPolicyLock;
+    std::vector<ExecutorBundle*> mBundles;
+    bool mIsActive;
+};
+
+}
diff --git a/camera/hal/intel/ipu6/src/fd/FaceBase.h b/camera/hal/intel/ipu6/src/fd/FaceBase.h
new file mode 100644
index 000000000000..1405e9ccac93
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/fd/FaceBase.h
@@ -0,0 +1,81 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#ifdef FACE_DETECTION
+#include <pvl_config.h>
+#include <pvl_eye_detection.h>
+#include <pvl_face_detection.h>
+#include <pvl_mouth_detection.h>
+#include <pvl_types.h>
+#endif
+
+namespace icamera {
+
+#define RECT_SIZE 4
+#define LM_SIZE 6
+#define MAX_STORE_FACE_DATA_BUF_NUM 3
+
+#define MAX_FACES_DETECTABLE 10
+#define MAX_FACE_FRAME_WIDTH 1920
+#define MAX_FACE_FRAME_HEIGHT 1280
+
+#define MAX_FACE_FRAME_SIZE_ASYNC (MAX_FACE_FRAME_WIDTH * MAX_FACE_FRAME_HEIGHT)  // only using Y
+#define MAX_FACE_FRAME_SIZE_SYNC (MAX_FACE_FRAME_WIDTH * MAX_FACE_FRAME_HEIGHT * 3 / 2)
+
+typedef enum {
+    FD_MODE_OFF,
+    FD_MODE_SIMPLE,  /**< Provide face area */
+    FD_MODE_FULL,    /**< Provide face area, eye and mouth coordinates */
+} face_detection_mode;
+
+/* Face Detection results */
+typedef struct CVFaceDetectionAbstractResult {
+    int faceNum;
+    int faceIds[MAX_FACES_DETECTABLE];
+    int faceLandmarks[LM_SIZE * MAX_FACES_DETECTABLE];
+    int faceRect[RECT_SIZE * MAX_FACES_DETECTABLE];
+    uint8_t faceScores[MAX_FACES_DETECTABLE];
+} CVFaceDetectionAbstractResult;
+
+struct FaceDetectionInitParams {
+    unsigned int max_face_num;
+};
+
+#ifdef FACE_DETECTION
+struct FaceDetectionResult {
+    int faceNum;
+    pvl_face_detection_result faceResults[MAX_FACES_DETECTABLE];
+    pvl_eye_detection_result eyeResults[MAX_FACES_DETECTABLE];
+    pvl_mouth_detection_result mouthResults[MAX_FACES_DETECTABLE];
+};
+
+struct FaceDetectionRunParams {
+    uint8_t data[MAX_FACE_FRAME_SIZE_ASYNC];
+    int32_t bufferHandle;
+    uint32_t size;
+    int32_t width;
+    int32_t height;
+    pvl_image_format format;
+    int32_t stride;
+    int32_t rotation;
+
+    FaceDetectionResult results;
+};
+#endif
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/fd/FaceDetection.cpp b/camera/hal/intel/ipu6/src/fd/FaceDetection.cpp
new file mode 100644
index 000000000000..d75a6664d42e
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/fd/FaceDetection.cpp
@@ -0,0 +1,530 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "FaceDetection"
+#include "src/fd/FaceDetection.h"
+
+#include <algorithm>
+#include <fstream>
+
+#include "AiqUtils.h"
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+#include "PlatformData.h"
+
+namespace icamera {
+std::unordered_map<int, FaceDetection*> FaceDetection::sInstances;
+Mutex FaceDetection::sLock;
+FaceDetection *FaceDetection::getInstance(int cameraId) {
+    LOG1("@%s, cameraId:%d", __func__, cameraId);
+    CheckError(cameraId < 0 || cameraId >= PlatformData::numberOfCameras(),
+               nullptr, "cameraId %d is error", cameraId);
+
+    if (sInstances.find(cameraId) == sInstances.end()) {
+        return nullptr;
+    }
+
+    return sInstances[cameraId];
+}
+
+FaceDetection *FaceDetection::createInstance(int cameraId,
+                                             unsigned int maxFaceNum, int32_t halStreamId,
+                                             int width, int height) {
+    LOG1("@%s, cameraId:%d, maxFaceNum:%u, halStreamId:%u, width:%d, height:%d",
+         __func__, cameraId, maxFaceNum, halStreamId, width, height);
+    CheckError(maxFaceNum > MAX_FACES_DETECTABLE, nullptr,
+               "maxFaceNum %d is error", maxFaceNum);
+    CheckError(cameraId < 0 || cameraId >= PlatformData::numberOfCameras(),
+               nullptr, "cameraId %d is error", cameraId);
+
+    AutoMutex lock(sLock);
+    if (sInstances.find(cameraId) == sInstances.end()) {
+        sInstances[cameraId] = new FaceDetection(cameraId, maxFaceNum,
+                                                 halStreamId, width, height);
+    }
+
+    return sInstances[cameraId];
+}
+
+void FaceDetection::destoryInstance(int cameraId) {
+    LOG1("@%s, cameraId:%d", __func__, cameraId);
+    CheckError(cameraId < 0 || cameraId >= PlatformData::numberOfCameras(),
+               VOID_VALUE, "cameraId is error");
+
+    AutoMutex lock(sLock);
+    if (sInstances.find(cameraId) != sInstances.end()) {
+        delete sInstances[cameraId];
+        sInstances.erase(cameraId);
+    }
+}
+
+FaceDetection::FaceDetection(int cameraId, unsigned int maxFaceNum, int32_t halStreamId,
+                             int width, int height) :
+    mCameraId(cameraId),
+    mInitialized(false),
+    mHalStreamId(halStreamId),
+    mWidth(width),
+    mHeight(height) {
+    LOG1("@%s, cameraId:%d, maxFaceNum:%d", __func__, cameraId, maxFaceNum);
+    CLEAR(mResult);
+
+    /* TODO: we should add CameraOrientationDetector to change the camera orientation */
+    camera_info_t info;
+    PlatformData::getCameraInfo(mCameraId, info);
+    mSensorOrientation = info.orientation;
+
+    /* start face engine pthread */
+    int ret = run("FaceDetection" + std::to_string(cameraId), PRIORITY_NORMAL);
+    CheckError(ret != OK, VOID_VALUE, "Camera thread failed to start, ret %d", ret);
+
+    /* init IntelFaceDetection */
+    FaceDetectionInitParams params;
+    params.max_face_num = maxFaceNum;
+    mFace = std::unique_ptr<IntelFaceDetection>(new IntelFaceDetection());
+    ret = mFace->init(&params, sizeof(FaceDetectionInitParams));
+    CheckError(ret != OK, VOID_VALUE, "mFace.init fails, ret %d", ret);
+
+    for (int i = 0; i < MAX_STORE_FACE_DATA_BUF_NUM; i++) {
+        FaceDetectionRunParams *memRunBuf = mFace->prepareRunBuffer(i);
+        CheckError(!memRunBuf, VOID_VALUE, "prepareRunBuffer fails");
+        mMemRunPool.push(memRunBuf);
+    }
+
+    mInitialized = true;
+}
+
+FaceDetection::~FaceDetection() {
+    LOG1("@%s", __func__);
+    mFace->deinit();
+    requestExit();
+
+    AutoMutex l(mRunBufQueueLock);
+    mRunCondition.notify_one();
+}
+
+FaceDetectionRunParams *FaceDetection::acquireRunBuf() {
+    AutoMutex l(mMemRunPoolLock);
+    LOG2("@%s, mRunPool.size is %zu", __func__, mMemRunPool.size());
+
+    FaceDetectionRunParams *runBuffer = nullptr;
+    if (!mMemRunPool.empty()) {
+        runBuffer = mMemRunPool.front();
+        mMemRunPool.pop();
+        CLEAR(*runBuffer);
+    }
+    return runBuffer;
+}
+
+void FaceDetection::returnRunBuf(FaceDetectionRunParams *memRunBuf) {
+    LOG2("@%s, Push back run face engine buffer", __func__);
+
+    AutoMutex l(mMemRunPoolLock);
+    mMemRunPool.push(memRunBuf);
+}
+
+void FaceDetection::runFaceDetection(const camera_buffer_t &buffer) {
+    LOG1("@%s", __func__);
+    CheckError(mInitialized == false, VOID_VALUE, "mInitialized is false");
+
+    if (PlatformData::isFaceEngineSyncRunning(mCameraId)) {
+        runFaceDetectionBySync(buffer);
+    } else {
+        runFaceDetectionByAsync(buffer);
+    }
+}
+
+void FaceDetection::runFaceDetectionBySync(const camera_buffer_t &buffer) {
+    LOG1("@%s", __func__);
+    CheckError(mInitialized == false, VOID_VALUE, "mInitialized is false");
+
+    int size = buffer.s.size;
+    CheckError(size > MAX_FACE_FRAME_SIZE_SYNC, VOID_VALUE,
+               "face frame buffer is too small!, w:%d,h:%d,size:%d",
+               buffer.s.width, buffer.s.height, size);
+
+    FaceDetectionRunParams *params = acquireRunBuf();
+    CheckError(!params, VOID_VALUE, "Fail to acquire face engine buffer");
+
+    params->size = size;
+    params->width = buffer.s.width;
+    params->height = buffer.s.height;
+    /* TODO: image.rotation is (mSensorOrientation + mCamOriDetector->getOrientation()) % 360 */
+    params->rotation = mSensorOrientation % 360;
+    params->format = pvl_image_format_nv12;
+    params->stride = buffer.s.stride;
+    params->bufferHandle = -1;
+
+    nsecs_t startTime = CameraUtils::systemTime();
+
+#ifdef ENABLE_SANDBOXING
+    int ret = mFace->run(params, sizeof(FaceDetectionRunParams), buffer.dmafd);
+#else
+    int ret = mFace->run(params, sizeof(FaceDetectionRunParams), buffer.addr);
+#endif
+
+    LOG2("@%s: ret:%d, it takes need %ums", __func__, ret,
+         (unsigned)((CameraUtils::systemTime() - startTime) / 1000000));
+
+    {
+        AutoMutex l(mFaceResultLock);
+        if (ret == OK) {
+            mResult = params->results;
+        } else {
+            CLEAR(mResult);
+        }
+    }
+
+    returnRunBuf(params);
+}
+
+void FaceDetection::runFaceDetectionByAsync(const camera_buffer_t &buffer) {
+    LOG1("@%s", __func__);
+    CheckError(mInitialized == false, VOID_VALUE, "mInitialized is false");
+
+    int size = buffer.s.stride * buffer.s.height;
+    CheckError(size > MAX_FACE_FRAME_SIZE_ASYNC, VOID_VALUE,
+               "face frame buffer is too small!, w:%d,h:%d,size:%d",
+               buffer.s.width, buffer.s.height, size);
+
+    FaceDetectionRunParams *params = acquireRunBuf();
+    CheckError(!params, VOID_VALUE, "Fail to acquire face engine buffer");
+
+    params->size = size;
+    MEMCPY_S(params->data, MAX_FACE_FRAME_SIZE_ASYNC, buffer.addr, size);
+    params->width = buffer.s.width;
+    params->height = buffer.s.height;
+    /* TODO: image.rotation is (mSensorOrientation + mCamOriDetector->getOrientation()) % 360 */
+    params->rotation = mSensorOrientation % 360;
+    params->format = pvl_image_format_gray;
+    params->stride = buffer.s.stride;
+    params->bufferHandle = -1;
+
+    AutoMutex l(mRunBufQueueLock);
+    mRunBufQueue.push(params);
+    mRunCondition.notify_one();
+}
+
+bool FaceDetection::threadLoop() {
+    LOG1("@%s", __func__);
+
+    FaceDetectionRunParams *faceParams = nullptr;
+
+    {
+        ConditionLock lock(mRunBufQueueLock);
+        if (mRunBufQueue.empty()) {
+            std::cv_status ret = mRunCondition.wait_for(
+                                     lock,
+                                     std::chrono::nanoseconds(kMaxDuration * SLOWLY_MULTIPLIER));
+            if (ret == std::cv_status::timeout) {
+                LOGW("@%s, wait request time out", __func__);
+            }
+
+            return true;
+        }
+        faceParams = mRunBufQueue.front();
+        mRunBufQueue.pop();
+    }
+
+    nsecs_t startTime = CameraUtils::systemTime();
+
+    int ret = mFace->run(faceParams, sizeof(FaceDetectionRunParams));
+    LOG2("@%s: ret:%d, it takes need %ums", __func__, ret,
+         (unsigned)((CameraUtils::systemTime() - startTime) / 1000000));
+
+    {
+        AutoMutex l(mFaceResultLock);
+        if (ret == OK) {
+            mResult = faceParams->results;
+        } else {
+            CLEAR(mResult);
+        }
+    }
+
+    returnRunBuf(faceParams);
+    return true;
+}
+
+int FaceDetection::getFaceNum() {
+    LOG2("@%s", __func__);
+    CheckError(mInitialized == false, 0, "mInitialized is false");
+
+    AutoMutex l(mFaceResultLock);
+    return mResult.faceNum;
+}
+
+/* The result is pvl's original reuslt */
+int FaceDetection::getFaceDetectionResult(FaceDetectionResult *result) {
+    LOG1("@%s", __func__);
+    CheckError(mInitialized == false, UNKNOWN_ERROR, "mInitialized is false");
+    CheckError(!result, UNKNOWN_ERROR, "mResult is nullptr");
+
+    AutoMutex l(mFaceResultLock);
+    MEMCPY_S(result, sizeof(FaceDetectionResult), &mResult, sizeof(FaceDetectionResult));
+    return OK;
+}
+
+/* Get current frame width and hight */
+void FaceDetection::getCurrentFrameWidthAndHight(int *frameWidth, int *frameHigth) {
+    LOG2("@%s", __func__);
+    CheckError(mInitialized == false, VOID_VALUE, "mInitialized is false");
+    CheckError(!frameWidth || !frameHigth, VOID_VALUE, "input paramter is error");
+
+    *frameWidth = mWidth;
+    *frameHigth = mHeight;
+}
+
+/* Get current hal stream id */
+void FaceDetection::getHalStreamId(int32_t *halStreamId) {
+    LOG2("@%s", __func__);
+    CheckError(mInitialized == false, VOID_VALUE, "mInitialized is false");
+    CheckError(!halStreamId, VOID_VALUE, "halStreamId is nullptr");
+
+    *halStreamId = mHalStreamId;
+}
+
+/* The result for 3A AE */
+int FaceDetection::getResult(int cameraId, ia_atbx_face_state *faceState) {
+    LOG1("@%s", __func__);
+    CheckError(!faceState, UNKNOWN_ERROR, "faceState is nullptr");
+    CheckError(cameraId < 0 || cameraId >= PlatformData::numberOfCameras(),
+               UNKNOWN_ERROR, "cameraId %d is error", cameraId);
+
+    int width = 0;
+    int height = 0;
+    int32_t halStreamId = 0;
+
+    FaceDetectionResult faceDetectionResult;
+    {
+        AutoMutex lock(sLock);
+        FaceDetection *fdInstance = FaceDetection::getInstance(cameraId);
+        CheckError(!fdInstance, UNKNOWN_ERROR, "Failed to get instance");
+
+        int ret = fdInstance->getFaceDetectionResult(&faceDetectionResult);
+        CheckError(ret != OK, UNKNOWN_ERROR, "Failed to get result, ret %d", ret);
+        fdInstance->getCurrentFrameWidthAndHight(&width, &height);
+        fdInstance->getHalStreamId(&halStreamId);
+    }
+
+    /*
+    face rectangle from face lib: (Ln, Tn, Rn, Bn)
+    3A statistics Surface: ((IA_COORDINATE_RIGHT - IA_COORDINATE_LEFT) *
+                            (IA_COORDINATE_BOTTOM - IA_COORDINATE_TOP))
+    target coordinate of face rectangle to the 3A lib: (LL, TT, RR, BB)
+    FOV ratio (which is <= 1): (fovRatioW * fovRatioH)
+
+    formular:
+    LL = Ln * fovRatioW + (1 - fovRatioW) / 2 * (IA_COORDINATE_RIGHT - IA_COORDINATE_LEFT)
+    TT = Tn * fovRatioH + (1 - fovRatioH) / 2 * (IA_COORDINATE_BOTTOM - IA_COORDINATE_TOP)
+    RR and BB are the similar.
+    */
+
+    float fovRatioW = 1;
+    float fovRatioH = 1;
+    int ret = PlatformData::getScalerInfo(cameraId, halStreamId, &fovRatioW, &fovRatioH);
+    LOG2("@%s, getScalerInfo ret:%d, fovRatioW:%f, fovRatioH:%f",
+         __func__, ret, fovRatioW, fovRatioH);
+
+    camera_coordinate_system_t activePixelArray = PlatformData::getActivePixelArray(cameraId);
+    float fovRatioWTmp = fovRatioW * width / (activePixelArray.right - activePixelArray.left);
+    float fovRatioHTmp = fovRatioH * height / (activePixelArray.bottom - activePixelArray.top);
+    float offsetW = (1.0 - fovRatioWTmp) / 2.0 * (IA_COORDINATE_RIGHT - IA_COORDINATE_LEFT);
+    float offsetH = (1.0 - fovRatioHTmp) / 2.0 * (IA_COORDINATE_BOTTOM - IA_COORDINATE_TOP);
+
+    LOG1("@%s, faceNum:%d, mHeight:%d, mWidth:%d", __func__,
+         faceDetectionResult.faceNum, height, width);
+
+    faceState->num_faces = faceDetectionResult.faceNum;
+
+    for (int i = 0; i < faceDetectionResult.faceNum; i++) {
+        CLEAR(faceState->faces[i]);
+        faceState->faces[i].face_area.left =
+         static_cast<int>(faceDetectionResult.faceResults[i].rect.left * fovRatioWTmp + offsetW);
+        faceState->faces[i].face_area.top =
+         static_cast<int>(faceDetectionResult.faceResults[i].rect.top * fovRatioHTmp + offsetH);
+        faceState->faces[i].face_area.bottom =
+         static_cast<int>(
+                         faceDetectionResult.faceResults[i].rect.bottom * fovRatioHTmp + offsetH);
+        faceState->faces[i].face_area.right =
+         static_cast<int>(
+                          faceDetectionResult.faceResults[i].rect.right * fovRatioWTmp + offsetW);
+        faceState->faces[i].rip_angle = faceDetectionResult.faceResults[i].rip_angle;
+        faceState->faces[i].rop_angle = faceDetectionResult.faceResults[i].rop_angle;
+        faceState->faces[i].tracking_id = faceDetectionResult.faceResults[i].tracking_id;
+        faceState->faces[i].confidence = faceDetectionResult.faceResults[i].confidence;
+        faceState->faces[i].person_id = -1;
+        faceState->faces[i].similarity = 0;
+        faceState->faces[i].best_ratio = 0;
+        faceState->faces[i].face_condition = 0;
+
+        faceState->faces[i].smile_state = 0;
+        faceState->faces[i].smile_score = 0;
+        faceState->faces[i].mouth.x =
+          static_cast<int>(faceDetectionResult.mouthResults[i].mouth.x * fovRatioWTmp + offsetW);
+         faceState->faces[i].mouth.y =
+          static_cast<int>(faceDetectionResult.mouthResults[i].mouth.y * fovRatioHTmp + offsetH);
+
+        faceState->faces[i].eye_validity = 0;
+    }
+
+    return OK;
+}
+
+/* The result for android statistics metadata */
+int FaceDetection::getResult(int cameraId, CVFaceDetectionAbstractResult *result) {
+    LOG1("@%s", __func__);
+    CheckError(!result, UNKNOWN_ERROR, "result is nullptr");
+    CheckError(cameraId < 0 || cameraId >= PlatformData::numberOfCameras(),
+               UNKNOWN_ERROR, "cameraId %d is error", cameraId);
+
+    int width = 0;
+    int height = 0;
+    FaceDetectionResult faceDetectionResult;
+    {
+        AutoMutex lock(sLock);
+        FaceDetection *fdInstance = FaceDetection::getInstance(cameraId);
+        CheckError(!fdInstance, UNKNOWN_ERROR, "Failed to get instance");
+
+        int ret = fdInstance->getFaceDetectionResult(&faceDetectionResult);
+        CheckError(ret != OK, UNKNOWN_ERROR, "Failed to get result");
+        fdInstance->getCurrentFrameWidthAndHight(&width, &height);
+    }
+
+    const camera_coordinate_system_t iaCoord = {IA_COORDINATE_LEFT, IA_COORDINATE_TOP,
+                                                IA_COORDINATE_RIGHT, IA_COORDINATE_BOTTOM};
+
+    // construct android coordinate based on active pixel array
+    camera_coordinate_system_t activePixelArray = PlatformData::getActivePixelArray(cameraId);
+
+    int activeHeight = activePixelArray.bottom - activePixelArray.top;
+    int activeWidth = activePixelArray.right - activePixelArray.left;
+    const camera_coordinate_system_t sysCoord = {0, 0,  activeWidth, activeHeight};
+    camera_coordinate_t srcCoord = {0, 0};
+    camera_coordinate_t destCoord = {0, 0};
+
+    int verticalCrop = 0, horizontalCrop = 0;
+    bool imageRotationUnchanged = true;
+
+    // do extra conversion if the image ratio is not the same ratio with the android coordinate.
+    if (height * activeWidth != width * activeHeight) {
+        imageRotationUnchanged = false;
+        int gap = (width * activeHeight / activeWidth) - height;
+
+        if (gap > 0) {
+            // vertical crop pixel
+            verticalCrop = gap;
+        } else if (gap < 0) {
+            // horizontal crop pixel
+            horizontalCrop = height * activeWidth / activeHeight - width;
+        }
+    }
+
+    const camera_coordinate_system_t fillFrameCoord = {0, 0,
+                                                       width + horizontalCrop,
+                                                       height + verticalCrop};
+    const camera_coordinate_system_t frameCoord = {0, 0, width, height};
+
+    CLEAR(*result);
+    for (int i = 0; i < faceDetectionResult.faceNum; i++) {
+        if (i == MAX_FACES_DETECTABLE)
+            break;
+
+        camera_coordinate_t pointCoord = {0, 0};
+        result->faceScores[i] = faceDetectionResult.faceResults[i].confidence;
+        result->faceIds[i] = faceDetectionResult.faceResults[i].tracking_id;
+
+        if (imageRotationUnchanged) {
+            srcCoord = {faceDetectionResult.faceResults[i].rect.left,
+                        faceDetectionResult.faceResults[i].rect.top};
+            destCoord = AiqUtils::convertCoordinateSystem(iaCoord, sysCoord, srcCoord);
+            result->faceRect[i * 4] = destCoord.x;  // rect.left
+            result->faceRect[i * 4 + 1] = destCoord.y;  // rect.top
+
+            srcCoord = {faceDetectionResult.faceResults[i].rect.right,
+                        faceDetectionResult.faceResults[i].rect.bottom};
+            destCoord = AiqUtils::convertCoordinateSystem(iaCoord, sysCoord, srcCoord);
+            result->faceRect[i * 4 + 2] = destCoord.x;  // rect.right
+            result->faceRect[i * 4 + 3] = destCoord.y;  // rect.bottom
+
+            srcCoord = {faceDetectionResult.eyeResults[i].left_eye.x,
+                        faceDetectionResult.eyeResults[i].left_eye.y};
+            destCoord = AiqUtils::convertCoordinateSystem(iaCoord, sysCoord, srcCoord);
+            result->faceLandmarks[i * 6] = destCoord.x;  // left_eye.x;
+            result->faceLandmarks[i * 6 + 1] = destCoord.y;  // left_eye.y;
+
+            srcCoord = {faceDetectionResult.eyeResults[i].right_eye.x,
+                        faceDetectionResult.eyeResults[i].right_eye.y};
+            destCoord = AiqUtils::convertCoordinateSystem(iaCoord, sysCoord, srcCoord);
+            result->faceLandmarks[i * 6 + 2] = destCoord.x;  // right_eye.x;
+            result->faceLandmarks[i * 6 + 3] = destCoord.y;  // right_eye.y;
+
+            srcCoord = {faceDetectionResult.mouthResults[i].mouth.x,
+                        faceDetectionResult.mouthResults[i].mouth.y};
+            destCoord = AiqUtils::convertCoordinateSystem(iaCoord, sysCoord, srcCoord);
+            result->faceLandmarks[i * 6 + 4] = destCoord.x;  // mouth.x;
+            result->faceLandmarks[i * 6 + 5] = destCoord.y;  // mouth.y;
+        } else {
+            srcCoord = {faceDetectionResult.faceResults[i].rect.left,
+                        faceDetectionResult.faceResults[i].rect.top};
+            pointCoord = AiqUtils::convertCoordinateSystem(iaCoord, frameCoord, srcCoord);
+            pointCoord.x += horizontalCrop / 2;
+            pointCoord.y += verticalCrop / 2;
+            destCoord = AiqUtils::convertCoordinateSystem(fillFrameCoord, sysCoord, pointCoord);
+            result->faceRect[i * 4] = destCoord.x;  // rect.left
+            result->faceRect[i * 4 + 1] = destCoord.y;  // rect.top
+
+            srcCoord = {faceDetectionResult.faceResults[i].rect.right,
+                        faceDetectionResult.faceResults[i].rect.bottom};
+            pointCoord = AiqUtils::convertCoordinateSystem(iaCoord, frameCoord, srcCoord);
+            pointCoord.x += horizontalCrop / 2;
+            pointCoord.y += verticalCrop / 2;
+            destCoord = AiqUtils::convertCoordinateSystem(fillFrameCoord, sysCoord, pointCoord);
+            result->faceRect[i * 4 + 2] = destCoord.x;  // rect.right
+            result->faceRect[i * 4 + 3] = destCoord.y;  // rect.bottom
+
+            srcCoord = {faceDetectionResult.eyeResults[i].left_eye.x,
+                        faceDetectionResult.eyeResults[i].left_eye.y};
+            pointCoord = AiqUtils::convertCoordinateSystem(iaCoord, frameCoord, srcCoord);
+            pointCoord.x += horizontalCrop / 2;
+            pointCoord.y += verticalCrop / 2;
+            destCoord = AiqUtils::convertCoordinateSystem(fillFrameCoord, sysCoord, pointCoord);
+            result->faceLandmarks[i * 6] = destCoord.x;  // left_eye.x;
+            result->faceLandmarks[i * 6 + 1] = destCoord.y;  // left_eye.y;
+
+            srcCoord = {faceDetectionResult.eyeResults[i].right_eye.x,
+                        faceDetectionResult.eyeResults[i].right_eye.y};
+            pointCoord = AiqUtils::convertCoordinateSystem(iaCoord, frameCoord, srcCoord);
+            pointCoord.x += horizontalCrop / 2;
+            pointCoord.y += verticalCrop / 2;
+            destCoord = AiqUtils::convertCoordinateSystem(fillFrameCoord, sysCoord, pointCoord);
+            result->faceLandmarks[i * 6 + 2] = destCoord.x;  // right_eye.x;
+            result->faceLandmarks[i * 6 + 3] = destCoord.y;  // right_eye.y;
+
+            srcCoord = {faceDetectionResult.mouthResults[i].mouth.x,
+                        faceDetectionResult.mouthResults[i].mouth.y};
+            pointCoord = AiqUtils::convertCoordinateSystem(iaCoord, frameCoord, srcCoord);
+            pointCoord.x += horizontalCrop / 2;
+            pointCoord.y += verticalCrop / 2;
+            destCoord = AiqUtils::convertCoordinateSystem(fillFrameCoord, sysCoord, pointCoord);
+            result->faceLandmarks[i * 6 + 4] = destCoord.x;  // mouth.x;
+            result->faceLandmarks[i * 6 + 5] = destCoord.y;  // mouth.y;
+        }
+    }
+    result->faceNum = (faceDetectionResult.faceNum < MAX_FACES_DETECTABLE ?
+                       faceDetectionResult.faceNum : MAX_FACES_DETECTABLE);
+    return OK;
+}
+}  // namespace icamera
+
diff --git a/camera/hal/intel/ipu6/src/fd/FaceDetection.h b/camera/hal/intel/ipu6/src/fd/FaceDetection.h
new file mode 100644
index 000000000000..9cc2433b05d3
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/fd/FaceDetection.h
@@ -0,0 +1,120 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#ifdef FACE_DETECTION
+#ifdef ENABLE_SANDBOXING
+#include "modules/sandboxing/client/IntelFaceDetection.h"
+#else
+#include "modules/algowrapper/IntelFaceDetection.h"
+#endif
+#include "iutils/Errors.h"
+#include "iutils/Thread.h"
+#endif
+
+#include <ia_types.h>
+#include <unordered_map>
+#include <memory>
+#include <queue>
+
+#include "iutils/Utils.h"
+#include "Parameters.h"
+#include "FaceBase.h"
+
+namespace icamera {
+
+#ifdef FACE_DETECTION
+class FaceDetection : public Thread {
+ public:
+    FaceDetection(int cameraId, unsigned int maxFaceNum, int32_t halStreamId,
+                  int width, int height);
+    ~FaceDetection();
+
+    static FaceDetection *createInstance(int cameraId, unsigned int maxFaceNum,
+                                         int32_t halStreamId, int width, int height);
+    static void destoryInstance(int cameraId);
+    static FaceDetection *getInstance(int cameraId);
+
+    void runFaceDetection(const camera_buffer_t &buffer);
+    void runFaceDetectionBySync(const camera_buffer_t &buffer);
+    void runFaceDetectionByAsync(const camera_buffer_t &buffer);
+    int getFaceNum();
+    virtual bool threadLoop();
+    static int getResult(int cameraId, ia_atbx_face_state *faceState);
+    static int getResult(int cameraId, CVFaceDetectionAbstractResult *result);
+
+ private:
+    int getFaceDetectionResult(FaceDetectionResult *mResult);
+    FaceDetectionRunParams *acquireRunBuf();
+    void returnRunBuf(FaceDetectionRunParams *memRunBuf);
+    void getCurrentFrameWidthAndHight(int *frameWidth, int *frameHigth);
+    void getHalStreamId(int32_t *halStreamId);
+
+    // Guard for face engine instance
+    static Mutex sLock;
+    static std::unordered_map<int, FaceDetection*> sInstances;
+
+    int mCameraId;
+    bool mInitialized;
+
+    // Guard for face detection result
+    std::mutex mFaceResultLock;
+    FaceDetectionResult mResult;
+
+    std::unique_ptr<IntelFaceDetection> mFace;
+
+    std::condition_variable mRunCondition;
+    // Guard for running buffer queue of thread
+    std::mutex mRunBufQueueLock;
+    std::queue<FaceDetectionRunParams *> mRunBufQueue;
+    const uint64_t kMaxDuration = 2000000000;  // 2000ms
+
+    // Guard for running buffer pool of face engine
+    std::mutex mMemRunPoolLock;
+    std::queue<FaceDetectionRunParams *> mMemRunPool;
+
+    int mSensorOrientation;
+
+    int32_t mHalStreamId;
+    int mWidth;
+    int mHeight;
+
+    DISALLOW_COPY_AND_ASSIGN(FaceDetection);
+};
+#else
+class FaceDetection {
+ public:
+    static int getResult(int cameraId, ia_atbx_face_state *faceState) {
+        faceState->num_faces = 0;
+        return 0;
+    }
+    static int getResult(int cameraId, CVFaceDetectionAbstractResult *result) {
+        CLEAR(*result);
+        return 0;
+    }
+    static FaceDetection *createInstance(int cameraId,
+                                         unsigned int maxFaceNum, int32_t halStreamId,
+                                         int width, int height) {
+        return nullptr;
+    }
+    static void destoryInstance(int cameraId) {}
+    void runFaceDetection(const camera_buffer_t &buffer) {}
+    int getFaceNum() {return 0;}
+};
+#endif
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/hal/CameraHal.cpp b/camera/hal/intel/ipu6/src/hal/CameraHal.cpp
new file mode 100644
index 000000000000..53dd2c1e6ec8
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/hal/CameraHal.cpp
@@ -0,0 +1,268 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "CameraHal"
+
+#include <vector>
+
+#include "iutils/CameraLog.h"
+
+#include "ICamera.h"
+#include "PlatformData.h"
+#include "SyncManager.h"
+#include "CameraHal.h"
+#include "Parameters.h"
+
+namespace icamera {
+
+#define checkCameraDevice(device, err_code) \
+    do { \
+        if (mState == HAL_UNINIT) { \
+            LOGE("HAL is not init."); \
+            return err_code; \
+        } \
+        if (!(device)) { \
+            LOGE("device is not open."); \
+            return err_code; \
+        } \
+    } while (0)
+
+CameraHal::CameraHal() :
+    mInitTimes(0),
+    mState(HAL_UNINIT)
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s", __func__);
+
+    CLEAR(mCameraDevices);
+}
+
+CameraHal::~CameraHal()
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s", __func__);
+}
+
+int CameraHal::init()
+{
+    LOG1("@%s", __func__);
+    AutoMutex lock(mLock);
+
+    if (mInitTimes++ > 0) {
+        LOGD("@%s, mInitTimes:%d, return without running", __func__, mInitTimes);
+        return OK;
+    }
+
+    int ret = PlatformData::init();
+    CheckError(ret != OK, NO_INIT, "PlatformData init failed");
+
+    mState = HAL_INIT;
+
+    return OK;
+}
+
+int CameraHal::deinit()
+{
+    LOG1("@%s", __func__);
+    AutoMutex l(mLock);
+
+    if (--mInitTimes > 0) {
+        LOGD("@%s, mInitTimes:%d, return without set state", __func__, mInitTimes);
+        return OK;
+    }
+
+    // SyncManager is used to do synchronization with multi-devices.
+    // Release it when the last device exit
+    SyncManager::releaseInstance();
+    // Release the PlatformData instance here due to it was
+    // created in init() period
+    PlatformData::releaseInstance();
+
+    mState = HAL_UNINIT;
+
+    return OK;
+}
+
+int CameraHal::deviceOpen(int cameraId)
+{
+    LOG1("@%s, camera id:%d", __func__, cameraId);
+    PERF_CAMERA_ATRACE();
+
+    AutoMutex l(mLock);
+    CheckError(mState == HAL_UNINIT, NO_INIT,"HAL is not initialized");
+
+    //Create the camera device that will be freed in close
+    if (mCameraDevices[cameraId]) {
+        LOGD("@%s: open multi times", __func__);
+        return INVALID_OPERATION;
+    }
+
+    mCameraOpenNum++;
+    mCameraDevices[cameraId] = new CameraDevice(cameraId);
+
+    if (mCameraOpenNum == 1) {
+        MediaControl::getInstance()->resetAllLinks();
+    }
+
+    return mCameraDevices[cameraId]->init();
+}
+
+void CameraHal::deviceClose(int cameraId)
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s, camera id:%d", __func__, cameraId);
+    AutoMutex l(mLock);
+
+    if (mCameraDevices[cameraId]) {
+        mCameraDevices[cameraId]->deinit();
+        delete mCameraDevices[cameraId];
+        mCameraDevices[cameraId] = nullptr;
+
+        mCameraOpenNum--;
+    }
+}
+
+void CameraHal::deviceCallbackRegister(int cameraId, const camera_callback_ops_t* callback)
+{
+    LOG1("@%s", __func__);
+    AutoMutex l(mLock);
+
+    CameraDevice *device = mCameraDevices[cameraId];
+    checkCameraDevice(device, VOID_VALUE);
+#ifdef ENABLE_SANDBOXING
+    IntelAlgoClient::getInstance()->registerErrorCallback(callback);
+#endif
+    device->callbackRegister(callback);
+}
+
+// Assume the inputConfig is already checked in upper layer
+int CameraHal::deviceConfigInput(int cameraId, const stream_t *inputConfig)
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s, camera id:%d", __func__, cameraId);
+    AutoMutex lock(mLock);
+
+    CameraDevice *device = mCameraDevices[cameraId];
+    checkCameraDevice(device, BAD_VALUE);
+
+    device->configureInput(inputConfig);
+
+    return OK;
+}
+
+// Assume the streamList is already checked in upper layer
+int CameraHal::deviceConfigStreams(int cameraId, stream_config_t *streamList)
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s, camera id:%d", __func__, cameraId);
+    AutoMutex lock(mLock);
+
+    CameraDevice *device = mCameraDevices[cameraId];
+    checkCameraDevice(device, BAD_VALUE);
+
+    int ret = device->configure(streamList);
+    if (ret != OK) {
+        LOGE("failed to config streams.");
+        return INVALID_OPERATION;
+    }
+
+    return ret;
+}
+
+int CameraHal::deviceStart(int cameraId)
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s, cameraId is %d", __func__, cameraId);
+
+    ConditionLock lock(mLock);
+
+    CameraDevice *device = mCameraDevices[cameraId];
+    checkCameraDevice(device, BAD_VALUE);
+
+    return device->start();
+}
+
+int CameraHal::deviceStop(int cameraId)
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s, cameraId is %d", __func__, cameraId);
+
+    AutoMutex lock(mLock);
+
+    CameraDevice *device = mCameraDevices[cameraId];
+    checkCameraDevice(device, BAD_VALUE);
+
+    return device->stop();
+}
+
+int CameraHal::deviceAllocateMemory(int cameraId, camera_buffer_t *ubuffer)
+{
+    PERF_CAMERA_ATRACE();
+    LOG1("@%s, cameraId is %d", __func__, cameraId);
+
+    CameraDevice *device = mCameraDevices[cameraId];
+
+    checkCameraDevice(device, BAD_VALUE);
+
+    return device->allocateMemory(ubuffer);
+}
+
+int CameraHal::streamQbuf(int cameraId, camera_buffer_t **ubuffer,
+                          int bufferNum, const Parameters* settings)
+{
+    PERF_CAMERA_ATRACE();
+    LOG2("@%s, cameraId is %d, fd:%d", __func__, cameraId, (*ubuffer)->dmafd);
+
+    CameraDevice *device = mCameraDevices[cameraId];
+
+    checkCameraDevice(device, BAD_VALUE);
+
+    return device->qbuf(ubuffer, bufferNum, settings);
+}
+
+int CameraHal::streamDqbuf(int cameraId, int streamId, camera_buffer_t **ubuffer,
+                           Parameters* settings)
+{
+    PERF_CAMERA_ATRACE();
+    LOG2("@%s, cameraId is %d, streamId is %d", __func__, cameraId, streamId);
+
+    CameraDevice *device = mCameraDevices[cameraId];
+    checkCameraDevice(device, BAD_VALUE);
+
+    return device->dqbuf(streamId, ubuffer, settings);
+}
+
+int CameraHal::getParameters(int cameraId, Parameters& param, long sequence)
+{
+    LOG1("@%s, cameraId is %d", __func__, cameraId);
+
+    CameraDevice *device = mCameraDevices[cameraId];
+    checkCameraDevice(device, BAD_VALUE);
+
+    return device->getParameters(param, sequence);
+}
+
+int CameraHal::setParameters(int cameraId, const Parameters& param)
+{
+    LOG1("@%s, cameraId is %d", __func__, cameraId);
+
+    CameraDevice *device = mCameraDevices[cameraId];
+    checkCameraDevice(device, BAD_VALUE);
+
+    return device->setParameters(param);
+}
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/hal/CameraHal.h b/camera/hal/intel/ipu6/src/hal/CameraHal.h
new file mode 100644
index 000000000000..1d46319862dd
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/hal/CameraHal.h
@@ -0,0 +1,85 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "CameraDevice.h"
+#include "Parameters.h"
+
+namespace icamera {
+
+/**
+ * CameraHal class is the real HAL API.
+ * There is only one instance of CameraHal which is created when HAL loading
+ * It creates the CameraDevice based on the camera ID to support multi-cameras.
+ *
+ * The main job of the Class is
+ * 1. Maintain a list of CameraDevice
+ * 2. Pass the Camera HAL API to the correct CameraDevice based on CameraId
+ *
+ * If open dual cameras in different process, the shared memory must be used to
+ * keep the account of the open times.
+ *
+ * The CameraHal create and maintains followings singleton instancs
+ * 1. MediaControl Instance
+ * 2. PlatformData Instance
+ */
+
+class CameraHal {
+//HAL API
+public:
+    CameraHal();
+    ~CameraHal();
+    int init();
+    int deinit();
+
+//Device API
+public:
+    int deviceOpen(int cameraId);
+    void deviceClose(int cameraId);
+
+    void deviceCallbackRegister(int cameraId, const camera_callback_ops_t* callback);
+    int deviceConfigInput(int cameraId, const stream_t *inputConfig);
+    int deviceConfigStreams(int cameraId, stream_config_t *streamList);
+    int deviceStart(int cameraId);
+    int deviceStop(int cameraId);
+    int deviceAllocateMemory(int cameraId, camera_buffer_t *ubuffer);
+//Stream API
+    int streamQbuf(int cameraId, camera_buffer_t **ubuffer,
+                   int bufferNum = 1, const Parameters* settings = nullptr);
+    int streamDqbuf(int cameraId, int streamId, camera_buffer_t **ubuffer,
+                    Parameters* settings = nullptr);
+    int setParameters(int cameraId, const Parameters& param);
+    int getParameters(int cameraId, Parameters& param, long sequence);
+
+private:
+    DISALLOW_COPY_AND_ASSIGN(CameraHal);
+
+private:
+    CameraDevice* mCameraDevices[MAX_CAMERA_NUMBER];
+    int mInitTimes;
+    // Guard for CameraHal public API.
+    Mutex mLock;
+
+    enum {
+        HAL_UNINIT,
+        HAL_INIT
+    } mState;
+
+    int mCameraOpenNum = 0;
+};
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/hal/ICamera.cpp b/camera/hal/intel/ipu6/src/hal/ICamera.cpp
new file mode 100644
index 000000000000..4fdad9b8a940
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/hal/ICamera.cpp
@@ -0,0 +1,371 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "ICamera"
+
+#include "iutils/CameraDump.h"
+#include "iutils/CameraLog.h"
+
+#include "ICamera.h"
+#include "CameraHal.h"
+#include "PlatformData.h"
+
+/**
+ * This is the wrapper to the CameraHal Class to provide the HAL interface
+ * Main job of this file
+ * 1. Check the argument from user
+ * 2. Transfer HAL API to CameraHal class
+ * 3. Implement the HAL static function: get_number_of_cameras and get_camera_info
+ */
+namespace icamera {
+
+static CameraHal * gCameraHal = nullptr;
+
+#define CheckCameraId(camera_id, err_code) \
+    do { \
+        int max_cam = PlatformData::numberOfCameras(); \
+        if (((camera_id) < 0) || (camera_id) >= max_cam) { \
+            LOGE("camera index(%d) is invaild., max_cam:%d", camera_id, max_cam); \
+            return err_code; \
+        } \
+    } while (0)
+
+/**
+ * Return the numbers of camera
+ * This should be called before any other calls
+ *
+ * \return > 0  return camera numbers
+ * \return == 0 failed to get camera numbers
+ **/
+int get_number_of_cameras()
+{
+    PERF_CAMERA_ATRACE();
+    HAL_TRACE_CALL(1);
+
+    return PlatformData::numberOfCameras();
+}
+
+/**
+ * Get capability related camera info.
+ * Should be called after get_number_of_cameras
+ *
+ * \return error code
+ */
+int get_camera_info(int camera_id, camera_info_t& info)
+{
+    PERF_CAMERA_ATRACE();
+    HAL_TRACE_CALL(1);
+    CheckCameraId(camera_id, BAD_VALUE);
+
+    int ret = PlatformData::getCameraInfo(camera_id, info);
+
+    return ret;
+}
+
+/**
+ * Initialize camera hal
+ *
+ * \return error code
+ **/
+int camera_hal_init()
+{
+    PERF_CAMERA_ATRACE();
+    HAL_TRACE_CALL(1);
+
+    if (gCameraHal) {
+        LOGW("camera hal is initialized multiple times.");
+        return 0;
+    }
+    gCameraHal = new CameraHal;
+
+    CheckError(!gCameraHal, INVALID_OPERATION, "camera hal is NULL.");
+    return gCameraHal->init();
+}
+
+/**
+ * De-initialize camera hal
+ *
+ * \return error code
+ **/
+int camera_hal_deinit()
+{
+    PERF_CAMERA_ATRACE();
+    HAL_TRACE_CALL(1);
+
+    CheckError(!gCameraHal, INVALID_OPERATION, "camera hal is NULL.");
+    int ret = gCameraHal->deinit();
+
+    delete gCameraHal;
+    gCameraHal = nullptr;
+
+    return ret;
+}
+
+/**
+ * Register callback function
+ **/
+void camera_callback_register(int camera_id, const camera_callback_ops_t* callback)
+{
+    PERF_CAMERA_ATRACE();
+    HAL_TRACE_CALL(1);
+
+    CheckError(!gCameraHal, VOID_VALUE, "camera hal is NULL.");
+    gCameraHal->deviceCallbackRegister(camera_id, callback);
+}
+
+/**
+ * Open one camera device
+ *
+ * \param camera_id camera index
+ *
+ * \return error code
+ **/
+int camera_device_open(int camera_id)
+{
+    PERF_CAMERA_ATRACE();
+    HAL_TRACE_CALL(1);
+
+    CheckError(!gCameraHal, INVALID_OPERATION, "camera hal is NULL.");
+    CheckCameraId(camera_id, BAD_VALUE);
+
+    return gCameraHal->deviceOpen(camera_id);
+}
+
+/**
+ * Close camera device
+ *
+ * \param camera_id The ID that opened before
+ **/
+void camera_device_close(int camera_id)
+{
+    PERF_CAMERA_ATRACE();
+    HAL_TRACE_CALL(1);
+
+    CheckError(!gCameraHal, VOID_VALUE, "camera hal is NULL.");
+    CheckCameraId(camera_id,);
+
+    gCameraHal->deviceClose(camera_id);
+}
+
+/**
+ * Configure the sensor input of the device
+ *
+ * \param camera_id The camera ID that was opened
+ * \param input_config  sensor input configuration
+ *
+ * \return 0 succeed <0 error
+ **/
+int camera_device_config_sensor_input(int camera_id, const stream_t *input_config)
+{
+    PERF_CAMERA_ATRACE();
+    HAL_TRACE_CALL(1);
+
+    CheckError(!gCameraHal, INVALID_OPERATION, "camera hal is NULL.");
+    CheckError(!input_config, BAD_VALUE, "camera input_config is NULL.");
+    CheckCameraId(camera_id, BAD_VALUE);
+
+    return gCameraHal->deviceConfigInput(camera_id, input_config);
+}
+
+/**
+ * Add stream to device
+ *
+ * \param camera_id The camera ID that was opened
+ * \param stream_id
+ * \param stream_conf stream configuration
+ *
+ * \return 0 succeed <0 error
+ **/
+int camera_device_config_streams(int camera_id, stream_config_t *stream_list)
+{
+    PERF_CAMERA_ATRACE();
+    HAL_TRACE_CALL(1);
+
+    CheckError(!gCameraHal, INVALID_OPERATION, "camera hal is NULL.");
+    CheckError(!stream_list, BAD_VALUE, "camera stream is NULL.");
+    CheckCameraId(camera_id, BAD_VALUE);
+
+    if (stream_list->operation_mode == CAMERA_STREAM_CONFIGURATION_MODE_STILL_CAPTURE) {
+        for (int i = 0; i < stream_list->num_streams; i++) {
+            stream_list->streams[i].usage = CAMERA_STREAM_STILL_CAPTURE;
+        }
+    }
+
+    return gCameraHal->deviceConfigStreams(camera_id, stream_list);
+}
+
+/**
+ * Start device
+ *
+ * Start all streams in device.
+ *
+ * \param camera_id The Caemra ID that opened before
+ *
+ * \return error code
+ **/
+int camera_device_start(int camera_id)
+{
+    PERF_CAMERA_ATRACE();
+    HAL_TRACE_CALL(1);
+    CheckError(!gCameraHal, INVALID_OPERATION ,"camera hal is NULL.");
+    CheckCameraId(camera_id, BAD_VALUE);
+
+    return gCameraHal->deviceStart(camera_id);
+}
+
+/**
+ * Stop device
+ *
+ * Stop all streams in device.
+ *
+ * \param camera_id The Caemra ID that opened before
+ *
+ * \return error code
+ **/
+int camera_device_stop(int camera_id)
+{
+    PERF_CAMERA_ATRACE();
+    HAL_TRACE_CALL(1);
+    CheckError(!gCameraHal, INVALID_OPERATION, "camera hal is NULL.");
+    CheckCameraId(camera_id, BAD_VALUE);
+
+    return gCameraHal->deviceStop(camera_id);
+}
+
+/**
+ * Allocate memory for mmap & dma export io-mode
+ *
+ * \param camera_id The camera ID that opened before
+ * \param camera_buff stream buff
+ *
+ * \return error code
+ **/
+int camera_device_allocate_memory(int camera_id, camera_buffer_t *buffer)
+{
+    PERF_CAMERA_ATRACE();
+    HAL_TRACE_CALL(2);
+    CheckError(!gCameraHal, INVALID_OPERATION, "camera hal is NULL.");
+    CheckCameraId(camera_id, BAD_VALUE);
+    CheckError(!buffer, BAD_VALUE, "buffer is NULL.");
+    CheckError(buffer->s.memType != V4L2_MEMORY_MMAP, BAD_VALUE, "memory type %d is not supported.", buffer->s.memType);
+
+    return gCameraHal->deviceAllocateMemory(camera_id, buffer);
+}
+
+/**
+ * Queue a buffer to a stream (deprecated)
+ *
+ * \param camera_id The camera ID that opened before
+ * \param stream_id the stream ID that add to device before
+ * \param camera_buff stream buff
+ *
+ * \return error code
+ **/
+int camera_stream_qbuf(int camera_id, int stream_id, camera_buffer_t *buffer,
+                       int num_buffers, const Parameters* settings)
+{
+    PERF_CAMERA_ATRACE();
+    HAL_TRACE_CALL(2);
+    CheckError(!gCameraHal, INVALID_OPERATION, "camera hal is NULL.");
+    CheckCameraId(camera_id, BAD_VALUE);
+
+    LOGW("camera_stream_qbuf(cam_id, stream_id, *buffer, num_buffers, *settings) is deprecated and will be removed soon.");
+    LOGW("Please start to use camera_stream_qbuf(cam_id, **buffer, num_buffers, *settings)");
+
+    return gCameraHal->streamQbuf(camera_id, &buffer, num_buffers, settings);
+}
+
+/**
+ * Queue a buffer(or more buffers) to a stream
+ *
+ * \param camera_id The camera ID that opened before
+ * \param buffer The array of pointers to the camera_buffer_t
+ * \param num_buffers The number of buffers in the array
+ *
+ * \return error code
+ **/
+int camera_stream_qbuf(int camera_id, camera_buffer_t **buffer,
+                       int num_buffers, const Parameters* settings)
+{
+    PERF_CAMERA_ATRACE();
+    HAL_TRACE_CALL(2);
+    CheckError(!gCameraHal, INVALID_OPERATION, "camera hal is NULL.");
+    CheckCameraId(camera_id, BAD_VALUE);
+
+    return gCameraHal->streamQbuf(camera_id, buffer, num_buffers, settings);
+}
+
+/**
+ * Dequeue a buffer from a stream
+ *
+ * \param camera_id The camera ID that opened before
+ * \param stream_id the stream ID that add to device before
+ * \param camera_buff stream buff
+ *
+ * \return error code
+ **/
+int camera_stream_dqbuf(int camera_id, int stream_id, camera_buffer_t **buffer,
+                        Parameters* settings)
+{
+    PERF_CAMERA_ATRACE();
+    HAL_TRACE_CALL(2);
+    CheckError(!gCameraHal, INVALID_OPERATION, "camera hal is NULL.");
+    CheckCameraId(camera_id, BAD_VALUE);
+    CheckError(!buffer, BAD_VALUE, "camera stream buffer is null.");
+
+    return gCameraHal->streamDqbuf(camera_id, stream_id, buffer, settings);
+}
+
+int camera_set_parameters(int camera_id, const Parameters& param)
+{
+    HAL_TRACE_CALL(2);
+    CheckCameraId(camera_id, BAD_VALUE);
+    CheckError(!gCameraHal, INVALID_OPERATION, "camera device is not open before setting parameters.");
+
+    return gCameraHal->setParameters(camera_id, param);
+}
+
+int camera_get_parameters(int camera_id, Parameters& param, long sequence)
+{
+    HAL_TRACE_CALL(2);
+    CheckCameraId(camera_id, BAD_VALUE);
+    CheckError(!gCameraHal, INVALID_OPERATION, "camera device is not open before getting parameters.");
+
+    return gCameraHal->getParameters(camera_id, param, sequence);
+}
+
+int get_frame_size(int camera_id, int format, int width, int height, int field, int *bpp)
+{
+    CheckError(width <= 0, BAD_VALUE, "width <=0");
+    CheckError(height <= 0, BAD_VALUE, "height <=0");
+    CheckError(field < 0, BAD_VALUE, "field <0");
+
+    int frameSize = 0;
+    bool isOFSCompression = PlatformData::getOFSCompression(camera_id);
+
+   *bpp = CameraUtils::getBpp(format);
+    if(isOFSCompression) {
+        frameSize = CameraUtils::getFrameSize(format, width, height, false, true, true);
+    } else {
+        frameSize = CameraUtils::getFrameSize(format, width, height);
+    }
+    LOG2("@%s: output compression frame: %d, frame size from HAL:%d\n",
+               __func__, isOFSCompression, frameSize);
+
+    return frameSize;
+}
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/image_process/IImageProcessor.h b/camera/hal/intel/ipu6/src/image_process/IImageProcessor.h
new file mode 100644
index 000000000000..c4b2d2fb8603
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/image_process/IImageProcessor.h
@@ -0,0 +1,46 @@
+/*
+ * Copyright (C) 2019 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "iutils/Utils.h"
+#include "iutils/Errors.h"
+#include "Camera3Buffer.h"
+#include "ProcessType.h"
+
+namespace icamera {
+
+class IImageProcessor {
+public:
+    IImageProcessor() {};
+    virtual ~IImageProcessor() {};
+
+    static std::unique_ptr<IImageProcessor> createImageProcessor();
+    static bool isProcessingTypeSupported(PostProcessType type);
+
+    virtual status_t cropFrame(const std::shared_ptr<camera3::Camera3Buffer> &input,
+                               std::shared_ptr<camera3::Camera3Buffer> &output) = 0;
+    virtual status_t scaleFrame(const std::shared_ptr<camera3::Camera3Buffer> &input,
+                                std::shared_ptr<camera3::Camera3Buffer> &output) = 0;
+    virtual status_t rotateFrame(const std::shared_ptr<camera3::Camera3Buffer> &input,
+                                 std::shared_ptr<camera3::Camera3Buffer> &output,
+                                 int angle, std::vector<uint8_t> &rotateBuf) = 0;
+    virtual status_t convertFrame(const std::shared_ptr<camera3::Camera3Buffer> &input,
+                                  std::shared_ptr<camera3::Camera3Buffer> &output) = 0;
+private:
+    DISALLOW_COPY_AND_ASSIGN(IImageProcessor);
+};
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/image_process/ImageConverter.cpp b/camera/hal/intel/ipu6/src/image_process/ImageConverter.cpp
new file mode 100644
index 000000000000..8e600ba937de
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/image_process/ImageConverter.cpp
@@ -0,0 +1,799 @@
+/*
+ * Copyright (C) 2016-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "ColorConverter"
+
+#include <sys/types.h>
+#include <linux/videodev2.h>
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+#include "iutils/Errors.h"
+#include "ImageConverter.h"
+
+namespace icamera {
+namespace ImageConverter {
+
+void YUV420ToRGB565(int width, int height, void *src, void *dst)
+{
+    int line, col, linewidth;
+    int y, u, v, yy, vr, ug, vg, ub;
+    int r, g, b;
+    const unsigned char *py, *pu, *pv;
+    unsigned short *rgbs = (unsigned short *) dst;
+
+    linewidth = width >> 1;
+    py = (unsigned char *) src;
+    pu = py + (width * height);
+    pv = pu + (width * height) / 4;
+
+    y = *py++;
+    yy = y << 8;
+    u = *pu - 128;
+    ug = 88 * u;
+    ub = 454 * u;
+    v = *pv - 128;
+    vg = 183 * v;
+    vr = 359 * v;
+
+    for (line = 0; line < height; line++) {
+        for (col = 0; col < width; col++) {
+            r = (yy + vr) >> 8;
+            g = (yy - ug - vg) >> 8;
+            b = (yy + ub ) >> 8;
+            if (r < 0) r = 0;
+            if (r > 255) r = 255;
+            if (g < 0) g = 0;
+            if (g > 255) g = 255;
+            if (b < 0) b = 0;
+            if (b > 255) b = 255;
+            *rgbs++ = (((unsigned short)r>>3)<<11) | (((unsigned short)g>>2)<<5)
+                   | (((unsigned short)b>>3)<<0);
+
+            y = *py++;
+            yy = y << 8;
+            if (col & 1) {
+                pu++;
+                pv++;
+                u = *pu - 128;
+                ug = 88 * u;
+                ub = 454 * u;
+                v = *pv - 128;
+                vg = 183 * v;
+                vr = 359 * v;
+            }
+        }
+        if ((line & 1) == 0) {
+            pu -= linewidth;
+            pv -= linewidth;
+        }
+    }
+}
+
+void trimConvertNV12ToRGB565(int width, int height, int srcStride, void *src, void *dst)
+{
+
+    unsigned char *yuvs = (unsigned char *) src;
+    unsigned char *rgbs = (unsigned char *) dst;
+
+    //the end of the luminance data
+    int lumEnd = srcStride * height;
+    int i = 0, j = 0;
+    for( i=0; i < height; i++) {
+        //points to the next luminance value pair
+        int lumPtr = i * srcStride;
+        //points to the next chromiance value pair
+        int chrPtr = i / 2 * srcStride + lumEnd;
+        for ( j=0; j < width; j+=2 ) {
+            //read the luminance and chromiance values
+            int Y1 = yuvs[lumPtr++] & 0xff;
+            int Y2 = yuvs[lumPtr++] & 0xff;
+            int Cb = (yuvs[chrPtr++] & 0xff) - 128;
+            int Cr = (yuvs[chrPtr++] & 0xff) - 128;
+            int R, G, B;
+
+            //generate first RGB components
+            B = Y1 + ((454 * Cb) >> 8);
+            if(B < 0) B = 0; else if(B > 255) B = 255;
+            G = Y1 - ((88 * Cb + 183 * Cr) >> 8);
+            if(G < 0) G = 0; else if(G > 255) G = 255;
+            R = Y1 + ((359 * Cr) >> 8);
+            if(R < 0) R = 0; else if(R > 255) R = 255;
+            //NOTE: this assume little-endian encoding
+            *rgbs++ = (unsigned char) (((G & 0x3c) << 3) | (B >> 3));
+            *rgbs++ = (unsigned char) ((R & 0xf8) | (G >> 5));
+
+            //generate second RGB components
+            B = Y2 + ((454 * Cb) >> 8);
+            if(B < 0) B = 0; else if(B > 255) B = 255;
+            G = Y2 - ((88 * Cb + 183 * Cr) >> 8);
+            if(G < 0) G = 0; else if(G > 255) G = 255;
+            R = Y2 + ((359 * Cr) >> 8);
+            if(R < 0) R = 0; else if(R > 255) R = 255;
+            //NOTE: this assume little-endian encoding
+            *rgbs++ = (unsigned char) (((G & 0x3c) << 3) | (B >> 3));
+            *rgbs++ = (unsigned char) ((R & 0xf8) | (G >> 5));
+        }
+    }
+}
+
+// covert YV12 (Y plane, V plane, U plane) to NV21 (Y plane, interlaced VU bytes)
+void convertYV12ToNV21(int width, int height, int srcStride, int dstStride, void *src, void *dst)
+{
+    const int cStride = srcStride>>1;
+    const int vuStride = dstStride;
+    const int hhalf = height>>1;
+    const int whalf = width>>1;
+
+    // copy the entire Y plane
+    unsigned char *srcPtr = (unsigned char *)src;
+    unsigned char *dstPtr = (unsigned char *)dst;
+    if (srcStride == dstStride) {
+        MEMCPY_S(dstPtr, dstStride*height, srcPtr, dstStride*height);
+    } else {
+        for (int i = 0; i < height; i++) {
+            MEMCPY_S(dstPtr, width, srcPtr, width);
+            srcPtr += srcStride;
+            dstPtr += dstStride;
+        }
+    }
+
+    // interlace the VU data
+    unsigned char *srcPtrV = (unsigned char *)src + height*srcStride;
+    unsigned char *srcPtrU = srcPtrV + cStride*hhalf;
+    dstPtr = (unsigned char *)dst + dstStride*height;
+    for (int i = 0; i < hhalf; ++i) {
+        unsigned char *pDstVU = dstPtr;
+        unsigned char *pSrcV = srcPtrV;
+        unsigned char *pSrcU = srcPtrU;
+        for (int j = 0; j < whalf; ++j) {
+            *pDstVU ++ = *pSrcV ++;
+            *pDstVU ++ = *pSrcU ++;
+        }
+        dstPtr += vuStride;
+        srcPtrV += cStride;
+        srcPtrU += cStride;
+    }
+}
+
+// copy YV12 to YV12 (Y plane, V plan, U plan) in case of different stride length
+void copyYV12ToYV12(int width, int height, int srcStride, int dstStride, void *src, void *dst)
+{
+    // copy the entire Y plane
+    if (srcStride == dstStride) {
+        MEMCPY_S(dst, dstStride * height, src, dstStride * height);
+    } else {
+        unsigned char *srcPtrY = (unsigned char *)src;
+        unsigned char *dstPtrY = (unsigned char *)dst;
+        for (int i = 0; i < height; i ++) {
+            MEMCPY_S(dstPtrY, width, srcPtrY, width);
+            srcPtrY += srcStride;
+            dstPtrY += dstStride;
+        }
+    }
+
+    // copy VU plane
+    const int scStride = srcStride >> 1;
+    const int dcStride = ALIGN_16(dstStride >> 1); // Android CTS required: U/V plane needs 16 bytes aligned!
+    if (dcStride == scStride) {
+        unsigned char *srcPtrVU = (unsigned char *)src + height * srcStride;
+        unsigned char *dstPtrVU = (unsigned char *)dst + height * dstStride;
+        MEMCPY_S(dstPtrVU, height * dcStride, srcPtrVU, height * dcStride);
+    } else {
+        const int wHalf = width >> 1;
+        const int hHalf = height >> 1;
+        unsigned char *srcPtrV = (unsigned char *)src + height * srcStride;
+        unsigned char *srcPtrU = srcPtrV + scStride * hHalf;
+        unsigned char *dstPtrV = (unsigned char *)dst + height * dstStride;
+        unsigned char *dstPtrU = dstPtrV + dcStride * hHalf;
+        for (int i = 0; i < hHalf; i ++) {
+            MEMCPY_S(dstPtrU, wHalf, srcPtrU, wHalf);
+            MEMCPY_S(dstPtrV, wHalf, srcPtrV, wHalf);
+            dstPtrU += dcStride, srcPtrU += scStride;
+            dstPtrV += dcStride, srcPtrV += scStride;
+        }
+    }
+}
+
+// covert NV12 (Y plane, interlaced UV bytes) to
+// NV21 (Y plane, interlaced VU bytes) and trim stride width to real width
+void trimConvertNV12ToNV21(int width, int height, int srcStride, void *src, void *dst)
+{
+    const int ysize = width * height;
+    unsigned const char *pSrc = (unsigned char *)src;
+    unsigned char *pDst = (unsigned char *)dst;
+
+    // Copy Y component
+    if (srcStride == width) {
+        MEMCPY_S(pDst, ysize, pSrc, ysize);
+    } else if (srcStride > width) {
+        int j = height;
+        while(j--) {
+            MEMCPY_S(pDst, width, pSrc, width);
+            pSrc += srcStride;
+            pDst += width;
+        }
+    } else {
+        ALOGE("bad stride value");
+        return;
+    }
+
+    // Convert UV to VU
+    pSrc = (unsigned char *)src + srcStride * height;
+    pDst = (unsigned char *)dst + width * height;
+    for (int j = 0; j < height / 2; j++) {
+        if (width >= 16) {
+            const uint32_t *ptr0 = (const uint32_t *)(pSrc);
+            uint32_t *ptr1 = (uint32_t *)(pDst);
+            int bNotLastLine = ((j+1) == (height/2)) ? 0 : 1;
+            int width_16 = (width + 15 * bNotLastLine) & ~0xf;
+            if ((((uint64_t)(pSrc)) & 0xf) == 0 && (((uint64_t)(pDst)) & 0xf) == 0) { // 16 bytes aligned for both src and dest
+                __asm__ volatile(\
+                                 "movl       %0,  %%eax      \n\t"
+                                 "movl       %1,  %%edx      \n\t"
+                                 "movl       %2,  %%ecx      \n\t"
+                                 "1:     \n\t"
+                                 "movdqa (%%eax), %%xmm1     \n\t"
+                                 "movdqa  %%xmm1, %%xmm0     \n\t"
+                                 "psllw       $8, %%xmm1     \n\t"
+                                 "psrlw       $8, %%xmm0     \n\t"
+                                 "por     %%xmm0, %%xmm1     \n\t"
+                                 "movdqa  %%xmm1, (%%edx)    \n\t"
+                                 "add        $16, %%eax      \n\t"
+                                 "add        $16, %%edx      \n\t"
+                                 "sub        $16, %%ecx      \n\t"
+                                 "jnz   1b \n\t"
+                                 : "+m"(ptr0), "+m"(ptr1), "+m"(width_16)
+                                 :
+                                 : "eax", "ecx", "edx", "xmm0", "xmm1"
+                                );
+            }
+            else { // either src or dest is not 16-bytes aligned
+                __asm__ volatile(\
+                                 "movl       %0,  %%eax      \n\t"
+                                 "movl       %1,  %%edx      \n\t"
+                                 "movl       %2,  %%ecx      \n\t"
+                                 "1:     \n\t"
+                                 "lddqu  (%%eax), %%xmm1     \n\t"
+                                 "movdqa  %%xmm1, %%xmm0     \n\t"
+                                 "psllw       $8, %%xmm1     \n\t"
+                                 "psrlw       $8, %%xmm0     \n\t"
+                                 "por     %%xmm0, %%xmm1     \n\t"
+                                 "movdqu  %%xmm1, (%%edx)    \n\t"
+                                 "add        $16, %%eax      \n\t"
+                                 "add        $16, %%edx      \n\t"
+                                 "sub        $16, %%ecx      \n\t"
+                                 "jnz   1b \n\t"
+                                 : "+m"(ptr0), "+m"(ptr1), "+m"(width_16)
+                                 :
+                                 : "eax", "ecx", "edx", "xmm0", "xmm1"
+                                );
+            }
+
+            // process remaining data of less than 16 bytes of last row
+            for (int i = width_16; i < width; i += 2) {
+                pDst[i] = pSrc[i + 1];
+                pDst[i + 1] = pSrc[i];
+            }
+        }
+        else if ((((uint64_t)(pSrc)) & 0x3) == 0 && (((uint64_t)(pDst)) & 0x3) == 0){  // 4 bytes aligned for both src and dest
+            const uint32_t *ptr0 = (const uint32_t *)(pSrc);
+            uint32_t *ptr1 = (uint32_t *)(pDst);
+            int width_4 = width & ~3;
+            for (int i = 0; i < width_4; i += 4) {
+                uint32_t data0 = *ptr0++;
+                uint32_t data1 = (data0 >> 8) & 0x00ff00ff;
+                uint32_t data2 = (data0 << 8) & 0xff00ff00;
+                *ptr1++ = data1 | data2;
+            }
+            // process remaining data of less than 4 bytes at end of each row
+            for (int i = width_4; i < width; i += 2) {
+                pDst[i] = pSrc[i + 1];
+                pDst[i + 1] = pSrc[i];
+            }
+        }
+        else {
+            unsigned const char *ptr0 = pSrc;
+            unsigned char *ptr1 = pDst;
+            for (int i = 0; i < width; i += 2) {
+                *ptr1++ = ptr0[1];
+                *ptr1++ = ptr0[0];
+                ptr0 += 2;
+            }
+        }
+        pDst += width;
+        pSrc += srcStride;
+    }
+}
+
+// convert NV12 (Y plane, interlaced UV bytes) to YV12 (Y plane, V plane, U plane)
+// without Y and C 16 bytes aligned
+void convertNV12ToYV12(int width, int height, int srcStride, void *src, void *dst)
+{
+    int yStride = width;
+    size_t ySize = yStride * height;
+    int cStride = yStride/2;
+    size_t cSize = cStride * height/2;
+
+    unsigned char *srcPtr = (unsigned char *) src;
+    unsigned char *dstPtr = (unsigned char *) dst;
+    unsigned char *dstPtrV = (unsigned char *) dst + ySize;
+    unsigned char *dstPtrU = (unsigned char *) dst + ySize + cSize;
+
+    // copy the entire Y plane
+    if (srcStride == yStride) {
+        MEMCPY_S(dstPtr, ySize, srcPtr, ySize);
+        srcPtr += ySize;
+    } else if (srcStride > width) {
+        for (int i = 0; i < height; i++) {
+            MEMCPY_S(dstPtr, width, srcPtr, width);
+            srcPtr += srcStride;
+            dstPtr += yStride;
+        }
+    } else {
+        ALOGE("bad src stride value");
+        return;
+    }
+
+    // deinterlace the UV data
+    int halfHeight = height / 2;
+    int halfWidth = width / 2;
+    for ( int i = 0; i < halfHeight; ++i) {
+        for ( int j = 0; j < halfWidth; ++j) {
+            dstPtrV[j] = srcPtr[j * 2 + 1];
+            dstPtrU[j] = srcPtr[j * 2];
+        }
+        srcPtr += srcStride;
+        dstPtrV += cStride;
+        dstPtrU += cStride;
+    }
+}
+
+// convert NV12 (Y plane, interlaced UV bytes) to YV12 (Y plane, V plane, U plane)
+// with Y and C 16 bytes aligned
+void align16ConvertNV12ToYV12(int width, int height, int srcStride, void *src, void *dst)
+{
+    int yStride = ALIGN_16(width);
+    size_t ySize = yStride * height;
+    int cStride = ALIGN_16(yStride/2);
+    size_t cSize = cStride * height/2;
+
+    unsigned char *srcPtr = (unsigned char *) src;
+    unsigned char *dstPtr = (unsigned char *) dst;
+    unsigned char *dstPtrV = (unsigned char *) dst + ySize;
+    unsigned char *dstPtrU = (unsigned char *) dst + ySize + cSize;
+
+    // copy the entire Y plane
+    if (srcStride == yStride) {
+        MEMCPY_S(dstPtr, ySize, srcPtr, ySize);
+        srcPtr += ySize;
+    } else if (srcStride > width) {
+        for (int i = 0; i < height; i++) {
+            MEMCPY_S(dstPtr, width, srcPtr, width);
+            srcPtr += srcStride;
+            dstPtr += yStride;
+        }
+    } else {
+        ALOGE("bad src stride value");
+        return;
+    }
+
+    // deinterlace the UV data
+    for ( int i = 0; i < height / 2; ++i) {
+        for ( int j = 0; j < width / 2; ++j) {
+            dstPtrV[j] = srcPtr[j * 2 + 1];
+            dstPtrU[j] = srcPtr[j * 2];
+        }
+        srcPtr += srcStride;
+        dstPtrV += cStride;
+        dstPtrU += cStride;
+    }
+}
+
+// P411's Y, U, V are seperated. But the YUY2's Y, U and V are interleaved.
+void YUY2ToP411(int width, int height, int stride, void *src, void *dst)
+{
+    int ySize = width * height;
+    int cSize = width * height / 4;
+    int wHalf = width >> 1;
+
+    unsigned char *srcPtr = (unsigned char *) src;
+    unsigned char *dstPtr = (unsigned char *) dst;
+    unsigned char *dstPtrU = (unsigned char *) dst + ySize;
+    unsigned char *dstPtrV = (unsigned char *) dst + ySize + cSize;
+
+    for (int i = 0; i < height; i++) {
+        //The first line of the source
+        //Copy first Y Plane first
+        for (int j=0; j < width; j++) {
+            dstPtr[j] = srcPtr[j*2];
+        }
+
+        if (i & 1) {
+            //Copy the V plane
+            for (int k = 0; k < wHalf; k++) {
+                dstPtrV[k] = srcPtr[k * 4 + 3];
+            }
+            dstPtrV = dstPtrV + wHalf;
+        } else {
+            //Copy the U plane
+            for (int k = 0; k< wHalf; k++) {
+                dstPtrU[k] = srcPtr[k * 4 + 1];
+            }
+            dstPtrU = dstPtrU + wHalf;
+        }
+
+        srcPtr = srcPtr + stride * 2;
+        dstPtr = dstPtr + width;
+    }
+}
+
+// P411's Y, U, V are separated. But the NV12's U and V are interleaved.
+void NV12ToP411Separate(int width, int height, int stride,
+                                void *srcY, void *srcUV, void *dst)
+{
+    int i, j, p, q;
+    unsigned char *psrcY = (unsigned char *) srcY;
+    unsigned char *pdstY = (unsigned char *) dst;
+    unsigned char *pdstU, *pdstV;
+    unsigned char *psrcUV;
+
+    // copy Y data
+    for (i = 0; i < height; i++) {
+        MEMCPY_S(pdstY, width, psrcY, width);
+        pdstY += width;
+        psrcY += stride;
+    }
+
+    // copy U data and V data
+    psrcUV = (unsigned char *)srcUV;
+    pdstU = (unsigned char *)dst + width * height;
+    pdstV = pdstU + width * height / 4;
+    p = q = 0;
+    for (i = 0; i < height / 2; i++) {
+        for (j = 0; j < width; j++) {
+            if (j % 2 == 0) {
+                pdstU[p] = (psrcUV[i * stride + j] & 0xFF);
+                p++;
+           } else {
+                pdstV[q] = (psrcUV[i * stride + j] & 0xFF);
+                q++;
+            }
+        }
+    }
+}
+
+// P411's Y, U, V are seperated. But the NV12's U and V are interleaved.
+void NV12ToP411(int width, int height, int stride, void *src, void *dst)
+{
+    NV12ToP411Separate(width, height, stride,
+                    src, (void *)((unsigned char *)src + width * height), dst);
+}
+
+// P411's Y, U, V are separated. But the NV21's U and V are interleaved.
+void NV21ToP411Separate(int width, int height, int stride,
+                        void *srcY, void *srcUV, void *dst)
+{
+    int i, j, p, q;
+    unsigned char *psrcY = (unsigned char *) srcY;
+    unsigned char *pdstY = (unsigned char *) dst;
+    unsigned char *pdstU, *pdstV;
+    unsigned char *psrcUV;
+
+    // copy Y data
+    for (i = 0; i < height; i++) {
+        MEMCPY_S(pdstY, width, psrcY, width);
+        pdstY += width;
+        psrcY += stride;
+    }
+
+    // copy U data and V data
+    psrcUV = (unsigned char *)srcUV;
+    pdstU = (unsigned char *)dst + width * height;
+    pdstV = pdstU + width * height / 4;
+    p = q = 0;
+    for (i = 0; i < height / 2; i++) {
+        for (j = 0; j < width; j++) {
+            if ((j & 1) == 0) {
+                pdstV[p] = (psrcUV[i * stride + j] & 0xFF);
+                p++;
+           } else {
+                pdstU[q] = (psrcUV[i * stride + j] & 0xFF);
+                q++;
+            }
+        }
+    }
+}
+
+// P411's Y, U, V are seperated. But the NV21's U and V are interleaved.
+void NV21ToP411(int width, int height, int stride, void *src, void *dst)
+{
+    NV21ToP411Separate(width, height, stride,
+                       src, (void *)((unsigned char *)src + width * height), dst);
+}
+
+// IMC3 Y, U, V are separated,the stride for U/V is the same as Y.
+// about IMC3 detail, please refer to http://www.fourcc.org/yuv.php
+// But the NV12's U and V are interleaved.
+void NV12ToIMC3(int width, int height, int stride, void *srcY, void *srcUV, void *dst)
+{
+    int i, j, p, q;
+    unsigned char *pdstU, *pdstV;
+    unsigned char *psrcUV;
+
+    // copy Y data even with stride
+    MEMCPY_S(dst, stride * height, srcY, stride * height);
+    // copy U data and V data
+    psrcUV = (unsigned char *)srcUV;
+    pdstU = (unsigned char *)dst + stride * height;
+    pdstV = pdstU + stride * height / 2;
+    p = q = 0;
+    for (i = 0; i < height / 2; i++) {
+        for (j = 0; j < width; j++) {
+            if (j % 2 == 0) {
+                pdstU[p]= (psrcUV[i * stride + j] & 0xFF) ;
+                p++;
+           } else {
+                pdstV[q]= (psrcUV[i * stride + j] & 0xFF);
+                q++;
+            }
+        }
+        p += stride - width/2;
+        q += stride - width/2;
+    }
+}
+
+// IMC1 Y, V,U are separated,the stride for U/V is the same as Y.
+// IMC's V is before U
+// But the NV12's U and V are interleaved.
+void NV12ToIMC1(int width, int height, int stride, void *srcY, void *srcUV, void *dst)
+{
+    int i, j, p, q;
+    unsigned char *pdstU, *pdstV;
+    unsigned char *psrcUV;
+
+    // copy Y data even with stride
+    MEMCPY_S(dst, stride * height, srcY, stride * height);
+    // copy U data and V data
+    psrcUV = (unsigned char *)srcUV;
+    pdstV = (unsigned char *)dst + stride * height;
+    pdstU = pdstV + stride * height / 2;
+    p = q = 0;
+    for (i = 0; i < height / 2; i++) {
+        for (j = 0; j < width; j++) {
+            if (j % 2 == 0) {
+                pdstU[p]= (psrcUV[i * stride + j] & 0xFF) ;
+                p++;
+           } else {
+                pdstV[q]= (psrcUV[i * stride + j] & 0xFF);
+                q++;
+            }
+        }
+        p += stride - width/2;
+        q += stride - width/2;
+    }
+}
+
+// Re-pad YUV420 format image, the format can be YV12, YU12 or YUV420 planar.
+// If buffer size: (height*dstStride*1.5) > (height*srcStride*1.5), src and dst
+// buffer start addresses are same, the re-padding can be done inplace.
+void repadYUV420(int width, int height, int srcStride, int dstStride, void *src, void *dst)
+{
+    unsigned char *dptr;
+    unsigned char *sptr;
+    void * (*myCopy)(void *dst, const void *src, size_t n);
+
+    const int whalf = width >> 1;
+    const int hhalf = height >> 1;
+    const int scStride = srcStride >> 1;
+    const int dcStride = dstStride >> 1;
+    const int sySize = height * srcStride;
+    const int dySize = height * dstStride;
+    const int scSize = hhalf * scStride;
+    const int dcSize = hhalf * dcStride;
+
+    // directly copy, if (srcStride == dstStride)
+    if (srcStride == dstStride) {
+        MEMCPY_S(dst, dySize + 2*dcSize, src, dySize + 2*dcSize);
+        return;
+    }
+
+    // copy V(YV12 case) or U(YU12 case) plane line by line
+    sptr = (unsigned char *)src + sySize + 2*scSize - scStride;
+    dptr = (unsigned char *)dst + dySize + 2*dcSize - dcStride;
+
+    // try to avoid overlapped memcpy()
+    myCopy = (abs(sptr -dptr) > dstStride) ? memcpy : memmove;
+
+    for (int i = 0; i < hhalf; i ++) {
+        myCopy(dptr, sptr, whalf);
+        sptr -= scStride;
+        dptr -= dcStride;
+    }
+
+    // copy  V(YV12 case) or U(YU12 case) U/V plane line by line
+    sptr = (unsigned char *)src + sySize + scSize - scStride;
+    dptr = (unsigned char *)dst + dySize + dcSize - dcStride;
+    for (int i = 0; i < hhalf; i ++) {
+        myCopy(dptr, sptr, whalf);
+        sptr -= scStride;
+        dptr -= dcStride;
+    }
+
+    // copy Y plane line by line
+    sptr = (unsigned char *)src + sySize - srcStride;
+    dptr = (unsigned char *)dst + dySize - dstStride;
+    for (int i = 0; i < height; i ++) {
+        myCopy(dptr, sptr, width);
+        sptr -= srcStride;
+        dptr -= dstStride;
+    }
+}
+
+// covert YUYV(YUY2, YUV422 format) to YV12 (Y plane, V plane, U plane)
+void convertYUYVToYV12(int width, int height, int srcStride, int dstStride, void *src, void *dst)
+{
+    int ySize = width * height;
+    int cSize = ALIGN_16(dstStride/2) * height / 2;
+    int wHalf = width >> 1;
+
+    unsigned char *srcPtr = (unsigned char *) src;
+    unsigned char *dstPtr = (unsigned char *) dst;
+    unsigned char *dstPtrV = (unsigned char *) dst + ySize;
+    unsigned char *dstPtrU = (unsigned char *) dst + ySize + cSize;
+
+    for (int i = 0; i < height; i++) {
+        //The first line of the source
+        //Copy first Y Plane first
+        for (int j=0; j < width; j++) {
+            dstPtr[j] = srcPtr[j*2];
+        }
+
+        if (i & 1) {
+            //Copy the V plane
+            for (int k = 0; k< wHalf; k++) {
+                dstPtrV[k] = srcPtr[k * 4 + 3];
+            }
+            dstPtrV = dstPtrV + ALIGN_16(dstStride>>1);
+        } else {
+            //Copy the U plane
+            for (int k = 0; k< wHalf; k++) {
+                dstPtrU[k] = srcPtr[k * 4 + 1];
+            }
+            dstPtrU = dstPtrU + ALIGN_16(dstStride>>1);
+        }
+
+        srcPtr = srcPtr + srcStride * 2;
+        dstPtr = dstPtr + width;
+    }
+}
+
+// covert YUYV(YUY2, YUV422 format) to NV21 (Y plane, interlaced VU bytes)
+void convertYUYVToNV21(int width, int height, int srcStride, void *src, void *dst)
+{
+    int ySize = width * height;
+    int u_counter=1, v_counter=0;
+
+    unsigned char *srcPtr = (unsigned char *) src;
+    unsigned char *dstPtr = (unsigned char *) dst;
+    unsigned char *dstPtrUV = (unsigned char *) dst + ySize;
+
+    for (int i=0; i < height; i++) {
+        //The first line of the source
+        //Copy first Y Plane first
+        for (int j=0; j < width * 2; j++) {
+            if (j % 2 == 0)
+                dstPtr[j/2] = srcPtr[j];
+            if (i%2) {
+                if (( j % 4 ) == 3) {
+                    dstPtrUV[v_counter] = srcPtr[j]; //V plane
+                    v_counter += 2;
+                }
+                if (( j % 4 ) == 1) {
+                    dstPtrUV[u_counter] = srcPtr[j]; //U plane
+                    u_counter += 2;
+                }
+            }
+        }
+
+        srcPtr = srcPtr + srcStride * 2;
+        dstPtr = dstPtr + width;
+    }
+}
+
+void convertNV12ToYUYV(int srcWidth, int srcHeight, int srcStride, int dstStride, const void *src, void *dst)
+{
+    int y_counter = 0, u_counter = 1, v_counter = 3, uv_counter = 0;
+    unsigned char *srcYPtr = (unsigned char *) src;
+    unsigned char *srcUVPtr = (unsigned char *)src + srcWidth * srcHeight;
+    unsigned char *dstPtr = (unsigned char *) dst;
+
+    for (int i = 0; i < srcHeight; i++) {
+        for (int k = 0; k < srcWidth; k++) {
+                dstPtr[y_counter] = srcYPtr[k];
+                y_counter += 2;
+                dstPtr[u_counter] = srcUVPtr[uv_counter];
+                u_counter += 4;
+                dstPtr[v_counter] = srcUVPtr[uv_counter + 1];
+                v_counter += 4;
+                uv_counter += 2;
+        }
+        if ((i % 2) == 0) {
+            srcUVPtr = srcUVPtr + srcStride;
+        }
+
+        dstPtr = dstPtr + 2 * dstStride;
+        srcYPtr = srcYPtr + srcStride;
+        u_counter = 1;
+        v_counter = 3;
+        y_counter = 0;
+        uv_counter = 0;
+    }
+}
+
+void convertBuftoYV12(int format, int width, int height, int srcStride,
+                      int dstStride, void *src, void *dst, bool align16)
+{
+    switch (format) {
+    case V4L2_PIX_FMT_NV12:
+        align16 ? align16ConvertNV12ToYV12(width, height, srcStride, src, dst)
+            : convertNV12ToYV12(width, height, srcStride, src, dst);
+        break;
+    case V4L2_PIX_FMT_YVU420:
+        copyYV12ToYV12(width, height, srcStride, dstStride, src, dst);
+        break;
+    case V4L2_PIX_FMT_YUYV:
+        convertYUYVToYV12(width, height, srcStride, dstStride, src, dst);
+        break;
+    default:
+        ALOGE("%s: unsupported format %d", __func__, format);
+        break;
+    }
+}
+
+void convertBuftoNV21(int format, int width, int height, int srcStride,
+                      int dstStride, void *src, void *dst)
+{
+    switch (format) {
+    case V4L2_PIX_FMT_NV12:
+        trimConvertNV12ToNV21(width, height, srcStride, src, dst);
+        break;
+    case V4L2_PIX_FMT_YVU420:
+        convertYV12ToNV21(width, height, srcStride, dstStride, src, dst);
+        break;
+    case V4L2_PIX_FMT_YUYV:
+        convertYUYVToNV21(width, height, srcStride, src, dst);
+        break;
+    default:
+        ALOGE("%s: unsupported format %d", __func__, format);
+        break;
+    }
+}
+
+void convertBuftoYUYV(int format, int width, int height, int srcStride,
+                      int dstStride, void *src, void *dst)
+{
+    switch (format) {
+    case V4L2_PIX_FMT_NV12:
+        convertNV12ToYUYV(width, height, srcStride, dstStride, src, dst);
+        break;
+    default:
+        LOGE("%s: unsupported format %d", __func__, format);
+        break;
+    }
+}
+} // namespace ImageConverter
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/image_process/ImageConverter.h b/camera/hal/intel/ipu6/src/image_process/ImageConverter.h
new file mode 100644
index 000000000000..f55479a57b63
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/image_process/ImageConverter.h
@@ -0,0 +1,59 @@
+/*
+ * Copyright (C) 2016-2019 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+namespace icamera {
+namespace ImageConverter {
+
+void YUV420ToRGB565(int width, int height, void *src, void *dst);
+
+void trimConvertNV12ToRGB565(int width, int height, int srcStride, void *src, void *dst);
+
+void convertYV12ToNV21(int width, int height, int srcStride, int dstStride, void *src, void *dst);
+void copyYV12ToYV12(int width, int height, int srcStride, int dstStride, void *src, void *dst);
+
+void trimConvertNV12ToNV21(int width, int height, int srcStride, void *src, void *dst);
+
+void convertNV12ToYV12(int width, int height, int srcStride, void *src, void *dst);
+void align16ConvertNV12ToYV12(int width, int height, int srcStride, void *src, void *dst);
+
+void NV12ToP411(int width, int height, int stride, void *src, void *dst);
+void NV21ToP411(int width, int height, int stride, void *src, void *dst);
+void NV12ToP411Separate(int width, int height, int stride,
+                                    void *srcY, void *srcUV, void *dst);
+void NV21ToP411Separate(int width, int height, int stride,
+                                    void *srcY, void *srcUV, void *dst);
+
+void YUY2ToP411(int width, int height, int stride, void *src, void *dst);
+void NV12ToIMC3(int width, int height, int stride,void *srcY, void *srcUV, void *dst);
+void NV12ToIMC1(int width, int height, int stride, void *srcY, void *srcUV, void *dst);
+void convertYUYVToYV12(int width, int height, int srcStride, int dstStride, void *src, void *dst);
+
+void convertYUYVToNV21(int width, int height, int srcStride, void *src, void *dst);
+void convertNV12ToYUYV(int srcWidth, int srcHeight, int srcStride, int dstStride, const void *src, void *dst);
+
+void convertBuftoYV12(int format, int width, int height, int srcStride,
+                      int dstStride, void *src, void *dst, bool align16 = true);
+void convertBuftoNV21(int format, int width, int height, int srcStride,
+                      int dstStride, void *src, void *dst);
+void convertBuftoYUYV(int format, int width, int height, int srcStride,
+                      int dstStride, void *src, void *dst);
+
+void repadYUV420(int width, int height, int srcStride, int dstStride, void *src, void *dst);
+
+} // namespace ImageConverter
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/image_process/ImageScalerCore.cpp b/camera/hal/intel/ipu6/src/image_process/ImageScalerCore.cpp
new file mode 100644
index 000000000000..1ea6f7104266
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/image_process/ImageScalerCore.cpp
@@ -0,0 +1,959 @@
+/*
+ * Copyright (C) 2012-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "ImageScalerCore"
+
+#include <memory>
+#include <linux/videodev2.h>
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+#include "iutils/CameraLog.h"
+#include "ImageScalerCore.h"
+
+#define RESOLUTION_VGA_WIDTH    640
+#define RESOLUTION_VGA_HEIGHT   480
+#define RESOLUTION_QVGA_WIDTH   320
+#define RESOLUTION_QVGA_HEIGHT  240
+#define RESOLUTION_QCIF_WIDTH   176
+#define RESOLUTION_QCIF_HEIGHT  144
+#define MIN(a,b) ((a)<(b)?(a):(b))
+
+namespace icamera {
+
+void ImageScalerCore::downScaleImage(void *src, void *dest,
+    int dest_w, int dest_h, int dest_stride,
+    int src_w, int src_h, int src_stride,
+    int format, int src_skip_lines_top, // number of lines that are skipped from src image start pointer
+    int src_skip_lines_bottom) // number of lines that are skipped after reading src_h (should be set always to reach full image height)
+{
+    unsigned char *m_dest = (unsigned char *)dest;
+    const unsigned char * m_src = (const unsigned char *)src;
+    switch (format) {
+        case V4L2_PIX_FMT_NV21:
+        case V4L2_PIX_FMT_NV12: {
+            if ((dest_w == src_w && dest_h <= src_h) || (dest_w <= src_w && dest_h == src_h)) {
+                // trim only if only src_h is larger than dest_h or src_w is larger than dest_w
+                ImageScalerCore::trimNv12Image(m_dest, m_src,
+                                               dest_w, dest_h, dest_stride,
+                                               src_w, src_h, src_stride,
+                                               src_skip_lines_top, src_skip_lines_bottom);
+            } else {
+                // downscale & crop
+                ImageScalerCore::downScaleAndCropNv12Image(m_dest, m_src,
+                                                           dest_w, dest_h, dest_stride,
+                                                           src_w, src_h, src_stride,
+                                                           src_skip_lines_top, src_skip_lines_bottom);
+            }
+            break;
+        }
+        case V4L2_PIX_FMT_YUYV: {
+            ImageScalerCore::downScaleYUY2Image(m_dest, m_src,
+                                                dest_w, dest_h, dest_stride,
+                                                src_w, src_h, src_stride);
+            break;
+        }
+        default: {
+            LOGE("no downscale support for format = %d", format);
+            break;
+        }
+    }
+}
+
+void ImageScalerCore::downScaleYUY2Image(unsigned char *dest, const unsigned char *src,
+                                         const int dest_w, const int dest_h, const int dest_stride,
+                                         const int src_w, const int src_h, const int src_stride)
+{
+    if (dest==NULL || dest_w <=0 || dest_h <=0 || src==NULL || src_w <=0 || src_h <= 0 )
+        return;
+
+    if (dest_w%2 != 0) // if the dest_w is not an even number, exit
+        return;
+
+    const int scale_w = (src_w<<8) / dest_w; // scale factors
+    const int scale_h = (src_h<<8) / dest_h;
+    int macro_pixel_width = dest_w >> 1;
+    unsigned int val_1, val_2; // for bi-linear-interpolation
+    int i,j,k;
+
+    for(i=0; i < dest_h; ++i) {
+        int src_i = i * scale_h;
+        int dy = src_i & 0xff;
+        src_i >>= 8;
+        for(j=0; j < macro_pixel_width; ++j) {
+            int src_j = j * scale_w;
+            int dx = src_j & 0xff;
+            src_j = src_j >> 8;
+            for(k = 0; k < 4; ++k) {
+                // bi-linear-interpolation
+                if(dx == 0 && dy == 0) {
+                    dest[i * 2 * dest_stride + 4 * j + k] = src[src_i * 2 * src_stride + src_j * 4 + k];
+                } else if(dx == 0 && dy != 0){
+                    val_1 = (unsigned int)src[src_i * 2 * src_stride + src_j * 4 + k];
+                    val_2 = (unsigned int)src[(src_i + 1) * 2 * src_stride + src_j * 4 + k];
+                    val_1 = (val_1 * (256 - dy) + val_2 * dy) >> 8;
+                    dest[i * 2 * dest_stride + 4 * j + k] = ((val_1 <= 255) ? val_1: 255);
+                } else if(dx != 0 && dy == 0) {
+                    val_1 = ((unsigned int)src[src_i * 2 * src_stride + src_j * 4 + k] * (256 - dx)
+                        + (unsigned int)src[src_i * 2 * src_stride + (src_j +1) * 4 + k] * dx) >> 8;
+                    dest[i * 2 * dest_stride + 4 * j + k] = ((val_1 <= 255) ? val_1: 255);
+                } else {
+                    val_1 = ((unsigned int)src[src_i * 2 * src_stride + src_j * 4 + k] * (256 - dx)
+                        + (unsigned int)src[src_i * 2 * src_stride + (src_j +1) * 4 + k] * dx) >> 8;
+                    val_2 = ((unsigned int)src[(src_i + 1) * 2 * src_stride + src_j * 4 + k] * (256 - dx)
+                        + (unsigned int)src[(src_i + 1) * 2 * src_stride + (src_j+1) * 4 + k] * dx) >> 8;
+                    val_1 = (val_1 * (256 - dy) + val_2 * dy) >> 8;
+                    dest[i * 2 * dest_stride + 4 * j + k] = ((val_1 <= 255) ? val_1: 255);
+                }
+            }
+        }
+    }
+}
+
+void ImageScalerCore::trimNv12Image(unsigned char *dest, const unsigned char *src,
+                                    const int dest_w, const int dest_h, const int dest_stride,
+                                    const int src_w, const int src_h, const int src_stride,
+                                    const int src_skip_lines_top, // number of lines that are skipped from src image start pointer
+                                    const int src_skip_lines_bottom) // number of lines that are skipped after reading src_h (should be set always to reach full image height)
+{
+    LOG1("@%s: dest_w: %d, dest_h: %d, dest_stride:%d, src_w: %d, src_h: %d, src_stride: %d, skip_top: %d, skip_bottom: %d",
+         __func__, dest_w,dest_h,dest_stride,src_w,src_h,src_stride,src_skip_lines_top,src_skip_lines_bottom);
+
+    const unsigned char *y = src;
+    const unsigned char *uv = src + src_h * src_stride;
+    if (dest_w < src_w) {
+        /*
+         *                     src_w
+         *  y    ---------------------------------
+         *       -    -        dest_w       -    -
+         *       -    -                     -    -
+         *       -    -                     -    -
+         * src_h -    - dest_h              -    -
+         *       -    -                     -    -
+         *       -    -                     -    -
+         *       -    -                     -    -
+         *       ---------------------------------
+         *
+         *                     src_w
+         *  uv   ---------------------------------
+         *       -    -        dest_w       -    -
+         * src_h -    - dest_h              -    -
+         *       -    -                     -    -
+         *       ---------------------------------
+         */
+        y += (src_w - dest_w) / 2;
+        uv += (src_w - dest_w) / 2;
+    } else if (dest_h < src_h) {
+        /*
+         *                     src_w
+         *  y    ---------------------------------
+         *       -             dest_w            -
+         *       ---------------------------------
+         *       -                               -
+         * src_h - dest_h                        -
+         *       -                               -
+         *       ---------------------------------
+         *       -                               -
+         *       ---------------------------------
+         *
+         *                     src_w
+         *  uv   ---------------------------------
+         *       ---------------------------------
+         * src_h - dest_h      dest_w            -
+         *       ---------------------------------
+         *       ---------------------------------
+         */
+        y += (src_h - dest_h) * src_stride / 2;
+        uv += (src_h - dest_h) * src_stride / 4;
+    }
+
+    // Y
+    for (int i = 0; i < dest_h; i++) {
+        MEMCPY_S(dest, src_stride, y, dest_stride);
+        dest += dest_stride;
+        y += src_stride;
+    }
+
+    // UV
+    for (int i = 0; i < dest_h / 2; i++) {
+        MEMCPY_S(dest, src_stride, uv, dest_stride);
+        dest += dest_stride;
+        uv += src_stride;
+    }
+}
+
+// VGA-QCIF begin (Enzo specific)
+void ImageScalerCore::downScaleAndCropNv12Image(unsigned char *dest, const unsigned char *src,
+                                                const int dest_w, const int dest_h, const int dest_stride,
+                                                const int src_w, const int src_h, const int src_stride,
+                                                const int src_skip_lines_top, // number of lines that are skipped from src image start pointer
+                                                const int src_skip_lines_bottom) // number of lines that are skipped after reading src_h (should be set always to reach full image height)
+{
+    LOG1("@%s: dest_w: %d, dest_h: %d, dest_stride: %d, src_w: %d, src_h: %d, src_stride: %d, skip_top: %d, skip_bottom: %d, dest: %p, src: %p",
+         __func__, dest_w, dest_h, dest_stride, src_w, src_h, src_stride, src_skip_lines_top, src_skip_lines_bottom, dest, src);
+
+    if (src_w == 800 && src_h == 600 && src_skip_lines_top == 0 && src_skip_lines_bottom == 0
+        && dest_w == RESOLUTION_QVGA_WIDTH && dest_h == RESOLUTION_QVGA_HEIGHT) {
+        downScaleNv12ImageFrom800x600ToQvga(dest, src, dest_stride, src_stride);
+        return;
+    }
+    if (src_w == RESOLUTION_VGA_WIDTH && src_h == RESOLUTION_VGA_HEIGHT
+        && src_skip_lines_top == 0 && src_skip_lines_bottom == 0
+        && dest_w == RESOLUTION_QVGA_WIDTH && dest_h == RESOLUTION_QVGA_HEIGHT) {
+        downScaleAndCropNv12ImageQvga(dest, src, dest_stride, src_stride);
+        return;
+    }
+    if (src_w == RESOLUTION_VGA_WIDTH && src_h == RESOLUTION_VGA_HEIGHT
+        && src_skip_lines_top == 0 && src_skip_lines_bottom == 0
+        && dest_w == RESOLUTION_QCIF_WIDTH && dest_h == RESOLUTION_QCIF_WIDTH) {
+        downScaleAndCropNv12ImageQcif(dest, src, dest_stride, src_stride);
+        return;
+    }
+
+    // skip lines from top
+    if (src_skip_lines_top > 0)
+        src += src_skip_lines_top * src_stride;
+
+    // Correct aspect ratio is defined by destination buffer
+    long int aspect_ratio = (dest_w << 16) / dest_h;
+    // Then, we calculate what should be the width of source image
+    // (should be multiple by four)
+    int proper_source_width = (aspect_ratio * (long int)(src_h) + 0x8000L) >> 16;
+    proper_source_width = (proper_source_width + 2) & ~0x3;
+    // Now, the source image should have some surplus width
+    if (src_w < proper_source_width) {
+        LOGE("%s: source image too narrow", __func__);
+    }
+    // Let's divide the surplus to both sides
+    int l_skip = src_w < proper_source_width ? 0 : ((src_w - proper_source_width) >> 1);
+    int r_skip = src_w < proper_source_width ? 0 : (src_w - proper_source_width - l_skip);
+    int skip = l_skip + r_skip;
+
+    int i, j, x1, y1, x2, y2;
+    unsigned int val_1, val_2;
+    int dx, dy;
+    int src_Y_data = src_stride * (src_h + src_skip_lines_bottom + (src_skip_lines_top >> 1));
+    int dest_Y_data = dest_stride * dest_h;
+    int width, height;
+    if (0 == dest_w || 0 == dest_h) {
+        LOGE("%s,dest_w or dest_h should not be 0", __func__);
+        return;
+    }
+    const int scaling_w = ((src_w - skip) << 8) / dest_w;
+    const int scaling_h = (src_h << 8) / dest_h;
+    dx = 0;
+    dy = 0;
+    // get Y data
+    for (i = 0; i < dest_h; i++) {
+        y1 = i * scaling_h;
+        dy = y1 & 0xff;
+        y2 = y1 >> 8;
+        for (j = 0; j < dest_w; j++) {
+            x1 = j * scaling_w;
+            dx = x1 & 0xff;
+            x2 = (x1 >> 8) + l_skip;
+            val_1 = ((unsigned int)src[y2 * src_stride + x2] * (256 - dx)
+                    + (unsigned int)src[y2 * src_stride + x2 + 1] * dx) >> 8;
+            val_2 = ((unsigned int)src[(y2 + 1) * src_stride + x2] * (256 - dx)
+                    + (unsigned int)src[(y2 + 1) * src_stride + x2 + 1] * dx) >> 8;
+            dest[i * dest_stride + j] = MIN(((val_1 * (256 - dy) + val_2 * dy) >> 8), 0xff);
+        }
+    }
+    i = 0;
+    j = 0;
+    width = dest_w >> 1;
+    height = dest_h >> 1;
+    //get UV data
+    for (i = 0; i < height; i++) {
+        y1 = i * scaling_h;
+        dy = y1 & 0xff;
+        y2 = y1 >> 8;
+        for (j = 0; j < width; j++) {
+            x1 = j * scaling_w;
+            dx = x1 & 0xff;
+            x2 = (x1 >> 8) + l_skip / 2;
+            //fill U data
+            val_1 = ((unsigned int)src[y2 * src_stride + (x2 << 1) + src_Y_data] * (256 - dx)
+                     + (unsigned int)src[y2 * src_stride + ((x2 + 1) << 1) + src_Y_data] * dx) >> 8;
+            val_2 = ((unsigned int)src[(y2 + 1) * src_stride + (x2 << 1) + src_Y_data] * (256 -dx)
+                     + (unsigned int)src[(y2 +1) * src_stride + ((x2 + 1) << 1) + src_Y_data] * dx) >> 8;
+            dest[i * dest_stride + (j << 1) + dest_Y_data] = MIN(((val_1 * (256 - dy) + val_2 * dy) >> 8), 0xff);
+            //fill V data
+            val_1 = ((unsigned int)src[y2 * src_stride + (x2 << 1) + 1 + src_Y_data] * (256 - dx)
+                     + (unsigned int)src[y2 * src_stride + ((x2 + 1) << 1) + 1 + src_Y_data] * dx) >> 8;
+            val_2 = ((unsigned int)src[(y2 + 1) * src_stride + (x2 << 1) + 1 + src_Y_data] * (256 -dx)
+                     + (unsigned int)src[(y2 +1) * src_stride + ((x2 + 1) << 1) + 1 + src_Y_data] * dx) >> 8;
+            dest[i * dest_stride + (j << 1) + 1 + dest_Y_data] = MIN(((val_1 * (256 - dy) + val_2 * dy) >> 8), 0xff);
+        }
+    }
+}
+
+void ImageScalerCore::downScaleAndCropNv12ImageQvga(unsigned char *dest, const unsigned char *src,
+                                                    const int dest_stride, const int src_stride)
+{
+    LOG1("@%s", __func__);
+    const int dest_w = RESOLUTION_QVGA_WIDTH;
+    const int dest_h = RESOLUTION_QVGA_HEIGHT;
+    const int src_h = RESOLUTION_VGA_HEIGHT;
+    const int scale = 2;
+
+    // Y component
+    for (int i = 0; i < dest_h; i++) {
+        u_int32_t *s1 = (u_int32_t *)(&src[(i * scale + 0) * src_stride]);
+        u_int32_t *s2 = (u_int32_t *)(&src[(i * scale + 1) * src_stride]);
+        u_int32_t  *d = (u_int32_t *)(&dest[i * dest_stride]);
+        // This processes 4 dest pixels at a time
+        for (int j = 0; j < dest_w; j+=4) {
+            u_int32_t a1; // Input data upper row
+            u_int32_t a2; // Input data lower row
+            u_int32_t b;  // Output data
+            a1 = *s1++;
+            a2 = *s2++;
+            b  = ((a1 & 0xff) + ((a1 >> 8) & 0xff) + (a2 & 0xff) + ((a2 >> 8) & 0xff) + 2) / 4;
+            b |= ((((a1 >> 16) & 0xff) + ((a1 >> 24) & 0xff) + ((a2 >> 16) & 0xff) + ((a2 >> 24) & 0xff) + 2) / 4) << 8;
+            a1 = *s1++;
+            a2 = *s2++;
+            b |= (((a1 & 0xff) + ((a1 >> 8) & 0xff) + (a2 & 0xff) + ((a2 >> 8) & 0xff) + 2) / 4) << 16;
+            b |= ((((a1 >> 16) & 0xff) + ((a1 >> 24) & 0xff) + ((a2 >> 16) & 0xff) + ((a2 >> 24) & 0xff) + 2) / 4) << 24;
+            *d++ = b;
+        }
+    }
+
+    // UV components
+    src = &src[src_stride * src_h];
+    dest = &dest[dest_stride * dest_h];
+
+    for (int i = 0; i < dest_h/2; i++) {
+        u_int32_t *s1 = (u_int32_t *)(&src[(i * scale + 0) * src_stride]);
+        u_int32_t *s2 = (u_int32_t *)(&src[(i * scale + 1) * src_stride]);
+        u_int32_t  *d = (u_int32_t *)(&dest[i * dest_stride]);
+        // This processes 2 dest UV pairs at a time
+        for (int j = 0; j < dest_w/2; j+=2) {
+            u_int32_t a1; // Input data upper row
+            u_int32_t a2; // Input data lower row
+            u_int32_t b;  // Output data
+            a1 = *s1++;
+            a2 = *s2++;
+            b  = ((a1 & 0xff) + ((a1 >> 16) & 0xff) + (a2 & 0xff) + ((a2 >> 16) & 0xff) + 2) / 4;
+            b |= ((((a1 >> 8) & 0xff) + ((a1 >> 24) & 0xff) + ((a2 >> 8) & 0xff) + ((a2 >> 24) & 0xff) + 2) / 4) << 8;
+            a1 = *s1++;
+            a2 = *s2++;
+            b |= (((a1 & 0xff) + ((a1 >> 16) & 0xff) + (a2 & 0xff) + ((a2 >> 16) & 0xff) + 2) / 4) << 16;
+            b |= ((((a1 >> 8) & 0xff) + ((a1 >> 24) & 0xff) + ((a2 >> 8) & 0xff) + ((a2 >> 24) & 0xff) + 2) / 4) << 24;
+            *d++ = b;
+        }
+    }
+}
+
+void ImageScalerCore::downScaleAndCropNv12ImageQcif(unsigned char *dest, const unsigned char *src,
+                                                    const int dest_stride, const int src_stride)
+{
+    LOG1("@%s", __func__);
+    const int dest_w = RESOLUTION_QCIF_WIDTH;
+    const int dest_h = RESOLUTION_QCIF_HEIGHT;
+    const int src_w = RESOLUTION_VGA_WIDTH;
+    const int src_h = RESOLUTION_VGA_HEIGHT;
+
+    // Correct aspect ratio is defined by destination buffer
+    long int aspect_ratio = (dest_w << 16) / dest_h;
+    // Then, we calculate what should be the width of source image
+    // (should be multiple by four)
+    int proper_source_width = (aspect_ratio * (long int)(src_h) + 0x8000L) >> 16;
+    proper_source_width = (proper_source_width + 2) & ~0x3;
+    // Now, the source image should have some surplus width
+    if (src_w < proper_source_width) {
+        LOGE("%s: source image too narrow", __func__);
+        return;
+    }
+    // Let's divide the surplus to both sides
+    int l_skip = (src_w - proper_source_width) >> 1;
+    int r_skip = src_w - proper_source_width - l_skip;
+    int skip = l_skip + r_skip;
+
+    int i, j, x1, y1, x2, y2;
+    unsigned int val_1, val_2;
+    int dx, dy;
+    int src_Y_data = src_stride * src_h;
+    int dest_Y_data = dest_stride * dest_h;
+    int width, height;
+    const int scaling_w = ((src_w - skip) << 8) / dest_w;
+    const int scaling_h = (src_h << 8) / dest_h;
+    dx = 0;
+    dy = 0;
+    // get Y data
+    for (i = 0; i < dest_h; i++) {
+        y1 = i * scaling_h;
+        dy = y1 & 0xff;
+        y2 = y1 >> 8;
+        for (j = 0; j < dest_w; j++) {
+            x1 = j * scaling_w;
+            dx = x1 & 0xff;
+            x2 = (x1 >> 8) + l_skip;
+            val_1 = ((unsigned int)src[y2 * src_stride + x2] * (256 - dx)
+                    + (unsigned int)src[y2 * src_stride + x2 + 1] * dx) >> 8;
+            val_2 = ((unsigned int)src[(y2 + 1) * src_stride + x2] * (256 - dx)
+                    + (unsigned int)src[(y2 + 1) * src_stride + x2 + 1] * dx) >> 8;
+            dest[i * dest_stride + j] = MIN(((val_1 * (256 - dy) + val_2 * dy) >> 8), 0xff);
+        }
+    }
+    i = 0;
+    j = 0;
+    width = dest_w >> 1;
+    height = dest_h >> 1;
+    //get UV data
+    for (i = 0; i < height; i++) {
+        y1 = i * scaling_h;
+        dy = y1 & 0xff;
+        y2 = y1 >> 8;
+        for (j = 0; j < width; j++) {
+            x1 = j * scaling_w;
+            dx = x1 & 0xff;
+            x2 = (x1 >> 8) + l_skip / 2;
+            //fill U data
+            val_1 = ((unsigned int)src[y2 * src_stride + (x2 << 1) + src_Y_data] * (256 - dx)
+                     + (unsigned int)src[y2 * src_stride + ((x2 + 1) << 1) + src_Y_data] * dx) >> 8;
+            val_2 = ((unsigned int)src[(y2 + 1) * src_stride + (x2 << 1) + src_Y_data] * (256 -dx)
+                     + (unsigned int)src[(y2 +1) * src_stride + ((x2 + 1) << 1) + src_Y_data] * dx) >> 8;
+            dest[i * dest_stride + (j << 1) + dest_Y_data] = MIN(((val_1 * (256 - dy) + val_2 * dy) >> 8), 0xff);
+            //fill V data
+            val_1 = ((unsigned int)src[y2 * src_w + (x2 << 1) + 1 + src_Y_data] * (256 - dx)
+                     + (unsigned int)src[y2 * src_w + ((x2 + 1) << 1) + 1 + src_Y_data] * dx) >> 8;
+            val_2 = ((unsigned int)src[(y2 + 1) * src_w + (x2 << 1) + 1 + src_Y_data] * (256 -dx)
+                     + (unsigned int)src[(y2 +1) * src_w + ((x2 + 1) << 1) + 1 + src_Y_data] * dx) >> 8;
+            dest[i * dest_stride + (j << 1) + 1 + dest_Y_data] = MIN(((val_1 * (256 - dy) + val_2 * dy) >> 8), 0xff);
+        }
+    }
+}
+
+void ImageScalerCore::downScaleNv12ImageFrom800x600ToQvga(unsigned char *dest, const unsigned char *src,
+                                                          const int dest_stride, const int src_stride)
+{
+    LOG1("@%s", __func__);
+    const int dest_w = RESOLUTION_QVGA_WIDTH;
+    const int dest_h = RESOLUTION_QVGA_HEIGHT;
+    const int src_h = 600;
+
+    // Y component
+
+    // Processing 2 dest rows and 5 src rows at a time
+    for (int i = 0; i < dest_h / 2; i++) {
+        u_int32_t *s1 = (u_int32_t *)(&src[(i * 5 + 0) * src_stride]);
+        u_int32_t *s2 = (u_int32_t *)(&src[(i * 5 + 1) * src_stride]);
+        u_int32_t *s3 = (u_int32_t *)(&src[(i * 5 + 2) * src_stride]);
+        u_int32_t *d = (u_int32_t *)(&dest[(i * 2 + 0) * dest_stride]);
+        // This processes 8 dest pixels at a time
+        for (int j = 0; j < dest_w; j+=8) {
+            u_int32_t a1; // Input data upper row
+            u_int32_t a2; // Input data middle row
+            u_int32_t a3; // Input data lower row
+            u_int32_t t;  // Temp data (for constructing the output)
+            u_int32_t b;  // Output data
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            t = (4 * ((a1 >> 0) & 0xff) + 4 * ((a1 >> 8) & 0xff) + 2 * ((a1 >> 16) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 4 * ((a2 >> 0) & 0xff) + 4 * ((a2 >> 8) & 0xff) + 2 * ((a2 >> 16) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 2 * ((a3 >> 0) & 0xff) + 2 * ((a3 >> 8) & 0xff) + 1 * ((a3 >> 16) & 0xff) + 0 * ((a3 >> 24) & 0xff));
+            t = (t + 12) / 25;
+            b = t; // First pixel
+            t = (0 * ((a1 >> 0) & 0xff) + 0 * ((a1 >> 8) & 0xff) + 2 * ((a1 >> 16) & 0xff) + 4 * ((a1 >> 24) & 0xff) +
+                 0 * ((a2 >> 0) & 0xff) + 0 * ((a2 >> 8) & 0xff) + 2 * ((a2 >> 16) & 0xff) + 4 * ((a2 >> 24) & 0xff) +
+                 0 * ((a3 >> 0) & 0xff) + 0 * ((a3 >> 8) & 0xff) + 1 * ((a3 >> 16) & 0xff) + 2 * ((a3 >> 24) & 0xff));
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            t = (4 * ((a1 >> 0) & 0xff) + 0 * ((a1 >> 8) & 0xff) + 0 * ((a1 >> 16) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 4 * ((a2 >> 0) & 0xff) + 0 * ((a2 >> 8) & 0xff) + 0 * ((a2 >> 16) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 2 * ((a3 >> 0) & 0xff) + 0 * ((a3 >> 8) & 0xff) + 0 * ((a3 >> 16) & 0xff) + 0 * ((a3 >> 24) & 0xff))+ t;
+            t = (t + 12) / 25;
+            b |= t << 8; // Second pixel
+            t = (0 * ((a1 >> 0) & 0xff) + 4 * ((a1 >> 8) & 0xff) + 4 * ((a1 >> 16) & 0xff) + 2 * ((a1 >> 24) & 0xff) +
+                 0 * ((a2 >> 0) & 0xff) + 4 * ((a2 >> 8) & 0xff) + 4 * ((a2 >> 16) & 0xff) + 2 * ((a2 >> 24) & 0xff) +
+                 0 * ((a3 >> 0) & 0xff) + 2 * ((a3 >> 8) & 0xff) + 2 * ((a3 >> 16) & 0xff) + 1 * ((a3 >> 24) & 0xff));
+            t = (t + 12) / 25;
+            b |= t << 16; // Third pixel
+            t = (2 * ((a1 >> 0) & 0xff) + 0 * ((a1 >> 8) & 0xff) + 0 * ((a1 >> 16) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 2 * ((a2 >> 0) & 0xff) + 0 * ((a2 >> 8) & 0xff) + 0 * ((a2 >> 16) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 1 * ((a3 >> 0) & 0xff) + 0 * ((a3 >> 8) & 0xff) + 0 * ((a3 >> 16) & 0xff) + 0 * ((a3 >> 24) & 0xff));
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            t = (4 * ((a1 >> 0) & 0xff) + 4 * ((a1 >> 8) & 0xff) + 0 * ((a1 >> 16) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 4 * ((a2 >> 0) & 0xff) + 4 * ((a2 >> 8) & 0xff) + 0 * ((a2 >> 16) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 2 * ((a3 >> 0) & 0xff) + 2 * ((a3 >> 8) & 0xff) + 0 * ((a3 >> 16) & 0xff) + 0 * ((a3 >> 24) & 0xff))+ t;
+            t = (t + 12) / 25;
+            b |= t << 24; // Fourth pixel
+            *d++ = b;
+            t = (0 * ((a1 >> 0) & 0xff) + 0 * ((a1 >> 8) & 0xff) + 4 * ((a1 >> 16) & 0xff) + 4 * ((a1 >> 24) & 0xff) +
+                 0 * ((a2 >> 0) & 0xff) + 0 * ((a2 >> 8) & 0xff) + 4 * ((a2 >> 16) & 0xff) + 4 * ((a2 >> 24) & 0xff) +
+                 0 * ((a3 >> 0) & 0xff) + 0 * ((a3 >> 8) & 0xff) + 2 * ((a3 >> 16) & 0xff) + 2 * ((a3 >> 24) & 0xff));
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            t = (2 * ((a1 >> 0) & 0xff) + 0 * ((a1 >> 8) & 0xff) + 0 * ((a1 >> 16) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 2 * ((a2 >> 0) & 0xff) + 0 * ((a2 >> 8) & 0xff) + 0 * ((a2 >> 16) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 1 * ((a3 >> 0) & 0xff) + 0 * ((a3 >> 8) & 0xff) + 0 * ((a3 >> 16) & 0xff) + 0 * ((a3 >> 24) & 0xff))+ t;
+            t = (t + 12) / 25;
+            b = t; // Fifth pixel
+            t = (2 * ((a1 >> 0) & 0xff) + 4 * ((a1 >> 8) & 0xff) + 4 * ((a1 >> 16) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 2 * ((a2 >> 0) & 0xff) + 4 * ((a2 >> 8) & 0xff) + 4 * ((a2 >> 16) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 1 * ((a3 >> 0) & 0xff) + 2 * ((a3 >> 8) & 0xff) + 2 * ((a3 >> 16) & 0xff) + 0 * ((a3 >> 24) & 0xff));
+            t = (t + 12) / 25;
+            b |= t << 8; // Sixth pixel
+            t = (0 * ((a1 >> 0) & 0xff) + 0 * ((a1 >> 8) & 0xff) + 0 * ((a1 >> 16) & 0xff) + 4 * ((a1 >> 24) & 0xff) +
+                 0 * ((a2 >> 0) & 0xff) + 0 * ((a2 >> 8) & 0xff) + 0 * ((a2 >> 16) & 0xff) + 4 * ((a2 >> 24) & 0xff) +
+                 0 * ((a3 >> 0) & 0xff) + 0 * ((a3 >> 8) & 0xff) + 0 * ((a3 >> 16) & 0xff) + 2 * ((a3 >> 24) & 0xff));
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            t = (4 * ((a1 >> 0) & 0xff) + 2 * ((a1 >> 8) & 0xff) + 0 * ((a1 >> 16) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 4 * ((a2 >> 0) & 0xff) + 2 * ((a2 >> 8) & 0xff) + 0 * ((a2 >> 16) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 2 * ((a3 >> 0) & 0xff) + 1 * ((a3 >> 8) & 0xff) + 0 * ((a3 >> 16) & 0xff) + 0 * ((a3 >> 24) & 0xff))+ t;
+            t = (t + 12) / 25;
+            b |= t << 16; // Seventh pixel
+            t = (0 * ((a1 >> 0) & 0xff) + 2 * ((a1 >> 8) & 0xff) + 4 * ((a1 >> 16) & 0xff) + 4 * ((a1 >> 24) & 0xff) +
+                 0 * ((a2 >> 0) & 0xff) + 2 * ((a2 >> 8) & 0xff) + 4 * ((a2 >> 16) & 0xff) + 4 * ((a2 >> 24) & 0xff) +
+                 0 * ((a3 >> 0) & 0xff) + 1 * ((a3 >> 8) & 0xff) + 2 * ((a3 >> 16) & 0xff) + 2 * ((a3 >> 24) & 0xff));
+            t = (t + 12) / 25;
+            b |= t << 24; // Eigth pixel
+            *d++ = b;
+        }
+        s1 = (u_int32_t *)(&src[(i * 5 + 4) * src_stride]);
+        s2 = (u_int32_t *)(&src[(i * 5 + 3) * src_stride]);
+        s3 = (u_int32_t *)(&src[(i * 5 + 2) * src_stride]);
+        d = (u_int32_t *)(&dest[(i * 2 + 1) * dest_stride]);
+        // This processes 8 dest pixels at a time
+        for (int j = 0; j < dest_w; j+=8) {
+            u_int32_t a1; // Input data lower row
+            u_int32_t a2; // Input data middle row
+            u_int32_t a3; // Input data upper row
+            u_int32_t t;  // Temp data (for constructing the output)
+            u_int32_t b;  // Output data
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            t = (4 * ((a1 >> 0) & 0xff) + 4 * ((a1 >> 8) & 0xff) + 2 * ((a1 >> 16) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 4 * ((a2 >> 0) & 0xff) + 4 * ((a2 >> 8) & 0xff) + 2 * ((a2 >> 16) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 2 * ((a3 >> 0) & 0xff) + 2 * ((a3 >> 8) & 0xff) + 1 * ((a3 >> 16) & 0xff) + 0 * ((a3 >> 24) & 0xff));
+            t = (t + 12) / 25;
+            b = t; // First pixel
+            t = (0 * ((a1 >> 0) & 0xff) + 0 * ((a1 >> 8) & 0xff) + 2 * ((a1 >> 16) & 0xff) + 4 * ((a1 >> 24) & 0xff) +
+                 0 * ((a2 >> 0) & 0xff) + 0 * ((a2 >> 8) & 0xff) + 2 * ((a2 >> 16) & 0xff) + 4 * ((a2 >> 24) & 0xff) +
+                 0 * ((a3 >> 0) & 0xff) + 0 * ((a3 >> 8) & 0xff) + 1 * ((a3 >> 16) & 0xff) + 2 * ((a3 >> 24) & 0xff));
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            t = (4 * ((a1 >> 0) & 0xff) + 0 * ((a1 >> 8) & 0xff) + 0 * ((a1 >> 16) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 4 * ((a2 >> 0) & 0xff) + 0 * ((a2 >> 8) & 0xff) + 0 * ((a2 >> 16) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 2 * ((a3 >> 0) & 0xff) + 0 * ((a3 >> 8) & 0xff) + 0 * ((a3 >> 16) & 0xff) + 0 * ((a3 >> 24) & 0xff))+ t;
+            t = (t + 12) / 25;
+            b |= t << 8; // Second pixel
+            t = (0 * ((a1 >> 0) & 0xff) + 4 * ((a1 >> 8) & 0xff) + 4 * ((a1 >> 16) & 0xff) + 2 * ((a1 >> 24) & 0xff) +
+                 0 * ((a2 >> 0) & 0xff) + 4 * ((a2 >> 8) & 0xff) + 4 * ((a2 >> 16) & 0xff) + 2 * ((a2 >> 24) & 0xff) +
+                 0 * ((a3 >> 0) & 0xff) + 2 * ((a3 >> 8) & 0xff) + 2 * ((a3 >> 16) & 0xff) + 1 * ((a3 >> 24) & 0xff));
+            t = (t + 12) / 25;
+            b |= t << 16; // Third pixel
+            t = (2 * ((a1 >> 0) & 0xff) + 0 * ((a1 >> 8) & 0xff) + 0 * ((a1 >> 16) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 2 * ((a2 >> 0) & 0xff) + 0 * ((a2 >> 8) & 0xff) + 0 * ((a2 >> 16) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 1 * ((a3 >> 0) & 0xff) + 0 * ((a3 >> 8) & 0xff) + 0 * ((a3 >> 16) & 0xff) + 0 * ((a3 >> 24) & 0xff));
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            t = (4 * ((a1 >> 0) & 0xff) + 4 * ((a1 >> 8) & 0xff) + 0 * ((a1 >> 16) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 4 * ((a2 >> 0) & 0xff) + 4 * ((a2 >> 8) & 0xff) + 0 * ((a2 >> 16) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 2 * ((a3 >> 0) & 0xff) + 2 * ((a3 >> 8) & 0xff) + 0 * ((a3 >> 16) & 0xff) + 0 * ((a3 >> 24) & 0xff))+ t;
+            t = (t + 12) / 25;
+            b |= t << 24; // Fourth pixel
+            *d++ = b;
+            t = (0 * ((a1 >> 0) & 0xff) + 0 * ((a1 >> 8) & 0xff) + 4 * ((a1 >> 16) & 0xff) + 4 * ((a1 >> 24) & 0xff) +
+                 0 * ((a2 >> 0) & 0xff) + 0 * ((a2 >> 8) & 0xff) + 4 * ((a2 >> 16) & 0xff) + 4 * ((a2 >> 24) & 0xff) +
+                 0 * ((a3 >> 0) & 0xff) + 0 * ((a3 >> 8) & 0xff) + 2 * ((a3 >> 16) & 0xff) + 2 * ((a3 >> 24) & 0xff));
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            t = (2 * ((a1 >> 0) & 0xff) + 0 * ((a1 >> 8) & 0xff) + 0 * ((a1 >> 16) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 2 * ((a2 >> 0) & 0xff) + 0 * ((a2 >> 8) & 0xff) + 0 * ((a2 >> 16) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 1 * ((a3 >> 0) & 0xff) + 0 * ((a3 >> 8) & 0xff) + 0 * ((a3 >> 16) & 0xff) + 0 * ((a3 >> 24) & 0xff))+ t;
+            t = (t + 12) / 25;
+            b = t; // Fifth pixel
+            t = (2 * ((a1 >> 0) & 0xff) + 4 * ((a1 >> 8) & 0xff) + 4 * ((a1 >> 16) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 2 * ((a2 >> 0) & 0xff) + 4 * ((a2 >> 8) & 0xff) + 4 * ((a2 >> 16) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 1 * ((a3 >> 0) & 0xff) + 2 * ((a3 >> 8) & 0xff) + 2 * ((a3 >> 16) & 0xff) + 0 * ((a3 >> 24) & 0xff));
+            t = (t + 12) / 25;
+            b |= t << 8; // Sixth pixel
+            t = (0 * ((a1 >> 0) & 0xff) + 0 * ((a1 >> 8) & 0xff) + 0 * ((a1 >> 16) & 0xff) + 4 * ((a1 >> 24) & 0xff) +
+                 0 * ((a2 >> 0) & 0xff) + 0 * ((a2 >> 8) & 0xff) + 0 * ((a2 >> 16) & 0xff) + 4 * ((a2 >> 24) & 0xff) +
+                 0 * ((a3 >> 0) & 0xff) + 0 * ((a3 >> 8) & 0xff) + 0 * ((a3 >> 16) & 0xff) + 2 * ((a3 >> 24) & 0xff));
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            t = (4 * ((a1 >> 0) & 0xff) + 2 * ((a1 >> 8) & 0xff) + 0 * ((a1 >> 16) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 4 * ((a2 >> 0) & 0xff) + 2 * ((a2 >> 8) & 0xff) + 0 * ((a2 >> 16) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 2 * ((a3 >> 0) & 0xff) + 1 * ((a3 >> 8) & 0xff) + 0 * ((a3 >> 16) & 0xff) + 0 * ((a3 >> 24) & 0xff))+ t;
+            t = (t + 12) / 25;
+            b |= t << 16; // Seventh pixel
+            t = (0 * ((a1 >> 0) & 0xff) + 2 * ((a1 >> 8) & 0xff) + 4 * ((a1 >> 16) & 0xff) + 4 * ((a1 >> 24) & 0xff) +
+                 0 * ((a2 >> 0) & 0xff) + 2 * ((a2 >> 8) & 0xff) + 4 * ((a2 >> 16) & 0xff) + 4 * ((a2 >> 24) & 0xff) +
+                 0 * ((a3 >> 0) & 0xff) + 1 * ((a3 >> 8) & 0xff) + 2 * ((a3 >> 16) & 0xff) + 2 * ((a3 >> 24) & 0xff));
+            t = (t + 12) / 25;
+            b |= t << 24; // Eigth pixel
+            *d++ = b;
+        }
+    }
+
+    // UV components
+    src = &src[src_stride * src_h];
+    dest = &dest[dest_stride * dest_h];
+
+    // Processing 2 dest rows and 5 src rows at a time
+    for (int i = 0; i < (dest_h/2) / 2; i++) {
+        u_int32_t *s1 = (u_int32_t *)(&src[(i * 5 + 0) * src_stride]);
+        u_int32_t *s2 = (u_int32_t *)(&src[(i * 5 + 1) * src_stride]);
+        u_int32_t *s3 = (u_int32_t *)(&src[(i * 5 + 2) * src_stride]);
+        u_int16_t *d = (u_int16_t *)(&dest[(i * 2 + 0) * dest_stride]);
+        // This processes 4 dest UV pairs at a time
+        for (int j = 0; j < dest_w/2; j+=4) {
+            u_int32_t a1; // Input data upper row
+            u_int32_t a2; // Input data middle row
+            u_int32_t a3; // Input data lower row
+            u_int32_t u;  // Temp data (for constructing the output)
+            u_int32_t v;  // Temp data (for constructing the output)
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            u = (4 * ((a1 >> 0) & 0xff) + 4 * ((a1 >> 16) & 0xff) +
+                 4 * ((a2 >> 0) & 0xff) + 4 * ((a2 >> 16) & 0xff) +
+                 2 * ((a3 >> 0) & 0xff) + 2 * ((a3 >> 16) & 0xff));
+            v = (4 * ((a1 >> 8) & 0xff) + 4 * ((a1 >> 24) & 0xff) +
+                 4 * ((a2 >> 8) & 0xff) + 4 * ((a2 >> 24) & 0xff) +
+                 2 * ((a3 >> 8) & 0xff) + 2 * ((a3 >> 24) & 0xff));
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            u = (2 * ((a1 >> 0) & 0xff) + 0 * ((a1 >> 16) & 0xff) +
+                 2 * ((a2 >> 0) & 0xff) + 0 * ((a2 >> 16) & 0xff) +
+                 1 * ((a3 >> 0) & 0xff) + 0 * ((a3 >> 16) & 0xff))+ u;
+            v = (2 * ((a1 >> 8) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 2 * ((a2 >> 8) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 1 * ((a3 >> 8) & 0xff) + 0 * ((a3 >> 24) & 0xff))+ v;
+            u = (u + 12) / 25;
+            v = (v + 12) / 25;
+            *d++ = u | (v << 8); // First uv pair;
+            u = (2 * ((a1 >> 0) & 0xff) + 4 * ((a1 >> 16) & 0xff) +
+                 2 * ((a2 >> 0) & 0xff) + 4 * ((a2 >> 16) & 0xff) +
+                 1 * ((a3 >> 0) & 0xff) + 2 * ((a3 >> 16) & 0xff));
+            v = (2 * ((a1 >> 8) & 0xff) + 4 * ((a1 >> 24) & 0xff) +
+                 2 * ((a2 >> 8) & 0xff) + 4 * ((a2 >> 24) & 0xff) +
+                 1 * ((a3 >> 8) & 0xff) + 2 * ((a3 >> 24) & 0xff));
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            u = (4 * ((a1 >> 0) & 0xff) + 0 * ((a1 >> 16) & 0xff) +
+                 4 * ((a2 >> 0) & 0xff) + 0 * ((a2 >> 16) & 0xff) +
+                 2 * ((a3 >> 0) & 0xff) + 0 * ((a3 >> 16) & 0xff))+ u;
+            v = (4 * ((a1 >> 8) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 4 * ((a2 >> 8) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 2 * ((a3 >> 8) & 0xff) + 0 * ((a3 >> 24) & 0xff))+ v;
+            u = (u + 12) / 25;
+            v = (v + 12) / 25;
+            *d++ = u | (v << 8); // Second uv pair;
+            u = (0 * ((a1 >> 0) & 0xff) + 4 * ((a1 >> 16) & 0xff) +
+                 0 * ((a2 >> 0) & 0xff) + 4 * ((a2 >> 16) & 0xff) +
+                 0 * ((a3 >> 0) & 0xff) + 2 * ((a3 >> 16) & 0xff));
+            v = (0 * ((a1 >> 8) & 0xff) + 4 * ((a1 >> 24) & 0xff) +
+                 0 * ((a2 >> 8) & 0xff) + 4 * ((a2 >> 24) & 0xff) +
+                 0 * ((a3 >> 8) & 0xff) + 2 * ((a3 >> 24) & 0xff));
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            u = (4 * ((a1 >> 0) & 0xff) + 2 * ((a1 >> 16) & 0xff) +
+                 4 * ((a2 >> 0) & 0xff) + 2 * ((a2 >> 16) & 0xff) +
+                 2 * ((a3 >> 0) & 0xff) + 1 * ((a3 >> 16) & 0xff))+ u;
+            v = (4 * ((a1 >> 8) & 0xff) + 2 * ((a1 >> 24) & 0xff) +
+                 4 * ((a2 >> 8) & 0xff) + 2 * ((a2 >> 24) & 0xff) +
+                 2 * ((a3 >> 8) & 0xff) + 1 * ((a3 >> 24) & 0xff))+ v;
+            u = (u + 12) / 25;
+            v = (v + 12) / 25;
+            *d++ = u | (v << 8); // Third uv pair;
+            u = (0 * ((a1 >> 0) & 0xff) + 2 * ((a1 >> 16) & 0xff) +
+                 0 * ((a2 >> 0) & 0xff) + 2 * ((a2 >> 16) & 0xff) +
+                 0 * ((a3 >> 0) & 0xff) + 1 * ((a3 >> 16) & 0xff));
+            v = (0 * ((a1 >> 8) & 0xff) + 2 * ((a1 >> 24) & 0xff) +
+                 0 * ((a2 >> 8) & 0xff) + 2 * ((a2 >> 24) & 0xff) +
+                 0 * ((a3 >> 8) & 0xff) + 1 * ((a3 >> 24) & 0xff));
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            u = (4 * ((a1 >> 0) & 0xff) + 4 * ((a1 >> 16) & 0xff) +
+                 4 * ((a2 >> 0) & 0xff) + 4 * ((a2 >> 16) & 0xff) +
+                 2 * ((a3 >> 0) & 0xff) + 2 * ((a3 >> 16) & 0xff))+ u;
+            v = (4 * ((a1 >> 8) & 0xff) + 4 * ((a1 >> 24) & 0xff) +
+                 4 * ((a2 >> 8) & 0xff) + 4 * ((a2 >> 24) & 0xff) +
+                 2 * ((a3 >> 8) & 0xff) + 2 * ((a3 >> 24) & 0xff))+ v;
+            u = (u + 12) / 25;
+            v = (v + 12) / 25;
+            *d++ = u | (v << 8); // Fourth uv pair;
+        }
+        s1 = (u_int32_t *)(&src[(i * 5 + 4) * src_stride]);
+        s2 = (u_int32_t *)(&src[(i * 5 + 3) * src_stride]);
+        s3 = (u_int32_t *)(&src[(i * 5 + 2) * src_stride]);
+        d = (u_int16_t *)(&dest[(i * 2 + 1) * dest_stride]);
+        // This processes 4 dest UV pairs at a time
+        for (int j = 0; j < dest_w/2; j+=4) {
+            u_int32_t a1; // Input data lower row
+            u_int32_t a2; // Input data middle row
+            u_int32_t a3; // Input data upper row
+            u_int32_t u;  // Temp data (for constructing the output)
+            u_int32_t v;  // Temp data (for constructing the output)
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            u = (4 * ((a1 >> 0) & 0xff) + 4 * ((a1 >> 16) & 0xff) +
+                 4 * ((a2 >> 0) & 0xff) + 4 * ((a2 >> 16) & 0xff) +
+                 2 * ((a3 >> 0) & 0xff) + 2 * ((a3 >> 16) & 0xff));
+            v = (4 * ((a1 >> 8) & 0xff) + 4 * ((a1 >> 24) & 0xff) +
+                 4 * ((a2 >> 8) & 0xff) + 4 * ((a2 >> 24) & 0xff) +
+                 2 * ((a3 >> 8) & 0xff) + 2 * ((a3 >> 24) & 0xff));
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            u = (2 * ((a1 >> 0) & 0xff) + 0 * ((a1 >> 16) & 0xff) +
+                 2 * ((a2 >> 0) & 0xff) + 0 * ((a2 >> 16) & 0xff) +
+                 1 * ((a3 >> 0) & 0xff) + 0 * ((a3 >> 16) & 0xff))+ u;
+            v = (2 * ((a1 >> 8) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 2 * ((a2 >> 8) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 1 * ((a3 >> 8) & 0xff) + 0 * ((a3 >> 24) & 0xff))+ v;
+            u = (u + 12) / 25;
+            v = (v + 12) / 25;
+            *d++ = u | (v << 8); // First uv pair;
+            u = (2 * ((a1 >> 0) & 0xff) + 4 * ((a1 >> 16) & 0xff) +
+                 2 * ((a2 >> 0) & 0xff) + 4 * ((a2 >> 16) & 0xff) +
+                 1 * ((a3 >> 0) & 0xff) + 2 * ((a3 >> 16) & 0xff));
+            v = (2 * ((a1 >> 8) & 0xff) + 4 * ((a1 >> 24) & 0xff) +
+                 2 * ((a2 >> 8) & 0xff) + 4 * ((a2 >> 24) & 0xff) +
+                 1 * ((a3 >> 8) & 0xff) + 2 * ((a3 >> 24) & 0xff));
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            u = (4 * ((a1 >> 0) & 0xff) + 0 * ((a1 >> 16) & 0xff) +
+                 4 * ((a2 >> 0) & 0xff) + 0 * ((a2 >> 16) & 0xff) +
+                 2 * ((a3 >> 0) & 0xff) + 0 * ((a3 >> 16) & 0xff))+ u;
+            v = (4 * ((a1 >> 8) & 0xff) + 0 * ((a1 >> 24) & 0xff) +
+                 4 * ((a2 >> 8) & 0xff) + 0 * ((a2 >> 24) & 0xff) +
+                 2 * ((a3 >> 8) & 0xff) + 0 * ((a3 >> 24) & 0xff))+ v;
+            u = (u + 12) / 25;
+            v = (v + 12) / 25;
+            *d++ = u | (v << 8); // Second uv pair;
+            u = (0 * ((a1 >> 0) & 0xff) + 4 * ((a1 >> 16) & 0xff) +
+                 0 * ((a2 >> 0) & 0xff) + 4 * ((a2 >> 16) & 0xff) +
+                 0 * ((a3 >> 0) & 0xff) + 2 * ((a3 >> 16) & 0xff));
+            v = (0 * ((a1 >> 8) & 0xff) + 4 * ((a1 >> 24) & 0xff) +
+                 0 * ((a2 >> 8) & 0xff) + 4 * ((a2 >> 24) & 0xff) +
+                 0 * ((a3 >> 8) & 0xff) + 2 * ((a3 >> 24) & 0xff));
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            u = (4 * ((a1 >> 0) & 0xff) + 2 * ((a1 >> 16) & 0xff) +
+                 4 * ((a2 >> 0) & 0xff) + 2 * ((a2 >> 16) & 0xff) +
+                 2 * ((a3 >> 0) & 0xff) + 1 * ((a3 >> 16) & 0xff))+ u;
+            v = (4 * ((a1 >> 8) & 0xff) + 2 * ((a1 >> 24) & 0xff) +
+                 4 * ((a2 >> 8) & 0xff) + 2 * ((a2 >> 24) & 0xff) +
+                 2 * ((a3 >> 8) & 0xff) + 1 * ((a3 >> 24) & 0xff))+ v;
+            u = (u + 12) / 25;
+            v = (v + 12) / 25;
+            *d++ = u | (v << 8); // Third uv pair;
+            u = (0 * ((a1 >> 0) & 0xff) + 2 * ((a1 >> 16) & 0xff) +
+                 0 * ((a2 >> 0) & 0xff) + 2 * ((a2 >> 16) & 0xff) +
+                 0 * ((a3 >> 0) & 0xff) + 1 * ((a3 >> 16) & 0xff));
+            v = (0 * ((a1 >> 8) & 0xff) + 2 * ((a1 >> 24) & 0xff) +
+                 0 * ((a2 >> 8) & 0xff) + 2 * ((a2 >> 24) & 0xff) +
+                 0 * ((a3 >> 8) & 0xff) + 1 * ((a3 >> 24) & 0xff));
+            a1 = *s1++;
+            a2 = *s2++;
+            a3 = *s3++;
+            u = (4 * ((a1 >> 0) & 0xff) + 4 * ((a1 >> 16) & 0xff) +
+                 4 * ((a2 >> 0) & 0xff) + 4 * ((a2 >> 16) & 0xff) +
+                 2 * ((a3 >> 0) & 0xff) + 2 * ((a3 >> 16) & 0xff))+ u;
+            v = (4 * ((a1 >> 8) & 0xff) + 4 * ((a1 >> 24) & 0xff) +
+                 4 * ((a2 >> 8) & 0xff) + 4 * ((a2 >> 24) & 0xff) +
+                 2 * ((a3 >> 8) & 0xff) + 2 * ((a3 >> 24) & 0xff))+ v;
+            u = (u + 12) / 25;
+            v = (v + 12) / 25;
+            *d++ = u | (v << 8); // Fourth uv pair;
+        }
+    }
+
+}
+// VGA-QCIF end
+
+int ImageScalerCore::cropCompose(
+    void *src, unsigned int srcW, unsigned int srcH, unsigned int srcStride, int srcFormat,
+    void *dst, unsigned int dstW, unsigned int dstH, unsigned int dstStride, int dstFormat,
+    unsigned int srcCropW, unsigned int srcCropH, unsigned int srcCropLeft, unsigned int srcCropTop,
+    unsigned int dstCropW, unsigned int dstCropH, unsigned int dstCropLeft, unsigned int dstCropTop)
+{
+    static const unsigned int MAXVAL = 65536;
+    static const int ALLOW_DOWNSCALING = 1;
+
+    // Check that we support the formats
+    if ((srcFormat != V4L2_PIX_FMT_NV12 &&
+         srcFormat != V4L2_PIX_FMT_NV21) ||
+        srcFormat != dstFormat) {
+        LOGE("Format conversion is not yet supported");
+        return UNKNOWN_ERROR;
+    }
+
+    if (srcW >= MAXVAL || srcH >= MAXVAL || srcCropLeft >= MAXVAL || srcCropH >= MAXVAL ||
+        dstW >= MAXVAL || dstH >= MAXVAL || dstCropLeft >= MAXVAL || dstCropH >= MAXVAL) {
+        LOGE("Values out of range");
+        return UNKNOWN_ERROR;
+    }
+
+    // Check that the windows are within limits
+    if (srcCropLeft + srcCropW > srcW || srcCropTop + srcCropH > srcH ||
+        dstCropLeft + dstCropW > dstW || dstCropTop + dstCropH > dstH) {
+        LOGE("Crop region is outside of the image");
+        return UNKNOWN_ERROR;
+    }
+
+    // Check that widths are even
+    if ((srcW & 1) || (dstW & 1)) {
+        LOGE("Image width must be even");
+        return UNKNOWN_ERROR;
+    }
+
+    if (srcStride == dstStride && srcCropW == srcW && srcW == dstW &&
+        srcCropH == srcH && srcH == dstH &&
+        dstCropW == dstW && dstCropH == dstH) {
+        // If no cropping/scaling is requested, just copy data
+        cropComposeCopy(src, dst, srcStride * srcH * 3 / 2);
+        return 0;
+    }
+
+    if (!ALLOW_DOWNSCALING &&
+        (dstCropH < srcCropH || dstCropW < srcCropW)) {
+        LOGE("Trying to downscale when it is disabled");
+        return UNKNOWN_ERROR;
+    }
+
+    if (srcStride == srcW && dstStride == dstW) {
+        // Upscaling both horizontally and vertically
+        cropComposeUpscaleNV12_bl(
+            src, srcH, srcStride, srcCropLeft, srcCropTop, srcCropW, srcCropH,
+            dst, dstH, dstStride, dstCropLeft, dstCropTop, dstCropW, dstCropH);
+        return 0;
+    }
+
+    LOGE("Unsupported scaling parameters");
+    return UNKNOWN_ERROR;
+}
+
+/**
+ * CropComposeZoom
+ *
+ * Crop and compose algorithm is need to perform digital zooming.
+ * Both source and destination images are in same format and resolution,
+ * the crop rectangle in the source image is scaled to fill whole image
+ * in the destination image.
+ */
+int ImageScalerCore::cropComposeZoom(void *src, void *dst,
+                                     unsigned int width, unsigned int height, unsigned int stride, int format,
+                                     unsigned int srcCropW, unsigned int srcCropH, unsigned int srcCropLeft, unsigned int srcCropTop)
+{
+    return cropCompose(src, width, height, stride, format,
+                       dst, width, height, stride, format,
+                       srcCropW, srcCropH, srcCropLeft, srcCropTop,
+                       width, height, 0, 0);
+}
+
+void ImageScalerCore::cropComposeCopy(void *src, void *dst, unsigned int size)
+{
+    MEMCPY_S((int8_t *) dst, size, (int8_t *) src, size);
+}
+
+// Bilinear scaling, chrominance with nearest neighbor
+void ImageScalerCore::cropComposeUpscaleNV12_bl(
+    void *src, unsigned int srcH, unsigned int srcStride,
+    unsigned int srcCropLeft, unsigned int srcCropTop,
+    unsigned int srcCropW, unsigned int srcCropH,
+    void *dst, unsigned int dstH, unsigned int dstStride,
+    unsigned int dstCropLeft, unsigned int dstCropTop,
+    unsigned int dstCropW, unsigned int dstCropH)
+{
+    static const int BILINEAR = 1;
+    static const unsigned int FP_1  = 1 << MFP;       // Fixed point 1.0
+    static const unsigned int FRACT = (1 << MFP) - 1; // Fractional part mask
+    unsigned int dx, dy, sx, sy;
+    unsigned char *s = (unsigned char *)src;
+    unsigned char *d = (unsigned char *)dst;
+    unsigned int sx0, sy0, dx0, dy0, dx1, dy1;
+
+    unsigned int sxd = ((srcCropW<<MFP) + (dstCropW>>1)) / dstCropW;
+    unsigned int syd = ((srcCropH<<MFP) + (dstCropH>>1)) / dstCropH;
+
+    if (!src || !dst) {
+        LOGE("buffer pointer is NULL");
+        return;
+    }
+
+    // Upscale luminance
+    sx0 = srcCropLeft << MFP;
+    sy0 = srcCropTop << MFP;
+    dx0 = dstCropLeft;
+    dy0 = dstCropTop;
+    dx1 = dstCropLeft + dstCropW;
+    dy1 = dstCropTop + dstCropH;
+    for (dy = dy0, sy = sy0; dy < dy1; dy++, sy += syd) {
+        for (dx = dx0, sx = sx0; dx < dx1; dx++, sx += sxd) {
+            unsigned int sxi = sx >> MFP;
+            unsigned int syi = sy >> MFP;
+            unsigned int s0 = s[srcStride*syi+sxi];
+            if (BILINEAR) {
+                unsigned int fx = sx & FRACT;             // Fractional part
+                unsigned int fy = sy & FRACT;
+                unsigned int fx1 = FP_1 - fx;               // 1 - fractional part
+                unsigned int fy1 = FP_1 - fy;
+                unsigned int s1 = s[srcStride*syi+sxi+1];
+                unsigned int s2 = s[srcStride*(syi+1)+sxi];
+                unsigned int s3 = s[srcStride*(syi+1)+sxi+1];
+                unsigned int s4 = (s0 * fx1 + s1 * fx) >> MFP;
+                unsigned int s5 = (s2 * fx1 + s3 * fx) >> MFP;
+                s0 = (s4 * fy1 + s5 * fy) >> MFP;
+            }
+            d[dstStride*dy+dx] = s0;
+        }
+    }
+
+    // Upscale chrominance
+    s = (unsigned char *)src + srcStride*srcH;
+    d = (unsigned char *)dst + dstStride*dstH;
+    sx0 = srcCropLeft << (MFP - 1);
+    sy0 = srcCropTop << (MFP - 1);
+    dx0 = dstCropLeft >> 1;
+    dy0 = dstCropTop >> 1;
+    dx1 = (dstCropLeft + dstCropW) >> 1;
+    dy1 = (dstCropTop + dstCropH) >> 1;
+    for (dy = dy0, sy = sy0; dy < dy1; dy++, sy += syd) {
+        for (dx = dx0, sx = sx0; dx < dx1; dx++, sx += sxd) {
+            unsigned int sxi = sx >> MFP;
+            unsigned int syi = sy >> MFP;
+            d[dstStride*dy+dx*2+0] = s[srcStride*syi+sxi*2+0];
+            d[dstStride*dy+dx*2+1] = s[srcStride*syi+sxi*2+1];
+        }
+    }
+}
+
+} // namespace icamera
+
diff --git a/camera/hal/intel/ipu6/src/image_process/ImageScalerCore.h b/camera/hal/intel/ipu6/src/image_process/ImageScalerCore.h
new file mode 100644
index 000000000000..eaeb536e4430
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/image_process/ImageScalerCore.h
@@ -0,0 +1,78 @@
+/*
+ * Copyright (C) 2012-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+namespace icamera {
+/**
+ * \class ImageScalerCore
+ *
+ */
+class ImageScalerCore {
+public:
+    static void downScaleImage(void *src, void *dest,
+                               int dest_w, int dest_h, int dest_stride,
+                               int src_w, int src_h, int src_stride,
+                               int format, int src_skip_lines_top = 0,
+                               int src_skip_lines_bottom = 0);
+    static int cropCompose(void *src, unsigned int srcW, unsigned int srcH, unsigned int srcStride, int srcFormat,
+                           void *dst, unsigned int dstW, unsigned int dstH, unsigned int dstStride, int dstFormat,
+                           unsigned int srcCropW, unsigned int srcCropH, unsigned int srcCropLeft, unsigned int srcCropTop,
+                           unsigned int dstCropW, unsigned int dstCropH, unsigned int dstCropLeft, unsigned int dstCropTop);
+    static int cropComposeZoom(void *src, void *dst,
+                               unsigned int width, unsigned int height, unsigned int stride, int format,
+                               unsigned int srcCropW, unsigned int srcCropH, unsigned int srcCropLeft, unsigned int srcCropTop);
+
+protected:
+    static void downScaleYUY2Image(unsigned char *dest, const unsigned char *src,
+                                   const int dest_w, const int dest_h, const int dest_stride,
+                                   const int src_w, const int src_h, const int src_stride);
+
+    static void downScaleAndCropNv12Image(unsigned char *dest, const unsigned char *src,
+                                          const int dest_w, const int dest_h, const int dest_stride,
+                                          const int src_w, const int src_h, const int src_stride,
+                                          const int src_skip_lines_top = 0,
+                                          const int src_skip_lines_bottom = 0);
+
+    static void trimNv12Image(unsigned char *dest, const unsigned char *src,
+                              const int dest_w, const int dest_h, const int dest_stride,
+                              const int src_w, const int src_h, const int src_stride,
+                              const int src_skip_lines_top = 0,
+                              const int src_skip_lines_bottom = 0);
+
+    static void downScaleAndCropNv12ImageQvga(unsigned char *dest, const unsigned char *src,
+                                              const int dest_stride, const int src_stride);
+
+    static void downScaleAndCropNv12ImageQcif(unsigned char *dest, const unsigned char *src,
+                                              const int dest_stride, const int src_stride);
+
+    static void downScaleNv12ImageFrom800x600ToQvga(unsigned char *dest, const unsigned char *src,
+                                                    const int dest_stride, const int src_stride);
+
+private:
+    static const int MFP = 16;            // Fractional bits for fixed point calculations
+
+private:
+    static void cropComposeCopy(void *src, void *dst, unsigned int size);
+    static void cropComposeUpscaleNV12_bl(
+        void *src, unsigned int srcH, unsigned int srcStride,
+        unsigned int srcCropLeft, unsigned int srcCropTop,
+        unsigned int srcCropW, unsigned int srcCropH,
+        void *dst, unsigned int dstH, unsigned int dstStride,
+        unsigned int dstCropLeft, unsigned int dstCropTop,
+        unsigned int dstCropW, unsigned int dstCropH);
+
+};
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/image_process/PostProcessorBase.cpp b/camera/hal/intel/ipu6/src/image_process/PostProcessorBase.cpp
new file mode 100644
index 000000000000..3c3777bbdfe7
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/image_process/PostProcessorBase.cpp
@@ -0,0 +1,318 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#define LOG_TAG "PostProcessorBase"
+
+#include <vector>
+#include <hardware/camera3.h>
+#include "stdlib.h"
+#include "PostProcessorBase.h"
+#include "iutils/CameraLog.h"
+
+using std::shared_ptr;
+
+namespace icamera {
+
+PostProcessorBase::PostProcessorBase(std::string processName) :
+    mName(processName),
+    mProcessor(nullptr)
+{
+    LOG1("@%s PostProcessorBase created", __func__);
+}
+
+PostProcessorBase::~PostProcessorBase()
+{
+    LOG1("@%s PostProcessorBase destory", __func__);
+}
+
+ScaleProcess::ScaleProcess() :
+    PostProcessorBase("Scaler")
+{
+    LOG1("@%s create scaler processor", __func__);
+    mProcessor = IImageProcessor::createImageProcessor();
+}
+
+status_t ScaleProcess::doPostProcessing(const shared_ptr<camera3::Camera3Buffer> &inBuf,
+                                        shared_ptr<camera3::Camera3Buffer> &outBuf)
+{
+    LOG1("@%s processor name: %s", __func__, mName.c_str());
+    CheckError(!inBuf, UNKNOWN_ERROR, "%s, the inBuf is nullptr", __func__);
+    CheckError(!outBuf, UNKNOWN_ERROR, "%s, the outBuf is nullptr", __func__);
+
+    int ret = mProcessor->scaleFrame(inBuf, outBuf);
+    CheckError(ret != OK, UNKNOWN_ERROR, "Failed to do post processing, name: %s", mName.c_str());
+
+    return OK;
+}
+
+RotateProcess::RotateProcess(int angle) :
+    PostProcessorBase("Rotate"),
+    mAngle(angle)
+{
+    LOG1("@%s create rotate processor, degree: %d", __func__, mAngle);
+    mProcessor = IImageProcessor::createImageProcessor();
+};
+
+status_t RotateProcess::doPostProcessing(const shared_ptr<camera3::Camera3Buffer> &inBuf,
+                                         shared_ptr<camera3::Camera3Buffer> &outBuf)
+{
+    LOG1("@%s processor name: %s", __func__, mName.c_str());
+    CheckError(!inBuf, UNKNOWN_ERROR, "%s, the inBuf is nullptr", __func__);
+    CheckError(!outBuf, UNKNOWN_ERROR, "%s, the outBuf is nullptr", __func__);
+    std::vector<uint8_t> rotateBuf;
+
+    int ret = mProcessor->rotateFrame(inBuf, outBuf, mAngle, rotateBuf);
+    CheckError(ret != OK, UNKNOWN_ERROR, "Failed to do post processing, name: %s", mName.c_str());
+
+    return OK;
+}
+
+CropProcess::CropProcess() :
+    PostProcessorBase("Crop")
+{
+    LOG1("@%s create crop processor", __func__);
+    mProcessor = IImageProcessor::createImageProcessor();
+};
+
+status_t CropProcess::doPostProcessing(const shared_ptr<camera3::Camera3Buffer> &inBuf,
+                                       shared_ptr<camera3::Camera3Buffer> &outBuf)
+{
+    LOG1("@%s processor name: %s", __func__, mName.c_str());
+    CheckError(!inBuf, UNKNOWN_ERROR, "%s, the inBuf is nullptr", __func__);
+    CheckError(!outBuf, UNKNOWN_ERROR, "%s, the outBuf is nullptr", __func__);
+
+    int ret = mProcessor->cropFrame(inBuf, outBuf);
+    CheckError(ret != OK, UNKNOWN_ERROR, "Failed to do post processing, name: %s", mName.c_str());
+
+    return OK;
+}
+
+ConvertProcess::ConvertProcess() :
+    PostProcessorBase("Convert")
+{
+    LOG1("@%s create convert processor", __func__);
+    mProcessor = IImageProcessor::createImageProcessor();
+};
+
+status_t ConvertProcess::doPostProcessing(const shared_ptr<camera3::Camera3Buffer> &inBuf,
+                                          shared_ptr<camera3::Camera3Buffer> &outBuf)
+{
+    LOG1("@%s processor name: %s", __func__, mName.c_str());
+    CheckError(!inBuf, UNKNOWN_ERROR, "%s, the inBuf is nullptr", __func__);
+    CheckError(!outBuf, UNKNOWN_ERROR, "%s, the outBuf is nullptr", __func__);
+
+    int ret = mProcessor->convertFrame(inBuf, outBuf);
+    CheckError(ret != OK, UNKNOWN_ERROR, "Failed to do post processing, name: %s", mName.c_str());
+
+    return OK;
+}
+
+JpegProcess::JpegProcess(int cameraId) :
+    PostProcessorBase("JpegEncode"),
+    mCameraId(cameraId),
+    mCropBuffer(nullptr),
+    mScaleBuffer(nullptr),
+    mThumbOutput(nullptr),
+    mExifData(nullptr)
+{
+    LOG1("@%s create jpeg encode processor", __func__);
+
+    mProcessor = IImageProcessor::createImageProcessor();
+    mJpegEncoder = IJpegEncoder::createJpegEncoder();
+    mJpegMaker = std::unique_ptr<JpegMaker>(new JpegMaker());
+};
+
+JpegProcess::~JpegProcess()
+{
+}
+
+void JpegProcess::attachJpegBlob(const EncodePackage &package)
+{
+    LOG2("@%s, encoded data size: %d, exif data size: %d",
+         __func__, package.encodedDataSize, package.exifDataSize);
+    uint8_t *resultPtr = static_cast<uint8_t*>(package.outputData) +
+                         package.outputSize - sizeof(struct camera3_jpeg_blob);
+
+    // save jpeg size at the end of file
+    auto *blob = reinterpret_cast<struct camera3_jpeg_blob*>(resultPtr);
+    blob->jpeg_blob_id = CAMERA3_JPEG_BLOB_ID;
+    blob->jpeg_size = package.encodedDataSize + package.exifDataSize;
+}
+
+std::shared_ptr<camera3::Camera3Buffer> JpegProcess::cropAndDownscaleThumbnail(int thumbWidth, int thumbHeight,
+                                                                               const shared_ptr<camera3::Camera3Buffer> &inBuf)
+{
+    LOG1("@%s, input size: %dx%d, thumbnail info: %dx%d",
+         __func__, inBuf->width(), inBuf->height(), thumbWidth, thumbHeight);
+
+    if (thumbWidth <= 0 || thumbHeight <= 0) {
+        LOGW("@%s, skip, thumbWidth:%d, thumbHeight:%d", __func__, thumbWidth, thumbHeight);
+        return nullptr;
+    }
+
+    int ret = OK;
+    shared_ptr<camera3::Camera3Buffer> tempBuffer = inBuf;
+
+    // Do crop first if needed
+    if (IImageProcessor::isProcessingTypeSupported(POST_PROCESS_CROP) &&
+        inBuf->width() * thumbHeight != inBuf->height() * thumbWidth) {
+        int width = 0, height = 0;
+        if (inBuf->width() * thumbHeight < inBuf->height() * thumbWidth) {
+            width = inBuf->width();
+            height = inBuf->width() * thumbHeight / thumbWidth;
+        } else {
+            width = inBuf->height() * thumbWidth / thumbHeight;
+            height = inBuf->height();
+        }
+
+        if (mCropBuffer && (mCropBuffer->width() != width || mCropBuffer->height() != height))
+            mCropBuffer.reset();
+        if (!mCropBuffer) {
+            int size = CameraUtils::getFrameSize(inBuf->v4l2Fmt(), width, height);
+            mCropBuffer = camera3::MemoryUtils::allocateHeapBuffer(width, height,
+                                                                   width, inBuf->v4l2Fmt(),
+                                                                   mCameraId, size);
+            CheckError(!mCropBuffer, nullptr, "%s, Failed to allocate the internal crop buffer", __func__);
+        }
+
+        LOG2("@%s, Crop the main buffer from %dx%d to %dx%d",
+             __func__, inBuf->width(), inBuf->height(), width, height);
+        ret = mProcessor->cropFrame(inBuf, mCropBuffer);
+        CheckError(ret != OK, nullptr, "%s, Failed to crop the frame", __func__);
+        tempBuffer = mCropBuffer;
+    }
+
+    if (IImageProcessor::isProcessingTypeSupported(POST_PROCESS_SCALING)) {
+        if (mScaleBuffer && (mScaleBuffer->width() != thumbWidth
+            || mScaleBuffer->height() != thumbHeight))
+            mScaleBuffer.reset();
+        if (!mScaleBuffer) {
+            int size = CameraUtils::getFrameSize(inBuf->v4l2Fmt(), thumbWidth, thumbHeight);
+            mScaleBuffer = camera3::MemoryUtils::allocateHeapBuffer(thumbWidth, thumbHeight,
+                                                                    thumbWidth, inBuf->v4l2Fmt(),
+                                                                    mCameraId, size);
+            CheckError(!mScaleBuffer, nullptr, "%s, Failed to allocate the internal scale buffer", __func__);
+        }
+
+        LOG2("@%s, Scale the buffer from %dx%d to %dx%d",
+              __func__, inBuf->width(), inBuf->height(), thumbWidth, thumbHeight);
+        ret = mProcessor->scaleFrame(tempBuffer, mScaleBuffer);
+        CheckError(ret != OK, nullptr, "%s, Failed to crop the frame", __func__);
+        tempBuffer = mScaleBuffer;
+    }
+
+    if (tempBuffer->width() != thumbWidth || tempBuffer->height() != thumbHeight) {
+        LOGE("%s, Failed to crop & downscale the main buffer to thumbnail buffer", __func__);
+        return nullptr;
+    }
+
+    return tempBuffer;
+}
+
+void JpegProcess::fillEncodeInfo(const shared_ptr<camera3::Camera3Buffer> &inBuf,
+                                 const shared_ptr<camera3::Camera3Buffer> &outBuf,
+                                 EncodePackage &package)
+{
+    package.inputWidth = inBuf->width();
+    package.inputHeight = inBuf->height();
+    package.inputStride = inBuf->stride();
+    package.inputFormat = inBuf->v4l2Fmt();
+    package.inputSize = inBuf->size();
+    package.inputBufferHandle = static_cast<void*>(inBuf->getBufferHandle());
+    package.inputData = inBuf->data();
+
+    package.outputWidth = outBuf->width();
+    package.outputHeight = outBuf->height();
+    package.outputSize = outBuf->size();
+    package.outputBufferHandle = static_cast<void*>(outBuf->getBufferHandle());
+    package.outputData = outBuf->data();
+}
+
+status_t JpegProcess::doPostProcessing(const shared_ptr<camera3::Camera3Buffer> &inBuf,
+                                       const icamera::Parameters &parameter,
+                                       shared_ptr<camera3::Camera3Buffer> &outBuf)
+{
+    LOG1("@%s", __func__);
+    bool isEncoded = false;
+
+    icamera::ExifMetaData exifMetadata;
+    status_t status = mJpegMaker->setupExifWithMetaData(inBuf->width(), inBuf->height(), parameter, &exifMetadata);
+    CheckError(status != OK, UNKNOWN_ERROR, "@%s, Setup exif metadata failed.", __func__);
+    LOG2("@%s: setting exif metadata done!", __func__);
+
+    std::shared_ptr<camera3::Camera3Buffer> thumbInput =
+        cropAndDownscaleThumbnail(exifMetadata.mJpegSetting.thumbWidth,
+                                  exifMetadata.mJpegSetting.thumbHeight,
+                                  inBuf);
+
+    EncodePackage thumbnailPackage;
+    if (thumbInput) {
+        if (mThumbOutput == nullptr ||
+            mThumbOutput->width() != exifMetadata.mJpegSetting.thumbWidth ||
+            mThumbOutput->height() != exifMetadata.mJpegSetting.thumbHeight ||
+            mThumbOutput->v4l2Fmt() != outBuf->v4l2Fmt()) {
+            mThumbOutput = camera3::MemoryUtils::allocateHeapBuffer(
+                               exifMetadata.mJpegSetting.thumbWidth, exifMetadata.mJpegSetting.thumbHeight,
+                               exifMetadata.mJpegSetting.thumbWidth, outBuf->v4l2Fmt(),
+                               mCameraId,
+                               exifMetadata.mJpegSetting.thumbWidth * exifMetadata.mJpegSetting.thumbHeight * 3 / 2);
+            CheckError(!mThumbOutput, NO_MEMORY, "%s, Failed to allocate the mThumbOutput", __func__);
+        }
+
+        // encode thumbnail image
+        fillEncodeInfo(thumbInput, mThumbOutput, thumbnailPackage);
+        thumbnailPackage.quality = exifMetadata.mJpegSetting.jpegThumbnailQuality;
+        // the exifDataSize should be 0 for encoding thumbnail
+        thumbnailPackage.exifData = nullptr;
+        thumbnailPackage.exifDataSize = 0;
+
+        do {
+            isEncoded = mJpegEncoder->doJpegEncode(&thumbnailPackage);
+            thumbnailPackage.quality -= 5;
+        } while (thumbnailPackage.encodedDataSize > THUMBNAIL_SIZE_LIMITATION &&
+            thumbnailPackage.quality > 0);
+
+        if (!isEncoded || thumbnailPackage.quality < 0) {
+            LOGW("Failed to generate thumbnail, isEncoded: %d, encoded thumbnail size: %d, quality:%d",
+                 isEncoded, thumbnailPackage.encodedDataSize, thumbnailPackage.quality);
+        }
+    }
+
+    // save exif data
+    uint32_t exifBufSize = ENABLE_APP2_MARKER ? EXIF_SIZE_LIMITATION * 2 : EXIF_SIZE_LIMITATION;
+    if (mExifData == nullptr) {
+        mExifData = std::unique_ptr<unsigned char[]>(new unsigned char[exifBufSize]);
+    }
+    uint8_t* finalExifDataPtr = static_cast<uint8_t*>(mExifData.get());
+    uint32_t finalExifDataSize = 0;
+    status = mJpegMaker->getExif(thumbnailPackage, finalExifDataPtr, &finalExifDataSize);
+    CheckError(status != OK, status, "@%s, Failed to get Exif", __func__);
+    LOG2("%s, exifBufSize %d, finalExifDataSize %d", __func__, exifBufSize, finalExifDataSize);
+
+    // encode main image
+    EncodePackage finalEncodePackage;
+    fillEncodeInfo(inBuf, outBuf, finalEncodePackage);
+    finalEncodePackage.quality = exifMetadata.mJpegSetting.jpegQuality;
+    finalEncodePackage.exifData = finalExifDataPtr;
+    finalEncodePackage.exifDataSize = finalExifDataSize;
+    isEncoded = mJpegEncoder->doJpegEncode(&finalEncodePackage);
+    CheckError(!isEncoded, UNKNOWN_ERROR, "@%s, Failed to encode main image", __func__);
+    mJpegMaker->writeExifData(&finalEncodePackage);
+
+    attachJpegBlob(finalEncodePackage);
+
+    return OK;
+}
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/image_process/PostProcessorBase.h b/camera/hal/intel/ipu6/src/image_process/PostProcessorBase.h
new file mode 100644
index 000000000000..2039e38a0080
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/image_process/PostProcessorBase.h
@@ -0,0 +1,118 @@
+/*
+ * Copyright (C) 2019 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+#include "Parameters.h"
+#include "Camera3Buffer.h"
+#include "IImageProcessor.h"
+#include "EXIFMetaData.h"
+#include "IJpegEncoder.h"
+#include "JpegMaker.h"
+
+namespace icamera {
+
+class PostProcessorBase {
+public:
+    PostProcessorBase(std::string processName);
+    virtual ~PostProcessorBase();
+
+    std::string getName() { return mName; }
+    virtual status_t doPostProcessing(const std::shared_ptr<camera3::Camera3Buffer> &inBuf,
+                                      std::shared_ptr<camera3::Camera3Buffer> &outBuf) { return OK; }
+
+    virtual status_t doPostProcessing(const std::shared_ptr<camera3::Camera3Buffer> &inBuf,
+                                      const Parameters &parameter,
+                                      std::shared_ptr<camera3::Camera3Buffer> &outBuf) { return OK; }
+private:
+    DISALLOW_COPY_AND_ASSIGN(PostProcessorBase);
+
+protected:
+    std::string mName;
+    std::unique_ptr<IImageProcessor> mProcessor;
+};
+
+class ScaleProcess : public PostProcessorBase {
+public:
+    ScaleProcess();
+    ~ScaleProcess() {};
+
+    virtual status_t doPostProcessing(const std::shared_ptr<camera3::Camera3Buffer> &inBuf,
+                                      std::shared_ptr<camera3::Camera3Buffer> &outBuf);
+};
+
+class RotateProcess : public PostProcessorBase {
+public:
+    RotateProcess(int angle);
+    ~RotateProcess() {};
+
+    virtual status_t doPostProcessing(const std::shared_ptr<camera3::Camera3Buffer> &inBuf,
+                                      std::shared_ptr<camera3::Camera3Buffer> &outBuf);
+private:
+    int mAngle;
+};
+
+class CropProcess : public PostProcessorBase {
+public:
+    CropProcess();
+    ~CropProcess() {};
+
+    virtual status_t doPostProcessing(const std::shared_ptr<camera3::Camera3Buffer> &inBuf,
+                                      std::shared_ptr<camera3::Camera3Buffer> &outBuf);
+};
+
+class ConvertProcess : public PostProcessorBase {
+public:
+    ConvertProcess();
+    ~ConvertProcess() {};
+
+    virtual status_t doPostProcessing(const std::shared_ptr<camera3::Camera3Buffer> &inBuf,
+                                      std::shared_ptr<camera3::Camera3Buffer> &outBuf);
+};
+
+class JpegProcess : public PostProcessorBase {
+public:
+    JpegProcess(int cameraId);
+    ~JpegProcess();
+
+    virtual status_t doPostProcessing(const std::shared_ptr<camera3::Camera3Buffer> &inBuf,
+                                      const Parameters &parameter,
+                                      std::shared_ptr<camera3::Camera3Buffer> &outBuf);
+private:
+    void attachJpegBlob(const EncodePackage &package);
+    std::shared_ptr<camera3::Camera3Buffer>
+    cropAndDownscaleThumbnail(int thumbWidth, int thumbHeight,
+                              const std::shared_ptr<camera3::Camera3Buffer> &inBuf);
+    void fillEncodeInfo(const std::shared_ptr<camera3::Camera3Buffer> &inBuf,
+                        const std::shared_ptr<camera3::Camera3Buffer> &outBuf,
+                        EncodePackage &package);
+
+private:
+    int mCameraId;
+
+    std::shared_ptr<camera3::Camera3Buffer> mCropBuffer;
+    std::shared_ptr<camera3::Camera3Buffer> mScaleBuffer;
+    std::shared_ptr<camera3::Camera3Buffer> mThumbOutput;
+
+    std::unique_ptr<JpegMaker> mJpegMaker;
+    std::unique_ptr<IJpegEncoder> mJpegEncoder;
+    std::unique_ptr<unsigned char[]> mExifData;
+};
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/image_process/PostProcessorCore.cpp b/camera/hal/intel/ipu6/src/image_process/PostProcessorCore.cpp
new file mode 100644
index 000000000000..f25e37f634c9
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/image_process/PostProcessorCore.cpp
@@ -0,0 +1,141 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "PostProcessorCore"
+
+#include "iutils/CameraLog.h"
+#include "PostProcessorCore.h"
+
+using std::shared_ptr;
+
+namespace icamera {
+
+PostProcessorCore::PostProcessorCore(int cameraId) :
+    mCameraId(cameraId)
+{
+    LOG1("@%s, cameraId: %d", __func__, mCameraId);
+}
+
+PostProcessorCore::~PostProcessorCore()
+{
+    LOG1("@%s", __func__);
+}
+
+bool PostProcessorCore::isPostProcessTypeSupported(PostProcessType type)
+{
+    return IImageProcessor::isProcessingTypeSupported(type);
+}
+
+status_t PostProcessorCore::createProcessor()
+{
+    LOG2("@%s, cameraId: %d", __func__, mCameraId);
+
+    mProcessorVector.clear();
+    for (const auto &order : mProcessorsInfo) {
+        shared_ptr<PostProcessorBase> processor = nullptr;
+        switch (order.type) {
+        case POST_PROCESS_SCALING :
+            processor = std::make_shared<ScaleProcess>();
+            break;
+        case POST_PROCESS_ROTATE :
+            processor = std::make_shared<RotateProcess>(order.angle);
+            break;
+        case POST_PROCESS_CROP :
+            processor = std::make_shared<CropProcess>();
+            break;
+        case POST_PROCESS_CONVERT :
+            processor = std::make_shared<ConvertProcess>();
+            break;
+        case POST_PROCESS_JPEG_ENCODING :
+            processor = std::make_shared<JpegProcess>(mCameraId);
+            break;
+        case POST_PROCESS_NONE:
+            break;
+        default:
+            LOGE("%s, Doesn't support this kind of post-processor");
+            return UNKNOWN_ERROR;
+        }
+
+        CheckError(!processor, UNKNOWN_ERROR, "%s, Failed to create the post processor: 0x%x", __func__, order.type);
+        mProcessorVector.push_back(processor);
+    }
+
+    LOG2("%s, the number of post processor unit is %zu", __func__, mProcessorVector.size());
+    return OK;
+}
+
+status_t PostProcessorCore::allocateBuffers()
+{
+    LOG2("@%s,mProcessorVector.size: %zu cameraId: %d", __func__, mProcessorVector.size(), mCameraId);
+
+    mInterBuffers.clear();
+    for (size_t i = 0; i < mProcessorsInfo.size() - 1; i++) {
+        const stream_t &info = mProcessorsInfo[i].outputInfo;
+        shared_ptr<camera3::Camera3Buffer> buf = camera3::MemoryUtils::allocateHeapBuffer(info.width, info.height,
+                                                                                          info.stride, info.format,
+                                                                                          mCameraId, info.size);
+        CheckError(!buf, NO_MEMORY, "@%s: Failed to allocate internal buffer: processor: %s",
+              __func__, mProcessorVector[i]->getName().c_str());
+        mInterBuffers[mProcessorVector[i]] = buf;
+    }
+
+    return OK;
+}
+
+status_t PostProcessorCore::configure(const std::vector<PostProcessInfo> &processorOrder)
+{
+    if (processorOrder.empty())
+        return OK;
+
+    mProcessorsInfo = processorOrder;
+    int ret = createProcessor();
+    CheckError(ret != OK, ret, "%s, Failed to create the post processor", __func__);
+
+    ret = allocateBuffers();
+    CheckError(ret != OK, ret, "%s, Failed allocate the internal buffers", __func__);
+
+    return OK;
+}
+
+status_t PostProcessorCore::doPostProcessing(const shared_ptr<camera3::Camera3Buffer> &inBuf,
+                                             const Parameters &parameter,
+                                             shared_ptr<camera3::Camera3Buffer> outBuf)
+{
+    CheckError(!inBuf, UNKNOWN_ERROR, "%s, the inBuf is nullptr", __func__);
+    CheckError(!outBuf, UNKNOWN_ERROR, "%s, the outBuf is nullptr", __func__);
+
+    shared_ptr<camera3::Camera3Buffer> input = inBuf;
+    shared_ptr<camera3::Camera3Buffer> output = nullptr;
+    for (size_t i = 0; i < mProcessorVector.size(); i++) {
+        if (i == (mProcessorVector.size() - 1)) {
+            output = outBuf;
+        } else {
+            output = mInterBuffers[mProcessorVector[i]];
+        }
+
+        int ret = OK;
+        if (mProcessorsInfo[i].type == POST_PROCESS_JPEG_ENCODING)
+            ret = mProcessorVector[i]->doPostProcessing(input, parameter, output);
+        else
+            ret = mProcessorVector[i]->doPostProcessing(input, output);
+        CheckError(ret != OK, ret, "%s, Failed to do post processing: %s", __func__, mProcessorVector[i]->getName().c_str());
+
+        input = output;
+    }
+
+    return OK;
+}
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/image_process/PostProcessorCore.h b/camera/hal/intel/ipu6/src/image_process/PostProcessorCore.h
new file mode 100644
index 000000000000..6871a1705750
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/image_process/PostProcessorCore.h
@@ -0,0 +1,68 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <vector>
+#include <map>
+
+#include "iutils/Utils.h"
+#include "iutils/Errors.h"
+#include "PostProcessorBase.h"
+#include "ProcessType.h"
+
+namespace icamera {
+
+struct PostProcessInfo {
+    stream_t inputInfo;
+    stream_t outputInfo;
+    PostProcessType type;
+    int angle;
+    PostProcessInfo() : type(POST_PROCESS_NONE),angle(0) { CLEAR(inputInfo); CLEAR(outputInfo); }
+};
+
+/**
+ * \class PostProcessorCore
+ *
+ * This class is used to encode JPEG and rotate image.
+ *
+ */
+class PostProcessorCore {
+
+public:
+    PostProcessorCore(int cameraId);
+    virtual ~PostProcessorCore();
+
+    bool isPostProcessTypeSupported(PostProcessType type);
+    status_t configure(const std::vector<PostProcessInfo> &processorOrder);
+    status_t doPostProcessing(const std::shared_ptr<camera3::Camera3Buffer> &mainBuf,
+                              const Parameters &parameter,
+                              std::shared_ptr<camera3::Camera3Buffer> outBuf);
+private:
+    status_t createProcessor();
+    status_t allocateBuffers();
+
+private:
+    DISALLOW_COPY_AND_ASSIGN(PostProcessorCore);
+
+private:
+    int mCameraId;
+    std::map<std::shared_ptr<PostProcessorBase>, std::shared_ptr<camera3::Camera3Buffer>>
+            mInterBuffers;
+    std::vector<PostProcessInfo> mProcessorsInfo;
+    std::vector<std::shared_ptr<PostProcessorBase>> mProcessorVector;
+};
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/image_process/ProcessType.h b/camera/hal/intel/ipu6/src/image_process/ProcessType.h
new file mode 100644
index 000000000000..d7354ddd1268
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/image_process/ProcessType.h
@@ -0,0 +1,30 @@
+/*
+ * Copyright (C) 2019 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+namespace icamera {
+
+enum PostProcessType {
+    POST_PROCESS_NONE = 0,
+    POST_PROCESS_ROTATE = 1 << 0,
+    POST_PROCESS_SCALING = 1 << 1,
+    POST_PROCESS_CROP = 1 << 2,
+    POST_PROCESS_CONVERT = 1 << 3,
+    POST_PROCESS_JPEG_ENCODING = 1 << 4
+};
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/image_process/chrome/ImageProcessorCore.cpp b/camera/hal/intel/ipu6/src/image_process/chrome/ImageProcessorCore.cpp
new file mode 100644
index 000000000000..c3156ddca111
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/image_process/chrome/ImageProcessorCore.cpp
@@ -0,0 +1,234 @@
+/*
+ * Copyright (C) 2019 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "ImageProcessorCore"
+
+#include <libyuv.h>
+#include "iutils/CameraLog.h"
+#include "ImageProcessorCore.h"
+#include "ImageConverter.h"
+
+namespace icamera {
+
+ImageProcessorCore::ImageProcessorCore()
+{
+    LOG2("enter %s", __func__);
+    mRotationMode = {
+        {0, libyuv::RotationMode::kRotate0},
+        {90, libyuv::RotationMode::kRotate90},
+        {180, libyuv::RotationMode::kRotate180},
+        {270, libyuv::RotationMode::kRotate270}
+    };
+}
+
+ImageProcessorCore::~ImageProcessorCore()
+{
+    LOG2("enter %s", __func__);
+}
+
+std::unique_ptr<IImageProcessor> IImageProcessor::createImageProcessor()
+{
+    return std::unique_ptr<ImageProcessorCore>(new ImageProcessorCore());
+}
+
+//If support this kind of post process type in current OS
+bool IImageProcessor::isProcessingTypeSupported(PostProcessType type)
+{
+    int supportedType = POST_PROCESS_ROTATE |
+                        POST_PROCESS_SCALING |
+                        POST_PROCESS_CROP |
+                        POST_PROCESS_CONVERT |
+                        POST_PROCESS_JPEG_ENCODING;
+
+    return supportedType & type;
+}
+
+status_t ImageProcessorCore::cropFrame(const std::shared_ptr<camera3::Camera3Buffer> &input,
+                                       std::shared_ptr<camera3::Camera3Buffer> &output)
+{
+    LOG2("%s: src: %dx%d,format 0x%x, dest: %dx%d format 0x%x",
+         __func__, input->width(), input->height(), input->v4l2Fmt(),
+         output->width(), output->height(), output->v4l2Fmt());
+
+    int srcW = input->stride();
+    int srcH = input->height();
+    int dstW = output->stride();
+    int dstH = output->height();
+
+    std::unique_ptr<uint8_t[]> srcI420Buf;
+    unsigned int srcI420BufSize = srcW * srcH * 3 / 2;
+    srcI420Buf.reset(new uint8_t[srcI420BufSize]);
+
+    const uint8_t* srcBufY = static_cast<uint8_t*>(input->data());
+    const uint8_t* srcBufUV = srcBufY + srcW * srcH;
+    uint8_t* srcI420BufY = static_cast<uint8_t*>(srcI420Buf.get());
+    uint8_t* srcI420BufU = srcI420BufY + srcW * srcH;
+    uint8_t* srcI420BufV = srcI420BufU + srcW * srcH / 4;
+    int ret = libyuv::NV12ToI420(srcBufY, srcW,
+                                 srcBufUV, srcW,
+                                 srcI420BufY, srcW,
+                                 srcI420BufU, srcW / 2,
+                                 srcI420BufV, srcW / 2,
+                                 srcW, srcH);
+    CheckError((ret != 0), UNKNOWN_ERROR, "@%s, NV12ToI420 fails", __func__);
+
+    std::unique_ptr<uint8_t[]> dstI420BufUV;
+    unsigned int dstI420BufUVSize = dstW * dstH / 2;
+    dstI420BufUV.reset(new uint8_t[dstI420BufUVSize]);
+
+    uint8_t* dstI420BufU = static_cast<uint8_t*>(dstI420BufUV.get());
+    uint8_t* dstI420BufV = dstI420BufU + dstW * dstH / 4;
+    ret = libyuv::ConvertToI420(static_cast<uint8_t*>(srcI420Buf.get()), srcI420BufSize,
+                                static_cast<uint8_t*>(output->data()), dstW,
+                                dstI420BufU, (dstW + 1) / 2,
+                                dstI420BufV, (dstW + 1) / 2,
+                                (srcW - dstW) / 2, (srcH - dstH) / 2,
+                                srcW, srcH, dstW, dstH,
+                                libyuv::RotationMode::kRotate0, libyuv::FourCC::FOURCC_I420);
+    CheckError(ret != 0, UNKNOWN_ERROR, "@%s, ConvertToI420 fails", __func__);
+
+    uint8_t* dstBufUV = static_cast<uint8_t*>(output->data()) + dstW * dstH;
+    libyuv::MergeUVPlane(dstI420BufU, (dstW + 1) / 2,
+                         dstI420BufV, (dstW + 1) / 2,
+                         dstBufUV, dstW,
+                         (dstW + 1) / 2, (dstH + 1) / 2);
+
+    return OK;
+}
+
+status_t ImageProcessorCore::scaleFrame(const std::shared_ptr<camera3::Camera3Buffer> &input,
+                                        std::shared_ptr<camera3::Camera3Buffer> &output)
+{
+    LOG2("%s: src: %dx%d,format 0x%x, dest: %dx%d format 0x%x",
+         __func__, input->width(), input->height(), input->v4l2Fmt(),
+         output->width(), output->height(), output->v4l2Fmt());
+
+    // Y plane
+    libyuv::ScalePlane(static_cast<uint8_t*>(input->data()),
+                       input->stride(),
+                       input->width(),
+                       input->height(),
+                       static_cast<uint8_t*>(output->data()),
+                       output->stride(),
+                       output->width(),
+                       output->height(),
+                       libyuv::kFilterNone);
+
+    // UV plane
+    int inUVOffsetByte = input->stride() * input->height();
+    int outUVOffsetByte = output->stride() * output->height();
+    libyuv::ScalePlane_16(static_cast<uint16_t*>(input->data()) + inUVOffsetByte / sizeof(uint16_t),
+                          input->stride() / 2,
+                          input->width() / 2,
+                          input->height() / 2,
+                          static_cast<uint16_t*>(output->data()) + outUVOffsetByte / sizeof(uint16_t),
+                          output->stride() / 2,
+                          output->width() / 2,
+                          output->height() / 2,
+                          libyuv::kFilterNone);
+
+    return OK;
+}
+
+status_t ImageProcessorCore::rotateFrame(const std::shared_ptr<camera3::Camera3Buffer> &input,
+                                         std::shared_ptr<camera3::Camera3Buffer> &output,
+                                         int angle, std::vector<uint8_t> &rotateBuf)
+{
+    LOG2("%s: src: %dx%d,format 0x%x, dest: %dx%d format 0x%x",
+         __func__, input->width(), input->height(), input->v4l2Fmt(),
+         output->width(), output->height(), output->v4l2Fmt());
+
+    CheckError(output->width() != input->height() || output->height() != input->width(),
+          BAD_VALUE, "output resolution mis-match [%d x %d] -> [%d x %d]",
+          input->width(), input->height(), output->width(), output->height());
+    CheckError((angle != 90 && angle != 270), BAD_VALUE, "angle value:%d is wrong", angle);
+
+    const uint8_t* inBuffer = static_cast<uint8_t*>(input->data());
+    uint8_t* outBuffer = static_cast<uint8_t*>(output->data());
+    int outW = output->width();
+    int outH = output->height();
+    int outStride = output->stride();
+    int inW = input->width();
+    int inH = input->height();
+    int inStride = input->stride();
+    if (rotateBuf.size() < input->size()) {
+        rotateBuf.resize(input->size());
+    }
+
+    // TODO: find a way to rotate NV12 directly.
+    uint8_t* I420Buffer = rotateBuf.data();
+
+    if (mRotationMode[angle] == libyuv::RotationMode::kRotate0) {
+        libyuv::CopyPlane(inBuffer, inStride, outBuffer, outStride, inW, inH);
+        libyuv::CopyPlane(inBuffer + inH * inStride, inStride,
+                          outBuffer + outH * outStride, outStride,
+                          inW, inH / 2);
+    } else {
+        int ret = libyuv::NV12ToI420Rotate(
+                      inBuffer, inStride, inBuffer + inH * inStride, inStride,
+                      I420Buffer, outW,
+                      I420Buffer + outW * outH, outW / 2,
+                      I420Buffer + outW * outH * 5 / 4, outW / 2,
+                      inW, inH, mRotationMode[angle]);
+        CheckError((ret < 0), UNKNOWN_ERROR, "@%s, rotate fail [%d]!", __func__, ret);
+
+        ret = libyuv::I420ToNV12(I420Buffer, outW,
+                                 I420Buffer + outW * outH, outW / 2,
+                                 I420Buffer + outW * outH * 5 / 4, outW / 2,
+                                 outBuffer, outStride,
+                                 outBuffer +  outStride * outH, outStride,
+                                 outW, outH);
+        CheckError((ret < 0), UNKNOWN_ERROR, "@%s, convert fail [%d]!", __func__, ret);
+    }
+
+    return OK;
+}
+
+status_t ImageProcessorCore::convertFrame(const std::shared_ptr<camera3::Camera3Buffer> &input,
+                                          std::shared_ptr<camera3::Camera3Buffer> &output)
+{
+    LOG2("%s: src: %dx%d,format 0x%x, dest: %dx%d format 0x%x",
+         __func__, input->width(), input->height(), input->v4l2Fmt(),
+         output->width(), output->height(), output->v4l2Fmt());
+
+    switch (output->v4l2Fmt()) {
+        case V4L2_PIX_FMT_YVU420:
+            // XXX -> YV12
+            ImageConverter::convertBuftoYV12(input->v4l2Fmt(), input->width(),
+                                             input->height(), input->stride(),
+                                             output->stride(), input->data(), output->data());
+            break;
+        case V4L2_PIX_FMT_NV21:
+            // XXX -> NV21
+            ImageConverter::convertBuftoNV21(input->v4l2Fmt(), input->width(),
+                                             input->height(), input->stride(),
+                                             output->stride(), input->data(), output->data());
+            break;
+        case V4L2_PIX_FMT_YUYV:
+            // XXX -> YUYV
+            ImageConverter::convertBuftoYUYV(input->v4l2Fmt(), input->width(),
+                                             input->height(), input->stride(),
+                                             output->stride(), input->data(), output->data());
+            break;
+        default:
+            LOGE("%s: not implement for color conversion 0x%x -> 0x%x!",
+                 __func__, input->v4l2Fmt(), output->v4l2Fmt());
+            return UNKNOWN_ERROR;
+    }
+
+    return OK;
+}
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/image_process/chrome/ImageProcessorCore.h b/camera/hal/intel/ipu6/src/image_process/chrome/ImageProcessorCore.h
new file mode 100644
index 000000000000..d874f371a696
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/image_process/chrome/ImageProcessorCore.h
@@ -0,0 +1,44 @@
+/*
+ * Copyright (C) 2019 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+#include <unordered_map>
+#include "IImageProcessor.h"
+
+namespace icamera {
+
+class ImageProcessorCore : public IImageProcessor {
+public:
+    ImageProcessorCore();
+    ~ImageProcessorCore();
+
+    virtual status_t cropFrame(const std::shared_ptr<camera3::Camera3Buffer> &input,
+                               std::shared_ptr<camera3::Camera3Buffer> &output);
+    virtual status_t scaleFrame(const std::shared_ptr<camera3::Camera3Buffer> &input,
+                                std::shared_ptr<camera3::Camera3Buffer> &output);
+    virtual status_t rotateFrame(const std::shared_ptr<camera3::Camera3Buffer> &input,
+                                 std::shared_ptr<camera3::Camera3Buffer> &output,
+                                 int angle, std::vector<uint8_t> &rotateBuf);
+    virtual status_t convertFrame(const std::shared_ptr<camera3::Camera3Buffer> &input,
+                                  std::shared_ptr<camera3::Camera3Buffer> &output);
+
+private:
+    DISALLOW_COPY_AND_ASSIGN(ImageProcessorCore);
+
+    std::unordered_map<int, libyuv::RotationMode> mRotationMode;
+};
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/iutils/CameraDump.cpp b/camera/hal/intel/ipu6/src/iutils/CameraDump.cpp
new file mode 100644
index 000000000000..58676a31f0a7
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/iutils/CameraDump.cpp
@@ -0,0 +1,409 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "CameraDump"
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <dirent.h>
+#include <iostream>
+#include <fstream>
+#include <sstream>
+#include <math.h>
+
+#include "PlatformData.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+#include "iutils/CameraLog.h"
+#include "iutils/CameraDump.h"
+
+#include "3a/AiqResult.h"
+#include "3a/AiqResultStorage.h"
+
+using std::string;
+using std::shared_ptr;
+
+namespace icamera {
+
+int  gDumpType = 0;
+int  gDumpFormat = 0;
+uint32_t gDumpSkipNum = 0;
+uint32_t gDumpRangeMin = 0;
+uint32_t gDumpRangeMax = 0;
+int  gDumpFrequency = 1;
+char gDumpPath[50];
+bool gDumpRangeEnabled = false;
+static const char *ModuleName[] = {
+    "na",     // not available
+    "sensor",
+    "isys",
+    "psys",
+    "de-inter",
+    "swip-op"
+}; // map to the ModuleType
+
+static const char *StreamUsage[] = {
+    "preview",
+    "video",
+    "still",
+    "app",
+}; // map to the StreamUsage
+
+void CameraDump::setDumpLevel(void)
+{
+    const char* PROP_CAMERA_HAL_DUMP      = "cameraDump";
+    const char* PROP_CAMERA_HAL_DUMP_FORMAT = "cameraDumpFormat";
+    const char* PROP_CAMERA_HAL_DUMP_PATH = "cameraDumpPath";
+    const char* PROP_CAMERA_HAL_DUMP_SKIP_NUM = "cameraDumpSkipNum";
+    const char* PROP_CAMERA_HAL_DUMP_RANGE = "cameraDumpRange";
+    const char* PROP_CAMERA_HAL_DUMP_FREQUENCY = "cameraDumpFrequency";
+
+    // dump, it's used to dump images or some parameters to a file.
+    char *dumpType = getenv(PROP_CAMERA_HAL_DUMP);
+    if (dumpType) {
+        gDumpType = strtoul(dumpType, nullptr, 0);
+        LOGD("Dump type is 0x%x", gDumpType);
+    }
+
+    char *dumpFormat = getenv(PROP_CAMERA_HAL_DUMP_FORMAT);
+    if (dumpFormat) {
+        gDumpFormat = strtoul(dumpFormat, nullptr, 0);
+        LOGD("Dump format is 0x%x", gDumpFormat);
+    }
+
+    char* cameraDumpPath = getenv(PROP_CAMERA_HAL_DUMP_PATH);
+    snprintf(gDumpPath, sizeof(gDumpPath), "%s", "./");
+    if (cameraDumpPath) {
+        snprintf(gDumpPath, sizeof(gDumpPath), "%s", cameraDumpPath);
+    }
+
+    char* cameraDumpSkipNum = getenv(PROP_CAMERA_HAL_DUMP_SKIP_NUM);
+    if (cameraDumpSkipNum) {
+        gDumpSkipNum = strtoul(cameraDumpSkipNum, nullptr, 0);
+        LOGD("Dump skip num is %d", gDumpSkipNum);
+    }
+
+    char* cameraDumpRange = getenv(PROP_CAMERA_HAL_DUMP_RANGE);
+    if (cameraDumpRange) {
+        int sz = strlen(cameraDumpRange);
+        char dumpRange[sz + 1];
+        char *savePtr = nullptr, *tablePtr = nullptr;
+        MEMCPY_S(dumpRange, sz, cameraDumpRange, sz);
+        dumpRange[sz] = '\0';
+
+        tablePtr = strtok_r(dumpRange, ",~-", &savePtr);
+        if (tablePtr)
+            gDumpRangeMin = strtoul(tablePtr, nullptr, 0);
+
+        tablePtr = strtok_r(nullptr, ",~-", &savePtr);
+        if (tablePtr)
+            gDumpRangeMax = strtoul(tablePtr, nullptr, 0);
+
+        gDumpRangeEnabled = true;
+        LOGD("Dump range is %d-%d", gDumpRangeMin, gDumpRangeMax);
+    }
+
+    char* cameraDumpFrequency = getenv(PROP_CAMERA_HAL_DUMP_FREQUENCY);
+    if (cameraDumpFrequency) {
+        gDumpFrequency = strtoul(cameraDumpFrequency, nullptr, 0);
+        if (gDumpFrequency == 0)
+            gDumpFrequency = 1;
+        LOGD("Dump frequency is %d", gDumpFrequency);
+    }
+
+    // the PG dump is implemented in libiacss
+    if (gDumpType & DUMP_PSYS_PG) {
+        const char* PROP_CAMERA_CSS_DEBUG     = "camera_css_debug";
+        const char* PROP_CAMERA_CSS_DUMP_PATH = "camera_css_debug_dump_path";
+
+        char newCssDebugEnv[16];
+        char *cssDebugEnv = getenv(PROP_CAMERA_CSS_DEBUG);
+        int  cssDebugType = cssDebugEnv ? strtoul(cssDebugEnv, nullptr, 0) : 0;
+        // defined in ia_log.h IA_CSS_LOG_LEVEL_DUMP = 64
+        const int IA_CSS_LOG_LEVEL_DUMP = 64;
+        snprintf(newCssDebugEnv, sizeof(newCssDebugEnv), "%d",
+                (cssDebugType | IA_CSS_LOG_LEVEL_DUMP));
+        // enable dump env in libiacss
+        if (setenv(PROP_CAMERA_CSS_DEBUG, newCssDebugEnv, 1)) {
+            LOGE("setenv error for %s, current value:%d\n", PROP_CAMERA_CSS_DEBUG,
+                    cssDebugType);
+        }
+
+        const char* cssDumpPath = getenv(PROP_CAMERA_CSS_DUMP_PATH);
+        // set dump path to hal dump path
+        if (setenv(PROP_CAMERA_CSS_DUMP_PATH, gDumpPath, 1)) {
+            LOGE("setenv error for %s, current path:%s\n", PROP_CAMERA_CSS_DUMP_PATH,
+                    cssDumpPath ? cssDumpPath : "null");
+        }
+    }
+}
+
+bool CameraDump::isDumpTypeEnable(int dumpType)
+{
+    return gDumpType & dumpType;
+}
+
+bool CameraDump::isDumpFormatEnable(int dumpFormat)
+{
+    return gDumpFormat & dumpFormat;
+}
+
+const char* CameraDump::getDumpPath(void)
+{
+    return gDumpPath;
+}
+
+void CameraDump::writeData(const void* data, int size, const char* fileName) {
+    CheckError((data == nullptr || size == 0 || fileName == nullptr), VOID_VALUE, "Nothing needs to be dumped");
+
+    FILE *fp = fopen (fileName, "w+");
+    CheckError(fp == nullptr, VOID_VALUE, "open dump file %s failed", fileName);
+
+    LOG1("Write data to file:%s", fileName);
+    if ((fwrite(data, size, 1, fp)) != 1)
+        LOGW("Error or short count writing %d bytes to %s", size, fileName);
+    fclose (fp);
+}
+
+static string getNamePrefix(int cameraId, ModuleType_t type, Port port, int sUsage = 0)
+{
+    const char* dumpPath   = CameraDump::getDumpPath();
+    const char* sensorName = PlatformData::getSensorName(cameraId);
+    char prefix[MAX_NAME_LEN] = {'\0'};
+
+    if ((sUsage >= static_cast<int>(ARRAY_SIZE(StreamUsage))) || (sUsage < 0)) {
+        sUsage = 0;
+    }
+
+    if (icamera::CameraDump::isDumpFormatEnable(DUMP_FORMAT_IQSTUDIO)) {
+        snprintf(prefix, (MAX_NAME_LEN - 1), "%s/name#%s_%s", dumpPath, sensorName, StreamUsage[sUsage]);
+    } else {
+        if (port == INVALID_PORT) {
+            snprintf(prefix, (MAX_NAME_LEN - 1), "%s/cam%d_%s_%s_%s", dumpPath, cameraId,
+                 sensorName, ModuleName[type], StreamUsage[sUsage]);
+        } else {
+            snprintf(prefix, (MAX_NAME_LEN - 1), "%s/cam%d_%s_%s_port%d_%s", dumpPath, cameraId,
+                 sensorName, ModuleName[type], port, StreamUsage[sUsage]);
+        }
+    }
+
+    return string(prefix);
+}
+
+static string getAiqSettingAppendix(int cameraId, long sequence)
+{
+    char settingAppendix[MAX_NAME_LEN] = {'\0'};
+
+    AiqResult* aiqResults = const_cast<AiqResult*>(AiqResultStorage::getInstance(cameraId)->getAiqResult(sequence));
+    if (aiqResults == nullptr) {
+        LOGW("%s: no result for sequence %ld! use the latest instead", __func__, sequence);
+        aiqResults = const_cast<AiqResult*>(AiqResultStorage::getInstance(cameraId)->getAiqResult());
+        CheckError((aiqResults == nullptr), string(settingAppendix), "Cannot find available aiq result.");
+    }
+
+    ia_aiq_exposure_sensor_parameters *sensorExposure =
+                                       aiqResults->mAeResults.exposures[0].sensor_exposure;
+    ia_aiq_exposure_parameters *exposure =
+                                       aiqResults->mAeResults.exposures[0].exposure;
+
+    CheckError((sensorExposure == nullptr || exposure == nullptr), string(settingAppendix), "Cannot find aiq exposures");
+
+    double ag = sensorExposure->analog_gain_code_global;
+    double dg = sensorExposure->digital_gain_global;
+    float ispDg = 1.0f;
+
+    LOG2("%s: original sensorExposure AG: %f, DG: %f, exposure: AG: %f, DG: %f",
+           __func__, ag, dg, exposure->analog_gain, exposure->digital_gain);
+
+    if (icamera::CameraDump::isDumpFormatEnable(DUMP_FORMAT_IQSTUDIO)) {
+
+        // Convert AG and DG per sensor for IQ Studio input.
+        const int nSteps = 256;
+        const char* sensorName = PlatformData::getSensorName(cameraId);
+        ispDg = sensorExposure->digital_gain_global;
+
+        if (strstr(sensorName, "imx185") != nullptr) {
+            LOG2("%s: AG and DG conversion made for %s.", __func__, sensorName);
+            if ((double)sensorExposure->analog_gain_code_global * 0.3 > 24) {
+                ag = 16.0 * nSteps;
+                // real gain should be  pwd(10, (db value / 20))
+                dg = nSteps * pow(10, ((double)sensorExposure->analog_gain_code_global * 0.3 - 24) / 20);
+            } else {
+                ag = nSteps * pow(10, ((double)sensorExposure->analog_gain_code_global * 0.3) / 20);
+                dg = 1.0 * nSteps;
+            }
+            LOG2("%s: converted AG: %f, DG: %f ispDG: %f for %s", __func__, ag, dg, ispDg, sensorName);
+        } else if (strstr(sensorName, "imx274") != nullptr) {
+            ag = nSteps * exposure->analog_gain;
+            dg = nSteps * PlatformData::getSensorDigitalGain(cameraId, exposure->digital_gain);
+            ispDg = nSteps * PlatformData::getIspDigitalGain(cameraId, exposure->digital_gain);
+            LOG2("%s: converted AG: %f, DG: %f ispDG: %f for %s", __func__, ag, dg, ispDg, sensorName);
+        }
+
+        if (aiqResults->mAeResults.num_exposures == 2) {
+            snprintf(settingAppendix, (MAX_NAME_LEN - 1), "~ag#%.0f~dg#%.0f~cmnt#ispdg_%.0f~exp#%d,%d",
+                ag, dg, ispDg, exposure->exposure_time_us,
+                aiqResults->mAeResults.exposures[1].exposure->exposure_time_us);
+        } else {
+            snprintf(settingAppendix, (MAX_NAME_LEN - 1), "~ag#%.0f~dg#%.0f~cmnt#ispdg_%.0f~exp#%d",
+                ag, dg, ispDg, exposure->exposure_time_us);
+        }
+    } else {
+
+        if (PlatformData::isUsingIspDigitalGain(cameraId)) {
+            dg = PlatformData::getSensorDigitalGain(cameraId, exposure->digital_gain);
+            ispDg = PlatformData::getIspDigitalGain(cameraId, exposure->digital_gain);
+        }
+
+        if (aiqResults->mAeResults.num_exposures == 2) {
+            snprintf(settingAppendix, (MAX_NAME_LEN - 1), "_ag#%.0f_dg#%.0f_ispdg#%.3f_exp#%d,%d",
+                ag, dg, ispDg, exposure->exposure_time_us,
+                aiqResults->mAeResults.exposures[1].exposure->exposure_time_us);
+        } else {
+            snprintf(settingAppendix, (MAX_NAME_LEN - 1), "_ag#%.0f_dg#%.0f_ispdg#%.3f_exp#%d",
+                ag, dg, ispDg, exposure->exposure_time_us);
+        }
+    }
+
+    return string(settingAppendix);
+}
+
+static string formatFrameFileName(const char *prefix,
+                                  const char *appendix,
+                                  const char *suffix,
+                                  long sequence,
+                                  int width, int height)
+{
+    char fileName[MAX_NAME_LEN] = {'\0'};
+
+    if (icamera::CameraDump::isDumpFormatEnable(DUMP_FORMAT_IQSTUDIO)) {
+
+        if (strstr(suffix, "GRBG") || strstr(suffix, "RGGB")
+            || strstr(suffix, "GBRG") || strstr(suffix, "BGGR")) {
+            snprintf(fileName, (MAX_NAME_LEN - 1), "%s~rev#v1~type#studio%s~msid#4442075~rep#%ld.raw",
+                prefix, appendix, sequence);
+        } else {
+            snprintf(fileName, (MAX_NAME_LEN - 1), "%s~rev#v1~type#studio%s~msid#4442075~rep#%ld.%s",
+                prefix, appendix, sequence, suffix);
+        }
+    } else {
+
+        snprintf(fileName, (MAX_NAME_LEN - 1), "%s_frame_%04ld_%dx%d%s.%s",
+             prefix, sequence, width, height, appendix, suffix);
+    }
+    return string(fileName);
+}
+
+static string formatBinFileName(int cameraId, const char *prefix, BinParam_t *binParam)
+{
+    char fileName[MAX_NAME_LEN] = {'\0'};
+    string appendix;
+
+    switch(binParam->bType) {
+    case BIN_TYPE_GENERAL:
+        snprintf(fileName, (MAX_NAME_LEN - 1), "%s_bin_%04ld_%s.bin",
+                 prefix, binParam->sequence, binParam->gParam.appendix);
+        break;
+    case BIN_TYPE_STATISTIC:
+        snprintf(fileName, (MAX_NAME_LEN - 1),
+                 "%s_stat_%04ld_grid%dx%d_%s.bin",
+                 prefix, binParam->sequence,
+                 binParam->sParam.gridWidth, binParam->sParam.gridHeight,
+                 binParam->sParam.appendix);
+        break;
+    case BIN_TYPE_SENSOR_METADATA:
+        snprintf(fileName, (MAX_NAME_LEN - 1),
+                 "%s_metadata_%04ld_%dx%d_plane%d.%s",
+                 prefix, binParam->sequence,
+                 binParam->mParam.width, binParam->mParam.height,
+                 binParam->mParam.planeIdx,
+                 CameraUtils::format2string(binParam->mParam.metaFormat).c_str());
+        break;
+    case BIN_TYPE_BUFFER:
+        appendix = getAiqSettingAppendix(cameraId, binParam->sequence);
+        return formatFrameFileName(prefix, appendix.c_str(),
+                                   CameraUtils::format2string(binParam->bParam.format).c_str(),
+                                   binParam->sequence,
+                                   binParam->bParam.width, binParam->bParam.height);
+
+    default:
+        LOGW("Unknow binary type:%d", binParam->bType);
+        break;
+    }
+
+    return string(fileName);
+}
+
+void CameraDump::dumpImage(int cameraId, const shared_ptr<CameraBuffer> &camBuffer,
+                           ModuleType_t type, Port port)
+{
+    CheckError(camBuffer == nullptr, VOID_VALUE, "invalid param");
+
+    if (camBuffer->getSequence() < gDumpSkipNum) return;
+
+    if (gDumpRangeEnabled &&
+        (camBuffer->getSequence() < gDumpRangeMin
+         || camBuffer->getSequence() > gDumpRangeMax)) {
+        return;
+    }
+
+    if (camBuffer->getSequence() % gDumpFrequency != 0) return;
+
+    string prefix   = getNamePrefix(cameraId, type, port, camBuffer->getUserBuffer()->s.usage);
+    string appendix = getAiqSettingAppendix(cameraId, camBuffer->getSequence());
+
+    string fileName = formatFrameFileName(prefix.c_str(), appendix.c_str(),
+                                          CameraUtils::format2string(camBuffer->getFormat()).c_str(),
+                                          camBuffer->getSequence(),
+                                          camBuffer->getWidth(), camBuffer->getHeight());
+
+    int fd = camBuffer->getFd();
+    int bufferSize = camBuffer->getBufferSize();
+    int memoryType = camBuffer->getMemory();
+    void* pBuf = (memoryType == V4L2_MEMORY_DMABUF)
+                    ? CameraBuffer::mapDmaBufferAddr(fd, bufferSize)
+                    : camBuffer->getBufferAddr();
+    LOGD("@%s, fd:%d, buffersize:%d, buf:%p, memoryType:%d, fileName:%s",
+            __func__, fd, bufferSize, pBuf, memoryType, fileName.c_str());
+    writeData(pBuf, bufferSize, fileName.c_str());
+    if (memoryType == V4L2_MEMORY_DMABUF) {
+        CameraBuffer::unmapDmaBufferAddr(pBuf, bufferSize);
+    }
+}
+
+void CameraDump::dumpBinary(int cameraId, const void *data, int size, BinParam_t *binParam)
+{
+    CheckError(binParam == nullptr, VOID_VALUE, "invalid param");
+
+    if (binParam->sequence < gDumpSkipNum) return;
+
+    if (gDumpRangeEnabled &&
+        (binParam->sequence < gDumpRangeMin
+         || binParam->sequence > gDumpRangeMax)) {
+        return;
+    }
+
+    if (binParam->sequence % gDumpFrequency != 0) return;
+
+    string prefix   = getNamePrefix(cameraId, binParam->mType, INVALID_PORT, binParam->sUsage);
+    string fileName = formatBinFileName(cameraId, prefix.c_str(), binParam);
+    LOG2("@%s, fileName:%s", __func__, fileName.c_str());
+    writeData(data, size, fileName.c_str());
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/iutils/CameraDump.h b/camera/hal/intel/ipu6/src/iutils/CameraDump.h
new file mode 100644
index 000000000000..3b51c84468b9
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/iutils/CameraDump.h
@@ -0,0 +1,170 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <string.h>
+#include <vector>
+#include <string>
+#include <linux/v4l2-subdev.h>
+
+#include "CameraTypes.h"
+#include "CameraBuffer.h"
+
+namespace icamera {
+
+/**
+ * global dump level
+ * This global variable is set from system properties
+ * It is used to control the type of frame dump
+ */
+extern int gDumpType;
+extern char gDumpPath[50];
+
+// Dump bit mask definition
+enum {
+    // ISYS Buffer dump (bit[0-3])
+    DUMP_ISYS_BUFFER =          1 << 0,
+    // Reserve bit[1-3] for detailed buffer dump control
+
+    DUMP_JPEG_BUFFER =          1 << 3, // JPEG buffer
+    // ISYS PG/PAL/Stats dump (bit[4-7])
+    DUMP_ISYS_PAL =             1 << 4, // ISP param binary
+    DUMP_ISYS_PG =              1 << 5, // ISYS whole PG dump assisted by libiacss
+    DUMP_ISYS_ENCODED_STAT =    1 << 6, // p2p encoded statistics
+    DUMP_ISYS_AIQ_STAT =        1 << 7, // rgbs_grid format stats for AIQ use
+
+    // PSYS dump (bit[8-11])
+    DUMP_PSYS_OUTPUT_BUFFER =   1 << 8,
+    DUMP_PSYS_INTERM_BUFFER =   1 << 9, // dump Psys intermediate buffers like PreGDC output
+    // Reserve bit[10-11] for detailed buffer dump control
+
+    DUMP_AIQ_DVS_RESULT =       1 << 10, // dump dvs result
+
+    // PSYS PG/PAL/Stats dump (bit[12-15])
+    DUMP_PSYS_PAL =             1 << 12, // ISP param binary
+    DUMP_PSYS_PG =              1 << 13, // PSYS whole PG dump assisted by libiacss
+    DUMP_PSYS_AIQ_STAT =        1 << 14, // rgbs_grid format stats for AIQ use
+    DUMP_PSYS_DECODED_STAT =    1 << 15, // p2p decoded statistics
+
+    // Other dump
+    DUMP_MIPI_BUFFER =          1 << 16, // e.g. export cameraDump=0x10000
+    DUMP_UT_BUFFER =            1 << 17, // e.g. export cameraDump=0x20000
+    DUMP_EMBEDDED_METADATA =    1 << 18,
+    DUMP_DEINTERLACED_BUFFER =  1 << 19, // 0x80000  Decimal val 524288
+    DUMP_SW_IMG_PROC_OUTPUT =   1 << 20, // 0x100000 Decimal val 1048576
+
+    // Pipe executors' dump
+    DUMP_EXECUTOR_OUTPUT =   1 << 21, // 0x200000 Decimal val 2097152
+
+    // LTM output's dump
+    DUMP_LTM_OUTPUT = 1 << 22, // 0x400000 Decimal val 4194304
+
+    DUMP_AAL_OUTPUT = 1 << 23, // 0x800000 Decimal val 8388608
+    DUMP_AAL_INPUT = 1 << 24, // 0x1000000 Decimal val 16777216
+};
+
+enum {
+    DUMP_FORMAT_NORMAL =          1 << 0,  // Normal format
+    DUMP_FORMAT_IQSTUDIO =        1 << 1,  // IQStudio format
+};
+
+const int MAX_NAME_LEN = 256;
+
+typedef enum {
+    M_NA,
+    M_SENSOR, // MIPI frame dump
+    M_ISYS,   // ISYS param, payload, frame dump
+    M_PSYS,   // PSYS param, payload, frame dump
+    M_DEINTR, // De-interlaced frame dump
+    M_SWIPOP, // Sw Image processor frame dump
+} ModuleType_t;
+
+typedef enum {
+    BIN_TYPE_GENERAL,
+    BIN_TYPE_STATISTIC,
+    BIN_TYPE_SENSOR_METADATA,
+    BIN_TYPE_BUFFER,
+} BinType_t;
+
+typedef struct {
+    const char* appendix;
+} GeneralParam_t;
+
+typedef struct {
+    int gridWidth;
+    int gridHeight;
+    const char* appendix;
+} StatParam_t;
+
+typedef struct {
+    int width;
+    int height;
+    int planeIdx;
+    int metaFormat;
+} SensorMetadataParam_t;
+
+typedef struct {
+    int width;
+    int height;
+    int format;
+} BufferParam_t;
+
+typedef struct {
+    BinType_t       bType;
+    ModuleType_t    mType;
+    long sequence;
+    union {
+        GeneralParam_t        gParam;
+        StatParam_t           sParam;
+        SensorMetadataParam_t mParam;
+        BufferParam_t         bParam;
+    };
+    int sUsage;
+} BinParam_t;
+
+/**
+ * Dump files with formated file name, put under getDumpPath()
+ * Supported dump type:
+ *   Image(RAW/YUV/RGB)
+ *   PAL bin
+ *   Decoded statistics
+ *   Sensor Metadata
+ * File name format example:
+ * Path/cameraId_sensorName_isys(psys)_
+ *      frame(pal/stats/)_sequence_resolution_appendix.suffix
+ */
+namespace CameraDump {
+    /**
+     * File dump control functions.
+     */
+    void setDumpLevel(void);
+    bool isDumpTypeEnable(int dumpType);
+    bool isDumpFormatEnable(int dumpFormat);
+    void writeData(const void* data, int size, const char* fileName);
+    const char* getDumpPath(void);
+    /**
+     * Dump image according to CameraBuffer properties
+     */
+    void dumpImage(int cameraId, const std::shared_ptr<CameraBuffer> &camBuffer,
+                   ModuleType_t mType = M_NA, Port port = INVALID_PORT);
+    /**
+     * Dump any buffer to binary file
+     */
+    void dumpBinary(int cameraId, const void *data, int size, BinParam_t *binParam);
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/iutils/CameraLog.cpp b/camera/hal/intel/ipu6/src/iutils/CameraLog.cpp
new file mode 100644
index 000000000000..01f7d29c3e11
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/iutils/CameraLog.cpp
@@ -0,0 +1,270 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "CameraLog"
+
+#include <sys/time.h>
+#include <time.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <stdarg.h>
+
+#include "iutils/Utils.h"
+
+#include "CameraLog.h"
+#include "Trace.h"
+
+namespace icamera {
+int gLogLevel = 0;
+char *gLogModules = nullptr;
+int gPerfLevel = 0;
+int gEnforceDvs = 0;
+int gSlowlyRunRatio = 0;
+bool gIsDumpMediaTopo = false;
+bool gIsDumpMediaInfo = false;
+
+static void getLogTime(char *timeBuf, int bufLen)
+{
+    // The format of time is: 01-22 15:24:53.071
+    struct timeval tv;
+    gettimeofday(&tv, nullptr);
+    time_t nowtime = tv.tv_sec;
+    struct tm* nowtm = localtime(&nowtime);
+    if (nowtm) { // If nowtm is nullptr, simply print nothing for time info
+        char tmbuf[bufLen];
+        CLEAR(tmbuf);
+        strftime(tmbuf, bufLen, "%m-%d %H:%M:%S", nowtm);
+        snprintf(timeBuf, bufLen, "%s.%03ld", tmbuf, tv.tv_usec/1000);
+    }
+}
+
+__attribute__((__format__ (__printf__, 3, 0)))
+static void printLog(const char *module, const char *level, const char *fmt, va_list ap)
+{
+    // Add time into beginning of the log.
+    const int BUF_LEN = 64;
+    char timeBuf[BUF_LEN] = {'\0'};
+
+    getLogTime(timeBuf, BUF_LEN);
+
+    fprintf(stdout, "%s: [%s]: CamHAL_%s:", timeBuf, level, module);
+    vfprintf(stdout, fmt, ap);
+    fprintf(stdout, "\n");
+}
+
+namespace Log {
+
+void setDebugLevel(void)
+{
+    const char* PROP_CAMERA_HAL_DEBUG = "cameraDebug";
+    const char* PROP_CAMERA_HAL_MODULES = "cameraModules";
+    const char* PROP_CAMERA_HAL_PERF  = "cameraPerf";
+    const char* PROP_CAMERA_HAL_DVS = "cameraDvs";
+    const char* PROP_CAMERA_RUN_RATIO = "cameraRunRatio";
+
+    // debug
+    char *dbgLevel = getenv(PROP_CAMERA_HAL_DEBUG);
+    if (dbgLevel) {
+        gLogLevel = strtoul(dbgLevel, nullptr, 0);
+        LOG1("Debug level is 0x%x", gLogLevel);
+
+        // to enable both LOG1 and LOG2 traces
+        if (gLogLevel & CAMERA_DEBUG_LOG_LEVEL2)
+            gLogLevel |= CAMERA_DEBUG_LOG_LEVEL1;
+    }
+
+    char *slowlyRunRatio = getenv(PROP_CAMERA_RUN_RATIO);
+    if (slowlyRunRatio) {
+        gSlowlyRunRatio = strtoul(slowlyRunRatio, nullptr, 0);
+        LOG1("Slow run ratio is 0x%x", gSlowlyRunRatio);
+    }
+
+    //modules
+    gLogModules = getenv(PROP_CAMERA_HAL_MODULES);
+
+    // performance
+    char *perfLevel = getenv(PROP_CAMERA_HAL_PERF);
+    if (perfLevel) {
+        gPerfLevel = strtoul(perfLevel, nullptr, 0);
+        LOGD("Performance level is 0x%x", gPerfLevel);
+
+        // bitmask of tracing categories
+        if (gPerfLevel & CAMERA_DEBUG_LOG_PERF_TRACES) {
+            LOGD("Perf KPI start/end trace is not yet supported");
+        }
+        if (gPerfLevel & CAMERA_DEBUG_LOG_PERF_TRACES_BREAKDOWN) {
+            LOGD("Perf KPI breakdown trace is not yet supported");
+        }
+        if (gPerfLevel & CAMERA_DEBUG_LOG_PERF_IOCTL_BREAKDOWN) {
+            LOGD("Perf IOCTL breakdown trace is not yet supported");
+        }
+        if (gPerfLevel & CAMERA_DEBUG_LOG_PERF_MEMORY) {
+            LOGD("Perf memory breakdown trace is not yet supported");
+        }
+        if (gPerfLevel & CAMERA_DEBUG_LOG_MEDIA_TOPO_LEVEL) {
+            gIsDumpMediaTopo = true;
+        }
+        if (gPerfLevel & CAMERA_DEBUG_LOG_MEDIA_CONTROLLER_LEVEL) {
+            gIsDumpMediaInfo = true;
+        }
+        ScopedAtrace::setTraceLevel(gPerfLevel);
+    }
+
+    // Enforce DVS for debugging
+    char *dvs = getenv(PROP_CAMERA_HAL_DVS);
+    if (dvs) {
+        gEnforceDvs = strtoul(dvs, nullptr, 0);
+        LOGD("EnforceDvs level is 0x%x", gEnforceDvs);
+    }
+}
+
+bool isDebugLevelEnable(int level)
+{
+    return gLogLevel & level;
+}
+
+bool isModulePrintAble(const char *module)
+{
+    if (gLogModules == nullptr) {
+        return true;
+    } else if (strstr(gLogModules, module) != nullptr) {
+        return true;
+    } else {
+        return false;
+    }
+}
+
+bool isDumpMediaTopo(void)
+{
+    return gIsDumpMediaTopo;
+}
+
+bool isDumpMediaInfo(void)
+{
+    return gIsDumpMediaInfo;
+}
+
+__attribute__((__format__ (__printf__, 4, 0)))
+void print_log(bool enable, const char *module, const int level, const char *format, ...)
+{
+    if (!enable && (level != CAMERA_DEBUG_LOG_ERR))
+        return;
+
+    if (!isModulePrintAble(module)) {
+        return;
+    }
+
+    const char *levelStr = nullptr;
+    va_list arg;
+    va_start(arg, format);
+
+    switch(level) {
+        case CAMERA_DEBUG_LOG_LEVEL1:
+            levelStr = "LV1";
+        break;
+        case CAMERA_DEBUG_LOG_LEVEL2:
+            levelStr = "LV2";
+        break;
+        case CAMERA_DEBUG_LOG_REQ_STATE:
+            levelStr = "REQ";
+        break;
+        case CAMERA_DEBUG_LOG_AIQ:
+            levelStr = "AIQ";
+        break;
+        case CAMERA_DEBUG_LOG_XML:
+            levelStr = "XML";
+        break;
+        case CAMERA_DEBUG_LOG_DBG:
+            levelStr = "DBG";
+        break;
+        case CAMERA_DEBUG_LOG_INFO:
+            levelStr = "INF";
+        break;
+        case CAMERA_DEBUG_LOG_ERR:
+            levelStr = "ERR";
+        break;
+        case CAMERA_DEBUG_LOG_WARNING:
+            levelStr = "WAR";
+        break;
+        case CAMERA_DEBUG_LOG_VERBOSE:
+            levelStr = "VER";
+        break;
+        case CAMERA_DEBUG_LOG_VC_SYNC:
+            levelStr = "VCSYNC";
+        break;
+        case CAMERA_DEBUG_LOG_GRAPH:
+            levelStr = "GRAPH";
+        break;
+        default:
+            levelStr = "UKN";
+        break;
+    }
+
+    printLog(module, levelStr, format, arg);
+
+    va_end(arg);
+}
+
+__attribute__((__format__ (__printf__, 1, 0)))
+void ccaPrintError(const char *fmt, va_list ap)
+{
+    printLog("CCA_DEBUG", "ERROR", fmt, ap);
+}
+
+__attribute__((__format__ (__printf__, 1, 0)))
+void ccaPrintInfo(const char *fmt, va_list ap)
+{
+    if (gLogLevel & CAMERA_DEBUG_LOG_AIQ) {
+        printLog("CCA_DEBUG", "INFO", fmt, ap);
+    }
+}
+
+__attribute__((__format__ (__printf__, 1, 0)))
+void ccaPrintDebug(const char *fmt, va_list ap)
+{
+    if (gLogLevel & CAMERA_DEBUG_LOG_AIQ) {
+        printLog("CCA_DEBUG", "DBG", fmt, ap);
+    }
+}
+
+} // namespace Log
+
+#ifdef HAVE_ANDROID_OS
+
+void __camera_hal_log(bool condition, int prio, const char *tag,
+                      const char *fmt, ...)
+{
+    if (condition) {
+        va_list ap;
+        va_start(ap, fmt);
+        if (gLogLevel & CAMERA_DEBUG_LOG_PERSISTENT) {
+            int errnoCopy;
+            unsigned int maxTries = 20;
+            do {
+                errno = 0;
+                __android_log_vprint(prio, tag, fmt, ap);
+                errnoCopy = errno;
+                if (errnoCopy == EAGAIN)
+                    usleep(2000); /* sleep 2ms */
+            } while(errnoCopy == EAGAIN && maxTries--);
+        } else {
+            __android_log_vprint(prio, tag, fmt, ap);
+        }
+    }
+}
+
+#endif //HAVE_ANDROID_OS
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/iutils/CameraLog.h b/camera/hal/intel/ipu6/src/iutils/CameraLog.h
new file mode 100644
index 000000000000..c105cd0bbd6a
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/iutils/CameraLog.h
@@ -0,0 +1,206 @@
+/*
+ * Copyright (C) 2015-2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <stdarg.h>
+
+#include "utils/ScopedAtrace.h"
+#ifdef HAVE_ANDROID_OS
+#include <log/log.h>
+#endif
+
+namespace icamera {
+/**
+ * global log level
+ * This global variable is set from system properties
+ * It is used to control the level of verbosity of the traces in logcat
+ * It is also used to store the status of certain RD features
+ */
+extern int gLogLevel;
+extern int gPerfLevel;
+extern int gEnforceDvs;
+extern int gSlowlyRunRatio;
+
+/**
+ * LOG levels
+ *
+ * LEVEL 1 is used to track events in the HAL that are relevant during
+ * the operation of the camera, but are not happening on a per frame basis.
+ * this ensures that the level of logging is not too verbose
+ *
+ * LEVEL 2 is used to track information on a per request basis
+ *
+ * REQ_STATE is used to track the state of each request. By state we mean a one
+ * of the following request properties:
+ *  - metadata result
+ *  - buffer
+ *  - shutter
+ *  - error
+ *
+ * PERF TRACES enable only traces that provide performance metrics on the opera
+ * tion of the HAL
+ *
+ * PERF TRACES BREAKDOWN provides further level of detail on the performance
+ * metrics
+ */
+enum  {
+    /* verbosity level of general traces */
+    CAMERA_DEBUG_LOG_LEVEL1 = 1,
+    CAMERA_DEBUG_LOG_LEVEL2 = 1 << 1,
+
+    /* Bitmask to enable a concrete set of traces */
+    CAMERA_DEBUG_LOG_REQ_STATE = 1 << 2,
+    CAMERA_DEBUG_LOG_AIQ = 1 << 3,
+    CAMERA_DEBUG_LOG_XML = 1 << 4,
+    CAMERA_DEBUG_LOG_VC_SYNC = 1 << 5,
+    CAMERA_DEBUG_LOG_FPS = 1 << 6,
+    CAMERA_DEBUG_LOG_KERNEL_TOGGLE = 1 << 8,
+    CAMERA_DEBUG_LOG_SANDBOXING = 1 << 9,
+
+    /* Make logs persistent, retrying if logcat is busy */
+    CAMERA_DEBUG_LOG_PERSISTENT = 1 << 12, /* 4096 */
+
+    /* reserved for any components */
+    CAMERA_DEBUG_LOG_GRAPH = 1 << 13,
+
+    CAMERA_DEBUG_LOG_DBG = 1 <<16,
+    CAMERA_DEBUG_LOG_INFO = 1 <<17,
+    CAMERA_DEBUG_LOG_ERR = 1 <<18,
+    CAMERA_DEBUG_LOG_WARNING = 1 <<19,
+    CAMERA_DEBUG_LOG_VERBOSE = 1 <<20
+};
+
+enum  {
+    /* Emit well-formed performance traces */
+    CAMERA_DEBUG_LOG_PERF_TRACES = 1,
+
+    /* Print out detailed timing analysis */
+    CAMERA_DEBUG_LOG_PERF_TRACES_BREAKDOWN = 2,
+
+    /* Print out detailed timing analysis for IOCTL */
+    CAMERA_DEBUG_LOG_PERF_IOCTL_BREAKDOWN = 1<<2,
+
+    /* Print out detailed memory information analysis for IOCTL */
+    CAMERA_DEBUG_LOG_PERF_MEMORY = 1<<3,
+
+    /*enable camera atrace level 0 for camtune-record*/
+    CAMERA_DEBUG_LOG_ATRACE_LEVEL0 = 1<<4,
+
+    /*enable media topology dump*/
+    CAMERA_DEBUG_LOG_MEDIA_TOPO_LEVEL = 1<<5,
+
+    /*enable media controller info dump*/
+    CAMERA_DEBUG_LOG_MEDIA_CONTROLLER_LEVEL = 1<<6,
+
+    /*enable camera imaging atrace level 1 for camtune-record*/
+    CAMERA_DEBUG_LOG_ATRACE_LEVEL1 = 1<<7,
+};
+
+enum {
+    CAMERA_POWERBREAKDOWN_DISABLE_PREVIEW = 1<<0,
+    CAMERA_POWERBREAKDOWN_DISABLE_FDFR = 1<<1,
+    CAMERA_POWERBREAKDOWN_DISABLE_3A = 1<<2,
+};
+
+namespace Log {
+void setDebugLevel(void);
+void print_log(bool enable, const char *module, const int level, const char *format, ...);
+bool isDebugLevelEnable(int level);
+bool isModulePrintAble(const char *module);
+bool isDumpMediaTopo(void);
+bool isDumpMediaInfo(void);
+void ccaPrintError(const char *fmt, va_list ap);
+void ccaPrintInfo(const char *fmt, va_list ap);
+void ccaPrintDebug(const char *fmt, va_list ap);
+};
+
+#define SLOWLY_MULTIPLIER (gSlowlyRunRatio ? gSlowlyRunRatio : 1)
+
+#ifdef HAVE_LINUX_OS //Linux OS
+#define LOG1(format, args...) Log::print_log(gLogLevel & CAMERA_DEBUG_LOG_LEVEL1, LOG_TAG, CAMERA_DEBUG_LOG_LEVEL1, format, ##args)
+#define LOG2(format, args...) Log::print_log(gLogLevel & CAMERA_DEBUG_LOG_LEVEL2, LOG_TAG, CAMERA_DEBUG_LOG_LEVEL2, format, ##args)
+#define LOGR(format, args...) Log::print_log(gLogLevel & CAMERA_DEBUG_LOG_REQ_STATE, LOG_TAG, CAMERA_DEBUG_LOG_REQ_STATE, format, ##args)
+#define LOG3A(format, args...) Log::print_log(gLogLevel & CAMERA_DEBUG_LOG_AIQ, LOG_TAG, CAMERA_DEBUG_LOG_AIQ, format, ##args)
+#define LOGXML(format, args...) Log::print_log(gLogLevel & CAMERA_DEBUG_LOG_XML, LOG_TAG, CAMERA_DEBUG_LOG_XML, format, ##args)
+#define LOGVCSYNC(format, args...) Log::print_log(gLogLevel & CAMERA_DEBUG_LOG_VC_SYNC, LOG_TAG, CAMERA_DEBUG_LOG_VC_SYNC, format, ##args)
+#define LOGG(format, args...) Log::print_log(gLogLevel & CAMERA_DEBUG_LOG_GRAPH, LOG_TAG, CAMERA_DEBUG_LOG_GRAPH, format, ##args)
+#define LOGIPC(format, args...) Log::print_log(gLogLevel & CAMERA_DEBUG_LOG_SANDBOXING, LOG_TAG, CAMERA_DEBUG_LOG_SANDBOXING, format, ##args)
+
+#define LOGD(format, args...) Log::print_log(true, LOG_TAG, CAMERA_DEBUG_LOG_DBG, format, ##args)
+#define LOGI(format, args...) Log::print_log(true, LOG_TAG, CAMERA_DEBUG_LOG_INFO, format, ##args)
+#define LOGE(format, args...) Log::print_log(true, LOG_TAG, CAMERA_DEBUG_LOG_ERR, format, ##args)
+#define LOGW(format, args...) Log::print_log(true, LOG_TAG, CAMERA_DEBUG_LOG_WARNING, format, ##args)
+#define LOGV(format, args...) Log::print_log(true, LOG_TAG, CAMERA_DEBUG_LOG_VERBOSE, format, ##args)
+
+#define ALOGE LOGE
+#define ALOGD LOGD
+#define ALOGI LOGI
+#define ALOGW LOGW
+#define ALOGV LOGV
+#define ALOGW_IF
+#define LOG_ALWAYS_FATAL_IF
+#define LOG_FATAL_IF
+
+#else //Android OS
+
+void __camera_hal_log(bool condition, int prio, const char *tag, const char *fmt, ...);
+
+#define LOG1(...) __camera_hal_log(gLogLevel & CAMERA_DEBUG_LOG_LEVEL1, ANDROID_LOG_DEBUG, LOG_TAG, __VA_ARGS__)
+#define LOG2(...) __camera_hal_log(gLogLevel & CAMERA_DEBUG_LOG_LEVEL2, ANDROID_LOG_DEBUG, LOG_TAG, __VA_ARGS__)
+#define LOGR(...) __camera_hal_log(gLogLevel & CAMERA_DEBUG_LOG_REQ_STATE, ANDROID_LOG_DEBUG, LOG_TAG, __VA_ARGS__)
+#define LOG3A(...) __camera_hal_log(gLogLevel & CAMERA_DEBUG_LOG_AIQ, ANDROID_LOG_DEBUG, LOG_TAG, __VA_ARGS__)
+#define LOGXML(...) __camera_hal_log(gLogLevel & CAMERA_DEBUG_LOG_XML, ANDROID_LOG_DEBUG, LOG_TAG, __VA_ARGS__)
+#define LOGVCSYNC(...) __camera_hal_log(gLogLevel & CAMERA_DEBUG_LOG_VC_SYNC, ANDROID_LOG_DEBUG, LOG_TAG, __VA_ARGS__)
+#define LOGG(...) __camera_hal_log(gLogLevel & CAMERA_DEBUG_LOG_GRAPH, ANDROID_LOG_DEBUG, LOG_TAG, __VA_ARGS__)
+#define LOGIPC(format, args...) Log::print_log(gLogLevel & CAMERA_DEBUG_LOG_SANDBOXING, LOG_TAG, CAMERA_DEBUG_LOG_SANDBOXING, format, ##args)
+
+#define LOGE(...) __camera_hal_log(true, ANDROID_LOG_ERROR, LOG_TAG, __VA_ARGS__)
+#define LOGI(...) __camera_hal_log(true, ANDROID_LOG_INFO, LOG_TAG, __VA_ARGS__)
+#define LOGD(...) __camera_hal_log(true, ANDROID_LOG_DEBUG, LOG_TAG, __VA_ARGS__)
+#define LOGW(...) __camera_hal_log(true, ANDROID_LOG_WARN, LOG_TAG, __VA_ARGS__)
+#define LOGV(...) __camera_hal_log(true, ANDROID_LOG_VERBOSE, LOG_TAG, __VA_ARGS__)
+
+#endif
+#define HAL_TRACE_NAME(level, name) ScopedTrace ___tracer(level, name )
+#define HAL_TRACE_CALL(level) HAL_TRACE_NAME(level, __PRETTY_FUNCTION__)
+
+#define MAKE_COLOR(data) (data)
+#define TRACE_LOG_PROCESS(...)
+#define TRACE_STRUCT_PROCESS(name, struct_name, ...)
+#define TRACE_LOG_POINT(...)
+#define TRACE_STRUCT_POINT(name, struct_name, ...)
+
+class ScopedTrace {
+    public:
+        inline ScopedTrace(int level, const char* name) :
+            mLevel(level),
+            mName(name) {
+                if ((mLevel <= gLogLevel) && !(gLogLevel & CAMERA_DEBUG_LOG_VC_SYNC))
+                    LOGD("ENTER-%s",name);
+            }
+
+        inline ~ScopedTrace() {
+                if ((mLevel <= gLogLevel)  && !(gLogLevel & CAMERA_DEBUG_LOG_VC_SYNC))
+                    LOGD("EXIT-%s", mName);
+        }
+
+    private:
+        int mLevel;
+        const char* mName;
+};
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/iutils/Errors.h b/camera/hal/intel/ipu6/src/iutils/Errors.h
new file mode 100644
index 000000000000..8ef824f04fcc
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/iutils/Errors.h
@@ -0,0 +1,65 @@
+/*
+ * Copyright (C) 2007 The Android Open Source Project
+ * Copyright (C) 2015-2018 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <sys/types.h>
+#include <errno.h>
+
+namespace icamera {
+
+typedef int         status_t;
+
+/*
+ * Error codes.
+ * All error codes are negative values.
+ */
+
+enum {
+    OK                  = 0,    // Everything's swell.
+
+    UNKNOWN_ERROR       = (-2147483647-1), // INT32_MIN value
+
+    NO_MEMORY           = -ENOMEM,
+    INVALID_OPERATION   = -ENOSYS,
+    BAD_VALUE           = -EINVAL,
+    BAD_TYPE            = (UNKNOWN_ERROR + 1),
+    NAME_NOT_FOUND      = -ENOENT,
+    PERMISSION_DENIED   = -EPERM,
+    NO_INIT             = -ENODEV,
+    ALREADY_EXISTS      = -EEXIST,
+    DEAD_OBJECT         = -EPIPE,
+    FAILED_TRANSACTION  = (UNKNOWN_ERROR + 2),
+    JPARKS_BROKE_IT     = -EPIPE,
+#if !defined(HAVE_MS_C_RUNTIME)
+    BAD_INDEX           = -EOVERFLOW,
+    NOT_ENOUGH_DATA     = -ENODATA,
+    WOULD_BLOCK         = -EWOULDBLOCK,
+    TIMED_OUT           = -ETIMEDOUT,
+    UNKNOWN_TRANSACTION = -EBADMSG,
+#else
+    BAD_INDEX           = -E2BIG,
+    NOT_ENOUGH_DATA     = (UNKNOWN_ERROR + 3),
+    WOULD_BLOCK         = (UNKNOWN_ERROR + 4),
+    TIMED_OUT           = (UNKNOWN_ERROR + 5),
+    UNKNOWN_TRANSACTION = (UNKNOWN_ERROR + 6),
+#endif
+    FDS_NOT_ALLOWED     = (UNKNOWN_ERROR + 7),
+    NO_ENTRY            = (UNKNOWN_ERROR + 8),
+};
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/iutils/RWLock.h b/camera/hal/intel/ipu6/src/iutils/RWLock.h
new file mode 100644
index 000000000000..2082811ec9be
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/iutils/RWLock.h
@@ -0,0 +1,98 @@
+/*
+ * Copyright (C) 2007 The Android Open Source Project
+ * Copyright (C) 2015-2018 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "Errors.h"
+
+#if defined(HAVE_PTHREADS)
+#include <pthread.h>
+#endif
+
+// ---------------------------------------------------------------------------
+namespace icamera {
+// ---------------------------------------------------------------------------
+
+#if defined(HAVE_PTHREADS)
+
+/*
+ * Simple mutex class.  The implementation is system-dependent.
+ *
+ * The mutex must be unlocked by the thread that locked it.  They are not
+ * recursive, i.e. the same thread can't lock it multiple times.
+ */
+class RWLock {
+public:
+                RWLock() {};
+                ~RWLock();
+
+    status_t    readLock();
+    status_t    tryReadLock();
+    status_t    writeLock();
+    status_t    tryWriteLock();
+    void        unlock();
+
+    class AutoRLock {
+    public:
+        inline AutoRLock(RWLock& rwlock) : mLock(rwlock)  { mLock.readLock(); }
+        inline ~AutoRLock() { mLock.unlock(); }
+    private:
+        RWLock& mLock;
+    };
+
+    class AutoWLock {
+    public:
+        inline AutoWLock(RWLock& rwlock) : mLock(rwlock)  { mLock.writeLock(); }
+        inline ~AutoWLock() { mLock.unlock(); }
+    private:
+        RWLock& mLock;
+    };
+
+private:
+    // A RWLock cannot be copied
+                RWLock(const RWLock&);
+   RWLock&      operator = (const RWLock&);
+
+   pthread_rwlock_t mRWLock = PTHREAD_RWLOCK_INITIALIZER;
+};
+
+inline RWLock::~RWLock() {
+    pthread_rwlock_destroy(&mRWLock);
+}
+inline status_t RWLock::readLock() {
+    return -pthread_rwlock_rdlock(&mRWLock);
+}
+inline status_t RWLock::tryReadLock() {
+    return -pthread_rwlock_tryrdlock(&mRWLock);
+}
+inline status_t RWLock::writeLock() {
+    return -pthread_rwlock_wrlock(&mRWLock);
+}
+inline status_t RWLock::tryWriteLock() {
+    return -pthread_rwlock_trywrlock(&mRWLock);
+}
+inline void RWLock::unlock() {
+    pthread_rwlock_unlock(&mRWLock);
+}
+
+#endif // HAVE_PTHREADS
+typedef RWLock::AutoRLock AutoRMutex;
+typedef RWLock::AutoWLock AutoWMutex;
+
+// ---------------------------------------------------------------------------
+} // namespace icamera
+// ---------------------------------------------------------------------------
diff --git a/camera/hal/intel/ipu6/src/iutils/ScopedAtrace.cpp b/camera/hal/intel/ipu6/src/iutils/ScopedAtrace.cpp
new file mode 100644
index 000000000000..13533c7bc43d
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/iutils/ScopedAtrace.cpp
@@ -0,0 +1,63 @@
+/*
+ * Copyright (C) 2015-2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "utils/ScopedAtrace.h"
+#include "Trace.h"
+
+namespace icamera {
+
+const int ATRACE_LEN = 128;
+int gScopedAtraceLevel = 0;
+
+ScopedAtrace::ScopedAtrace(const int level, const char* func, const char* tag,
+                           const char* note, long value, const char* note2,
+                           int value2, const char* note3, int value3)
+{
+    mEnableAtraceEnd = false;
+    if(gScopedAtraceLevel & level) {
+        char buf[ATRACE_LEN];
+        if (value < 0 || note == nullptr) {
+            snprintf(buf, ATRACE_LEN, "<%s,%s>", func, tag);
+            atrace_begin(ATRACE_TAG, buf);
+        } else if (value2 < 0 || note2 == nullptr) {
+            snprintf(buf, ATRACE_LEN, "<%s,%s>:%s(%ld)", func, tag, note, value);
+            atrace_begin(ATRACE_TAG, buf);
+        } else if (value3 < 0 || note3 == nullptr) {
+            snprintf(buf, ATRACE_LEN, "<%s,%s>:%s(%ld) %s(%d)", func, tag, note,
+                     value, note2, value2);
+            atrace_begin(ATRACE_TAG, buf);
+        } else {
+            snprintf(buf, ATRACE_LEN, "<%s,%s>:%s(%ld) %s(%d) %s(%d)", func, tag,
+                     note, value, note2, value2, note3, value3);
+            atrace_begin(ATRACE_TAG, buf);
+        }
+        mEnableAtraceEnd = true;
+    }
+}
+
+ScopedAtrace::~ScopedAtrace()
+{
+    if(mEnableAtraceEnd) {
+        atrace_end(ATRACE_TAG);
+    }
+}
+
+void ScopedAtrace::setTraceLevel(int level)
+{
+    gScopedAtraceLevel = level;
+}
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/iutils/SwImageConverter.cpp b/camera/hal/intel/ipu6/src/iutils/SwImageConverter.cpp
new file mode 100644
index 000000000000..a6e63c6beffd
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/iutils/SwImageConverter.cpp
@@ -0,0 +1,406 @@
+/*
+ * Copyright (C) 2016-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "SwImageConverter"
+
+#include "Errors.h"
+#include "Utils.h"
+#include "CameraLog.h"
+#include "SwImageConverter.h"
+
+namespace icamera {
+
+void SwImageConverter::RGB2YUV(unsigned short R, unsigned short G, unsigned short B,
+     unsigned char *Y, unsigned char *U, unsigned char *V)
+{
+    int Rp, Gp, Bp;
+    int oY, oU, oV;
+    Rp = R; Gp = G; Bp = B;
+    oY = (257 * Rp + 504 * Gp + 98 * Bp) / 4000 + 16;
+    oU = (-148 * Rp - 291 * Gp + 439 * Bp) / 4000 + 128;
+    oV = (439 * Rp - 368 * Gp - 71 * Bp) / 4000 + 128;
+    if (oY > 255) oY = 255;
+    if (oY < 0) oY = 0;
+    if (oU > 255) oU = 255;
+    if (oU < 0) oU = 0;
+    if (oV > 255) oV = 255;
+    if (oV < 0) oV = 0;
+    *Y = (unsigned char)oY;
+    *U = (unsigned char)oU;
+    *V = (unsigned char)oV;
+}
+
+void SwImageConverter::YUV2RGB(unsigned char Y, unsigned char U, unsigned char V,
+    unsigned short *R, unsigned short *G, unsigned short *B)
+{
+    int Yp, Up, Vp, Ypp;
+    int oR, oG, oB;
+    Yp = Y - 16;
+    Up = (U - 128);
+    Vp = (V - 128);
+    Ypp = 9535 * Yp;
+
+    oB = (Ypp + 16531 * Up) >> 11;
+    oG = (Ypp - 6660 * Vp - 3203 * Up) >> 11;
+    oR = (Ypp + 13074 * Vp) >> 11;
+    if (oR > 1023) oR = 1023;
+    if (oR < 0) oR = 0;
+    if (oG > 1023) oG = 1023;
+    if (oG < 0) oG = 0;
+    if (oB > 1023) oB = 1023;
+    if (oB < 0) oB = 0;
+    *R = (unsigned short)oR;
+    *G = (unsigned short)oG;
+    *B = (unsigned short)oB;
+}
+
+void SwImageConverter::convertBayerBlock(unsigned int x, unsigned int y,
+    unsigned int width, unsigned int height,
+    unsigned short bayer_data[4], unsigned char *out_buf,
+    unsigned int src_fmt, unsigned int dst_fmt)
+{
+    unsigned char *Ybase;
+    unsigned char *UVbase;
+    unsigned char Y, U, V;
+    unsigned short R, Gr, Gb, B;
+    switch (src_fmt) {
+        case V4L2_PIX_FMT_SRGGB8: R = bayer_data[0] << 2; Gr = bayer_data[1] << 2; Gb = bayer_data[2] << 2; B = bayer_data[3] << 2; break;
+        case V4L2_PIX_FMT_SGRBG8: Gr = bayer_data[0] << 2; R = bayer_data[1] << 2; B = bayer_data[2] << 2; Gb = bayer_data[3] << 2; break;
+        case V4L2_PIX_FMT_SGBRG8: Gb = bayer_data[0] << 2; B = bayer_data[1] << 2; R = bayer_data[2] << 2; Gr = bayer_data[3] << 2; break;
+        case V4L2_PIX_FMT_SBGGR8: B = bayer_data[0] << 2; Gb = bayer_data[1] << 2; Gr = bayer_data[2] << 2; R = bayer_data[3] << 2; break;
+        case V4L2_PIX_FMT_SRGGB10: R = bayer_data[0]; Gr = bayer_data[1]; Gb = bayer_data[2]; B = bayer_data[3]; break;
+        case V4L2_PIX_FMT_SGRBG10: Gr = bayer_data[0]; R = bayer_data[1]; B = bayer_data[2]; Gb = bayer_data[3]; break;
+        case V4L2_PIX_FMT_SGBRG10: Gb = bayer_data[0]; B = bayer_data[1]; R = bayer_data[2]; Gr = bayer_data[3]; break;
+        case V4L2_PIX_FMT_SBGGR10: B = bayer_data[0]; Gb = bayer_data[1]; Gr = bayer_data[2]; R = bayer_data[3]; break;
+        case V4L2_PIX_FMT_SRGGB12: R = bayer_data[0] >> 2; Gr = bayer_data[1] >> 2; Gb = bayer_data[2] >> 2; B = bayer_data[3] >> 2; break;
+        case V4L2_PIX_FMT_SGRBG12: Gr = bayer_data[0] >> 2; R = bayer_data[1] >> 2; B = bayer_data[2] >> 2; Gb = bayer_data[3] >> 2; break;
+        case V4L2_PIX_FMT_SGBRG12: Gb = bayer_data[0] >> 2; B = bayer_data[1] >> 2; R = bayer_data[2] >> 2; Gr = bayer_data[3] >> 2; break;
+        case V4L2_PIX_FMT_SBGGR12: B = bayer_data[0] >> 2; Gb = bayer_data[1] >> 2; Gr = bayer_data[2] >> 2; R = bayer_data[3] >> 2; break;
+        default: return;
+    }
+
+    int dstStride = CameraUtils::getStride(dst_fmt, width);
+    switch(dst_fmt) {
+        case V4L2_PIX_FMT_SRGGB8:
+            out_buf[y * dstStride + x] = (R >> 2);
+            out_buf[y * dstStride + x + 1] = (Gr >> 2);
+            out_buf[(y + 1) * dstStride + x] = (Gb >> 2);
+            out_buf[(y + 1) * dstStride + x + 1] = (B >> 2);
+            break;
+        case V4L2_PIX_FMT_SGRBG8:
+            out_buf[y * dstStride + x] = (Gr >> 2);
+            out_buf[y * dstStride + x + 1] = (R >> 2);
+            out_buf[(y + 1) * dstStride + x] = (B >> 2);
+            out_buf[(y + 1) * dstStride + x + 1] = (Gb >> 2);
+            break;
+        case V4L2_PIX_FMT_SGBRG8:
+            out_buf[y * dstStride + x] = (Gb >> 2);
+            out_buf[y * dstStride + x + 1] = (B >> 2);
+            out_buf[(y + 1) * dstStride + x] = (R >> 2);
+            out_buf[(y + 1) * dstStride + x + 1] = (Gr >> 2);
+            break;
+        case V4L2_PIX_FMT_SBGGR8:
+            out_buf[y * dstStride + x] = (B >> 2);
+            out_buf[y * dstStride + x + 1] = (Gb >> 2);
+            out_buf[(y + 1) * dstStride + x] = (Gr >> 2);
+            out_buf[(y + 1) * dstStride + x + 1] = (R >> 2);
+            break;
+        case V4L2_PIX_FMT_SRGGB10:
+            *((unsigned short *) out_buf + y * dstStride + x) = R;
+            *((unsigned short *) out_buf + y * dstStride + x + 1) = Gr;
+            *((unsigned short *) out_buf + (y + 1) * dstStride + x) = Gb;
+            *((unsigned short *) out_buf + (y + 1) * dstStride + x + 1) = B;
+            break;
+        case V4L2_PIX_FMT_SGRBG10:
+            *((unsigned short *) out_buf + y * dstStride + x) = Gr;
+            *((unsigned short *) out_buf + y * dstStride + x + 1) = R;
+            *((unsigned short *) out_buf + (y + 1) * dstStride + x) = B;
+            *((unsigned short *) out_buf + (y + 1) * dstStride + x + 1) = Gb;
+            break;
+        case V4L2_PIX_FMT_SGBRG10:
+            *((unsigned short *) out_buf + y * dstStride + x) = Gb;
+            *((unsigned short *) out_buf + y * dstStride + x + 1) = B;
+            *((unsigned short *) out_buf + (y + 1) * dstStride + x) = R;
+            *((unsigned short *) out_buf + (y + 1) * dstStride + x + 1) = Gr;
+            break;
+        case V4L2_PIX_FMT_SBGGR10:
+            *((unsigned short *) out_buf + y * dstStride + x) = B;
+            *((unsigned short *) out_buf + y * dstStride + x + 1) = Gb;
+            *((unsigned short *) out_buf + (y + 1) * dstStride + x) = Gr;
+            *((unsigned short *) out_buf + (y + 1) * dstStride + x + 1) = R;
+            break;
+        case V4L2_PIX_FMT_NV12:
+            Ybase = out_buf;
+            UVbase = Ybase + dstStride * height;
+            RGB2YUV(R, (Gr + Gb) / 2, B, &Y, &U, &V);
+            Ybase[y * dstStride + x] = Ybase[y * dstStride + x + 1] =
+                Ybase[(y + 1) * dstStride + x] = Ybase[(y + 1) * dstStride + x + 1] = Y;
+            UVbase[y / 2 * dstStride + x / 2 * 2] = U;
+            UVbase[y / 2 * dstStride + x / 2 * 2 + 1] = V;
+            break;
+        case V4L2_PIX_FMT_UYVY:
+            RGB2YUV(R, (Gr + Gb) / 2, B, &Y, &U, &V);
+            out_buf[y * dstStride + x * 2] = U;
+            out_buf[y * dstStride + x * 2 + 1] = Y;
+            out_buf[y * dstStride + x * 2 + 2] = V;
+            out_buf[y * dstStride + x * 2 + 3] = Y;
+            out_buf[(y + 1) * dstStride + x * 2] = U;
+            out_buf[(y + 1) * dstStride + x * 2 + 1] = Y;
+            out_buf[(y + 1) * dstStride + x * 2 + 2] = V;
+            out_buf[(y + 1) * dstStride + x * 2 + 3] = Y;
+            break;
+        case V4L2_PIX_FMT_YUYV:
+            RGB2YUV(R, (Gr + Gb) / 2, B, &Y, &U, &V);
+            out_buf[y * dstStride + x * 2] = Y;
+            out_buf[y * dstStride + x * 2 + 1] = U;
+            out_buf[y * dstStride + x * 2 + 2] = Y;
+            out_buf[y * dstStride + x * 2 + 3] = V;
+            out_buf[(y + 1) * dstStride + x * 2] = Y;
+            out_buf[(y + 1) * dstStride + x * 2 + 1] = U;
+            out_buf[(y + 1) * dstStride + x * 2 + 2] = Y;
+            out_buf[(y + 1) * dstStride + x * 2 + 3] = V;
+            break;
+        case V4L2_PIX_FMT_YUV420:
+        {
+            RGB2YUV(R, (Gr + Gb) / 2, B, &Y, &U, &V);
+            Ybase = out_buf;
+            uint8_t* UBase = out_buf + dstStride * height;
+            uint8_t* VBase = out_buf + dstStride * (height + height / 4);
+            Ybase[y * dstStride + x] = Y;
+            Ybase[y * dstStride + x + 1] = Y;
+            Ybase[(y + 1) * dstStride + x] = Y;
+            Ybase[(y + 1) * dstStride + x + 1] = Y;
+            if (y % 4 == 0) {
+                UBase[y / 4 * dstStride + x / 2] = U;
+                VBase[y / 4 * dstStride + x / 2] = V;
+            } else {
+                UBase[y / 4 * dstStride + width / 2 + x / 2] = U;
+                VBase[y / 4 * dstStride + width / 2 + x / 2] = V;
+            }
+            break;
+        }
+        default:
+            break;
+    }
+}
+
+void SwImageConverter::convertYuvBlock(unsigned int x, unsigned int y,
+    unsigned int width, unsigned int height,
+    unsigned char *in_buf, unsigned char *out_buf,
+    unsigned int src_fmt, unsigned int dst_fmt)
+{
+    unsigned char *YBase;
+    unsigned char *UVBase;
+    unsigned char Y[4];
+    unsigned char U[4];
+    unsigned char V[4];
+    unsigned short R, G, B;
+    int srcStride = CameraUtils::getStride(src_fmt, width);
+
+    switch(src_fmt) {
+        case V4L2_PIX_FMT_NV12:
+            YBase = in_buf;
+            UVBase = in_buf + srcStride * height;
+            Y[0] = YBase[y * srcStride + x];
+            Y[1] = YBase[y * srcStride + x + 1];
+            Y[2] = YBase[(y + 1) * srcStride + x];
+            Y[3] = YBase[(y + 1) * srcStride + x + 1];
+            U[0] = U[1] = U[2] = U[3] = UVBase[y / 2 * srcStride + x / 2 * 2];
+            V[0] = V[1] = V[2] = V[3] = UVBase[y / 2 * srcStride + x / 2 * 2 + 1];
+            break;
+        case V4L2_PIX_FMT_UYVY:
+            Y[0] = in_buf[y * srcStride + x * 2 + 1];
+            Y[1] = in_buf[y * srcStride + x * 2 + 3];
+            Y[2] = in_buf[(y + 1) * srcStride + x * 2 + 1];
+            Y[3] = in_buf[(y + 1) * srcStride + x * 2 + 3];
+            U[0] = U[1] = in_buf[y * srcStride + x * 2];
+            U[2] = U[3] = in_buf[(y + 1) * srcStride + x * 2];
+            V[0] = V[1] = in_buf[y * srcStride + x * 2 + 2];
+            V[2] = V[3] = in_buf[(y + 1) * srcStride + x * 2 + 2];
+            break;
+        case V4L2_PIX_FMT_YUYV:
+            Y[0] = in_buf[y * srcStride + x * 2];
+            Y[1] = in_buf[y * srcStride + x * 2 + 2];
+            Y[2] = in_buf[(y + 1) * srcStride + x * 2];
+            Y[3] = in_buf[(y + 1) * srcStride + x * 2 + 2];
+            U[0] = U[1] = in_buf[y * srcStride + x * 2 + 1];
+            U[2] = U[3] = in_buf[(y + 1) * srcStride + x * 2 + 1];
+            V[0] = V[1] = in_buf[y * srcStride + x * 2 + 3];
+            V[2] = V[3] = in_buf[(y + 1) * srcStride + x * 2 + 3];
+            break;
+        default:
+            return;
+    }
+
+    int dstStride = CameraUtils::getStride(dst_fmt, width);
+    switch(dst_fmt) {
+        case V4L2_PIX_FMT_NV12:
+            YBase = out_buf;
+            UVBase = out_buf + dstStride * height;
+            YBase[y * dstStride + x] = Y[0];
+            YBase[y * dstStride + x + 1] = Y[1];
+            YBase[(y + 1) * dstStride + x] = Y[2];
+            YBase[(y + 1) * dstStride + x + 1] = Y[3];
+            UVBase[y / 2 * dstStride + x / 2 * 2] = U[0];
+            UVBase[y / 2 * dstStride + x / 2 * 2 + 1] = V[0];
+            break;
+        case V4L2_PIX_FMT_UYVY:
+            out_buf[y * dstStride + x * 2 + 1] = Y[0];
+            out_buf[y * dstStride + x * 2 + 3] = Y[1];
+            out_buf[(y + 1) * dstStride + x * 2 + 1] = Y[2];
+            out_buf[(y + 1) * dstStride + x * 2 + 3] = Y[3];
+            out_buf[y * dstStride + x * 2] = U[0];
+            out_buf[(y + 1) * dstStride + x * 2] = U[2];
+            out_buf[y * dstStride + x * 2 + 2] = V[0];
+            out_buf[(y + 1) * dstStride + x * 2 + 2] = V[2];
+            break;
+        case V4L2_PIX_FMT_YUYV:
+            out_buf[y * dstStride + x * 2] = Y[0];
+            out_buf[y * dstStride + x * 2 + 2] = Y[1];
+            out_buf[(y + 1) * dstStride + x * 2] = Y[2];
+            out_buf[(y + 1) * dstStride + x * 2 + 2] = Y[3];
+            out_buf[y * dstStride + x * 2 + 1] = U[0];
+            out_buf[(y + 1) * dstStride + x * 2 + 1] = U[2];
+            out_buf[y * dstStride + x * 2 + 3] = V[0];
+            out_buf[(y + 1) * dstStride + x * 2 + 3] = V[2];
+            break;
+        case V4L2_PIX_FMT_YUV420:
+        {
+            YBase = out_buf;
+            uint8_t* UBase = out_buf + dstStride * height;
+            uint8_t* VBase = out_buf + dstStride * (height + height / 4);
+            YBase[y * dstStride + x] = Y[0];
+            YBase[y * dstStride + x + 1] = Y[1];
+            YBase[(y + 1) * dstStride + x] = Y[2];
+            YBase[(y + 1) * dstStride + x + 1] = Y[3];
+            if (y % 4 == 0) {
+                UBase[y / 4 * dstStride + x / 2] = (U[0] + U[2]) / 2;
+                VBase[y / 4 * dstStride + x / 2] = (V[0] + V[2]) / 2;
+            } else {
+                UBase[y / 4 * dstStride + width / 2 + x / 2] = (U[0] + U[2]) / 2;
+                VBase[y / 4 * dstStride + width / 2 + x / 2] = (V[0] + V[2]) / 2;
+            }
+            break;
+        }
+        case V4L2_PIX_FMT_SRGGB8:
+            YUV2RGB(Y[0], U[0], V[0], &R, &G, &B);
+            out_buf[y * dstStride + x] = (R >> 2);
+            out_buf[y * dstStride + x + 1] = (G >> 2);
+            out_buf[(y + 1) * dstStride + x] = (G >> 2);
+            out_buf[(y + 1) * dstStride + x + 1] = (B >> 2);
+            break;
+        case V4L2_PIX_FMT_SGRBG8:
+            YUV2RGB(Y[0], U[0], V[0], &R, &G, &B);
+            out_buf[y * dstStride + x] = (G >> 2);
+            out_buf[y * dstStride + x + 1] = (R >> 2);
+            out_buf[(y + 1) * dstStride + x] = (B >> 2);
+            out_buf[(y + 1) * dstStride + x + 1] = (G >> 2);
+            break;
+        case V4L2_PIX_FMT_SGBRG8:
+            YUV2RGB(Y[0], U[0], V[0], &R, &G, &B);
+            out_buf[y * dstStride + x] = (G >> 2);
+            out_buf[y * dstStride + x + 1] = (B >> 2);
+            out_buf[(y + 1) * dstStride + x] = (R >> 2);
+            out_buf[(y + 1) * dstStride + x + 1] = (G >> 2);
+            break;
+        case V4L2_PIX_FMT_SBGGR8:
+            YUV2RGB(Y[0], U[0], V[0], &R, &G, &B);
+            out_buf[y * dstStride + x] = (B >> 2);
+            out_buf[y * dstStride + x + 1] = (G >> 2);
+            out_buf[(y + 1) * dstStride + x] = (G >> 2);
+            out_buf[(y + 1) * dstStride + x + 1] = (R >> 2);
+            break;
+        case V4L2_PIX_FMT_SRGGB10:
+            YUV2RGB(Y[0], U[0], V[0], &R, &G, &B);
+            *((unsigned short *) out_buf + y * dstStride + x) = R;
+            *((unsigned short *) out_buf + y * dstStride + x + 1) = G;
+            *((unsigned short *) out_buf + (y + 1) * dstStride + x) = G;
+            *((unsigned short *) out_buf + (y + 1) * dstStride + x + 1) = B;
+            break;
+        case V4L2_PIX_FMT_SGRBG10:
+            YUV2RGB(Y[0], U[0], V[0], &R, &G, &B);
+            *((unsigned short *) out_buf + y * dstStride + x) = G;
+            *((unsigned short *) out_buf + y * dstStride + x + 1) = R;
+            *((unsigned short *) out_buf + (y + 1) * dstStride + x) = B;
+            *((unsigned short *) out_buf + (y + 1) * dstStride + x + 1) = G;
+            break;
+        case V4L2_PIX_FMT_SGBRG10:
+            YUV2RGB(Y[0], U[0], V[0], &R, &G, &B);
+            *((unsigned short *) out_buf + y * dstStride + x) = G;
+            *((unsigned short *) out_buf + y * dstStride + x + 1) = B;
+            *((unsigned short *) out_buf + (y + 1) * dstStride + x) = R;
+            *((unsigned short *) out_buf + (y + 1) * dstStride + x + 1) = G;
+            break;
+        case V4L2_PIX_FMT_SBGGR10:
+            YUV2RGB(Y[0], U[0], V[0], &R, &G, &B);
+            *((unsigned short *) out_buf + y * dstStride + x) = B;
+            *((unsigned short *) out_buf + y * dstStride + x + 1) = G;
+            *((unsigned short *) out_buf + (y + 1) * dstStride + x) = G;
+            *((unsigned short *) out_buf + (y + 1) * dstStride + x + 1) = R;
+            break;
+        default:
+            return;
+    }
+}
+
+int SwImageConverter::convertFormat(unsigned int width, unsigned int height,
+                        unsigned char *inBuf, unsigned int inLength, unsigned int srcFmt,
+                        unsigned char *outBuf, unsigned int outLength, unsigned int dstFmt)
+{
+    CheckError((inBuf == nullptr || outBuf == nullptr), BAD_VALUE, "Invalid input(%p) or output buffer(%p)", inBuf, outBuf);
+
+    unsigned int x, y;
+    unsigned short bayer_data[4];
+
+    LOG2("%s srcFmt %s => dstFmt %s %dx%d", __func__,
+         CameraUtils::format2string(srcFmt).c_str(),
+         CameraUtils::format2string(dstFmt).c_str(), width, height);
+
+    if (dstFmt == srcFmt) {
+        // No need do format convertion.
+        LOG2("No conversion needed");
+        MEMCPY_S(outBuf, outLength, inBuf, inLength);
+        return 0;
+    }
+
+    // for not vector raw
+    int srcStride = CameraUtils::getStride(srcFmt, width);
+    for(y = 0; y < height; y += 2) {
+        for(x = 0; x < width; x += 2) {
+            if(CameraUtils::isRaw(srcFmt)) {
+                if(CameraUtils::getBpp(srcFmt) == 8) {
+                    bayer_data[0] = inBuf[y * srcStride + x];
+                    bayer_data[1] = inBuf[y * srcStride + x + 1];
+                    bayer_data[2] = inBuf[(y + 1) * srcStride + x];
+                    bayer_data[3] = inBuf[(y + 1) * srcStride + x + 1];
+                } else {
+                    int offset = srcStride / (CameraUtils::getBpp(srcFmt) / 8);
+                    bayer_data[0] = *((unsigned short *) inBuf + y * offset + x);
+                    bayer_data[1] = *((unsigned short *) inBuf + y * offset + x + 1);
+                    bayer_data[2] = *((unsigned short *) inBuf + (y + 1) * offset + x);
+                    bayer_data[3] =
+                        *((unsigned short *) inBuf + (y + 1) * offset + x + 1);
+                }
+                convertBayerBlock(x, y, width, height, bayer_data, outBuf, srcFmt, dstFmt);
+            } else {
+                convertYuvBlock(x, y, width, height, inBuf, outBuf, srcFmt, dstFmt);
+            }
+        }
+    }
+    return 0;
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/iutils/SwImageConverter.h b/camera/hal/intel/ipu6/src/iutils/SwImageConverter.h
new file mode 100644
index 000000000000..591f5b4029a8
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/iutils/SwImageConverter.h
@@ -0,0 +1,42 @@
+/*
+ * Copyright (C) 2016-2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+namespace icamera {
+
+namespace SwImageConverter {
+    void RGB2YUV(unsigned short R, unsigned short G, unsigned short B,
+         unsigned char *Y, unsigned char *U, unsigned char *V);
+
+    void YUV2RGB(unsigned char Y, unsigned char U, unsigned char V,
+        unsigned short *R, unsigned short *G, unsigned short *B);
+
+    void convertBayerBlock(unsigned int x, unsigned int y,
+        unsigned int width, unsigned int height, unsigned short bayer_data[4],
+        unsigned char *out_buf, unsigned int src_fmt, unsigned int dst_fmt);
+
+    void convertYuvBlock(unsigned int x, unsigned int y,
+        unsigned int width, unsigned int height, unsigned char *in_buf,
+        unsigned char *out_buf, unsigned int src_fmt, unsigned int dst_fmt);
+
+    //convert the buffer from the src_fmt to the dst_fmt
+    int convertFormat(unsigned int width, unsigned int height,
+        unsigned char *inBuf, unsigned int inLength, unsigned int srcFmt,
+        unsigned char *outBuf, unsigned int outLength, unsigned int dstFmt);
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/iutils/Thread.cpp b/camera/hal/intel/ipu6/src/iutils/Thread.cpp
new file mode 100644
index 000000000000..919f5a1cf6e0
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/iutils/Thread.cpp
@@ -0,0 +1,222 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Thread"
+
+#include "Errors.h"
+#include "Thread.h"
+#include "CameraLog.h"
+
+namespace icamera {
+
+int Condition::waitRelative(ConditionLock& lock, int64_t reltime) {
+    std::cv_status ret = mCondition.wait_for(lock, std::chrono::nanoseconds(reltime));
+    return ret == std::cv_status::timeout ? TIMED_OUT : OK;
+}
+
+Thread::Thread() : mState(NOT_STARTED), mThread(nullptr), mPriority(PRIORITY_DEFAULT)
+{
+    LOG1("%s", __func__);
+}
+
+Thread::~Thread()
+{
+    LOG1("%s", __func__);
+
+    requestExitAndWait();
+
+    delete mThread;
+}
+
+int Thread::run(std::string name, int priority)
+{
+    LOG1("%s", __func__);
+
+    AutoMutex lock(mLock);
+
+    if (mState != NOT_STARTED && mState != EXITED) {
+        LOGW("Cannot start thread(%s) in state(%d).", name.c_str(), mState);
+        return INVALID_OPERATION;
+    }
+
+    // Thread can be restarted only if the previous one has exited.
+    // Release the previous thread first if it's created already.
+    delete mThread;
+
+    mThread = new std::thread(_threadLoop, this);
+    mThread->detach();
+    mId = mThread->get_id();
+    mName = name;
+    mPriority = priority;
+    mState = RUNNING;
+
+    mStartCondition.signal();
+
+    return OK;
+}
+
+void Thread::requestExit()
+{
+    LOG1("%s", __func__);
+
+    AutoMutex lock(mLock);
+
+    if (mState == RUNNING) {
+        mState = EXITING;
+    }
+}
+
+int Thread::requestExitAndWait()
+{
+    LOG1("%s", __func__);
+
+    ConditionLock lock(mLock);
+
+    // No need exit if it's not started.
+    if (mState == NOT_STARTED) {
+        return NO_INIT;
+    }
+
+    // The function cannot be called by same thread.
+    if (std::this_thread::get_id() == mId) {
+        LOGE("The thread itself cannot call its own requestExitAndWait function.");
+        return WOULD_BLOCK;
+    }
+
+    while (mState != EXITED) {
+        mState = EXITING;
+        mExitedCondition.wait(lock);
+    }
+
+    return OK;
+}
+
+int Thread::join()
+{
+    LOG1("%s", __func__);
+
+    ConditionLock lock(mLock);
+
+    // No need join if it's not started.
+    if (mState == NOT_STARTED) {
+        return NO_INIT;
+    }
+
+    // The function cannot be called by same thread.
+    if (std::this_thread::get_id() == mId) {
+        LOGE("The thread itself cannot call its own join function.");
+        return WOULD_BLOCK;
+    }
+
+    while (mState != EXITED) {
+        mExitedCondition.wait(lock);
+    }
+
+    return OK;
+}
+
+bool Thread::isRunning() const
+{
+    AutoMutex lock(mLock);
+    // A thread in EXITING also means it's still running, but it's going to exit.
+    return mState == RUNNING || mState == EXITING;
+}
+
+bool Thread::isExiting() const
+{
+    AutoMutex lock(mLock);
+    return mState == EXITING;
+}
+
+bool Thread::isExited() const
+{
+    AutoMutex lock(mLock);
+    return mState == EXITED;
+}
+
+void Thread::_threadLoop(Thread* self)
+{
+    {
+        // Wait for function "run" to finish.
+        // If the thread is going to exit, then no need to wait anymore.
+        ConditionLock lock(self->mLock);
+        while (self->mState != RUNNING && self->mState != EXITING) {
+            self->mStartCondition.wait(lock);
+        }
+
+        if (self->mState == EXITING) {
+            self->mState = EXITED;
+            self->mExitedCondition.broadcast();
+            return;
+        }
+
+        self->setProperty();
+    }
+
+    while (true) {
+        bool loopAgain = self->threadLoop();
+
+        AutoMutex lock(self->mLock);
+        if (!loopAgain || self->mState == EXITING) {
+            self->mState = EXITED;
+            self->mExitedCondition.broadcast();
+            return;
+        }
+    }
+}
+
+// Platform specific implementation.
+#ifdef HAVE_PTHREADS
+#include <pthread.h>
+#include <sys/resource.h>
+
+void Thread::setProperty()
+{
+    LOG1("%s, name:%s, priority:%d", __func__, mName.c_str(), mPriority);
+
+#if __GLIBC__ >= 2 && __GLIBC_MINOR__ >= 12
+    // Set thread's name
+    std::string threadName = mName.empty() ? "NO_NAME" : mName;
+    if (mName.size() > MAX_THREAD_NAME_LEN) {
+        threadName = mName.substr(0, MAX_THREAD_NAME_LEN);
+        LOG2("The thread name(%s) is too long, modify it to %s", mName.c_str(), threadName.c_str());
+    }
+    pthread_setname_np(pthread_self(), threadName.c_str());
+#endif
+
+    // Set thread's priority
+    setpriority(PRIO_PROCESS, 0, mPriority);
+
+    const int policy = SCHED_OTHER;
+    int min = sched_get_priority_min(policy);
+    int max = sched_get_priority_max(policy);
+    LOG1("Priority range:(%d-%d)", min, max);
+
+    if (mPriority < min) mPriority = min;
+    if (mPriority > max) mPriority = max;
+
+    sched_param param;
+    param.sched_priority = mPriority;
+
+    int ret = pthread_setschedparam(pthread_self(), policy, &param);
+    LOG1("pthread_setschedparam ret:%d", ret);
+}
+#else
+#warning "Setting thread's property is not implemented yet on this platform."
+#endif
+
+} // namespace icamera
+
diff --git a/camera/hal/intel/ipu6/src/iutils/Thread.h b/camera/hal/intel/ipu6/src/iutils/Thread.h
new file mode 100644
index 000000000000..91623dca8b21
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/iutils/Thread.h
@@ -0,0 +1,198 @@
+/*
+ * Copyright (C) 2017-2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <string>
+#include <thread>
+#include <mutex>
+#include <condition_variable>
+
+namespace icamera {
+
+typedef std::mutex Mutex;
+typedef std::lock_guard<std::mutex> AutoMutex;
+typedef std::unique_lock<std::mutex> ConditionLock;
+
+enum {
+    PRIORITY_LOWEST = 19,
+    PRIORITY_BACKGROUND = 10,
+    PRIORITY_NORMAL = 0,
+    PRIORITY_FOREGROUND = -2,
+    PRIORITY_DISPLAY = -4,
+    PRIORITY_URGENT_DISPLAY = -8,
+    PRIORITY_AUDIO = -16,
+    PRIORITY_URGENT_AUDIO = -19,
+    PRIORITY_HIGHEST = -20,
+    PRIORITY_DEFAULT = 0,
+    PRIORITY_MORE_FAVORABLE = -1,
+    PRIORITY_LESS_FAVORABLE = 1,
+};
+
+class Condition {
+public:
+    Condition() {}
+    ~Condition() {}
+
+    /**
+     * Wait on the condition variable. MUST be locked with ConditionLock before being called.
+     *
+     * \param[in] lock: An object of type ConditionLock, which must be locked by the current thread.
+     */
+    void wait(ConditionLock& lock) {
+        mCondition.wait(lock);
+    }
+
+    /**
+     * Wait on the condition variable with a period of time.
+     *
+     * \param[in] lock: An object of type ConditionLock, which must be locked by the current thread.
+     * \param[in] reltime: The maximum time to spend waiting.
+     *
+     * \return TIMED_OUT if it's not notified to wake up within reltime, otherwise return OK.
+     */
+    int waitRelative(ConditionLock& lock, int64_t reltime);
+
+    /**
+     * Wake up one thread that is waiting on the condition variable.
+     */
+    void signal() { mCondition.notify_one(); }
+
+    /**
+     * Wake up all threads that are waiting on the condition variable.
+     */
+    void broadcast() { mCondition.notify_all(); }
+
+private:
+    Condition(const Condition& other) = delete;
+    Condition& operator=(const Condition&) = delete;
+
+    std::condition_variable mCondition;
+};
+
+/**
+ * Thread is a wrapper class to std::thread
+ *
+ * Thread helps manager the thread's state and make the std::thread is easier to used.
+ * Thread also hides the platform specific implementation details.
+ */
+class Thread {
+public:
+    Thread();
+    virtual ~Thread();
+
+    /**
+     * Start the thread.
+     */
+    virtual int run(std::string name = ("nameless"), int priority = PRIORITY_DEFAULT);
+
+    /**
+     * Ask this object's thread to exit. This function is asynchronous, so when it
+     * returns the thread might still be running.
+     */
+    virtual void requestExit();
+
+    /**
+     * Wait until this object's thread exits. Returns immediately if not yet running.
+     *
+     * join will not trigger the thread to exit, it just wait for the thread exits.
+     * Do not call it from this object's thread, will return WOULD_BLOCK in that case.
+     */
+    int join();
+
+    /**
+     * Ask this object's thread to exit. This function is synchronous, so when it
+     * returns the thread must exit already.
+     * It has same effect with calling requestExit and join combined.
+     *
+     * Do not call from this object's thread, will return WOULD_BLOCK in that case.
+     */
+    int requestExitAndWait();
+
+    /**
+     * Indicates whether this thread is running or not.
+     */
+    bool isRunning() const;
+
+    /**
+     * Indicates whether this thread is going to exit or not.
+     */
+    bool isExiting() const;
+
+    /**
+     * Indicates whether this thread exited or not.
+     */
+    bool isExited() const;
+
+private:
+
+    /**
+     * threadLoop is the function which is called by the thread.
+     * The derived class MUST override this function. The thread starts its life here.
+     *
+     * There are two ways of using the thread object:
+     * 1. loop: threadLoop will be called again as long as it returns true,
+     *          and requestExit() wasn't called.
+     * 2. once: If threadLoop() returns false, the thread will exit upon return.
+     *
+     * There are three ways of exiting the thread.
+     * 1. threadLoop return false.
+     * 2. requestExit is called.
+     * 3. requestExitAndWait is called.
+     */
+    virtual bool threadLoop() { return false; }
+
+private:
+    Thread(const Thread& other) = delete;
+    Thread& operator=(const Thread&) = delete;
+
+    /**
+     * The function which is used to create the std::thread.
+     */
+    static void _threadLoop(Thread* self);
+
+    /**
+     * Set thread's property such as thread's name or priority.
+     */
+    void setProperty();
+
+private:
+    enum {
+        NOT_STARTED,
+        RUNNING,
+        EXITING,
+        EXITED,
+    } mState;
+
+    // The max length for thread name is 15.
+    static const int MAX_THREAD_NAME_LEN = 15;
+    std::thread* mThread;
+    std::string mName;
+    std::thread::id mId;
+    int mPriority;
+
+    // A lock used to protect internal data and API accessing.
+    mutable Mutex mLock;
+
+    // To make sure the thread not dead before "run" not finish.
+    Condition mStartCondition;
+
+    // To make sure API like join be able to wait until thread exits.
+    Condition mExitedCondition;
+};
+
+} // namespace icamera
+
diff --git a/camera/hal/intel/ipu6/src/iutils/Trace.cpp b/camera/hal/intel/ipu6/src/iutils/Trace.cpp
new file mode 100644
index 000000000000..d50ab513eb2e
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/iutils/Trace.cpp
@@ -0,0 +1,54 @@
+/*
+ * Copyright (C) 2014-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Trace"
+
+#include <errno.h>
+#include <fcntl.h>
+#include <limits.h>
+#include <pthread.h>
+#include <stdbool.h>
+#include <stdlib.h>
+#include <string.h>
+#include <sys/types.h>
+
+#include "Trace.h"
+#include "iutils/CameraLog.h"
+
+namespace icamera {
+
+std::atomic<int>        atrace_is_ready(0);
+uint64_t                atrace_enabled_tags  = ATRACE_TAG_NOT_READY;
+int                     atrace_marker_fd     = -1;
+static pthread_once_t   atrace_once_control  = PTHREAD_ONCE_INIT;
+
+static void atrace_init_once()
+{
+    atrace_marker_fd = open("/sys/kernel/debug/tracing/trace_marker", O_WRONLY);
+    if (atrace_marker_fd == -1) {
+        ATRACE_LOGE("atrace %s open error: %s!\n", __func__, strerror(errno));
+        return;
+    }
+    atrace_enabled_tags = ATRACE_TAG_ALWAYS;
+    atrace_is_ready = 1;
+}
+
+void atrace_setup()
+{
+    pthread_once(&atrace_once_control, atrace_init_once);
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/iutils/Trace.h b/camera/hal/intel/ipu6/src/iutils/Trace.h
new file mode 100644
index 000000000000..849a6f19106b
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/iutils/Trace.h
@@ -0,0 +1,241 @@
+/*
+ * Copyright (C) 2014-2018 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <atomic>
+
+#include <inttypes.h>
+#include <stdbool.h>
+#include <stdint.h>
+#include <stdio.h>
+#include <sys/cdefs.h>
+#include <sys/types.h>
+#include <unistd.h>
+#include <errno.h>
+#include <string.h>
+
+namespace icamera {
+
+#ifndef ATRACE_NO_LOG
+#define ATRACE_LOGE(format, args...) printf(format, ##args)
+#else
+#define ATRACE_LOGE()
+#endif
+
+#define CAMERA_PRId32 "d"
+#define CAMERA_PRId64 "I64d"
+
+#define ATRACE_MESSAGE_LENGTH 1024
+#define PROPERTY_VALUE_MAX 1024
+/**
+ * The ATRACE_TAG macro can be defined before including this header to trace
+ * using one of the tags defined below.  It must be defined to one of the
+ * following ATRACE_TAG_* macros.  The trace tag is used to filter tracing in
+ * userland to avoid some of the runtime cost of tracing when it is not desired.
+ *
+ * Defining ATRACE_TAG to be ATRACE_TAG_ALWAYS will result in the tracing always
+ * being enabled - this should ONLY be done for debug code, as userland tracing
+ * has a performance cost even when the trace is not being recorded.  Defining
+ * ATRACE_TAG to be ATRACE_TAG_NEVER or leaving ATRACE_TAG undefined will result
+ * in the tracing always being disabled.
+ */
+#define ATRACE_TAG_NEVER            0       // This tag is never enabled.
+#define ATRACE_TAG_ALWAYS           (1<<0)  // This tag is always enabled.
+#define ATRACE_TAG_LAST             ATRACE_TAG_ALWAYS
+
+// Reserved for initialization.
+#define ATRACE_TAG_NOT_READY        (1LL<<63)
+
+#define ATRACE_TAG_VALID_MASK ((ATRACE_TAG_LAST - 1) | ATRACE_TAG_LAST)
+
+// define the ATRACE_TAG to ALWAYS for nestdemo.
+#define ATRACE_TAG ATRACE_TAG_ALWAYS
+
+/**
+ * Opens the trace file for writing and reads the property for initial tags.
+ * The atrace.tags.enableflags property sets the tags to trace.
+ * This function should not be explicitly called, the first call to any normal
+ * trace function will cause it to be run safely.
+ */
+void atrace_setup();
+
+/**
+ * Flag indicating whether setup has been completed, initialized to 0.
+ * Nonzero indicates setup has completed.
+ * Note: This does NOT indicate whether or not setup was successful.
+ */
+extern std::atomic<int> atrace_is_ready;
+
+/**
+ * Set of ATRACE_TAG flags to trace for, initialized to ATRACE_TAG_NOT_READY.
+ * A value of zero indicates setup has failed.
+ * Any other nonzero value indicates setup has succeeded, and tracing is on.
+ */
+extern uint64_t atrace_enabled_tags;
+
+/**
+ * Handle to the kernel's trace buffer, initialized to -1.
+ * Any other value indicates setup has succeeded, and is a valid fd for tracing.
+ */
+extern int atrace_marker_fd;
+
+/**
+ * atrace_init readies the process for tracing by opening the trace_marker file.
+ * Calling any trace function causes this to be run, so calling it is optional.
+ * This can be explicitly run to avoid setup delay on first trace function.
+ */
+#define ATRACE_INIT() atrace_init()
+static inline void atrace_init()
+{
+    if (!atrace_is_ready.load()) {
+        atrace_setup();
+    }
+}
+
+/**
+ * Get the mask of all tags currently enabled.
+ * It can be used as a guard condition around more expensive trace calculations.
+ * Every trace function calls this, which ensures atrace_init is run.
+ */
+#define ATRACE_GET_ENABLED_TAGS() atrace_get_enabled_tags()
+static inline uint64_t atrace_get_enabled_tags()
+{
+    atrace_init();
+    return atrace_enabled_tags;
+}
+
+/**
+ * Test if a given tag is currently enabled.
+ * Returns nonzero if the tag is enabled, otherwise zero.
+ * It can be used as a guard condition around more expensive trace calculations.
+ */
+#define ATRACE_ENABLED() atrace_is_tag_enabled(ATRACE_TAG)
+static inline uint64_t atrace_is_tag_enabled(uint64_t tag)
+{
+    return atrace_get_enabled_tags() & tag;
+}
+
+/**
+ * Trace the beginning of a context.  name is used to identify the context.
+ * This is often used to time function execution.
+ */
+#define ATRACE_BEGIN(name) atrace_begin(ATRACE_TAG, name)
+static inline void atrace_begin(uint64_t tag, const char* name)
+{
+    if (atrace_is_tag_enabled(tag)) {
+        char buf[ATRACE_MESSAGE_LENGTH];
+        ssize_t len;
+
+        len = snprintf(buf, ATRACE_MESSAGE_LENGTH, "B|%d|%s", static_cast<int>(getpid()), name);
+        if (write(atrace_marker_fd, buf, len) != len)
+            ATRACE_LOGE("atrace %s write %s error: %s!\n", __func__, buf, strerror(errno));
+    }
+}
+
+/**
+ * Trace the end of a context.
+ * This should match up (and occur after) a corresponding ATRACE_BEGIN.
+ */
+#define ATRACE_END() atrace_end(ATRACE_TAG)
+static inline void atrace_end(uint64_t tag)
+{
+    if (atrace_is_tag_enabled(tag)) {
+        char c = 'E';
+        if (write(atrace_marker_fd, &c, 1) !=1)
+            ATRACE_LOGE("atrace %s write error: %s!\n", __func__, strerror(errno));
+    }
+}
+
+/**
+ * Trace the beginning of an asynchronous event. Unlike ATRACE_BEGIN/ATRACE_END
+ * contexts, asynchronous events do not need to be nested. The name describes
+ * the event, and the cookie provides a unique identifier for distinguishing
+ * simultaneous events. The name and cookie used to begin an event must be
+ * used to end it.
+ */
+#define ATRACE_ASYNC_BEGIN(name, cookie) \
+    atrace_async_begin(ATRACE_TAG, name, cookie)
+static inline void atrace_async_begin(uint64_t tag, const char* name,
+        int32_t cookie)
+{
+    if (atrace_is_tag_enabled(tag)) {
+        char buf[ATRACE_MESSAGE_LENGTH];
+        ssize_t len;
+
+        len = snprintf(buf, ATRACE_MESSAGE_LENGTH, "S|%d|%s|%" CAMERA_PRId32,
+                       static_cast<int>(getpid()), name, cookie);
+        if (write(atrace_marker_fd, buf, len) != len)
+            ATRACE_LOGE("atrace %s write %s error: %s!\n", __func__, buf, strerror(errno));
+    }
+}
+
+/**
+ * Trace the end of an asynchronous event.
+ * This should have a corresponding ATRACE_ASYNC_BEGIN.
+ */
+#define ATRACE_ASYNC_END(name, cookie) atrace_async_end(ATRACE_TAG, name, cookie)
+static inline void atrace_async_end(uint64_t tag, const char* name,
+        int32_t cookie)
+{
+    if (atrace_is_tag_enabled(tag)) {
+        char buf[ATRACE_MESSAGE_LENGTH];
+        ssize_t len;
+
+        len = snprintf(buf, ATRACE_MESSAGE_LENGTH, "F|%d|%s|%" CAMERA_PRId32,
+                       static_cast<int>(getpid()), name, cookie);
+        if (write(atrace_marker_fd, buf, len) != len)
+            ATRACE_LOGE("atrace %s write %s error: %s!\n", __func__, buf, strerror(errno));
+    }
+}
+
+/**
+ * Traces an integer counter value.  name is used to identify the counter.
+ * This can be used to track how a value changes over time.
+ */
+#define ATRACE_INT(name, value) atrace_int(ATRACE_TAG, name, value)
+static inline void atrace_int(uint64_t tag, const char* name, int32_t value)
+{
+    if (atrace_is_tag_enabled(tag)) {
+        char buf[ATRACE_MESSAGE_LENGTH];
+        ssize_t len;
+
+        len = snprintf(buf, ATRACE_MESSAGE_LENGTH, "C|%d|%s|%" CAMERA_PRId32,
+                       static_cast<int>(getpid()), name, value);
+        if (write(atrace_marker_fd, buf, len) != len)
+            ATRACE_LOGE("atrace %s write %s error: %s!\n", __func__, buf, strerror(errno));
+    }
+}
+
+/**
+ * Traces a 64-bit integer counter value.  name is used to identify the
+ * counter. This can be used to track how a value changes over time.
+ */
+#define ATRACE_INT64(name, value) atrace_int64(ATRACE_TAG, name, value)
+static inline void atrace_int64(uint64_t tag, const char* name, int64_t value)
+{
+    if (atrace_is_tag_enabled(tag)) {
+        char buf[ATRACE_MESSAGE_LENGTH];
+        ssize_t len;
+
+        len = snprintf(buf, ATRACE_MESSAGE_LENGTH, "C|%d|%s|%" PRId64 " " CAMERA_PRId64,
+                       static_cast<int>(getpid()), name, value);
+        if (write(atrace_marker_fd, buf, len) != len)
+            ATRACE_LOGE("atrace %s write %s error: %s!\n", __func__, buf, strerror(errno));
+    }
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/iutils/Utils.cpp b/camera/hal/intel/ipu6/src/iutils/Utils.cpp
new file mode 100644
index 000000000000..d6d61f2c7c43
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/iutils/Utils.cpp
@@ -0,0 +1,740 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Utils"
+
+#include <stdio.h>
+#include <stdlib.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <dlfcn.h>
+#include <dirent.h>
+#include <iostream>
+#include <fstream>
+#include <sstream>
+
+#include "PlatformData.h"
+#include "iutils/Utils.h"
+#include "iutils/CameraLog.h"
+#include "linux/media-bus-format.h"
+#include "linux/ipu-isys.h"
+
+using std::string;
+
+namespace icamera {
+
+int CameraUtils::getFileContent(const char* filename, char* buffer, int maxSize) {
+
+    std::ifstream stream(filename);
+
+    stream.seekg(0, std::ios::end);
+    long copyLength = stream.tellg();
+    stream.seekg(0, std::ios::beg);
+
+    if (copyLength > maxSize) {
+        copyLength = maxSize;
+    }
+
+    stream.read(buffer, copyLength);
+    return copyLength;
+}
+
+#define GET_FOURCC_FMT(a, b, c, d) ((uint32_t)(d) | ((uint32_t)(c) << 8) \
+                                   | ((uint32_t)(b) << 16) | ((uint32_t)(a) << 24))
+
+enum FormatType {
+    FORMAT_RAW,
+    FORMAT_RAW_VEC,
+    FORMAT_YUV,
+    FORMAT_YUV_VEC,
+    FORMAT_RGB,
+    FORMAT_MBUS,
+    FORMAT_JPEG,
+    FORMAT_FOURCC,
+};
+
+struct FormatInfo {
+    int pixelCode;
+    const char* fullName;
+    const char* shortName;
+    int bpp;
+    FormatType type;
+};
+
+static const FormatInfo gFormatMapping[] = {
+    { V4L2_PIX_FMT_GREY, "V4L2_PIX_FMT_GREY", "GREY", 8, FORMAT_RAW },
+
+    { V4L2_PIX_FMT_SBGGR8, "V4L2_PIX_FMT_SBGGR8", "BGGR8", 8, FORMAT_RAW },
+    { V4L2_PIX_FMT_SGBRG8, "V4L2_PIX_FMT_SGBRG8", "GBRG8", 8, FORMAT_RAW },
+    { V4L2_PIX_FMT_SGRBG8, "V4L2_PIX_FMT_SGRBG8", "GRBG8", 8, FORMAT_RAW },
+    { V4L2_PIX_FMT_SRGGB8, "V4L2_PIX_FMT_SRGGB8", "RGGB8", 8, FORMAT_RAW },
+
+    { V4L2_PIX_FMT_SBGGR10, "V4L2_PIX_FMT_SBGGR10", "BGGR10", 16, FORMAT_RAW },
+    { V4L2_PIX_FMT_SGBRG10, "V4L2_PIX_FMT_SGBRG10", "GBRG10", 16, FORMAT_RAW },
+    { V4L2_PIX_FMT_SGRBG10, "V4L2_PIX_FMT_SGRBG10", "GRBG10", 16, FORMAT_RAW },
+    { V4L2_PIX_FMT_SRGGB10, "V4L2_PIX_FMT_SRGGB10", "RGGB10", 16, FORMAT_RAW },
+
+    { V4L2_PIX_FMT_SBGGR12, "V4L2_PIX_FMT_SBGGR12", "BGGR12", 16, FORMAT_RAW },
+    { V4L2_PIX_FMT_SGBRG12, "V4L2_PIX_FMT_SGBRG12", "GBRG12", 16, FORMAT_RAW },
+    { V4L2_PIX_FMT_SGRBG12, "V4L2_PIX_FMT_SGRBG12", "GRBG12", 16, FORMAT_RAW },
+    { V4L2_PIX_FMT_SRGGB12, "V4L2_PIX_FMT_SRGGB12", "RGGB12", 16, FORMAT_RAW },
+
+    { V4L2_PIX_FMT_SBGGR10P, "V4L2_PIX_FMT_SBGGR10P", "BGGR10P", 10, FORMAT_RAW },
+    { V4L2_PIX_FMT_SGBRG10P, "V4L2_PIX_FMT_SGBRG10P", "GBRG10P", 10, FORMAT_RAW },
+    { V4L2_PIX_FMT_SGRBG10P, "V4L2_PIX_FMT_SGRBG10P", "GRBG10P", 10, FORMAT_RAW },
+    { V4L2_PIX_FMT_SRGGB10P, "V4L2_PIX_FMT_SRGGB10P", "RGGB10P", 10, FORMAT_RAW },
+
+    { V4L2_PIX_FMT_NV12, "V4L2_PIX_FMT_NV12", "NV12", 12, FORMAT_YUV },
+    { V4L2_PIX_FMT_NV21, "V4L2_PIX_FMT_NV21", "NV21", 12, FORMAT_YUV },
+    { V4L2_PIX_FMT_NV16, "V4L2_PIX_FMT_NV16", "NV16", 16, FORMAT_YUV },
+    { V4L2_PIX_FMT_YUYV, "V4L2_PIX_FMT_YUYV", "YUYV", 16, FORMAT_YUV },
+    { V4L2_PIX_FMT_UYVY, "V4L2_PIX_FMT_UYVY", "UYVY", 16, FORMAT_YUV },
+
+    { V4L2_PIX_FMT_YUV420, "V4L2_PIX_FMT_YUV420", "YUV420", 12, FORMAT_YUV },
+    { V4L2_PIX_FMT_YVU420, "V4L2_PIX_FMT_YVU420", "YVU420", 12, FORMAT_YUV },
+    { V4L2_PIX_FMT_YUV422P, "V4L2_PIX_FMT_YUV422P", "YUV422P", 16, FORMAT_YUV },
+
+    { V4L2_PIX_FMT_BGR24, "V4L2_PIX_FMT_BGR24", "BGR24", 24, FORMAT_RGB },
+    { V4L2_PIX_FMT_BGR32, "V4L2_PIX_FMT_BGR32", "BGR32", 32, FORMAT_RGB },
+    { V4L2_PIX_FMT_RGB24, "V4L2_PIX_FMT_RGB24", "RGB24", 24, FORMAT_RGB },
+    { V4L2_PIX_FMT_RGB32, "V4L2_PIX_FMT_RGB32", "RGB32", 32, FORMAT_RGB },
+    { V4L2_PIX_FMT_XBGR32, "V4L2_PIX_FMT_XBGR32", "XBGR32", 32, FORMAT_RGB },
+    { V4L2_PIX_FMT_XRGB32, "V4L2_PIX_FMT_XRGB32", "XRGB32", 32, FORMAT_RGB },
+    { V4L2_PIX_FMT_RGB565, "V4L2_PIX_FMT_RGB565", "RGB565", 16, FORMAT_RGB },
+
+    { V4L2_PIX_FMT_JPEG, "V4L2_PIX_FMT_JPEG", "JPG", 0, FORMAT_JPEG },
+
+    { V4L2_MBUS_FMT_SBGGR12_1X12, "V4L2_MBUS_FMT_SBGGR12_1X12", "SBGGR12_1X12", 12, FORMAT_MBUS },
+    { V4L2_MBUS_FMT_SGBRG12_1X12, "V4L2_MBUS_FMT_SGBRG12_1X12", "SGBRG12_1X12", 12, FORMAT_MBUS },
+    { V4L2_MBUS_FMT_SGRBG12_1X12, "V4L2_MBUS_FMT_SGRBG12_1X12", "SGRBG12_1X12", 12, FORMAT_MBUS },
+    { V4L2_MBUS_FMT_SRGGB12_1X12, "V4L2_MBUS_FMT_SRGGB12_1X12", "SRGGB12_1X12", 12, FORMAT_MBUS },
+
+    { V4L2_MBUS_FMT_SBGGR10_1X10, "V4L2_MBUS_FMT_SBGGR10_1X10", "SBGGR10_1X10", 10, FORMAT_MBUS },
+    { V4L2_MBUS_FMT_SGBRG10_1X10, "V4L2_MBUS_FMT_SGBRG10_1X10", "SGBRG10_1X10", 10, FORMAT_MBUS },
+    { V4L2_MBUS_FMT_SGRBG10_1X10, "V4L2_MBUS_FMT_SGRBG10_1X10", "SGRBG10_1X10", 10, FORMAT_MBUS },
+    { V4L2_MBUS_FMT_SRGGB10_1X10, "V4L2_MBUS_FMT_SRGGB10_1X10", "SRGGB10_1X10", 10, FORMAT_MBUS },
+
+    { V4L2_MBUS_FMT_SBGGR8_1X8, "V4L2_MBUS_FMT_SBGGR8_1X8", "SBGGR8_1X8", 8, FORMAT_MBUS },
+    { V4L2_MBUS_FMT_SGBRG8_1X8, "V4L2_MBUS_FMT_SGBRG8_1X8", "SGBRG8_1X8", 8, FORMAT_MBUS },
+    { V4L2_MBUS_FMT_SGRBG8_1X8, "V4L2_MBUS_FMT_SGRBG8_1X8", "SGRBG8_1X8", 8, FORMAT_MBUS },
+    { V4L2_MBUS_FMT_SRGGB8_1X8, "V4L2_MBUS_FMT_SRGGB8_1X8", "SRGGB8_1X8", 8, FORMAT_MBUS },
+
+    { V4L2_MBUS_FMT_UYVY8_1X16, "V4L2_MBUS_FMT_UYVY8_1X16", "UYVY8_1X16", 16, FORMAT_MBUS },
+    { V4L2_MBUS_FMT_YUYV8_1X16, "V4L2_MBUS_FMT_YUYV8_1X16", "YUYV8_1X16", 16, FORMAT_MBUS },
+    { V4L2_MBUS_FMT_UYVY8_2X8, "V4L2_MBUS_FMT_UYVY8_2X8","UYVY8_2X8", 8, FORMAT_MBUS},
+
+    { MEDIA_BUS_FMT_RGB888_1X24, "MEDIA_BUS_FMT_RGB888_1X24", "RGB888_1X24", 0, FORMAT_MBUS },
+    { MEDIA_BUS_FMT_RGB565_1X16, "MEDIA_BUS_FMT_RGB565_1X16", "RGB565_1X16", 0, FORMAT_MBUS },
+    { MEDIA_BUS_FMT_YUYV12_1X24, "MEDIA_BUS_FMT_YUYV12_1X24", "YUYV12_1X24", 0, FORMAT_MBUS },
+    { MEDIA_BUS_FMT_SGRBG10_1X10, "MEDIA_BUS_FMT_SGRBG10_1X10", "SGRBG10_1X10", 0, FORMAT_MBUS },
+
+    { MEDIA_BUS_FMT_RGB888_1X32_PADHI, "MEDIA_BUS_FMT_RGB888_1X32_PADHI", "RGB888_1X32_PADHI", 0, FORMAT_MBUS },
+
+    { V4L2_FMT_IPU_ISYS_META, "V4L2_FMT_IPU_ISYS_META", "META_DATA", 0, FORMAT_MBUS },
+
+    { GET_FOURCC_FMT('y','0','3','2'), "y032", "y032", 24, FORMAT_FOURCC },
+    { GET_FOURCC_FMT('N','V','1','2'), "YUV420_8_SP", "NV12", 12, FORMAT_FOURCC },
+    { GET_FOURCC_FMT('I','Y','U','V'), "YUV420_8_PL", "IYUV", 12, FORMAT_FOURCC },
+    { GET_FOURCC_FMT('b','V','0','K'), "bV0K", "bV0K", 16, FORMAT_FOURCC },
+    { GET_FOURCC_FMT('b','V','0','G'), "bV0G", "bV0G", 16, FORMAT_FOURCC },
+    { GET_FOURCC_FMT('V','4','2','0'), "YUV420_10_PL", "V420", 24, FORMAT_FOURCC },
+    { GET_FOURCC_FMT('B','A','1','0'), "BA10", "BA10", 16, FORMAT_FOURCC },
+    { GET_FOURCC_FMT('B','A','1','2'), "BA12", "BA12", 16, FORMAT_FOURCC },
+    { GET_FOURCC_FMT('G','R','1','0'), "GR10", "GR10", 16, FORMAT_FOURCC },
+    { GET_FOURCC_FMT('C','S','L','6'), "GRBG_12_LI", "CSL6", 15, FORMAT_FOURCC },
+    { GET_FOURCC_FMT('P','0','1','0'), "P010", "P010", 24, FORMAT_FOURCC },
+    { GET_FOURCC_FMT('Y','U','Y','2'), "YUY2", "YUY2", 16, FORMAT_FOURCC },
+    { GET_FOURCC_FMT('G','R','B','G'), "GRBG", "GRBG", 8, FORMAT_FOURCC },
+};
+
+struct TuningModeStringInfo {
+    TuningMode mode;
+    const char *str;
+};
+
+static const TuningModeStringInfo TuningModeStringInfoTable[] = {
+    { TUNING_MODE_VIDEO,               "VIDEO" },
+    { TUNING_MODE_VIDEO_ULL,           "VIDEO-ULL" },
+    { TUNING_MODE_VIDEO_CUSTOM_AIC,    "VIDEO-CUSTOM_AIC" },
+    { TUNING_MODE_VIDEO_LL,            "VIDEO-LL" },
+    { TUNING_MODE_VIDEO_REAR_VIEW,     "VIDEO-REAR-VIEW" },
+    { TUNING_MODE_VIDEO_HITCH_VIEW,    "VIDEO-HITCH-VIEW" },
+    { TUNING_MODE_STILL_CAPTURE,       "STILL_CAPTURE" },
+};
+
+const char *CameraUtils::tuningMode2String(TuningMode mode)
+{
+    int size = ARRAY_SIZE(TuningModeStringInfoTable);
+    for (int i = 0; i < size; i++) {
+        if (TuningModeStringInfoTable[i].mode == mode) {
+            return TuningModeStringInfoTable[i].str;
+        }
+    }
+    LOGW("Invalid TuningMode %d, use string VIDEO as default", mode);
+    return TuningModeStringInfoTable[0].str;
+}
+
+TuningMode CameraUtils::string2TuningMode(const char *str)
+{
+    int size = ARRAY_SIZE(TuningModeStringInfoTable);
+    for (int i = 0; i < size; i++) {
+        if (strcmp(TuningModeStringInfoTable[i].str, str) == 0) {
+            return TuningModeStringInfoTable[i].mode;
+        }
+    }
+    LOGW("Invalid TuningMode string %s, use TUNING_MODE_VIDEO as default", str);
+    return TuningModeStringInfoTable[0].mode;
+}
+
+const char *CameraUtils::pixelCode2String(int code)
+{
+    int size = ARRAY_SIZE(gFormatMapping);
+    for (int i = 0; i < size; i++) {
+        if (gFormatMapping[i].pixelCode == code) {
+            return gFormatMapping[i].fullName;
+        }
+    }
+
+    LOGE("Invalid Pixel Format: %d", code);
+    return "INVALID FORMAT";
+}
+
+int CameraUtils::string2PixelCode(const char *code)
+{
+    CheckError(code == nullptr, -1, "Invalid null pixel format.");
+
+    int size = ARRAY_SIZE(gFormatMapping);
+    for (int i = 0; i < size; i++) {
+        if (strcmp(gFormatMapping[i].fullName, code) == 0) {
+            return gFormatMapping[i].pixelCode;
+        }
+    }
+
+    LOGE("Invalid Pixel Format: %s", code);
+    return -1;
+}
+
+int CameraUtils::string2IaFourccCode(const char *code)
+{
+    CheckError(code == nullptr, -1, "Invalid null pixel format.");
+
+    int size = ARRAY_SIZE(gFormatMapping);
+    for (int i = 0; i < size; i++) {
+        if (gFormatMapping[i].type == FORMAT_FOURCC){
+            if (!strcmp(gFormatMapping[i].fullName, code) ||
+                 !strcmp(gFormatMapping[i].shortName, code)) {
+                return gFormatMapping[i].pixelCode;
+            }
+        }
+    }
+
+    LOGE("Invalid Pixel Format: %s", code);
+    return -1;
+}
+
+const string CameraUtils::fourcc2String(int format4cc)
+{
+    char fourccBuf[5];
+    CLEAR(fourccBuf);
+    snprintf(fourccBuf, sizeof(fourccBuf), "%c%c%c%c", (format4cc >> 24) & 0xff,
+            (format4cc >> 16) & 0xff, (format4cc >> 8) & 0xff, format4cc & 0xff);
+
+    return string(fourccBuf);
+}
+
+std::string CameraUtils::format2string(int format)
+{
+    int size = ARRAY_SIZE(gFormatMapping);
+    for (int i = 0; i < size; i++) {
+        if (gFormatMapping[i].pixelCode == format) {
+            return std::string(gFormatMapping[i].shortName);
+        }
+    }
+
+    LOG2("%s, Not in our format list :%x", __func__, format);
+    return fourcc2String(format);
+}
+
+unsigned int CameraUtils::fourcc2UL(char *str4cc)
+{
+    CheckError(str4cc == nullptr, 0, "Invalid null string.");
+    CheckError(strlen(str4cc) != 4, 0, "Invalid string %s, should be 4cc.", str4cc);
+
+    return FOURCC_TO_UL(str4cc[0], str4cc[1], str4cc[2], str4cc[3]);
+}
+
+bool CameraUtils::isPlanarFormat(int format)
+{
+    return (format == V4L2_PIX_FMT_NV12 || format == V4L2_PIX_FMT_NV21
+         || format == V4L2_PIX_FMT_YUV420 || format == V4L2_PIX_FMT_YVU420
+         || format == V4L2_PIX_FMT_YUV422P || format == V4L2_PIX_FMT_NV16);
+}
+
+bool CameraUtils::isRaw(int format)
+{
+    int size = ARRAY_SIZE(gFormatMapping);
+    for (int i = 0; i < size; i++) {
+        if (gFormatMapping[i].pixelCode == format) {
+            // Both normal raw and vector raw treated as raw here.
+            return gFormatMapping[i].type == FORMAT_RAW_VEC || gFormatMapping[i].type == FORMAT_RAW;
+        }
+    }
+
+    return false;
+}
+
+int CameraUtils::getBpp(int format)
+{
+    int size = ARRAY_SIZE(gFormatMapping);
+    for (int i = 0; i < size; i++) {
+        if (gFormatMapping[i].pixelCode == format) {
+            return gFormatMapping[i].bpp;
+        }
+    }
+
+    LOGE("There is no bpp supplied for format %s", pixelCode2String(format));
+    return -1;
+}
+
+/**
+ * Get the stride which is also known as aligned bype per line in some context.
+ * Mainly used for locate the start of next line.
+ */
+int CameraUtils::getStride(int format, int width)
+{
+    int bpl = width * getBpp(format) / 8;
+    if (isPlanarFormat(format)) {
+        bpl = width;
+    }
+    return ALIGN_64(bpl);
+}
+
+/*
+ * Calc frame size for compression
+ */
+int CameraUtils::getCompressedFrameSize(int format, int width, int height)
+{
+   int alignedBpl = getStride(format, width);
+   int alignedHeight, imageBufferSize, frameSize;
+
+   switch (format) {
+       case V4L2_PIX_FMT_SBGGR8:
+       case V4L2_PIX_FMT_SGBRG8:
+       case V4L2_PIX_FMT_SGRBG8:
+       case V4L2_PIX_FMT_SRGGB8:
+       case V4L2_PIX_FMT_SBGGR10:
+       case V4L2_PIX_FMT_SGBRG10:
+       case V4L2_PIX_FMT_SGRBG10:
+       case V4L2_PIX_FMT_SRGGB10:
+       {
+           alignedBpl = ALIGN(alignedBpl, ISYS_COMPRESSION_STRIDE_ALIGNMENT_BYTES);
+           alignedHeight = ALIGN(height, ISYS_COMPRESSION_HEIGHT_ALIGNMENT);
+           imageBufferSize = ALIGN(alignedBpl * alignedHeight, ISYS_COMPRESSION_PAGE_SIZE);
+           int singlePlanarTileStatusSize = CAMHAL_CEIL_DIV(((alignedBpl * alignedHeight / ISYS_COMPRESSION_TILE_SIZE_BYTES) *
+                                                                                               ISYS_COMPRESSION_TILE_STATUS_BITS), 8);
+           int singleTileStatusSize = ALIGN(singlePlanarTileStatusSize, ISYS_COMPRESSION_PAGE_SIZE);
+           LOG1("@%s: format:%s, aligned stride:%d, buffer height:%d, pixel buffer size:%d, single planner TS size:%d",
+                __func__, pixelCode2String(format), alignedBpl, alignedHeight, imageBufferSize, singleTileStatusSize);
+           frameSize = imageBufferSize + singleTileStatusSize;
+           break;
+       }
+       case GET_FOURCC_FMT('V','4','2','0'):
+       case GET_FOURCC_FMT('I','Y','U','V'):
+       {
+           //alignedBpl needs accurate stride, not equivalent value from getStride()
+           alignedBpl = (format == GET_FOURCC_FMT('V','4','2','0')) ? width * 2 : width;
+           alignedBpl = ALIGN(alignedBpl, PSYS_COMPRESSION_PSA_Y_STRIDE_ALIGNMENT);
+           alignedHeight = ALIGN(height, PSYS_COMPRESSION_PSA_HEIGHT_ALIGNMENT);
+           int alignBplUV = ALIGN(alignedBpl / UV_STRIDE_DIVIDER, PSYS_COMPRESSION_PSA_UV_STRIDE_ALIGNMENT);
+           int alignHeightUV = ALIGN(alignedHeight / UV_HEIGHT_DIVIDER, PSYS_COMPRESSION_PSA_HEIGHT_ALIGNMENT);
+           imageBufferSize = ALIGN((alignedBpl * alignedHeight + alignBplUV * alignHeightUV * 2), PSYS_COMPRESSION_PAGE_SIZE);
+
+           int planarYTileStatus = CAMHAL_CEIL_DIV((alignedBpl * alignedHeight / TILE_SIZE_YUV420_Y) *
+                                                                                   TILE_STATUS_BITS_YUV420_Y, 8);
+           planarYTileStatus = ALIGN(planarYTileStatus, PSYS_COMPRESSION_PAGE_SIZE);
+           int planarUVTileStatus = CAMHAL_CEIL_DIV((alignBplUV * alignHeightUV / TILE_SIZE_YUV420_Y) *
+                                                                                   TILE_STATUS_BITS_YUV420_Y, 8);
+           planarUVTileStatus = ALIGN(planarUVTileStatus, PSYS_COMPRESSION_PAGE_SIZE);
+
+           LOG1("@%s: format:%s, stride:%d, height:%d, imageSize:%d, tile_status_Y:%d, two tile_status_UV:%d",
+                       __func__, pixelCode2String(format), alignedBpl, alignedHeight,
+                       imageBufferSize, planarYTileStatus, planarUVTileStatus*2);
+           frameSize = imageBufferSize + planarYTileStatus + planarUVTileStatus * 2;
+           break;
+       }
+       case V4L2_PIX_FMT_NV12:
+       {
+           int bpl = width;
+           alignedBpl = ALIGN(bpl, PSYS_COMPRESSION_OFS_STRIDE_ALIGNMENT);
+           alignedHeight = ALIGN(height, PSYS_COMPRESSION_OFS_LINEAR_HEIGHT_ALIGNMENT);
+           int alignedHeightUV = ALIGN(alignedHeight / UV_HEIGHT_DIVIDER, PSYS_COMPRESSION_OFS_LINEAR_HEIGHT_ALIGNMENT);
+           int imageBufferSize = ALIGN(alignedBpl * (alignedHeight + alignedHeightUV), PSYS_COMPRESSION_PAGE_SIZE);
+
+           int planarYTileStatus = CAMHAL_CEIL_DIV((alignedBpl *  alignedHeight / TILE_SIZE_OFS8_10_LINEAR) *
+                                                                                   TILE_STATUS_BITS_OFS8_10_LINEAR, 8);
+           planarYTileStatus = ALIGN(planarYTileStatus, PSYS_COMPRESSION_PAGE_SIZE);
+           int planarUVTileStatus = CAMHAL_CEIL_DIV((alignedBpl * alignedHeightUV / TILE_SIZE_OFS8_10_LINEAR) *
+                                                                                   TILE_STATUS_BITS_OFS8_10_LINEAR, 8);
+           planarUVTileStatus = ALIGN(planarUVTileStatus, PSYS_COMPRESSION_PAGE_SIZE);
+
+           LOG1("@%s: format: %s, stride:%d, height:%d, imageSize:%d, tile_status_Y:%d, tile_status_UV:%d",
+                    __func__, pixelCode2String(format), alignedBpl, alignedHeight, imageBufferSize, planarYTileStatus, planarUVTileStatus);
+           frameSize = imageBufferSize + planarYTileStatus + planarUVTileStatus;
+           break;
+       }
+       default:
+           LOGE("@%s: unexpected format 0x%x in string %s, unsupported compression format", __func__, format, pixelCode2String(format));
+           frameSize = 0;
+           break;
+    }
+
+    return frameSize;
+}
+
+/*
+ * Calc frame buffer size.
+ *
+ *  Why alignment is 64?
+ *  The IPU DMA unit must transimit at leat 64 bytes one time.
+ *
+ *  Why need extra size? It's due to a hardware issue: the DMA unit is a power of
+ *  two, and a line should be transferred as few units as possible.
+ *  The result is that up to line length more data than the image size
+ *  may be transferred to memory after the image.
+ *
+ *  Another limition is the GDA(Global Dynamic Allocator) allocation unit size(1024). For low
+ *  resolution it gives a bigger number. Use larger one to avoid
+ *  memory corruption.
+ *  for example: 320x480 UVVY, which bpl is 640, less than 1024, in this case, driver will
+ *  allocate 1024 bytes for the last line.
+ */
+int CameraUtils::getFrameSize(int format, int width, int height, bool needAlignedHeight, bool needExtraSize, bool needCompression)
+{
+    LOG1("@%s get buffer size for %s %dx%d", __func__, pixelCode2String(format), width, height);
+    LOG1("@%s needAlignedHeight:%d, needExtraSize:%d, needCompression: %d", __func__,
+              needAlignedHeight, needExtraSize, needCompression);
+
+    int alignedBpl = getStride(format, width);
+
+    // Get frame size with aligned height taking in count for internal buffers.
+    // To garantee PSYS kernel like GDC always get enough buffer size to process.
+    // This is to satisfy the PSYS kernel, like GDC, input alignment requirement.
+    if (needAlignedHeight) {
+        height = ALIGN_64(height);
+        LOG1("@%s buffer aligned height %d", __func__, height);
+    }
+    int bufferHeight = isPlanarFormat(format) ? (height * getBpp(format) / 8) : height;
+
+    if (!needExtraSize) {
+        LOG1("%s: no need extra size, frame size is %d", __func__, alignedBpl * bufferHeight);
+        return alignedBpl * bufferHeight;
+    }
+
+    if (needCompression) {
+        int compressedFrameSize = getCompressedFrameSize(format, width, height);
+        LOG1("%s: Compressed frame size %d for original format: %s",
+                   __func__, compressedFrameSize, pixelCode2String(format));
+        return compressedFrameSize;
+    }
+
+    // Extra size should be at least one alignedBpl
+    int extraSize = isPlanarFormat(format) ? alignedBpl * getBpp(format) / 8 : alignedBpl;
+    extraSize = std::max(extraSize , 1024);
+
+    return alignedBpl * bufferHeight + extraSize;
+}
+
+int CameraUtils::getNumOfPlanes(int format)
+{
+    switch(format) {
+        case V4L2_PIX_FMT_NV12:
+        case V4L2_PIX_FMT_SGRBG8:
+        case V4L2_FMT_IPU_ISYS_META:
+            return 1;
+        //Add more when needed...
+        default:
+            return 1;
+    }
+}
+
+void CameraUtils::getDeviceName(const char* entityName, string& deviceNodeName, bool isSubDev)
+{
+
+    const char *filePrefix = "video";
+    const char *dirPath = "/sys/class/video4linux/";
+    if (isSubDev)
+        filePrefix = "v4l-subdev";
+
+    DIR *dp = opendir(dirPath);
+    CheckError((dp == nullptr), VOID_VALUE, "@%s, Fail open : %s", __func__, dirPath);
+
+    struct dirent *dirp = nullptr;
+    while ((dirp = readdir(dp)) != nullptr) {
+        if ((dirp->d_type == DT_LNK) && (strncmp(dirp->d_name, filePrefix, strlen(filePrefix)) == 0)) {
+            string subDeviceName = dirPath;
+            subDeviceName += dirp->d_name;
+            subDeviceName += "/name";
+            int fd = open(subDeviceName.c_str(), O_RDONLY);
+            CheckError((fd < 0), VOID_VALUE, "@%s, open file %s failed. err: %s",
+                  __func__, subDeviceName.c_str(), strerror(errno));
+
+            char buf[128] = {'\0'};
+            int len = read(fd, buf, sizeof(buf));
+            close(fd);
+            len--; // remove "\n"
+            if (len == (int)strlen(entityName) && memcmp(buf, entityName, len) == 0) {
+                deviceNodeName = "/dev/";
+                deviceNodeName += dirp->d_name;
+                break;
+            }
+        }
+    }
+    closedir(dp);
+}
+
+void CameraUtils::getSubDeviceName(const char* entityName, string& deviceNodeName)
+{
+     getDeviceName(entityName, deviceNodeName, true);
+}
+
+int CameraUtils::getInterlaceHeight(int field, int height)
+{
+    if (SINGLE_FIELD(field))
+        return height/2;
+    else
+        return height;
+}
+
+bool CameraUtils::isMultiExposureCase(TuningMode tuningMode)
+{
+    LOG2("%s, tuningMode %d", __func__, tuningMode);
+
+    return false;
+}
+
+bool CameraUtils::isUllPsysPipe(TuningMode tuningMode)
+{
+    return (tuningMode == TUNING_MODE_VIDEO_ULL ||
+            tuningMode == TUNING_MODE_VIDEO_CUSTOM_AIC);
+}
+
+ConfigMode CameraUtils::getConfigModeByName(const char* ConfigName)
+{
+    ConfigMode configMode = CAMERA_STREAM_CONFIGURATION_MODE_END;
+
+    if (ConfigName == nullptr) {
+        LOGE("%s, the ConfigName is nullptr", __func__);
+    } else if (strcmp(ConfigName, "AUTO") == 0) {
+        configMode = CAMERA_STREAM_CONFIGURATION_MODE_AUTO;
+    } else if (strcmp(ConfigName, "ULL") == 0) {
+        configMode = CAMERA_STREAM_CONFIGURATION_MODE_ULL;
+    } else if (strcmp(ConfigName, "NORMAL") == 0) {
+        configMode = CAMERA_STREAM_CONFIGURATION_MODE_NORMAL;
+    } else if (strcmp(ConfigName, "HIGH_SPEED") == 0) {
+        configMode = CAMERA_STREAM_CONFIGURATION_MODE_CONSTRAINED_HIGH_SPEED;
+    } else if (strcmp(ConfigName, "CUSTOM_AIC") == 0) {
+        configMode = CAMERA_STREAM_CONFIGURATION_MODE_CUSTOM_AIC;
+    } else if (strcmp(ConfigName, "VIDEO_LL") == 0) {
+        configMode = CAMERA_STREAM_CONFIGURATION_MODE_VIDEO_LL;
+    } else if (strcmp(ConfigName, "STILL_CAPTURE") == 0) {
+        configMode = CAMERA_STREAM_CONFIGURATION_MODE_STILL_CAPTURE;
+    } else if (strcmp(ConfigName, "NONE") == 0) {
+        LOG1("%s, the detected internal 'NONE' ConfigName", __func__);
+    } else {
+        configMode = CAMERA_STREAM_CONFIGURATION_MODE_NORMAL;
+        LOG2("%s, the ConfigName %s is not supported, use normal as default", __func__, ConfigName);
+    }
+
+    LOG2("%s, configMode = %d", __func__, configMode);
+    return configMode;
+}
+
+void CameraUtils::getConfigModeFromString(string str, std::vector<ConfigMode> &cfgModes)
+{
+    bool split = true;
+    ConfigMode mode;
+    string resultStr, modeStr = str;
+
+    while(split) {
+        size_t pos = 0;
+        if ((pos = modeStr.find(",")) == string::npos) {
+            mode = getConfigModeByName(modeStr.c_str());
+            split = false;
+        } else {
+            resultStr = modeStr.substr(0, pos);
+            modeStr = modeStr.substr(pos + 1);
+            mode = getConfigModeByName(resultStr.c_str());
+        }
+        cfgModes.push_back(mode);
+    }
+}
+
+ConfigMode CameraUtils::getConfigModeBySceneMode(camera_scene_mode_t sceneMode)
+{
+    ConfigMode configMode = CAMERA_STREAM_CONFIGURATION_MODE_END;
+
+    switch(sceneMode) {
+        case SCENE_MODE_NORMAL:
+            configMode = CAMERA_STREAM_CONFIGURATION_MODE_NORMAL;
+            break;
+        case SCENE_MODE_ULL:
+            configMode = CAMERA_STREAM_CONFIGURATION_MODE_ULL;
+            break;
+        case SCENE_MODE_CUSTOM_AIC:
+            configMode = CAMERA_STREAM_CONFIGURATION_MODE_CUSTOM_AIC;
+            break;
+        case SCENE_MODE_VIDEO_LL:
+            configMode = CAMERA_STREAM_CONFIGURATION_MODE_VIDEO_LL;
+            break;
+        default:
+            // There is no corresponding ConfigMode for some scene.
+            LOG2("there is no corresponding ConfigMode for scene %d", sceneMode);
+            break;
+    }
+    return configMode;
+}
+
+camera_scene_mode_t CameraUtils::getSceneModeByName(const char* sceneName)
+{
+    if (sceneName == nullptr) return SCENE_MODE_MAX;
+    else if (strcmp(sceneName, "AUTO") == 0) return SCENE_MODE_AUTO;
+    else if (strcmp(sceneName, "ULL") == 0) return SCENE_MODE_ULL;
+    else if (strcmp(sceneName, "VIDEO_LL") == 0) return SCENE_MODE_VIDEO_LL;
+    else if (strcmp(sceneName, "NORMAL") == 0) return SCENE_MODE_NORMAL;
+    else if (strcmp(sceneName, "CUSTOM_AIC") == 0) return SCENE_MODE_CUSTOM_AIC;
+
+    return SCENE_MODE_MAX;
+}
+
+camera_awb_mode_t CameraUtils::getAwbModeByName(const char* awbName)
+{
+    if (awbName == nullptr) return AWB_MODE_MAX;
+    else if (strcmp(awbName, "AUTO") == 0) return AWB_MODE_AUTO;
+    else if (strcmp(awbName, "INCANDESCENT") == 0) return AWB_MODE_INCANDESCENT;
+    else if (strcmp(awbName, "FLUORESCENT") == 0) return AWB_MODE_FLUORESCENT;
+    else if (strcmp(awbName, "DAYLIGHT") == 0) return AWB_MODE_DAYLIGHT;
+    else if (strcmp(awbName, "FULL_OVERCAST") == 0) return AWB_MODE_FULL_OVERCAST;
+    else if (strcmp(awbName, "PARTLY_OVERCAST") == 0) return AWB_MODE_PARTLY_OVERCAST;
+    else if (strcmp(awbName, "SUNSET") == 0) return AWB_MODE_SUNSET;
+    else if (strcmp(awbName, "VIDEO_CONFERENCE") == 0) return AWB_MODE_VIDEO_CONFERENCE;
+    else if (strcmp(awbName, "MANUAL_CCT_RANGE") == 0) return AWB_MODE_MANUAL_CCT_RANGE;
+    else if (strcmp(awbName, "MANUAL_WHITE_POINT") == 0) return AWB_MODE_MANUAL_WHITE_POINT;
+    else if (strcmp(awbName, "MANUAL_GAIN") == 0) return AWB_MODE_MANUAL_GAIN;
+    else if (strcmp(awbName, "MANUAL_COLOR_TRANSFORM") == 0) return AWB_MODE_MANUAL_COLOR_TRANSFORM;
+
+    return AWB_MODE_MAX;
+}
+
+unsigned int CameraUtils::getMBusFormat(int cameraId, unsigned int isysFmt)
+{
+    unsigned int pixelCode = 0;
+
+    switch (isysFmt) {
+    case V4L2_PIX_FMT_UYVY:
+    case V4L2_PIX_FMT_NV16:
+        pixelCode = V4L2_MBUS_FMT_UYVY8_1X16;
+        break;
+    case V4L2_PIX_FMT_YUYV:
+        pixelCode = V4L2_MBUS_FMT_YUYV8_1X16;
+        break;
+    case V4L2_PIX_FMT_BGR24:
+    case V4L2_PIX_FMT_XBGR32:
+        pixelCode = MEDIA_BUS_FMT_RGB888_1X24;
+        break;
+    case V4L2_PIX_FMT_RGB565:
+    case V4L2_PIX_FMT_XRGB32:
+        pixelCode = MEDIA_BUS_FMT_RGB565_1X16;
+        break;
+    case V4L2_PIX_FMT_SGRBG8:
+        pixelCode = V4L2_MBUS_FMT_SGRBG8_1X8;
+        break;
+    default:
+        LOGE("No input format to match the output: %s", pixelCode2String(isysFmt));
+        break;
+    }
+
+    return pixelCode;
+}
+
+void* CameraUtils::dlopenLibrary(const char* name, int flags)
+{
+    CheckError((name == nullptr), nullptr, "%s, invalid parameters", __func__);
+
+    void* handle = dlopen(name, flags);
+
+    const char* lError = dlerror();
+    if (lError) {
+        if (handle == nullptr) {
+            LOGW("%s, handle is NULL", __func__);
+        }
+        LOGW("%s, dlopen Error: %s", __func__, lError);
+        return nullptr;
+    }
+
+    LOG1("%s, handle %p, name %s has been opened", __func__, handle, name);
+    return handle;
+}
+
+void* CameraUtils::dlsymLibrary(void* handle, const char* str)
+{
+    CheckError((handle == nullptr || str == nullptr), nullptr, "%s, invalid parameters", __func__);
+
+    void* sym = dlsym(handle, str);
+
+    const char* lError = dlerror();
+    if (lError) {
+        if (sym == nullptr) {
+            LOGW("%s, symbol is nullptr", __func__);
+        }
+        LOGW("%s, dlopen Error: %s", __func__, lError);
+        return nullptr;
+    }
+
+    LOG1("%s, handle %p, str %s has been found", __func__, handle, str);
+    return sym;
+}
+
+int CameraUtils::dlcloseLibrary(void* handle)
+{
+    CheckError((handle == nullptr), BAD_VALUE, "%s, invalid parameters", __func__);
+
+    dlclose(handle);
+    LOG1("%s, handle %p has been closed", __func__, handle);
+    return OK;
+}
+
+std::vector<string> CameraUtils::splitString(const char* srcStr, char delim)
+{
+    std::vector<string> tokens;
+    std::stringstream ss(srcStr);
+    string item;
+
+    for (size_t i = 0; std::getline(ss, item, delim); i++) {
+        tokens.push_back(item);
+    }
+
+    return tokens;
+}
+
+nsecs_t CameraUtils::systemTime()
+{
+    struct timespec t;
+    t.tv_sec = t.tv_nsec = 0;
+    clock_gettime(CLOCK_MONOTONIC, &t);
+    return nsecs_t(t.tv_sec)*1000000000LL + t.tv_nsec;
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/iutils/Utils.h b/camera/hal/intel/ipu6/src/iutils/Utils.h
new file mode 100644
index 000000000000..0bae788fa802
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/iutils/Utils.h
@@ -0,0 +1,277 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <linux/videodev2.h>
+#include <v4l2_device.h>
+
+#include <string.h>
+#include <vector>
+#include <string>
+
+#include "CameraTypes.h"
+
+namespace icamera {
+
+typedef int64_t nsecs_t;
+
+typedef ::cros::V4L2DevicePoller V4L2DevicePoller;
+typedef ::cros::V4L2Device V4L2Device;
+typedef ::cros::V4L2VideoNode V4L2VideoNode;
+typedef ::cros::V4L2Subdevice V4L2Subdevice;
+typedef ::cros::V4L2Buffer V4L2Buffer;
+typedef ::cros::V4L2Format V4L2Format;
+
+#define ALIGN(val, alignment) (((val)+(alignment)-1) & ~((alignment)-1))
+#define ALIGN_64(val) ALIGN(val, 64)
+#define ALIGN_32(val) ALIGN(val, 32)
+#define ALIGN_16(val) ALIGN(val, 16)
+#define ALIGN_8(val)  ALIGN(val, 8)
+
+#define ARRAY_SIZE(array)    (sizeof(array) / sizeof((array)[0]))
+
+#define CLEAR(x) memset (&(x), 0, sizeof (x))
+
+// macro CLIP is used to clip the Number value to between the Min and Max
+#define CLIP(Number, Max, Min)    ((Number) > (Max) ? (Max) : ((Number) < (Min) ? (Min) : (Number)))
+
+#ifndef UNUSED
+#define UNUSED(param) (void)(param)
+#endif
+
+/**
+ * Align to page boundary
+ * \ingroup ia_tools
+ */
+#ifndef PAGE_ALIGN
+#define PAGE_SIZE_U (getpagesize())
+#define PAGE_ALIGN(x)  ALIGN(x, PAGE_SIZE_U)
+#endif
+
+/* Integer round-up division of a with b */
+#define CEIL_DIV(a, b)  ((b) ? (((a) + (b) - 1) / (b)) : 0)
+/* Align a to the upper multiple of b */
+#define CEIL_MUL(a, b)  CEIL_DIV(a, b) * (b)
+
+#define SINGLE_FIELD(field) ((field == V4L2_FIELD_TOP) || (field == V4L2_FIELD_BOTTOM) || \
+                             (field == V4L2_FIELD_ALTERNATE))
+/**
+ * Use to check input parameter and if failed, return err_code and print error message
+ */
+#define VOID_VALUE
+#define CheckError(condition, err_code, err_msg, args...) \
+            do { \
+                if (condition) { \
+                    LOGE(err_msg, ##args);\
+                    return err_code;\
+                }\
+            } while (0)
+
+#define CheckAndClean(condition, err_code, clean, err_msg, args...) \
+            do { \
+                if (condition) { \
+                    LOGE(err_msg, ##args);\
+                    clean;\
+                    return err_code;\
+                }\
+            } while (0)
+
+/**
+ * Use to check input parameter and if failed, return err_code and print warning message,
+ * this should be used for non-vital error checking.
+ */
+#define CheckWarning(condition, err_code, err_msg, args...) \
+            do { \
+                if (condition) { \
+                    LOGW(err_msg, ##args);\
+                    return err_code;\
+                }\
+            } while (0)
+
+// As above but no return.
+#define CheckWarningNoReturn(condition, err_msg, args...) \
+                            do { \
+                                if (condition) { \
+                                    LOGW(err_msg, ##args);\
+                                }\
+                            } while (0)
+
+// macro delete array and set it to null
+#define DELETE_ARRAY_AND_NULLIFY(var) \
+    do { \
+        delete[] (var); \
+        var = nullptr; \
+    } while(0)
+
+/**
+ *  \macro TIMEVAL2NSECS
+ *  Convert timeval struct to value in nanoseconds
+ *  Helper macro to convert timeval struct to nanosecond values stored in a
+ *  long long signed value (equivalent to int64_t)
+ */
+#define TIMEVAL2NSECS(x) (int64_t)((x).tv_sec*1000000000LL + (x).tv_usec*1000LL)
+
+/**
+ *  \macro TIMEVAL2USECS
+ *  Convert timeval struct to value in microseconds
+ *  Helper macro to convert timeval struct to microsecond values stored in a
+ *  long long signed value (equivalent to int64_t)
+ */
+#define TIMEVAL2USECS(x) (int64_t)(((x).tv_sec*1000000000LL + \
+                                    (x).tv_usec*1000LL)/1000LL)
+
+// macro for float comparaion with 0
+#define EPSILON 0.00001
+
+// macro for the MAX FILENAME
+#define MAX_SYS_NAME 64
+#define MAX_TARGET_NAME 256
+
+// A macro to disallow the copy constructor and operator= functions
+// This should be used in the private:declarations for a class
+#ifndef DISALLOW_COPY_AND_ASSIGN
+#define DISALLOW_COPY_AND_ASSIGN(TypeName) \
+    TypeName(const TypeName&) = delete; \
+    TypeName& operator=(const TypeName&) = delete
+#endif
+
+/**
+ *  Maker note maximum sizes
+ *  Section 1 is used for Normal capture
+ *  Section 2 is used for RAW captures
+ */
+#define MAKERNOTE_SECTION1_SIZE 56000
+#define MAKERNOTE_SECTION2_SIZE 110592
+
+// macro for memcpy
+#ifndef MEMCPY_S
+#define MEMCPY_S(dest, dmax, src, smax) memcpy((dest), (src), std::min((size_t)(dmax), (size_t)(smax)))
+#endif
+
+#define STDCOPY(dst, src, size) std::copy((src), ((src) + (size)), (dst))
+
+// macro for isys compression width/height calculation
+#define ISYS_COMPRESSION_STRIDE_ALIGNMENT_BYTES 512
+#define ISYS_COMPRESSION_HEIGHT_ALIGNMENT 1
+#define ISYS_COMPRESSION_PAGE_SIZE 0x1000
+// macro for psys compression width/height calculation
+#define PSYS_COMPRESSION_PSA_Y_STRIDE_ALIGNMENT 256
+#define PSYS_COMPRESSION_PSA_UV_STRIDE_ALIGNMENT 256
+#define PSYS_COMPRESSION_PSA_HEIGHT_ALIGNMENT 2
+#define PSYS_COMPRESSION_OFS_STRIDE_ALIGNMENT 128
+#define PSYS_COMPRESSION_OFS_TILE_HEIGHT_ALIGNMENT 32
+#define PSYS_COMPRESSION_OFS_LINEAR_HEIGHT_ALIGNMENT 2
+#define PSYS_COMPRESSION_PAGE_SIZE 0x1000
+#define UV_STRIDE_DIVIDER 2
+#define UV_HEIGHT_DIVIDER 2
+// tile size definition
+#define ISYS_COMPRESSION_TILE_SIZE_BYTES 512
+#define TILE_SIZE_YUV420_Y 256
+#define TILE_SIZE_YUV420_UV 128
+#define TILE_SIZE_OFS10_12_TILEY 256
+#define TILE_SIZE_OFS8_TILEY 256
+#define TILE_SIZE_OFS8_10_LINEAR 128
+
+//tile status bits definition
+#define ISYS_COMPRESSION_TILE_STATUS_BITS 4
+#define TILE_STATUS_BITS_YUV420_Y 2
+#define TILE_STATUS_BITS_YUV420_UV 1
+#define TILE_STATUS_BITS_OFS_NV12_TILE_Y	 8
+#define TILE_STATUS_BITS_OFS_P010_TILE_Y 8
+#define TILE_STATUS_BITS_OFS8_10_LINEAR 1
+#define TILE_STATUS_BITS_TNR_NV12_LINEAR 4
+
+#define CAMHAL_CEIL_DIV(a,b)   (((a) + (b) - 1) / (b))
+
+#define FOURCC_TO_UL(a,b,c,d) \
+    ((uint32_t)(a) | ((uint32_t)(b)<<8) | ((uint32_t)(c)<<16) | ((uint32_t)(d)<<24))
+
+#define DEFAULT_JPEG_QUALITY   95
+#define DEFAULT_THUMBNAIL_QUALITY 0
+
+//Internal useful tool for format
+namespace CameraUtils {
+
+    int getFileContent(const char* filename, char* buffer, int maxSize);
+
+    const char *tuningMode2String(TuningMode mode);
+
+    TuningMode string2TuningMode(const char *str);
+
+    const char *pixelCode2String(int code);
+
+    int string2PixelCode(const char *code);
+
+    int string2IaFourccCode(const char *code);
+
+    std::string format2string (int format);
+
+    int string2format(const char *str);
+
+    const std::string fourcc2String(int format4cc);
+
+    unsigned int fourcc2UL(char *str4cc);
+
+    bool isPlanarFormat(int format);
+
+    bool isRaw(int format);
+
+    int getBpp(int format);
+
+    int getStride (int format, int width);
+
+    int getCompressedFrameSize(int format, int width, int height);
+
+    int getFrameSize(int format, int width, int height, bool needAlignedHeight = false, bool needExtraSize = true, bool needCompression = false);
+
+    int getNumOfPlanes(int format);
+
+    void getDeviceName(const char* entityName, std::string& deviceNodeName, bool isSubDev = false);
+
+    void getSubDeviceName(const char* entityName, std::string& deviceNodeName);
+
+    int getInterlaceHeight(int field, int height);
+
+    bool isMultiExposureCase(TuningMode tuningMode);
+
+    bool isUllPsysPipe(TuningMode tuningMode);
+
+    ConfigMode getConfigModeByName(const char* ConfigName);
+
+    ConfigMode getConfigModeBySceneMode(camera_scene_mode_t sceneMode);
+
+    void getConfigModeFromString(std::string str, std::vector<ConfigMode> &cfgModes);
+
+    camera_scene_mode_t getSceneModeByName(const char* sceneName);
+
+    camera_awb_mode_t getAwbModeByName(const char* awbName);
+
+    unsigned int getMBusFormat(int cameraId, unsigned int isysFmt);
+
+    void* dlopenLibrary(const char* name, int flags);
+    void* dlsymLibrary(void* handle, const char* str);
+    int dlcloseLibrary(void* handle);
+
+    /**
+     * Spit the given srcStr by delim into a vector of sub strings.
+     */
+    std::vector<std::string> splitString(const char* srcStr, char delim);
+
+    nsecs_t systemTime();
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/jpeg/EXIFMaker.cpp b/camera/hal/intel/ipu6/src/jpeg/EXIFMaker.cpp
new file mode 100644
index 000000000000..0cd7f6712fd0
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/jpeg/EXIFMaker.cpp
@@ -0,0 +1,620 @@
+/*
+ * Copyright (C) 2016-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "EXIFMaker"
+
+#include "EXIFMaker.h"
+
+#include <limits.h>
+
+#include <fstream>
+#include <string>
+#include <unordered_map>
+
+#include "ParameterHelper.h"
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+#define DEFAULT_ISO_SPEED 100
+
+// The property file directory
+static const char* CAMERA_CACHE_DIR = "/var/cache/camera/";
+/*
+ * Property file defines product name and manufactory info
+ * Used for EXIF header of JPEG. Format: key=value in each line
+ */
+static const char* CAMERA_PROPERTY_FILE = "camera.prop";
+
+EXIFMaker::EXIFMaker()
+        : mExifSize(-1),
+          mInitialized(false),
+          mProductName("<not_set>"),
+          mManufacturerName("<not set>") {
+    LOG2("@%s", __func__);
+
+    CLEAR(mExifAttributes);
+    mMakernoteSection = new unsigned char[MAKERNOTE_SECTION1_SIZE + MAKERNOTE_SECTION2_SIZE];
+    readProperty();
+}
+
+EXIFMaker::~EXIFMaker() {
+    LOG2("@%s", __func__);
+    delete[] mMakernoteSection;
+}
+
+void EXIFMaker::readProperty() {
+    LOG2("@%s", __func__);
+    std::string cameraPropertyPath =
+        std::string(CAMERA_CACHE_DIR) + std::string(CAMERA_PROPERTY_FILE);
+    std::fstream props(cameraPropertyPath.c_str(), std::ios::in);
+
+    if (!props.is_open()) {
+        LOG2("There isn't camera property file.");
+        return;
+    }
+
+    const std::string kModel = "ro.product.model";
+    const std::string kManufacturer = "ro.product.manufacturer";
+    const std::string kDelimiter = "=";
+    std::unordered_map<std::string, std::string> properties;
+
+    while (!props.eof()) {
+        size_t pos;
+        std::string line, key, value;
+
+        std::getline(props, line);
+        pos = line.find(kDelimiter);
+        if (pos != std::string::npos) {
+            key = line.substr(0, pos);
+            value = line.substr(pos + 1);
+            properties[key] = value;
+            LOG2("%s, new key,value: %s,%s", __func__, key.c_str(), value.c_str());
+        }
+    }
+
+    if (properties.find(kManufacturer) != properties.end()) {
+        mManufacturerName = properties[kManufacturer];
+    }
+    if (properties.find(kModel) != properties.end()) {
+        mProductName = properties[kModel];
+    }
+}
+
+/**
+ * Fills EXIF data after a picture has been taken to
+ * record the active sensor, 3A and ISP state to EXIF metadata.
+ *
+ * This function is intented to set EXIF tags belonging
+ * to the EXIF "Per Picture Camera Setting" group.
+ *
+ * @arg params active Android HAL parameters
+ */
+void EXIFMaker::pictureTaken(ExifMetaData* exifmetadata) {
+    LOG2("@%s", __func__);
+
+    // brightness, -99.99 to 99.99. FFFFFFFF.H means unknown.
+    float brightness;
+    // TODO: The check for getAeManualBrightness of 3A should be moved
+    //       to MetaData class, because the metadata collection happen
+    //       at capture time
+    brightness = 99;
+    mExifAttributes.brightness.num = static_cast<int>(brightness * 100);
+    mExifAttributes.brightness.den = 100;
+    LOG2("EXIF: brightness = %.2f", brightness);
+
+    mExifAttributes.contrast = 0;
+    mExifAttributes.saturation = 0;
+    mExifAttributes.sharpness = 0;
+    LOG2("EXIF: contrast=%d, saturation=%d, sharpness=%d (0:normal 1:low 2:high)",
+         mExifAttributes.contrast, mExifAttributes.saturation, mExifAttributes.sharpness);
+
+    // set the exposure program mode
+    icamera::camera_ae_mode_t aeMode = exifmetadata->aeMode;
+    switch (aeMode) {
+        case AE_MODE_MANUAL:
+            mExifAttributes.exposure_program = EXIF_EXPOSURE_PROGRAM_MANUAL;
+            mExifAttributes.exposure_mode = EXIF_EXPOSURE_MANUAL;
+            LOG2("EXIF: Exposure Program = Manual, Exposure Mode = Manual");
+            break;
+        case AE_MODE_AUTO:
+        default:
+            mExifAttributes.exposure_program = EXIF_EXPOSURE_PROGRAM_NORMAL;
+            mExifAttributes.exposure_mode = EXIF_EXPOSURE_AUTO;
+            LOG2("EXIF: Exposure Program = Normal, Exposure Mode = Auto");
+            break;
+    }
+
+    mExifAttributes.metering_mode = EXIF_METERING_AVERAGE;
+
+    // white balance mode. 0: auto; 1: manual
+    icamera::camera_awb_mode_t awbMode = exifmetadata->awbMode;
+    LOG2("EXIF: awbMode = %d", awbMode);
+    switch (awbMode) {
+        case AWB_MODE_AUTO:
+        case AWB_MODE_MAX:
+            mExifAttributes.white_balance = EXIF_WB_AUTO;
+            break;
+        default:
+            mExifAttributes.white_balance = EXIF_WB_MANUAL;
+            break;
+    }
+
+    // light source type. Refer to EXIF V2.2
+    // TBD. Now light source is only set to UNKNOWN, when WB is auto mode.
+    std::unordered_map<icamera::camera_awb_mode_t, CamExifLightSourceType> awbModeLightMap = {
+        {AWB_MODE_AUTO, EXIF_LIGHT_SOURCE_UNKNOWN},
+        {AWB_MODE_SUNSET, EXIF_LIGHT_SOURCE_TUNGSTEN},
+        {AWB_MODE_DAYLIGHT, EXIF_LIGHT_SOURCE_FINE_WEATHER},
+        {AWB_MODE_PARTLY_OVERCAST, EXIF_LIGHT_SOURCE_CLOUDY_WEATHER},
+        {AWB_MODE_FLUORESCENT, EXIF_LIGHT_SOURCE_FLUORESCENT},
+        {AWB_MODE_INCANDESCENT, EXIF_LIGHT_SOURCE_TUNGSTEN},
+        {AWB_MODE_MAX, EXIF_LIGHT_SOURCE_OTHER_LIGHT_SOURCE},
+    };
+    if (awbModeLightMap.find(awbMode) != awbModeLightMap.end()) {
+        mExifAttributes.light_source = awbModeLightMap[awbMode];
+    } else {
+        mExifAttributes.light_source = EXIF_LIGHT_SOURCE_UNKNOWN;
+    }
+
+    mExifAttributes.scene_capture_type = EXIF_SCENE_STANDARD;
+
+    int rotation = exifmetadata->mJpegSetting.orientation;
+    mExifAttributes.orientation = EXIF_ORIENTATION_UP;
+    if (0 == rotation)
+        mExifAttributes.orientation = EXIF_ORIENTATION_UP;
+    else if (90 == rotation)
+        mExifAttributes.orientation = EXIF_ORIENTATION_90;
+    else if (180 == rotation)
+        mExifAttributes.orientation = EXIF_ORIENTATION_180;
+    else if (270 == rotation)
+        mExifAttributes.orientation = EXIF_ORIENTATION_270;
+
+    // Platform has no HW rotation. No swap here
+    // if (rotation % 180 == 90)
+    //    swap(mExifAttributes.width, mExifAttributes.height);
+
+    mExifAttributes.zoom_ratio.num = exifmetadata->zoomRatio;
+    mExifAttributes.zoom_ratio.den = 100;
+    // the unit of subjectDistance is meter, focus distance from 3A is mm.
+    mExifAttributes.subject_distance.num =
+        static_cast<uint32_t>(exifmetadata->currentFocusDistance);
+    mExifAttributes.subject_distance.den = 1000;
+    mExifAttributes.custom_rendered =
+        exifmetadata->hdr ? EXIF_CUSTOM_RENDERED_HDR : EXIF_DEF_CUSTOM_RENDERED;
+    LOG2("subject_distance is %d", mExifAttributes.subject_distance.num);
+}
+
+/**
+ * Called when the the camera static configuration is known.
+ *
+ * @arg width: width of the main JPEG picture.
+ * @arg height: height of the main JPEG picture.
+ */
+void EXIFMaker::initialize(int width, int height) {
+    /* We clear the exif attributes, so we won't be using some old values
+     * from a previous EXIF generation.
+     */
+    clear();
+
+    // Initialize the mExifAttributes with specific values
+    // time information
+    time_t rawtime;
+    struct tm* timeinfo;
+    time(&rawtime);
+    timeinfo = localtime(&rawtime);
+    if (timeinfo) {
+        strftime(reinterpret_cast<char*>(mExifAttributes.date_time),
+                 sizeof(mExifAttributes.date_time), "%Y:%m:%d %H:%M:%S", timeinfo);
+        // fields: tm_sec, tm_min, tm_hour, tm_mday, tm_mon, tm_year, tm_wday, tm_yday, tm_isdst,
+        // tm_gmtoff, tm_zone
+    } else {
+        LOGW("nullptr timeinfo from localtime(), using defaults...");
+        struct tm tmpTime = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, "UTC"};
+        strftime(reinterpret_cast<char*>(mExifAttributes.date_time),
+                 sizeof(mExifAttributes.date_time), "%Y:%m:%d %H:%M:%S", &tmpTime);
+    }
+
+    // set default subsec time to 1000
+    const char subsecTime[] = "1000";
+    MEMCPY_S(reinterpret_cast<char*>(mExifAttributes.subsec_time),
+             sizeof(mExifAttributes.subsec_time), subsecTime, sizeof(subsecTime));
+
+    // conponents configuration.
+    // Default = 4 5 6 0(if RGB uncompressed), 1 2 3 0(other cases)
+    // 0 = does not exist; 1 = Y; 2 = Cb; 3 = Cr; 4 = R; 5 = G; 6 = B; other = reserved
+    mExifAttributes.components_configuration[0] = 1;
+    mExifAttributes.components_configuration[1] = 2;
+    mExifAttributes.components_configuration[2] = 3;
+    mExifAttributes.components_configuration[3] = 0;
+
+    // set default values for fnumber and focal length
+    // (see EXIFMaker::setDriverData() how to override these)
+    mExifAttributes.fnumber.num = EXIF_DEF_FNUMBER_NUM;
+    mExifAttributes.fnumber.den = EXIF_DEF_FNUMBER_DEN;
+    mExifAttributes.focal_length.num = EXIF_DEF_FOCAL_LEN_NUM;
+    mExifAttributes.focal_length.den = EXIF_DEF_FOCAL_LEN_DEN;
+
+    // TODO: should ISO be omitted if the value cannot be trusted?
+    mExifAttributes.iso_speed_rating = DEFAULT_ISO_SPEED;
+
+    mExifAttributes.aperture.den = EXIF_DEF_APEX_DEN;
+    mExifAttributes.aperture.num = EXIF_DEF_APEX_NUM;
+    // max aperture. the smallest F number of the lens. unit is APEX value.
+    mExifAttributes.max_aperture.num = mExifAttributes.aperture.num;
+    mExifAttributes.max_aperture.den = mExifAttributes.aperture.den;
+
+    // subject distance,    0 means distance unknown; (~0) means infinity.
+    mExifAttributes.subject_distance.num = EXIF_DEF_SUBJECT_DISTANCE_UNKNOWN;
+    mExifAttributes.subject_distance.den = 1;
+
+    // light source, 0 means light source unknown
+    mExifAttributes.light_source = 0;
+    // TODO: for awb mode
+
+    // gain control, 0 = none;
+    // 1 = low gain up; 2 = high gain up; 3 = low gain down; 4 = high gain down
+    mExifAttributes.gain_control = 0;
+
+    // contrast, 0 = normal; 1 = soft; 2 = hard; other = reserved
+    mExifAttributes.contrast = EXIF_CONTRAST_NORMAL;
+
+    // saturation, 0 = normal; 1 = Low saturation; 2 = High saturation; other = reserved
+    mExifAttributes.saturation = EXIF_SATURATION_NORMAL;
+
+    // sharpness, 0 = normal; 1 = soft; 2 = hard; other = reserved
+    mExifAttributes.sharpness = EXIF_SHARPNESS_NORMAL;
+
+    // the picture's width and height
+    mExifAttributes.width = width;
+    mExifAttributes.height = height;
+
+    mExifAttributes.orientation = 1;
+
+    mExifAttributes.custom_rendered = EXIF_DEF_CUSTOM_RENDERED;
+
+    // metering mode, 0 = normal; 1 = soft; 2 = hard; other = reserved
+    mExifAttributes.metering_mode = EXIF_METERING_UNKNOWN;
+    mInitialized = true;
+}
+
+void EXIFMaker::initializeLocation(ExifMetaData* metadata) {
+    LOG1("@%s", __func__);
+    // GIS information
+    bool gpsEnabled = false;
+    double latitude = metadata->mGpsSetting.latitude;
+    double longitude = metadata->mGpsSetting.longitude;
+    double altitude = metadata->mGpsSetting.altitude;
+    long timestamp = metadata->mGpsSetting.gpsTimeStamp;
+    char* pprocmethod = metadata->mGpsSetting.gpsProcessingMethod;
+
+    // check whether the GIS Information is valid
+    if (!(latitude >= -EPSILON && latitude <= EPSILON) ||
+        !(longitude >= -EPSILON && longitude <= EPSILON) ||
+        !(altitude >= -EPSILON && altitude <= EPSILON) || (timestamp != 0) ||
+        (strlen(pprocmethod) != 0))
+        gpsEnabled = true;
+
+    mExifAttributes.enableGps = 0;
+    LOG1("EXIF: gpsEnabled: %d", gpsEnabled);
+
+    // the version is given as 2.2.0.0, it is mandatory when GPSInfo tag is present
+    if (gpsEnabled) {
+        const unsigned char gpsversion[4] = {0x02, 0x02, 0x00, 0x00};
+        MEMCPY_S(mExifAttributes.gps_version_id, sizeof(mExifAttributes.gps_version_id), gpsversion,
+                 sizeof(gpsversion));
+    } else {
+        return;
+    }
+
+    // latitude, for example, 39.904214 degrees, N
+    if (latitude > 0)
+        MEMCPY_S(mExifAttributes.gps_latitude_ref, sizeof(mExifAttributes.gps_latitude_ref), "N",
+                 sizeof(mExifAttributes.gps_latitude_ref));
+    else
+        MEMCPY_S(mExifAttributes.gps_latitude_ref, sizeof(mExifAttributes.gps_latitude_ref), "S",
+                 sizeof(mExifAttributes.gps_latitude_ref));
+
+    latitude = fabs(latitude);
+    mExifAttributes.gps_latitude[0].num = (uint32_t)latitude;
+    mExifAttributes.gps_latitude[0].den = 1;
+    mExifAttributes.gps_latitude[1].num =
+        (uint32_t)((latitude - mExifAttributes.gps_latitude[0].num) * 60);
+    mExifAttributes.gps_latitude[1].den = 1;
+    mExifAttributes.gps_latitude[2].num =
+        (uint32_t)(((latitude - mExifAttributes.gps_latitude[0].num) * 60 -
+                    mExifAttributes.gps_latitude[1].num) *
+                   60 * 100);
+    mExifAttributes.gps_latitude[2].den = 100;
+    mExifAttributes.enableGps |= EXIF_GPS_LATITUDE;
+    LOG1("EXIF: latitude, ref:%s, dd:%d, mm:%d, ss:%d", mExifAttributes.gps_latitude_ref,
+         mExifAttributes.gps_latitude[0].num, mExifAttributes.gps_latitude[1].num,
+         mExifAttributes.gps_latitude[2].num);
+
+    // longitude, for example, 116.407413 degrees, E
+    if (longitude > 0)
+        MEMCPY_S(mExifAttributes.gps_longitude_ref, sizeof(mExifAttributes.gps_longitude_ref), "E",
+                 sizeof(mExifAttributes.gps_longitude_ref));
+    else
+        MEMCPY_S(mExifAttributes.gps_longitude_ref, sizeof(mExifAttributes.gps_longitude_ref), "W",
+                 sizeof(mExifAttributes.gps_longitude_ref));
+    longitude = fabs(longitude);
+    mExifAttributes.gps_longitude[0].num = (uint32_t)longitude;
+    mExifAttributes.gps_longitude[0].den = 1;
+    mExifAttributes.gps_longitude[1].num =
+        (uint32_t)((longitude - mExifAttributes.gps_longitude[0].num) * 60);
+    mExifAttributes.gps_longitude[1].den = 1;
+    mExifAttributes.gps_longitude[2].num =
+        (uint32_t)(((longitude - mExifAttributes.gps_longitude[0].num) * 60 -
+                    mExifAttributes.gps_longitude[1].num) *
+                   60 * 100);
+    mExifAttributes.gps_longitude[2].den = 100;
+    mExifAttributes.enableGps |= EXIF_GPS_LONGITUDE;
+    LOG1("EXIF: longitude, ref:%s, dd:%d, mm:%d, ss:%d", mExifAttributes.gps_longitude_ref,
+         mExifAttributes.gps_longitude[0].num, mExifAttributes.gps_longitude[1].num,
+         mExifAttributes.gps_longitude[2].num);
+
+    // altitude
+    // altitude, sea level or above sea level, set it to 0; below sea level, set it to 1
+    mExifAttributes.gps_altitude_ref = ((altitude > 0) ? 0 : 1);
+    altitude = fabs(altitude);
+    mExifAttributes.gps_altitude.num = (uint32_t)altitude;
+    mExifAttributes.gps_altitude.den = 1;
+    mExifAttributes.enableGps |= EXIF_GPS_ALTITUDE;
+    LOG1("EXIF: altitude, ref:%d, height:%d", mExifAttributes.gps_altitude_ref,
+         mExifAttributes.gps_altitude.num);
+
+    // timestamp
+    if (timestamp >= LONG_MAX || timestamp <= LONG_MIN) {
+        timestamp = 0;
+        LOGW("invalid timestamp was provided, defaulting to 0 (i.e. 1970)");
+    }
+    struct tm time;
+    gmtime_r(&timestamp, &time);
+    time.tm_year += 1900;
+    time.tm_mon += 1;
+    mExifAttributes.gps_timestamp[0].num = time.tm_hour;
+    mExifAttributes.gps_timestamp[0].den = 1;
+    mExifAttributes.gps_timestamp[1].num = time.tm_min;
+    mExifAttributes.gps_timestamp[1].den = 1;
+    mExifAttributes.gps_timestamp[2].num = time.tm_sec;
+    mExifAttributes.gps_timestamp[2].den = 1;
+    mExifAttributes.enableGps |= EXIF_GPS_TIMESTAMP;
+
+    snprintf(reinterpret_cast<char*>(mExifAttributes.gps_datestamp),
+             sizeof(mExifAttributes.gps_datestamp), "%04d:%02d:%02d", time.tm_year, time.tm_mon,
+             time.tm_mday);
+
+    LOG1("EXIF: timestamp, year:%d,mon:%d,day:%d,hour:%d,min:%d,sec:%d", time.tm_year, time.tm_mon,
+         time.tm_mday, time.tm_hour, time.tm_min, time.tm_sec);
+
+    // processing method
+    MEMCPY_S(mExifAttributes.gps_processing_method, sizeof(mExifAttributes.gps_processing_method),
+             metadata->mGpsSetting.gpsProcessingMethod,
+             sizeof(metadata->mGpsSetting.gpsProcessingMethod));
+    mExifAttributes.gps_processing_method[sizeof(mExifAttributes.gps_processing_method) - 1] = 0;
+
+    mExifAttributes.enableGps |= EXIF_GPS_PROCMETHOD;
+    LOG1("EXIF: GPS processing method:%s", mExifAttributes.gps_processing_method);
+}
+
+void EXIFMaker::setSensorAeConfig(const Parameters& params) {
+    LOG1("@%s", __func__);
+
+    int64_t expTime = 0;
+    params.getExposureTime(expTime);
+
+    if (expTime > 0) {
+        // EXIF exposure rational value is in seconds and the unit of exposure time in 3A is usecs
+        mExifAttributes.exposure_time.num = expTime;
+        mExifAttributes.exposure_time.den = 1000000;
+        uint32_t tv = APEX_EXPOSURE_TO_SHUTTER(static_cast<double>(expTime) /
+                                               mExifAttributes.exposure_time.den);
+        mExifAttributes.shutter_speed.num = tv * 65536;
+        mExifAttributes.shutter_speed.den = 65536;
+    } else {
+        mExifAttributes.exposure_time.num = 0;
+        mExifAttributes.exposure_time.den = 1;
+        mExifAttributes.shutter_speed.num = 0;
+        mExifAttributes.shutter_speed.den = 1;
+    }
+
+    float stepEv = 1 / 3.0f;
+    icamera::camera_rational_t aeCompensationStep;
+    if (params.getAeCompensationStep(aeCompensationStep) == 0) {
+        stepEv = static_cast<float>(aeCompensationStep.numerator) / aeCompensationStep.denominator;
+    }
+
+    int32_t ev = 0;
+    float evBias = 0;
+    if (params.getAeCompensation(ev) == 0) {
+        evBias = static_cast<float>(ev) * stepEv;
+    }
+
+    // exposure bias. unit is APEX value. -99.99 to 99.99
+    const int evLowerBound = -100, evUpperBound = 100;
+    if (evBias > evLowerBound && evBias < evUpperBound) {
+        mExifAttributes.exposure_bias.num = static_cast<int>(evBias) * 100;
+        mExifAttributes.exposure_bias.den = 100;
+        LOG2("EXIF: Ev = %.2f", evBias);
+    } else {
+        mExifAttributes.exposure_bias.num = 0;
+        mExifAttributes.exposure_bias.den = 100;
+        LOG2("EXIF: Invalid Ev!");
+    }
+
+    int32_t iso;
+    mExifAttributes.iso_speed_rating = DEFAULT_ISO_SPEED;
+    if (params.getSensitivityIso(iso) == 0) {
+        mExifAttributes.iso_speed_rating = iso;
+    }
+
+    LOG2("EXIF: ISO=%d", mExifAttributes.iso_speed_rating);
+    LOG2("EXIF: shutter speed=%u/%u", mExifAttributes.shutter_speed.num,
+         mExifAttributes.shutter_speed.den);
+    LOG2("EXIF: exposure time=%u/%u", mExifAttributes.exposure_time.num,
+         mExifAttributes.exposure_time.den);
+}
+
+/*
+ * more secure attribute copy routine.
+ * \param dst pointer to dst buffer
+ * \param dstSize dst buffer size
+ * \param src pointer to src character buffer
+ * \param srcLength src buffer length in characters, not including null byte
+ */
+void EXIFMaker::copyAttribute(uint8_t* dst, size_t dstSize, const char* src, size_t srcLength) {
+    size_t dstMaxLength = dstSize - 1;            // leave space for null
+    MEMCPY_S(dst, dstMaxLength, src, srcLength);  // copy chars (not null)
+    // add null termination
+    size_t len = std::min(dstMaxLength, srcLength);
+    dst[len] = '\0';
+}
+
+void EXIFMaker::clear() {
+    LOG1("@%s", __func__);
+    // Reset all the attributes
+    CLEAR(mExifAttributes);
+    // Initialize the common values
+    mExifAttributes.enableThumb = false;
+    copyAttribute(mExifAttributes.image_description, sizeof(mExifAttributes.image_description),
+                  EXIF_DEF_IMAGE_DESCRIPTION, strlen(EXIF_DEF_IMAGE_DESCRIPTION));
+
+    copyAttribute(mExifAttributes.maker, sizeof(mExifAttributes.maker), mManufacturerName.c_str(),
+                  strlen(mManufacturerName.c_str()));
+
+    copyAttribute(mExifAttributes.model, sizeof(mExifAttributes.model), mProductName.c_str(),
+                  strlen(mProductName.c_str()));
+
+    copyAttribute(mExifAttributes.software, sizeof(mExifAttributes.software), EXIF_DEF_SOFTWARE,
+                  strlen(EXIF_DEF_SOFTWARE));
+
+    copyAttribute(mExifAttributes.exif_version, sizeof(mExifAttributes.exif_version),
+                  EXIF_DEF_EXIF_VERSION, strlen(EXIF_DEF_EXIF_VERSION));
+
+    copyAttribute(mExifAttributes.flashpix_version, sizeof(mExifAttributes.flashpix_version),
+                  EXIF_DEF_FLASHPIXVERSION, strlen(EXIF_DEF_FLASHPIXVERSION));
+
+    // initially, set default flash
+    mExifAttributes.flash = EXIF_DEF_FLASH;
+
+    // normally it is sRGB, 1 means sRGB. FFFF.H means uncalibrated
+    mExifAttributes.color_space = EXIF_DEF_COLOR_SPACE;
+
+    // the number of pixels per ResolutionUnit in the w or h direction
+    // 72 means the image resolution is unknown
+    mExifAttributes.x_resolution.num = EXIF_DEF_RESOLUTION_NUM;
+    mExifAttributes.x_resolution.den = EXIF_DEF_RESOLUTION_DEN;
+    mExifAttributes.y_resolution.num = mExifAttributes.x_resolution.num;
+    mExifAttributes.y_resolution.den = mExifAttributes.x_resolution.den;
+    // resolution unit, 2 means inch
+    mExifAttributes.resolution_unit = EXIF_DEF_RESOLUTION_UNIT;
+    // when thumbnail uses JPEG compression, this tag 103H's value is set to 6
+    mExifAttributes.compression_scheme = EXIF_DEF_COMPRESSION;
+
+    // the TIFF default is 1 (centered)
+    mExifAttributes.ycbcr_positioning = EXIF_DEF_YCBCR_POSITIONING;
+
+    // Clear the Intel 3A Makernote information
+    mExifAttributes.makerNoteData = mMakernoteSection;
+    mExifAttributes.makerNoteDataSize = 0;
+    mExifAttributes.makernoteToApp2 = ENABLE_APP2_MARKER;
+
+    mInitialized = false;
+}
+
+void EXIFMaker::enableFlash(bool enable, int8_t aeMode, int8_t flashMode) {
+    mExifAttributes.flash = EXIF_DEF_FLASH;
+}
+
+void EXIFMaker::setThumbnail(unsigned char* data, size_t size, int width, int height) {
+    LOG1("@%s: data = %p, size = %zu", __func__, data, size);
+    mExifAttributes.enableThumb = true;
+    mExifAttributes.widthThumb = width;
+    mExifAttributes.heightThumb = height;
+    if (mEncoder.setThumbData(data, size) != EXIF_SUCCESS) {
+        LOGE("Error in setting EXIF thumbnail");
+    }
+}
+
+bool EXIFMaker::isThumbnailSet() const {
+    LOG1("@%s", __func__);
+    return mEncoder.isThumbDataSet();
+}
+
+size_t EXIFMaker::makeExif(unsigned char* data) {
+    LOG1("@%s", __func__);
+    CheckError(!data, 0, "nullptr passed for EXIF. Cannot generate EXIF!");
+
+    if (mEncoder.makeExif(data, &mExifAttributes, &mExifSize) == EXIF_SUCCESS) {
+        LOG1("Generated EXIF (@%p) of size: %zu", data, mExifSize);
+        return mExifSize;
+    }
+    return 0;
+}
+
+void EXIFMaker::setMaker(const char* data) {
+    LOG1("@%s: data = %s", __func__, data);
+
+    snprintf((char*)mExifAttributes.maker, sizeof(mExifAttributes.maker), "%s", data);
+}
+
+void EXIFMaker::setModel(const char* data) {
+    LOG1("@%s: data = %s", __func__, data);
+
+    snprintf((char*)mExifAttributes.model, sizeof(mExifAttributes.model), "%s", data);
+}
+
+void EXIFMaker::setSoftware(const char* data) {
+    LOG1("@%s: data = %s", __func__, data);
+
+    snprintf((char*)mExifAttributes.software, sizeof(mExifAttributes.software), "%s", data);
+}
+
+void EXIFMaker::saveMakernote(const Parameters& params) {
+    unsigned int size = sizeof(unsigned char) * (MAKERNOTE_SECTION1_SIZE + MAKERNOTE_SECTION2_SIZE);
+    if (params.getMakernoteData(mMakernoteSection, &size) == OK) {
+        mExifAttributes.makerNoteDataSize = size;
+    }
+}
+
+void EXIFMaker::updateSensorInfo(const Parameters& params) {
+    float focal = 0.0;
+    params.getFocalLength(focal);
+
+    if (focal < EPSILON) {
+        // Focal length is not supported, set to default value
+        icamera::CameraMetadata meta;
+        icamera::ParameterHelper::copyMetadata(params, &meta);
+
+        icamera_metadata_entry entry = meta.find(CAMERA_LENS_INFO_AVAILABLE_FOCAL_LENGTHS);
+        if (entry.count >= 1) {
+            focal = entry.data.f[0];
+        }
+    }
+
+    LOG2("focal length is %f", focal);
+    mExifAttributes.focal_length.num = focal * mExifAttributes.focal_length.den;
+    float aperture = 0.0;
+    params.getAperture(aperture);
+    mExifAttributes.aperture.num = aperture * mExifAttributes.aperture.den;
+
+    mExifAttributes.fnumber.num = aperture * mExifAttributes.aperture.den;
+    mExifAttributes.fnumber.den = mExifAttributes.aperture.den;
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/jpeg/EXIFMaker.h b/camera/hal/intel/ipu6/src/jpeg/EXIFMaker.h
new file mode 100644
index 000000000000..adad8d072710
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/jpeg/EXIFMaker.h
@@ -0,0 +1,72 @@
+/*
+ * Copyright (C) 2016-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <string>
+
+#include "EXIFMetaData.h"
+#include "ExifCreater.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+/**
+ * \class EXIFMaker
+ *
+ */
+class EXIFMaker {
+ public:
+    EXIFMaker();
+    ~EXIFMaker();
+
+    void readProperty();
+    void initialize(int width, int height);
+    bool isInitialized() { return mInitialized; }
+    void initializeLocation(ExifMetaData* metadata);
+    uint32_t getMakerNoteDataSize() const;
+    void pictureTaken(ExifMetaData* exifmetadata);
+    void enableFlash(bool enable, int8_t aeMode, int8_t flashMode);
+    void setThumbnail(unsigned char* data, size_t size, int width, int height);
+    bool isThumbnailSet() const;
+    size_t makeExif(unsigned char* data);
+    void setMaker(const char* data);
+    void setModel(const char* data);
+    void setSoftware(const char* data);
+    void updateSensorInfo(const Parameters& params);
+    void saveMakernote(const Parameters& params);
+    void setSensorAeConfig(const Parameters& params);
+
+ private:  // member variables
+    ExifCreater mEncoder;
+    exif_attribute_t mExifAttributes;
+    size_t mExifSize;
+    bool mInitialized;
+    unsigned char* mMakernoteSection;
+    std::string mProductName;
+    std::string mManufacturerName;
+
+ private:
+    // prevent copy constructor and assignment operator
+    DISALLOW_COPY_AND_ASSIGN(EXIFMaker);
+
+ private:  // Methods
+    void copyAttribute(uint8_t* dst, size_t dstSize, const char* src, size_t srcLength);
+
+    void clear();
+};
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/jpeg/EXIFMetaData.cpp b/camera/hal/intel/ipu6/src/jpeg/EXIFMetaData.cpp
new file mode 100644
index 000000000000..6c1bd04ffe86
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/jpeg/EXIFMetaData.cpp
@@ -0,0 +1,57 @@
+/*
+ * Copyright (C) 2016-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "EXIFMetaData"
+
+#include "EXIFMetaData.h"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+#define DEFAULT_ISO_SPEED 100
+
+ExifMetaData::ExifMetaData()
+        : effectMode(CAM_EFFECT_NONE),
+          software(nullptr),
+          hdr(false),
+          flashFired(false),
+          v3AeMode(BAD_VALUE),
+          flashMode(BAD_VALUE),
+          saveMirrored(false),
+          cameraOrientation(0),
+          currentOrientation(0),
+          zoomRatio(1),
+          aeMode(AE_MODE_AUTO),
+          awbMode(AWB_MODE_AUTO),
+          currentFocusDistance(0.0) {
+    ALOGI("@%s", __func__);
+    mJpegSetting.jpegQuality = 90;
+    mJpegSetting.jpegThumbnailQuality = 90;
+    mJpegSetting.orientation = 0;
+    mJpegSetting.thumbWidth = 320;
+    mJpegSetting.thumbHeight = 240;
+    mGpsSetting.latitude = 0.0;
+    mGpsSetting.longitude = 0.0;
+    mGpsSetting.altitude = 0.0;
+    CLEAR(mGpsSetting.gpsProcessingMethod);
+    mGpsSetting.gpsTimeStamp = 0;
+}
+
+ExifMetaData::~ExifMetaData() {}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/jpeg/EXIFMetaData.h b/camera/hal/intel/ipu6/src/jpeg/EXIFMetaData.h
new file mode 100644
index 000000000000..7cd37f319ebc
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/jpeg/EXIFMetaData.h
@@ -0,0 +1,68 @@
+/*
+ * Copyright (C) 2016-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "Parameters.h"
+#include "iutils/Errors.h"
+namespace icamera {
+
+#define MAX_NUM_GPS_PROCESSING_METHOD 64
+
+/**
+ * \class ExifMetaData
+ *
+ */
+class ExifMetaData {
+ public:
+    ExifMetaData();
+    virtual ~ExifMetaData();
+
+    // jpeg info
+    struct JpegSetting {
+        uint8_t jpegQuality;
+        uint8_t jpegThumbnailQuality;
+        int thumbWidth;
+        int thumbHeight;
+        int orientation;
+    };
+    // GPS info
+    struct GpsSetting {
+        double latitude;
+        double longitude;
+        double altitude;
+        char gpsProcessingMethod[MAX_NUM_GPS_PROCESSING_METHOD];
+        long gpsTimeStamp;
+    };
+    // exif info
+    JpegSetting mJpegSetting;
+    GpsSetting mGpsSetting;
+    camera_effect_mode_t effectMode;
+    char* software;         /*!< software string from HAL */
+    bool hdr;               /*!< whether hdr was used */
+    bool flashFired;        /*!< whether flash was fired */
+    int8_t v3AeMode;        /*!< v3 ae mode (e.g. for flash) */
+    int8_t flashMode;       /*!< flash mode (e.g. TORCH,SINGLE,OFF) */
+    bool saveMirrored;      /*!< whether to do mirroring */
+    int cameraOrientation;  /*!< camera sensor orientation */
+    int currentOrientation; /*!< Current orientation of the device */
+    int zoomRatio;
+    icamera::camera_ae_mode_t aeMode;
+    icamera::camera_awb_mode_t awbMode;
+    float currentFocusDistance;
+};
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/jpeg/Exif.h b/camera/hal/intel/ipu6/src/jpeg/Exif.h
new file mode 100644
index 000000000000..4ccca42545c1
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/jpeg/Exif.h
@@ -0,0 +1,343 @@
+/*
+ * Copyright Samsung Electronics Co.,LTD.
+ * Copyright (C) 2010 The Android Open Source Project
+ * Copyright (C) 2016-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <math.h>
+
+#define EXIF_LOG2(x) (log((double)(x)) / log(2.0))
+#define APEX_FNUM_TO_APERTURE(x) (2 * (EXIF_LOG2((double)(x))))
+#define APEX_EXPOSURE_TO_SHUTTER(x) (-1.0 * (EXIF_LOG2((double)(x))))
+#define APEX_ISO_TO_FILMSENSITIVITY(x) ((int)(EXIF_LOG2((x) / 3.125) + 0.5))
+
+#define NUM_SIZE 2
+#define IFD_SIZE 12
+#define OFFSET_SIZE 4
+
+#define NUM_0TH_IFD_TIFF 14
+#define NUM_0TH_IFD_EXIF 36
+#define NUM_0TH_IFD_GPS 12
+#define NUM_1TH_IFD_TIFF 9
+// For QVGA: 320 * 240 * 1.5
+#define EXIF_SIZE_LIMITATION 0x10000
+// Limite the thumbnail size to 32k, to make sure the whole exif size does
+// not exceed the exif size limitation. We guess the total size of all the
+// other fields is smaller than 32k. (Currently the size is about 26k.)
+#define THUMBNAIL_SIZE_LIMITATION 0x8000
+
+/* Type */
+#define EXIF_TYPE_BYTE 1
+#define EXIF_TYPE_ASCII 2
+#define EXIF_TYPE_SHORT 3
+#define EXIF_TYPE_LONG 4
+#define EXIF_TYPE_RATIONAL 5
+#define EXIF_TYPE_UNDEFINED 7
+#define EXIF_TYPE_SLONG 9
+#define EXIF_TYPE_SRATIONAL 10
+
+#define EXIF_FILE_SIZE 28800
+
+/* 0th IFD TIFF Tags */
+#define EXIF_TAG_IMAGE_WIDTH 0x0100
+#define EXIF_TAG_IMAGE_HEIGHT 0x0101
+#define EXIF_TAG_IMAGE_DESCRIPTION 0x010e
+#define EXIF_TAG_MAKE 0x010f
+#define EXIF_TAG_MODEL 0x0110
+#define EXIF_TAG_ORIENTATION 0x0112
+#define EXIF_TAG_X_RESOLUTION 0x011A
+#define EXIF_TAG_Y_RESOLUTION 0x011B
+#define EXIF_TAG_RESOLUTION_UNIT 0x0128
+#define EXIF_TAG_SOFTWARE 0x0131
+#define EXIF_TAG_DATE_TIME 0x0132
+#define EXIF_TAG_YCBCR_POSITIONING 0x0213
+#define EXIF_TAG_EXIF_IFD_POINTER 0x8769
+#define EXIF_TAG_GPS_IFD_POINTER 0x8825
+
+/* 0th IFD Exif Private Tags */
+#define EXIF_TAG_EXPOSURE_TIME 0x829A
+#define EXIF_TAG_FNUMBER 0x829D
+#define EXIF_TAG_EXPOSURE_PROGRAM 0x8822
+#define EXIF_TAG_ISO_SPEED_RATING 0x8827
+#define EXIF_TAG_EXIF_VERSION 0x9000
+#define EXIF_TAG_DATE_TIME_ORG 0x9003
+#define EXIF_TAG_DATE_TIME_DIGITIZE 0x9004
+#define EXIF_TAG_COMPONENTS_CONFIGURATION 0x9101
+#define EXIF_TAG_SHUTTER_SPEED 0x9201
+#define EXIF_TAG_APERTURE 0x9202
+#define EXIF_TAG_BRIGHTNESS 0x9203
+#define EXIF_TAG_EXPOSURE_BIAS 0x9204
+#define EXIF_TAG_MAX_APERTURE 0x9205
+#define EXIF_TAG_SUBJECT_DISTANCE 0x9206
+#define EXIF_TAG_METERING_MODE 0x9207
+#define EXIF_TAG_LIGHT_SOURCE 0x9208
+#define EXIF_TAG_FLASH 0x9209
+#define EXIF_TAG_FOCAL_LENGTH 0x920A
+#define EXIF_TAG_MAKER_NOTE 0x927C
+#define EXIF_TAG_USER_COMMENT 0x9286
+#define EXIF_TAG_SUBSEC_TIME 0x9290
+#define EXIF_TAG_SUBSEC_TIME_ORIG 0x9291
+#define EXIF_TAG_SUBSEC_TIME_DIG 0x9292
+#define EXIF_TAG_FLASH_PIX_VERSION 0xA000
+#define EXIF_TAG_COLOR_SPACE 0xA001
+#define EXIF_TAG_PIXEL_X_DIMENSION 0xA002
+#define EXIF_TAG_PIXEL_Y_DIMENSION 0xA003
+#define EXIF_TAG_CUSTOM_RENDERED 0xA401
+#define EXIF_TAG_EXPOSURE_MODE 0xA402
+#define EXIF_TAG_WHITE_BALANCE 0xA403
+#define EXIF_TAG_JPEG_ZOOM_RATIO 0XA404
+#define EXIF_TAG_SCENCE_CAPTURE_TYPE 0xA406
+#define EXIF_TAG_GAIN_CONTROL 0xA407
+#define EXIF_TAG_CONTRAST 0xA408
+#define EXIF_TAG_SATURATION 0xA409
+#define EXIF_TAG_SHARPNESS 0xA40A
+
+/* 0th IFD GPS Info Tags */
+#define EXIF_TAG_GPS_VERSION_ID 0x0000
+#define EXIF_TAG_GPS_LATITUDE_REF 0x0001
+#define EXIF_TAG_GPS_LATITUDE 0x0002
+#define EXIF_TAG_GPS_LONGITUDE_REF 0x0003
+#define EXIF_TAG_GPS_LONGITUDE 0x0004
+#define EXIF_TAG_GPS_ALTITUDE_REF 0x0005
+#define EXIF_TAG_GPS_ALTITUDE 0x0006
+#define EXIF_TAG_GPS_TIMESTAMP 0x0007
+#define EXIF_TAG_GPS_IMG_DIRECTION_REF 0x0010
+#define EXIF_TAG_GPS_IMG_DIRECTION 0x0011
+#define EXIF_TAG_GPS_PROCESSING_METHOD 0x001B
+#define EXIF_TAG_GPS_DATESTAMP 0x001D
+
+/* 1th IFD TIFF Tags */
+#define EXIF_TAG_COMPRESSION_SCHEME 0x0103
+/*
+#define EXIF_TAG_X_RESOLUTION                   0x011A
+#define EXIF_TAG_Y_RESOLUTION                   0x011B
+#define EXIF_TAG_RESOLUTION_UNIT                0x0128
+*/
+#define EXIF_TAG_JPEG_INTERCHANGE_FORMAT 0x0201
+#define EXIF_TAG_JPEG_INTERCHANGE_FORMAT_LEN 0x0202
+
+typedef enum {
+    EXIF_ORIENTATION_UP = 1,
+    EXIF_ORIENTATION_90 = 6,
+    EXIF_ORIENTATION_180 = 3,
+    EXIF_ORIENTATION_270 = 8,
+} ExifOrientationType;
+
+typedef enum {
+    EXIF_SCENE_STANDARD,
+    EXIF_SCENE_LANDSCAPE,
+    EXIF_SCENE_PORTRAIT,
+    EXIF_SCENE_NIGHT,
+} CamExifSceneCaptureType;
+
+typedef enum {
+    EXIF_METERING_UNKNOWN,
+    EXIF_METERING_AVERAGE,
+    EXIF_METERING_CENTER,
+    EXIF_METERING_SPOT,
+    EXIF_METERING_MULTISPOT,
+    EXIF_METERING_PATTERN,
+    EXIF_METERING_PARTIAL,
+    EXIF_METERING_OTHER = 255,
+} CamExifMeteringModeType;
+
+typedef enum {
+    EXIF_EXPOSURE_AUTO,
+    EXIF_EXPOSURE_MANUAL,
+    EXIF_EXPOSURE_AUTO_BRACKET,
+} CamExifExposureModeType;
+
+typedef enum {
+    EXIF_WB_AUTO,
+    EXIF_WB_MANUAL,
+} CamExifWhiteBalanceType;
+
+typedef enum {
+    EXIF_LIGHT_SOURCE_UNKNOWN,
+    EXIF_LIGHT_SOURCE_DAYLIGHT,
+    EXIF_LIGHT_SOURCE_FLUORESCENT,
+    EXIF_LIGHT_SOURCE_TUNGSTEN,
+    EXIF_LIGHT_SOURCE_FLASH,
+    EXIF_LIGHT_SOURCE_FINE_WEATHER = 9,
+    EXIF_LIGHT_SOURCE_CLOUDY_WEATHER,
+    EXIF_LIGHT_SOURCE_SHADE,
+    EXIF_LIGHT_SOURCE_DAYLIGHT_FLUORESCENT,
+    EXIF_LIGHT_SOURCE_DAY_WHITE_FLUORESCENT,
+    EXIF_LIGHT_SOURCE_COOL_WHITE_FLUORESCENT,
+    EXIF_LIGHT_SOURCE_WHITE_FLUORESCENT,
+    EXIF_LIGHT_SOURCE_WARM_WHITE_FLUORESCENT,  // value 16 is used in EXIF V2.3, not for EXIF V2.2
+    EXIF_LIGHT_SOURCE_STANDARD_LIGHT_A,
+    EXIF_LIGHT_SOURCE_STANDARD_LIGHT_B,
+    EXIF_LIGHT_SOURCE_STANDARD_LIGHT_C,
+    EXIF_LIGHT_SOURCE_D55,
+    EXIF_LIGHT_SOURCE_D65,
+    EXIF_LIGHT_SOURCE_D75,
+    EXIF_LIGHT_SOURCE_D50,
+    EXIF_LIGHT_SOURCE_ISO_STUDIO_TUNGSTEN,
+    EXIF_LIGHT_SOURCE_OTHER_LIGHT_SOURCE = 255,
+} CamExifLightSourceType;
+
+typedef enum {
+    EXIF_EXPOSURE_PROGRAM_MANUAL = 1,
+    EXIF_EXPOSURE_PROGRAM_NORMAL = 2,
+    EXIF_EXPOSURE_PROGRAM_APERTURE_PRIORITY = 3,
+    EXIF_EXPOSURE_PROGRAM_SHUTTER_PRIORITY = 4
+} CamExifExposureProgramType;
+
+typedef enum {
+    EXIF_CONTRAST_NORMAL = 0,
+    EXIF_CONTRAST_SOFT = 1,
+    EXIF_CONTRAST_HARD = 2,
+} CamExifContrastType;
+
+typedef enum {
+    EXIF_SATURATION_NORMAL = 0,
+    EXIF_SATURATION_LOW = 1,
+    EXIF_SATURATION_HIGH = 2,
+} CamExifSaturationType;
+
+typedef enum {
+    EXIF_SHARPNESS_NORMAL = 0,
+    EXIF_SHARPNESS_SOFT = 1,
+    EXIF_SHARPNESS_HARD = 2,
+} CamExifSharpnessType;
+
+/* define the flag of enable gps info */
+const uint8_t EXIF_GPS_LATITUDE = 0x01;
+const uint8_t EXIF_GPS_LONGITUDE = 0x02;
+const uint8_t EXIF_GPS_ALTITUDE = 0x04;
+const uint8_t EXIF_GPS_TIMESTAMP = 0x08;
+const uint8_t EXIF_GPS_PROCMETHOD = 0x10;
+const uint8_t EXIF_GPS_IMG_DIRECTION = 0x20;
+
+/* Values */
+#define EXIF_DEF_IMAGE_DESCRIPTION "Jpeg"
+#define EXIF_DEF_SOFTWARE "Android"
+#define EXIF_DEF_EXIF_VERSION "0220"
+#define EXIF_DEF_USERCOMMENTS "  "
+#define EXIF_DEF_FLASHPIXVERSION "0100" /* Flashpix Format Version 1.0 */
+
+#define EXIF_DEF_YCBCR_POSITIONING 1 /* centered */
+#define EXIF_DEF_FNUMBER_NUM 26      /* 2.6 */
+#define EXIF_DEF_FNUMBER_DEN 10
+#define EXIF_DEF_EXPOSURE_PROGRAM 3 /* aperture priority */
+#define EXIF_DEF_FOCAL_LEN_NUM 278  /* 2.78mm */
+#define EXIF_DEF_FOCAL_LEN_DEN 100
+#define EXIF_DEF_FLASH 0                 /* O: off, 1: on*/
+#define EXIF_FLASH_FORCED_ON 1 << 3;     /* mode description */
+#define EXIF_FLASH_FORCED_OFF 1 << 4;    /* mode description */
+#define EXIF_FLASH_AUTO 1 << 3 | 1 << 4; /* mode description */
+#define EXIF_FLASH_ON 1                  /* O: off, 1: on - fired or not*/
+#define EXIF_DEF_COLOR_SPACE 1
+#define EXIF_DEF_CUSTOM_RENDERED 0
+#define EXIF_CUSTOM_RENDERED_HDR 1
+#define EXIF_DEF_EXPOSURE_MODE EXIF_EXPOSURE_AUTO
+#define EXIF_DEF_APEX_DEN 10
+#define EXIF_DEF_APEX_NUM 25
+#define EXIF_DEF_SUBJECT_DISTANCE_UNKNOWN 0
+
+#define EXIF_DEF_COMPRESSION 6
+#define EXIF_DEF_RESOLUTION_NUM 72
+#define EXIF_DEF_RESOLUTION_DEN 1
+#define EXIF_DEF_RESOLUTION_UNIT 2 /* inches */
+
+typedef struct {
+    uint32_t num;
+    uint32_t den;
+} rational_t;
+
+typedef struct {
+    int32_t num;
+    int32_t den;
+} srational_t;
+
+typedef struct {
+    bool enableThumb;
+
+    uint8_t image_description[32];
+    uint8_t flashpix_version[4];
+    uint8_t components_configuration[4];
+    uint8_t maker[32];
+    uint8_t model[32];
+    uint8_t software[32];
+    uint8_t exif_version[4];
+    uint8_t date_time[20];
+    uint8_t subsec_time[8];
+    uint8_t user_comment[150];
+
+    uint32_t width;
+    uint32_t height;
+    uint32_t widthThumb;
+    uint32_t heightThumb;
+
+    uint16_t orientation;
+    uint16_t ycbcr_positioning;
+    uint16_t exposure_program;
+    uint16_t iso_speed_rating;
+    uint16_t metering_mode;
+    uint16_t flash;
+    uint16_t color_space;
+    uint16_t custom_rendered;
+    uint16_t exposure_mode;
+    uint16_t white_balance;
+    rational_t zoom_ratio;
+    uint16_t scene_capture_type;
+    uint16_t light_source;
+    uint16_t gain_control;
+    uint16_t contrast;
+    uint16_t saturation;
+    uint16_t sharpness;
+
+    rational_t exposure_time;
+    rational_t fnumber;
+    rational_t aperture;
+    rational_t max_aperture;
+    rational_t focal_length;
+    rational_t subject_distance;
+
+    srational_t shutter_speed;
+    srational_t brightness;
+    srational_t exposure_bias;
+
+    // bit 0~4 indicate whether Gps items latitude, longitude, altitude, timestamp,
+    // datastamp exist or not.
+    uint8_t enableGps;
+    uint8_t gps_latitude_ref[2];
+    uint8_t gps_longitude_ref[2];
+
+    uint8_t gps_version_id[4];
+    uint8_t gps_altitude_ref;
+
+    rational_t gps_latitude[3];
+    rational_t gps_longitude[3];
+    rational_t gps_altitude;
+    rational_t gps_timestamp[3];
+    uint8_t gps_datestamp[11];
+    uint8_t gps_processing_method[100];
+
+    uint8_t gps_img_direction_ref[2];
+    rational_t gps_img_direction;
+
+    rational_t x_resolution;
+    rational_t y_resolution;
+    uint16_t resolution_unit;
+    uint16_t compression_scheme;
+
+    uint16_t makerNoteDataSize;
+    unsigned char* makerNoteData;
+    bool makernoteToApp2;
+} exif_attribute_t;
diff --git a/camera/hal/intel/ipu6/src/jpeg/ExifCreater.cpp b/camera/hal/intel/ipu6/src/jpeg/ExifCreater.cpp
new file mode 100644
index 000000000000..34f885ae48f8
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/jpeg/ExifCreater.cpp
@@ -0,0 +1,542 @@
+/*
+ * Copyright Samsung Electronics Co.,LTD.
+ * Copyright (C) 2010 The Android Open Source Project
+ * Copyright (C) 2016-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ *
+ * JPEG DRIVER MODULE (ExifCreater.cpp)
+ * Author  : ge.lee       -- initial version
+ * Date    : 03 June 2010
+ * Purpose : This file implements the JPEG encoder APIs as needed by Camera HAL
+ */
+
+#define LOG_TAG "ExifCreater"
+
+#include "ExifCreater.h"
+
+#include <fcntl.h>
+#include <sys/mman.h>
+
+#include "iutils/CameraLog.h"
+
+static const char ExifAsciiPrefix[] = {0x41, 0x53, 0x43, 0x49, 0x49, 0x0, 0x0, 0x0};
+
+namespace icamera {
+
+ExifCreater::ExifCreater() {
+    m_thumbBuf = nullptr;
+    m_thumbSize = 0;
+}
+
+ExifCreater::~ExifCreater() {}
+
+exif_status ExifCreater::setThumbData(const void* thumbBuf, unsigned int thumbSize) {
+    // TODO: Maybe we should take into account the rest of the EXIF data as well here,
+    // not just the thumbnail size.
+    if (thumbSize >= EXIF_SIZE_LIMITATION) {
+        LOGE("ERROR: Too big thumb size %d (limit: %d)", thumbSize, EXIF_SIZE_LIMITATION);
+        m_thumbBuf = nullptr;
+        m_thumbSize = 0;
+        return EXIF_FAIL;
+    }
+
+    m_thumbBuf = static_cast<unsigned char*>(const_cast<void*>(thumbBuf));
+    m_thumbSize = thumbSize;
+    return EXIF_SUCCESS;
+}
+
+bool ExifCreater::isThumbDataSet() const {
+    return m_thumbBuf != nullptr;
+}
+
+// if exif tags size + thumbnail size is > 64K, it will disable thumbnail
+exif_status ExifCreater::makeExif(void* exifOut, exif_attribute_t* exifInfo, size_t* size) {
+    LOG1("makeExif start");
+
+    unsigned char *pCur, *pApp1Start, *pIfdStart, *pGpsIfdPtr, *pNextIfdOffset;
+    unsigned int tmp, LongerTagOffset = 0, LongerTagOffsetWithoutThumbnail;
+    pApp1Start = pCur = static_cast<unsigned char*>(exifOut);
+
+    // 2 Exif Identifier Code & TIFF Header
+    pCur += 4;  // Skip 4 Byte for APP1 marker and length
+    unsigned char ExifIdentifierCode[6] = {0x45, 0x78, 0x69, 0x66, 0x00, 0x00};
+    MEMCPY_S(pCur, sizeof(ExifIdentifierCode), ExifIdentifierCode, sizeof(ExifIdentifierCode));
+    pCur += 6;
+
+    /* Byte Order - little endian, Offset of IFD - 0x00000008.H */
+    unsigned char TiffHeader[8] = {0x49, 0x49, 0x2A, 0x00, 0x08, 0x00, 0x00, 0x00};
+    MEMCPY_S(pCur, sizeof(TiffHeader), TiffHeader, sizeof(TiffHeader));
+    pIfdStart = pCur;
+    pCur += 8;
+
+    // 2 0th IFD TIFF Tags
+    if (exifInfo->enableGps)
+        tmp = NUM_0TH_IFD_TIFF;
+    else
+        tmp = NUM_0TH_IFD_TIFF - 1;
+
+    MEMCPY_S(pCur, NUM_SIZE, (int8_t*)&tmp, NUM_SIZE);
+    pCur += NUM_SIZE;
+
+    LongerTagOffset += 8 + NUM_SIZE + tmp * IFD_SIZE + OFFSET_SIZE;
+
+    writeExifIfd(&pCur, EXIF_TAG_IMAGE_WIDTH, EXIF_TYPE_LONG, 1, exifInfo->width);
+    writeExifIfd(&pCur, EXIF_TAG_IMAGE_HEIGHT, EXIF_TYPE_LONG, 1, exifInfo->height);
+    writeExifIfd(&pCur, EXIF_TAG_IMAGE_DESCRIPTION, EXIF_TYPE_ASCII,
+                 strlen(reinterpret_cast<char*>(exifInfo->image_description)) + 1,
+                 exifInfo->image_description, &LongerTagOffset, pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_MAKE, EXIF_TYPE_ASCII, strlen((char*)exifInfo->maker) + 1,
+                 exifInfo->maker, &LongerTagOffset, pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_MODEL, EXIF_TYPE_ASCII, strlen((char*)exifInfo->model) + 1,
+                 exifInfo->model, &LongerTagOffset, pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_ORIENTATION, EXIF_TYPE_SHORT, 1, exifInfo->orientation);
+    writeExifIfd(&pCur, EXIF_TAG_X_RESOLUTION, EXIF_TYPE_RATIONAL, 1, &exifInfo->x_resolution,
+                 &LongerTagOffset, pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_Y_RESOLUTION, EXIF_TYPE_RATIONAL, 1, &exifInfo->y_resolution,
+                 &LongerTagOffset, pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_RESOLUTION_UNIT, EXIF_TYPE_SHORT, 1, exifInfo->resolution_unit);
+    writeExifIfd(&pCur, EXIF_TAG_SOFTWARE, EXIF_TYPE_ASCII, strlen((char*)exifInfo->software) + 1,
+                 exifInfo->software, &LongerTagOffset, pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_DATE_TIME, EXIF_TYPE_ASCII, 20, exifInfo->date_time,
+                 &LongerTagOffset, pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_YCBCR_POSITIONING, EXIF_TYPE_SHORT, 1,
+                 exifInfo->ycbcr_positioning);
+    writeExifIfd(&pCur, EXIF_TAG_EXIF_IFD_POINTER, EXIF_TYPE_LONG, 1, LongerTagOffset);
+
+    pGpsIfdPtr = pCur;
+    if (exifInfo->enableGps) {
+        pCur += IFD_SIZE;  // Skip a ifd size for gps IFD pointer
+    }
+
+    pNextIfdOffset = pCur;  // Skip a offset size for next IFD offset
+    pCur += OFFSET_SIZE;
+
+    // 2 0th IFD Exif Private Tags
+    pCur = pIfdStart + LongerTagOffset;
+
+    int drop_num = 0;
+    if (exifInfo->exposure_time.den == 0) drop_num++;
+    if (exifInfo->shutter_speed.den == 0) drop_num++;
+    if (exifInfo->makerNoteDataSize == 0 || exifInfo->makernoteToApp2) {
+        // skip the makernote IFD in APP1, when we don't have any,
+        // or if we want it to APP2
+        drop_num++;
+    }
+    tmp = NUM_0TH_IFD_EXIF - drop_num;
+    MEMCPY_S(pCur, NUM_SIZE, &tmp, NUM_SIZE);
+    pCur += NUM_SIZE;
+
+    LongerTagOffset += NUM_SIZE + tmp * IFD_SIZE + OFFSET_SIZE;
+    if (exifInfo->exposure_time.den != 0) {
+        writeExifIfd(&pCur, EXIF_TAG_EXPOSURE_TIME, EXIF_TYPE_RATIONAL, 1, &exifInfo->exposure_time,
+                     &LongerTagOffset, pIfdStart);
+    }
+    writeExifIfd(&pCur, EXIF_TAG_FNUMBER, EXIF_TYPE_RATIONAL, 1, &exifInfo->fnumber,
+                 &LongerTagOffset, pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_EXPOSURE_PROGRAM, EXIF_TYPE_SHORT, 1, exifInfo->exposure_program);
+    writeExifIfd(&pCur, EXIF_TAG_ISO_SPEED_RATING, EXIF_TYPE_SHORT, 1, exifInfo->iso_speed_rating);
+    writeExifIfd(&pCur, EXIF_TAG_EXIF_VERSION, EXIF_TYPE_UNDEFINED, 4, exifInfo->exif_version);
+    writeExifIfd(&pCur, EXIF_TAG_DATE_TIME_ORG, EXIF_TYPE_ASCII, 20, exifInfo->date_time,
+                 &LongerTagOffset, pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_DATE_TIME_DIGITIZE, EXIF_TYPE_ASCII, 20, exifInfo->date_time,
+                 &LongerTagOffset, pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_COMPONENTS_CONFIGURATION, EXIF_TYPE_UNDEFINED, 4,
+                 exifInfo->components_configuration);
+    if (exifInfo->shutter_speed.den != 0) {
+        writeExifIfd(&pCur, EXIF_TAG_SHUTTER_SPEED, EXIF_TYPE_SRATIONAL, 1,
+                     reinterpret_cast<rational_t*>(&exifInfo->shutter_speed), &LongerTagOffset,
+                     pIfdStart);
+    }
+    writeExifIfd(&pCur, EXIF_TAG_APERTURE, EXIF_TYPE_RATIONAL, 1, &exifInfo->aperture,
+                 &LongerTagOffset, pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_BRIGHTNESS, EXIF_TYPE_SRATIONAL, 1,
+                 reinterpret_cast<rational_t*>(&exifInfo->brightness), &LongerTagOffset, pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_EXPOSURE_BIAS, EXIF_TYPE_SRATIONAL, 1,
+                 reinterpret_cast<rational_t*>(&exifInfo->exposure_bias), &LongerTagOffset,
+                 pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_MAX_APERTURE, EXIF_TYPE_RATIONAL, 1, &exifInfo->max_aperture,
+                 &LongerTagOffset, pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_SUBJECT_DISTANCE, EXIF_TYPE_RATIONAL, 1,
+                 &exifInfo->subject_distance, &LongerTagOffset, pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_METERING_MODE, EXIF_TYPE_SHORT, 1, exifInfo->metering_mode);
+    writeExifIfd(&pCur, EXIF_TAG_LIGHT_SOURCE, EXIF_TYPE_SHORT, 1, exifInfo->light_source);
+    writeExifIfd(&pCur, EXIF_TAG_FLASH, EXIF_TYPE_SHORT, 1, exifInfo->flash);
+    writeExifIfd(&pCur, EXIF_TAG_FOCAL_LENGTH, EXIF_TYPE_RATIONAL, 1, &exifInfo->focal_length,
+                 &LongerTagOffset, pIfdStart);
+    char code[8] = {0x41, 0x53, 0x43, 0x49, 0x49, 0x00, 0x00, 0x00};
+    size_t commentsLen = strlen((char*)exifInfo->user_comment) + 1;
+    if (commentsLen > (sizeof(exifInfo->user_comment) - sizeof(code))) return EXIF_FAIL;
+    memmove(exifInfo->user_comment + sizeof(code), exifInfo->user_comment, commentsLen);
+    MEMCPY_S(exifInfo->user_comment, sizeof(exifInfo->user_comment), code, sizeof(code));
+    writeExifIfd(&pCur, EXIF_TAG_USER_COMMENT, EXIF_TYPE_UNDEFINED, commentsLen + sizeof(code),
+                 exifInfo->user_comment, &LongerTagOffset, pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_SUBSEC_TIME, EXIF_TYPE_ASCII,
+                 strlen((char*)exifInfo->subsec_time) + 1, exifInfo->subsec_time, &LongerTagOffset,
+                 pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_SUBSEC_TIME_ORIG, EXIF_TYPE_ASCII,
+                 strlen((char*)exifInfo->subsec_time) + 1, exifInfo->subsec_time, &LongerTagOffset,
+                 pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_SUBSEC_TIME_DIG, EXIF_TYPE_ASCII,
+                 strlen((char*)exifInfo->subsec_time) + 1, exifInfo->subsec_time, &LongerTagOffset,
+                 pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_FLASH_PIX_VERSION, EXIF_TYPE_UNDEFINED, 4,
+                 exifInfo->flashpix_version);
+    writeExifIfd(&pCur, EXIF_TAG_COLOR_SPACE, EXIF_TYPE_SHORT, 1, exifInfo->color_space);
+    writeExifIfd(&pCur, EXIF_TAG_PIXEL_X_DIMENSION, EXIF_TYPE_LONG, 1, exifInfo->width);
+    writeExifIfd(&pCur, EXIF_TAG_PIXEL_Y_DIMENSION, EXIF_TYPE_LONG, 1, exifInfo->height);
+    writeExifIfd(&pCur, EXIF_TAG_CUSTOM_RENDERED, EXIF_TYPE_SHORT, 1, exifInfo->custom_rendered);
+    writeExifIfd(&pCur, EXIF_TAG_EXPOSURE_MODE, EXIF_TYPE_SHORT, 1, exifInfo->exposure_mode);
+    writeExifIfd(&pCur, EXIF_TAG_WHITE_BALANCE, EXIF_TYPE_SHORT, 1, exifInfo->white_balance);
+    writeExifIfd(&pCur, EXIF_TAG_JPEG_ZOOM_RATIO, EXIF_TYPE_RATIONAL, 1, &exifInfo->zoom_ratio,
+                 &LongerTagOffset, pIfdStart);
+    writeExifIfd(&pCur, EXIF_TAG_SCENCE_CAPTURE_TYPE, EXIF_TYPE_SHORT, 1,
+                 exifInfo->scene_capture_type);
+    writeExifIfd(&pCur, EXIF_TAG_GAIN_CONTROL, EXIF_TYPE_SHORT, 1, exifInfo->gain_control);
+    writeExifIfd(&pCur, EXIF_TAG_CONTRAST, EXIF_TYPE_SHORT, 1, exifInfo->contrast);
+    writeExifIfd(&pCur, EXIF_TAG_SATURATION, EXIF_TYPE_SHORT, 1, exifInfo->saturation);
+    writeExifIfd(&pCur, EXIF_TAG_SHARPNESS, EXIF_TYPE_SHORT, 1, exifInfo->sharpness);
+
+    // Save MakerNote data to APP1, unless we want it APP2
+    if (exifInfo->makerNoteDataSize > 0 && !exifInfo->makernoteToApp2) {
+        writeExifIfd(&pCur, EXIF_TAG_MAKER_NOTE, EXIF_TYPE_UNDEFINED, exifInfo->makerNoteDataSize,
+                     (unsigned char*)exifInfo->makerNoteData, &LongerTagOffset, pIfdStart);
+    }
+
+    tmp = 0;
+    MEMCPY_S(pCur, OFFSET_SIZE, (int8_t*)&tmp, OFFSET_SIZE);  // next IFD offset
+    pCur += OFFSET_SIZE;
+
+    // 2 0th IFD GPS Info Tags
+    if (exifInfo->enableGps) {
+        writeExifIfd(&pGpsIfdPtr, EXIF_TAG_GPS_IFD_POINTER, EXIF_TYPE_LONG, 1,
+                     LongerTagOffset);  // GPS IFD pointer skipped on 0th IFD
+
+        pCur = pIfdStart + LongerTagOffset;
+
+        tmp = NUM_0TH_IFD_GPS;
+        if ((exifInfo->enableGps & EXIF_GPS_LATITUDE) == 0) tmp -= 2;
+        if ((exifInfo->enableGps & EXIF_GPS_LONGITUDE) == 0) tmp -= 2;
+        if ((exifInfo->enableGps & EXIF_GPS_ALTITUDE) == 0) tmp -= 2;
+        if ((exifInfo->enableGps & EXIF_GPS_TIMESTAMP) == 0) tmp -= 1;
+        if ((exifInfo->enableGps & EXIF_GPS_PROCMETHOD) == 0) tmp -= 1;
+        if ((exifInfo->enableGps & EXIF_GPS_IMG_DIRECTION) == 0) tmp -= 2;
+
+        MEMCPY_S(pCur, NUM_SIZE, (int8_t*)&tmp, NUM_SIZE);
+        pCur += NUM_SIZE;
+
+        LongerTagOffset += NUM_SIZE + tmp * IFD_SIZE + OFFSET_SIZE;
+
+        writeExifIfd(&pCur, EXIF_TAG_GPS_VERSION_ID, EXIF_TYPE_BYTE, 4, exifInfo->gps_version_id);
+        if (exifInfo->enableGps & EXIF_GPS_LATITUDE) {
+            writeExifIfd(&pCur, EXIF_TAG_GPS_LATITUDE_REF, EXIF_TYPE_ASCII, 2,
+                         exifInfo->gps_latitude_ref);
+            writeExifIfd(&pCur, EXIF_TAG_GPS_LATITUDE, EXIF_TYPE_RATIONAL, 3,
+                         exifInfo->gps_latitude, &LongerTagOffset, pIfdStart);
+        }
+
+        if (exifInfo->enableGps & EXIF_GPS_LONGITUDE) {
+            writeExifIfd(&pCur, EXIF_TAG_GPS_LONGITUDE_REF, EXIF_TYPE_ASCII, 2,
+                         exifInfo->gps_longitude_ref);
+            writeExifIfd(&pCur, EXIF_TAG_GPS_LONGITUDE, EXIF_TYPE_RATIONAL, 3,
+                         exifInfo->gps_longitude, &LongerTagOffset, pIfdStart);
+        }
+
+        if (exifInfo->enableGps & EXIF_GPS_ALTITUDE) {
+            writeExifIfd(&pCur, EXIF_TAG_GPS_ALTITUDE_REF, EXIF_TYPE_BYTE, 1,
+                         exifInfo->gps_altitude_ref);
+            writeExifIfd(&pCur, EXIF_TAG_GPS_ALTITUDE, EXIF_TYPE_RATIONAL, 1,
+                         &exifInfo->gps_altitude, &LongerTagOffset, pIfdStart);
+        }
+
+        if (exifInfo->enableGps & EXIF_GPS_TIMESTAMP) {
+            writeExifIfd(&pCur, EXIF_TAG_GPS_TIMESTAMP, EXIF_TYPE_RATIONAL, 3,
+                         exifInfo->gps_timestamp, &LongerTagOffset, pIfdStart);
+        }
+
+        if (exifInfo->enableGps & EXIF_GPS_IMG_DIRECTION) {
+            writeExifIfd(&pCur, EXIF_TAG_GPS_IMG_DIRECTION_REF, EXIF_TYPE_ASCII, 2,
+                         exifInfo->gps_img_direction_ref);
+            writeExifIfd(&pCur, EXIF_TAG_GPS_IMG_DIRECTION, EXIF_TYPE_RATIONAL, 1,
+                         &exifInfo->gps_img_direction, &LongerTagOffset, pIfdStart);
+        }
+
+        if (exifInfo->enableGps & EXIF_GPS_PROCMETHOD) {
+            tmp = strlen((char*)exifInfo->gps_processing_method);
+            if (tmp > 0) {
+                if (tmp > 100) {
+                    tmp = 100;
+                }
+                unsigned char tmp_buf[100 + sizeof(ExifAsciiPrefix)];
+                MEMCPY_S(tmp_buf, sizeof(tmp_buf), ExifAsciiPrefix, sizeof(ExifAsciiPrefix));
+                MEMCPY_S(&tmp_buf[sizeof(ExifAsciiPrefix)], 100, exifInfo->gps_processing_method,
+                         tmp);
+                writeExifIfd(&pCur, EXIF_TAG_GPS_PROCESSING_METHOD, EXIF_TYPE_UNDEFINED,
+                             tmp + sizeof(ExifAsciiPrefix), tmp_buf, &LongerTagOffset, pIfdStart);
+            }
+        }
+        writeExifIfd(&pCur, EXIF_TAG_GPS_DATESTAMP, EXIF_TYPE_ASCII, 11, exifInfo->gps_datestamp,
+                     &LongerTagOffset, pIfdStart);
+        tmp = 0;
+        MEMCPY_S(pCur, OFFSET_SIZE, (int8_t*)&tmp, OFFSET_SIZE);  // next IFD offset
+        pCur += OFFSET_SIZE;
+    }
+
+    // backup LongerTagOffset, if the total exif size is > 64K, we will use it.
+    LongerTagOffsetWithoutThumbnail = LongerTagOffset;
+    if (LongerTagOffsetWithoutThumbnail >= EXIF_SIZE_LIMITATION) {
+        LOGE("line:%d, in the makeExif, the size exceeds 64K", __LINE__);
+        return EXIF_FAIL;
+    }
+
+    // 2 1th IFD TIFF Tags
+    if (exifInfo->enableThumb && (m_thumbBuf != nullptr) && (m_thumbSize > 0)) {
+        writeThumbData(pIfdStart, pNextIfdOffset, &LongerTagOffset, exifInfo);
+    } else {
+        tmp = 0;
+        MEMCPY_S(pNextIfdOffset, OFFSET_SIZE, (int8_t*)&tmp,
+                 OFFSET_SIZE);  // NEXT IFD offset skipped on 0th IFD
+    }
+
+    // fill APP1 maker
+    unsigned char App1Marker[2] = {0xff, 0xe1};
+    MEMCPY_S(pApp1Start, 2, App1Marker, 2);
+    pApp1Start += 2;
+
+    // calc and fill the APP1 segment total size, 2 is length; 6 is ExifIdentifierCode
+    *size = 2 + 6 + LongerTagOffset;
+
+    writeMarkerSizeToBuf(pApp1Start, *size);
+
+    unsigned app2StartOffset = *size;
+    *size += 2;  // APP1 marker size
+
+    exif_status status = EXIF_SUCCESS;
+
+    if (exifInfo->makernoteToApp2) {
+        LOG1("Makernote goes to APP2 segment.");
+        status = makeApp2((pApp1Start + app2StartOffset), *size, exifInfo);
+    }
+
+    if (status != EXIF_SUCCESS) LOGW("Failed to create EXIF APP2 section");
+
+    LOG1("makeExif End");
+
+    return EXIF_SUCCESS;
+}
+
+void ExifCreater::writeMarkerSizeToBuf(unsigned char* ptrTo, unsigned int size) {
+    unsigned char size_mm[2] = {static_cast<unsigned char>((size >> 8) & 0xFF),
+                                static_cast<unsigned char>(size & 0xFF)};
+
+    MEMCPY_S(ptrTo, 2, size_mm, 2);
+}
+
+/**
+ * makeApp2
+ *
+ * Write the makernote to APP2 segment. Use multiple APP2 segments if makernote
+ * size is more than one segment (64 kb)
+ *
+ * \param pStartApp2 [IN] APP2 start address
+ * \param exifInfo [IN] Data to be written
+ * \param writeId [IN] Whether to write the Intel Makernote ID string.
+ * \param size [OUT] Total size after APP2 is written
+ */
+exif_status ExifCreater::makeApp2(void* pStartApp2, size_t& size, exif_attribute_t* exifInfo,
+                                  bool writeId) {
+    LOG1("@%s", __func__);
+
+    // APP2 marker will be written starting from the pos pointed to by
+    // pStartApp2
+
+    if (exifInfo->makerNoteDataSize <= 0) return EXIF_SUCCESS;
+
+    int bytesLeftForSegment = EXIF_SIZE_LIMITATION;
+    int bytesToWrite = exifInfo->makerNoteDataSize;
+
+    unsigned char *pCur = nullptr, *pApp2Start = nullptr;
+    unsigned char App2Marker[SIZEOF_APP2_MARKER] = {0xff, 0xe2};
+    int writeCount = 0;
+    unsigned char* toWrite = exifInfo->makerNoteData;
+
+    pCur = static_cast<unsigned char*>(pStartApp2);
+
+    // Write Makernote up to ~64kB, then split to a new
+    // APP2 segment, if needed
+    while (bytesToWrite > 0) {
+        pApp2Start = pCur;
+        pCur += 4;  // Skip 4 bytes for APP2 marker and length
+
+        if (writeId) {
+            MEMCPY_S(pCur, sizeof(MAKERNOTE_ID), MAKERNOTE_ID, sizeof(MAKERNOTE_ID));
+            pCur += sizeof(MAKERNOTE_ID);
+            size += sizeof(MAKERNOTE_ID);
+            // ID overhead for one APP2 segment
+            bytesLeftForSegment -= sizeof(MAKERNOTE_ID);
+        }
+
+        // Overhead for one APP2 segment:
+        bytesLeftForSegment -= (sizeof(App2Marker) + SIZEOF_LENGTH_FIELD);
+
+        if (bytesToWrite > bytesLeftForSegment) {
+            // More data to write than what fits to one APP2 marker
+            writeCount = bytesLeftForSegment;
+        } else {
+            // All data fits to one APP2 segment
+            writeCount = bytesToWrite;
+        }
+
+        bytesToWrite -= writeCount;
+
+        MEMCPY_S(pCur, writeCount, toWrite, writeCount);
+        pCur += writeCount;
+        toWrite += writeCount;
+        size += writeCount;
+
+        // Last, put the APP2 marker to the beginning of the segment
+        MEMCPY_S(pApp2Start, sizeof(App2Marker), App2Marker, sizeof(App2Marker));
+        pApp2Start += sizeof(App2Marker);
+
+        // Length field goes after the APP2 marker
+        int app2SegmentSize = writeCount + SIZEOF_LENGTH_FIELD;  // Raw data written + overhead
+        if (writeId) app2SegmentSize += sizeof(MAKERNOTE_ID);
+
+        writeMarkerSizeToBuf(pApp2Start, app2SegmentSize);
+
+        // add the 2 bytes for both length field and APP2 marker, the caller has to know the total
+        // size
+        size += sizeof(App2Marker) + SIZEOF_LENGTH_FIELD;
+
+        // Reset byte counts for another APP2 segment, if needed
+        bytesLeftForSegment = EXIF_SIZE_LIMITATION;
+    }
+
+    return EXIF_SUCCESS;
+}
+
+void ExifCreater::writeThumbData(unsigned char* pIfdStart, unsigned char* pNextIfdOffset,
+                                 unsigned int* LongerTagOffset, exif_attribute_t* exifInfo) {
+    unsigned char* pCur;
+    unsigned int tmp;
+
+    // firstly calc the exif total size, if it's > 64K, we'll disable the thumbnail
+    tmp = 4 + 6 + *LongerTagOffset;  // 4 is APP1 marker and length; 6 is ExifIdentifierCode
+    tmp += NUM_SIZE + NUM_1TH_IFD_TIFF * IFD_SIZE + OFFSET_SIZE;
+    tmp += sizeof(exifInfo->x_resolution) + sizeof(exifInfo->y_resolution);
+    tmp += m_thumbSize;
+
+    if (tmp > EXIF_SIZE_LIMITATION) {
+        LOGD("line:%d, in makeExif, exif total size(%d) > 64K, we'll disable thumbnail.", __LINE__,
+             tmp);
+        m_thumbSize = 0;
+        m_thumbBuf = nullptr;
+        tmp = 0;
+        MEMCPY_S(pNextIfdOffset, OFFSET_SIZE, (int8_t*)&tmp,
+                 OFFSET_SIZE);  // NEXT IFD offset skipped on 0th IFD
+    } else {
+        tmp = *LongerTagOffset;
+        MEMCPY_S(pNextIfdOffset, OFFSET_SIZE, (int8_t*)&tmp,
+                 OFFSET_SIZE);  // NEXT IFD offset skipped on 0th IFD
+
+        pCur = pIfdStart + *LongerTagOffset;
+
+        tmp = NUM_1TH_IFD_TIFF;
+        MEMCPY_S(pCur, NUM_SIZE, (int8_t*)&tmp, NUM_SIZE);
+        pCur += NUM_SIZE;
+
+        *LongerTagOffset += NUM_SIZE + NUM_1TH_IFD_TIFF * IFD_SIZE + OFFSET_SIZE;
+
+        writeExifIfd(&pCur, EXIF_TAG_IMAGE_WIDTH, EXIF_TYPE_LONG, 1, exifInfo->widthThumb);
+        writeExifIfd(&pCur, EXIF_TAG_IMAGE_HEIGHT, EXIF_TYPE_LONG, 1, exifInfo->heightThumb);
+        writeExifIfd(&pCur, EXIF_TAG_COMPRESSION_SCHEME, EXIF_TYPE_SHORT, 1,
+                     exifInfo->compression_scheme);
+        writeExifIfd(&pCur, EXIF_TAG_ORIENTATION, EXIF_TYPE_SHORT, 1, exifInfo->orientation);
+        writeExifIfd(&pCur, EXIF_TAG_X_RESOLUTION, EXIF_TYPE_RATIONAL, 1, &exifInfo->x_resolution,
+                     LongerTagOffset, pIfdStart);
+        writeExifIfd(&pCur, EXIF_TAG_Y_RESOLUTION, EXIF_TYPE_RATIONAL, 1, &exifInfo->y_resolution,
+                     LongerTagOffset, pIfdStart);
+        writeExifIfd(&pCur, EXIF_TAG_RESOLUTION_UNIT, EXIF_TYPE_SHORT, 1,
+                     exifInfo->resolution_unit);
+        writeExifIfd(&pCur, EXIF_TAG_JPEG_INTERCHANGE_FORMAT, EXIF_TYPE_LONG, 1, *LongerTagOffset);
+        writeExifIfd(&pCur, EXIF_TAG_JPEG_INTERCHANGE_FORMAT_LEN, EXIF_TYPE_LONG, 1, m_thumbSize);
+
+        tmp = 0;
+        MEMCPY_S(pCur, OFFSET_SIZE, (int8_t*)&tmp, OFFSET_SIZE);  // next IFD offset
+        // pCur += OFFSET_SIZE;
+
+        MEMCPY_S(pIfdStart + *LongerTagOffset, m_thumbSize, m_thumbBuf, m_thumbSize);
+        *LongerTagOffset += m_thumbSize;
+    }
+}
+
+void ExifCreater::writeExifIfd(unsigned char** pCur, unsigned short tag, unsigned short type,
+                               unsigned int count, uint32_t value) {
+    MEMCPY_S(*pCur, 2, (int8_t*)&tag, 2);
+    *pCur += 2;
+    MEMCPY_S(*pCur, 2, (int8_t*)&type, 2);
+    *pCur += 2;
+    MEMCPY_S(*pCur, 4, (int8_t*)&count, 4);
+    *pCur += 4;
+    MEMCPY_S(*pCur, 4, (int8_t*)&value, 4);
+    *pCur += 4;
+}
+
+void ExifCreater::writeExifIfd(unsigned char** pCur, unsigned short tag, unsigned short type,
+                               unsigned int count, unsigned char* pValue) {
+    char buf[4] = {
+        0,
+    };
+
+    MEMCPY_S(buf, count, pValue, count);
+    MEMCPY_S(*pCur, 2, (int8_t*)&tag, 2);
+    *pCur += 2;
+    MEMCPY_S(*pCur, 2, (int8_t*)&type, 2);
+    *pCur += 2;
+    MEMCPY_S(*pCur, 4, (int8_t*)&count, 4);
+    *pCur += 4;
+    MEMCPY_S(*pCur, 4, (int8_t*)buf, 4);
+    *pCur += 4;
+}
+
+void ExifCreater::writeExifIfd(unsigned char** pCur, unsigned short tag, unsigned short type,
+                               unsigned int count, unsigned char* pValue, unsigned int* offset,
+                               unsigned char* start) {
+    MEMCPY_S(*pCur, 2, (int8_t*)&tag, 2);
+    *pCur += 2;
+    MEMCPY_S(*pCur, 2, (int8_t*)&type, 2);
+    *pCur += 2;
+    MEMCPY_S(*pCur, 4, (int8_t*)&count, 4);
+    *pCur += 4;
+    MEMCPY_S(*pCur, 4, (int8_t*)offset, 4);
+    *pCur += 4;
+    MEMCPY_S(start + *offset, count, pValue, count);
+    *offset += count;
+}
+
+void ExifCreater::writeExifIfd(unsigned char** pCur, unsigned short tag, unsigned short type,
+                               unsigned int count, rational_t* pValue, unsigned int* offset,
+                               unsigned char* start) {
+    MEMCPY_S(*pCur, 2, (int8_t*)&tag, 2);
+    *pCur += 2;
+    MEMCPY_S(*pCur, 2, (int8_t*)&type, 2);
+    *pCur += 2;
+    MEMCPY_S(*pCur, 4, (int8_t*)&count, 4);
+    *pCur += 4;
+    MEMCPY_S(*pCur, 4, (int8_t*)offset, 4);
+    *pCur += 4;
+    MEMCPY_S(start + *offset, 8 * count, (int8_t*)pValue, 8 * count);
+    *offset += 8 * count;
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/jpeg/ExifCreater.h b/camera/hal/intel/ipu6/src/jpeg/ExifCreater.h
new file mode 100644
index 000000000000..bf90e260567e
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/jpeg/ExifCreater.h
@@ -0,0 +1,114 @@
+/*
+ * Copyright Samsung Electronics Co.,LTD.
+ * Copyright (C) 2010 The Android Open Source Project
+ * Copyright (C) 2016-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ *
+ * JPEG DRIVER MODULE (JpegEncoder.h)
+ * Author  : ge.lee       -- initial version
+ * Date    : 03 June 2010
+ * Purpose : This file implements the JPEG encoder APIs as needed by Camera HAL
+ */
+
+#pragma once
+
+#include <stdint.h>
+#include <sys/ioctl.h>
+
+#include "Exif.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+#define MAX_JPG_WIDTH 4352
+#define MAX_JPG_HEIGHT 3264
+#define MAX_JPG_RESOLUTION (MAX_JPG_WIDTH * MAX_JPG_HEIGHT)
+
+#define MAX_JPG_THUMBNAIL_WIDTH 640
+#define MAX_JPG_THUMBNAIL_HEIGHT 480
+#define MAX_JPG_THUMBNAIL_RESOLUTION (MAX_JPG_THUMBNAIL_WIDTH * MAX_JPG_THUMBNAIL_HEIGHT)
+
+#define MAX_RGB_WIDTH 800
+#define MAX_RGB_HEIGHT 480
+#define MAX_RGB_RESOLUTION (MAX_RGB_WIDTH * MAX_RGB_HEIGHT)
+
+/*******************************************************************************/
+/* define JPG & image memory */
+/* memory area is 4k(PAGE_SIZE) aligned because of VirtualCopyEx() */
+#define JPG_STREAM_BUF_SIZE ((MAX_JPG_RESOLUTION / PAGE_SIZE + 1) * PAGE_SIZE)
+#define JPG_STREAM_THUMB_BUF_SIZE ((MAX_JPG_THUMBNAIL_RESOLUTION / PAGE_SIZE + 1) * PAGE_SIZE)
+#define JPG_FRAME_BUF_SIZE (((MAX_JPG_RESOLUTION * 3) / PAGE_SIZE + 1) * PAGE_SIZE)
+#define JPG_FRAME_THUMB_BUF_SIZE (((MAX_JPG_THUMBNAIL_RESOLUTION * 3) / PAGE_SIZE + 1) * PAGE_SIZE)
+#define JPG_RGB_BUF_SIZE (((MAX_RGB_RESOLUTION * 4) / PAGE_SIZE + 1) * PAGE_SIZE)
+
+#define JPG_TOTAL_BUF_SIZE                                                  \
+    (JPG_STREAM_BUF_SIZE + JPG_STREAM_THUMB_BUF_SIZE + JPG_FRAME_BUF_SIZE + \
+     JPG_FRAME_THUMB_BUF_SIZE + JPG_RGB_BUF_SIZE)
+
+#define JPG_MAIN_START 0x00
+#define JPG_THUMB_START JPG_STREAM_BUF_SIZE
+#define IMG_MAIN_START (JPG_STREAM_BUF_SIZE + JPG_STREAM_THUMB_BUF_SIZE)
+#define IMG_THUMB_START (IMG_MAIN_START + JPG_FRAME_BUF_SIZE)
+/*******************************************************************************/
+
+const char MAKERNOTE_ID[] = {0x49, 0x6e, 0x74, 0x65, 0x6c, 0x4d,
+                             0x6b, 0x6e, 0x6f, 0x74, 0x65, 0x0 /* "IntelMknote\0" */};
+
+const unsigned SIZEOF_LENGTH_FIELD = 2;
+const unsigned SIZEOF_APP2_MARKER = 2;
+const unsigned SIZEOF_APP2_OVERHEAD =
+    sizeof(MAKERNOTE_ID) + SIZEOF_APP2_MARKER + SIZEOF_LENGTH_FIELD;
+const bool ENABLE_APP2_MARKER = true;
+typedef enum { EXIF_FAIL = -1, EXIF_SUCCESS = 0 } exif_status;
+
+class ExifCreater {
+ public:
+    ExifCreater();
+    virtual ~ExifCreater();
+
+    exif_status setThumbData(const void* thumbBuf, unsigned int thumbSize);
+
+    bool isThumbDataSet() const;
+
+    exif_status makeExif(void* exifOut, exif_attribute_t* exifInfo, size_t* size);
+
+ private:
+    exif_status makeApp2(void* pStartApp2, size_t& size, exif_attribute_t* exifInfo,
+                         bool writeId = true);
+    void writeMarkerSizeToBuf(unsigned char* ptrTo, unsigned int size);
+    /*
+        Every IFD has 12Bytes.
+        Tag ID, 2B; Type, 2B; Count, 4B; Value/Offset, 4B;
+        If it is Value, please use the first two functions.
+        If it is Offset, please use the last two functions
+        and store the data in the rear.
+    */
+    void writeExifIfd(unsigned char** pCur, unsigned short tag, unsigned short type,
+                      unsigned int count, uint32_t value);
+    void writeExifIfd(unsigned char** pCur, unsigned short tag, unsigned short type,
+                      unsigned int count, unsigned char* pValue);
+    void writeExifIfd(unsigned char** pCur, unsigned short tag, unsigned short type,
+                      unsigned int count, rational_t* pValue, unsigned int* offset,
+                      unsigned char* start);
+    void writeExifIfd(unsigned char** pCur, unsigned short tag, unsigned short type,
+                      unsigned int count, unsigned char* pValue, unsigned int* offset,
+                      unsigned char* start);
+    void writeThumbData(unsigned char* pIfdStart, unsigned char* pNextIfdOffset,
+                        unsigned int* LongerTagOffset, exif_attribute_t* exifInfo);
+
+    unsigned char* m_thumbBuf;  // MAP: Added to set thumbnail from external data
+    unsigned int m_thumbSize;   // MAP: Added to set thumbnail from external data
+};
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/jpeg/IJpegEncoder.h b/camera/hal/intel/ipu6/src/jpeg/IJpegEncoder.h
new file mode 100644
index 000000000000..9e810b07f13d
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/jpeg/IJpegEncoder.h
@@ -0,0 +1,78 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+#define DEFAULT_JPEG_QUALITY 95
+static const unsigned char mJpegMarkerSOI[2] = {0xFF, 0xD8};
+
+struct EncodePackage {
+    EncodePackage()
+            : inputWidth(0),
+              inputHeight(0),
+              inputStride(0),
+              inputFormat(0),
+              inputSize(0),
+              inputBufferHandle(nullptr),
+              inputData(nullptr),
+              outputWidth(0),
+              outputHeight(0),
+              outputSize(0),
+              outputBufferHandle(nullptr),
+              outputData(nullptr),
+              quality(0),
+              encodedDataSize(0),
+              exifData(nullptr),
+              exifDataSize(0) {}
+
+    /* input buffer info */
+    int inputWidth;
+    int inputHeight;
+    int inputStride;
+    int inputFormat;
+    unsigned int inputSize;
+    void* inputBufferHandle;
+    void* inputData;
+
+    /* output buffer info */
+    int outputWidth;
+    int outputHeight;
+    unsigned int outputSize;
+    void* outputBufferHandle;
+    void* outputData;
+
+    int quality;
+    uint32_t encodedDataSize;
+    uint8_t* exifData;
+    uint32_t exifDataSize;
+};
+
+class IJpegEncoder {
+ public:
+    IJpegEncoder(){};
+    virtual ~IJpegEncoder(){};
+
+    static std::unique_ptr<IJpegEncoder> createJpegEncoder();
+    virtual bool doJpegEncode(EncodePackage* package) = 0;
+
+ private:
+    DISALLOW_COPY_AND_ASSIGN(IJpegEncoder);
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/jpeg/JpegMaker.cpp b/camera/hal/intel/ipu6/src/jpeg/JpegMaker.cpp
new file mode 100644
index 000000000000..bbfe08556d8d
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/jpeg/JpegMaker.cpp
@@ -0,0 +1,222 @@
+/*
+ * Copyright (C) 2016-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "JpegMaker"
+
+#include "JpegMaker.h"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+JpegMaker::JpegMaker() {
+    LOG2("@%s", __func__);
+    mExifMaker = std::unique_ptr<EXIFMaker>(new EXIFMaker());
+}
+
+JpegMaker::~JpegMaker() {
+    LOG2("@%s", __func__);
+}
+
+status_t JpegMaker::setupExifWithMetaData(int bufWidth, int bufHeight, const Parameters& parameter,
+                                          ExifMetaData* metaData) {
+    LOG2("@%s", __func__);
+
+    status_t status = OK;
+
+    status = processJpegSettings(parameter, metaData);
+    CheckError(status != OK, status, "@%s: Process settngs for JPEG failed!", __func__);
+
+    mExifMaker->initialize(bufWidth, bufHeight);
+    mExifMaker->pictureTaken(metaData);
+
+    mExifMaker->enableFlash(metaData->flashFired, metaData->v3AeMode, metaData->flashMode);
+    mExifMaker->updateSensorInfo(parameter);
+    mExifMaker->saveMakernote(parameter);
+
+    status = processExifSettings(parameter, metaData);
+    if (status != OK) {
+        LOGE("@%s: Process settngs for Exif! %d", __func__, status);
+        return status;
+    }
+
+    mExifMaker->initializeLocation(metaData);
+    mExifMaker->setSensorAeConfig(parameter);
+
+    if (metaData->software) mExifMaker->setSoftware(metaData->software);
+
+    return status;
+}
+
+status_t JpegMaker::getExif(const EncodePackage& thumbnailPackage, uint8_t* exifPtr,
+                            uint32_t* exifSize) {
+    if (thumbnailPackage.encodedDataSize > 0 && thumbnailPackage.quality > 0) {
+        mExifMaker->setThumbnail(static_cast<unsigned char*>(thumbnailPackage.outputData),
+                                 thumbnailPackage.encodedDataSize, thumbnailPackage.outputWidth,
+                                 thumbnailPackage.outputHeight);
+    }
+    *exifSize = mExifMaker->makeExif(exifPtr);
+    return *exifSize > 0 ? OK : UNKNOWN_ERROR;
+}
+
+status_t JpegMaker::processExifSettings(const Parameters& params, ExifMetaData* metaData) {
+    LOG2("@%s:", __func__);
+    status_t status = OK;
+
+    status = processGpsSettings(params, metaData);
+    status |= processColoreffectSettings(params, metaData);
+    status |= processScalerCropSettings(params, metaData);
+
+    return status;
+}
+
+/* copy exif data into output buffer */
+void JpegMaker::writeExifData(EncodePackage* package) {
+    CheckError(package == nullptr, VOID_VALUE, "@%s, package is nullptr", __func__);
+
+    if (package->exifDataSize == 0) return;
+
+    CheckError(!package->outputData, VOID_VALUE, "@%s, outputData is nullptr", __func__);
+    CheckError(!package->exifData, VOID_VALUE, "@%s, exifData is nullptr", __func__);
+
+    unsigned int jSOISize = sizeof(mJpegMarkerSOI);
+    unsigned char* jpegOut = reinterpret_cast<unsigned char*>(package->outputData);
+    MEMCPY_S(jpegOut, jSOISize, mJpegMarkerSOI, jSOISize);
+    jpegOut += jSOISize;
+
+    MEMCPY_S(jpegOut, package->exifDataSize, reinterpret_cast<unsigned char*>(package->exifData),
+             package->exifDataSize);
+}
+
+/**
+ * processJpegSettings
+ *
+ * Store JPEG settings to the exif metadata
+ *
+ * \param [IN] jpeg parameters
+ * \ metaData [out] metadata of the request
+ *
+ */
+status_t JpegMaker::processJpegSettings(const Parameters& params, ExifMetaData* metaData) {
+    LOG2("@%s:", __func__);
+    status_t status = OK;
+
+    CheckError(!metaData, UNKNOWN_ERROR, "MetaData struct not intialized");
+
+    // make jpeg with thumbnail or not
+    camera_resolution_t thumbSize = {0};
+    params.getJpegThumbnailSize(thumbSize);
+    LOG2("%s request thumbname size %dx%d", __func__, thumbSize.width, thumbSize.height);
+
+    uint8_t new_jpeg_quality = DEFAULT_JPEG_QUALITY;
+    int ret = params.getJpegQuality(&new_jpeg_quality);
+    if (ret != icamera::OK) {
+        LOGW("cannot find jpeg quality, use default");
+    }
+    metaData->mJpegSetting.jpegQuality = new_jpeg_quality;
+
+    uint8_t new_jpeg_thumb_quality = DEFAULT_JPEG_QUALITY;
+    params.getJpegThumbnailQuality(&new_jpeg_thumb_quality);
+    metaData->mJpegSetting.jpegThumbnailQuality = new_jpeg_thumb_quality;
+    metaData->mJpegSetting.thumbWidth = thumbSize.width;
+    metaData->mJpegSetting.thumbHeight = thumbSize.height;
+
+    int new_rotation = 0;
+    params.getJpegRotation(new_rotation);
+    metaData->mJpegSetting.orientation = new_rotation;
+
+    LOG1("jpegQuality=%d,thumbQuality=%d,thumbW=%d,thumbH=%d,orientation=%d",
+         metaData->mJpegSetting.jpegQuality, metaData->mJpegSetting.jpegThumbnailQuality,
+         metaData->mJpegSetting.thumbWidth, metaData->mJpegSetting.thumbHeight,
+         metaData->mJpegSetting.orientation);
+
+    params.getAeMode(metaData->aeMode);
+    params.getAwbMode(metaData->awbMode);
+
+    metaData->currentFocusDistance = 0.0;
+    float focusDistance = 0.0;
+    params.getFocusDistance(focusDistance);
+    if (focusDistance != 0) {
+        metaData->currentFocusDistance = ceil(1000.0 / focusDistance);
+    }
+    LOG2("aeMode=%d, awbMode=%d, currentFocusDistance=%f", metaData->aeMode, metaData->awbMode,
+         metaData->currentFocusDistance);
+
+    return status;
+}
+
+/**
+ * This function will get GPS metadata from request setting
+ *
+ * \param[in] settings The Anroid metadata to process GPS settings from
+ * \param[out] metadata The EXIF data where the GPS setting are written to
+ */
+status_t JpegMaker::processGpsSettings(const Parameters& param, ExifMetaData* metadata) {
+    LOG2("@%s:", __func__);
+    status_t status = OK;
+
+    // gps latitude
+    double new_gps_latitude = 0.0;
+    param.getJpegGpsLatitude(new_gps_latitude);
+    metadata->mGpsSetting.latitude = new_gps_latitude;
+
+    double new_gps_longitude = 0.0;
+    param.getJpegGpsLongitude(new_gps_longitude);
+    metadata->mGpsSetting.longitude = new_gps_longitude;
+
+    double new_gps_altitude = 0.0;
+    param.getJpegGpsAltitude(new_gps_altitude);
+    metadata->mGpsSetting.altitude = new_gps_altitude;
+
+    // gps timestamp
+    int64_t new_gps_timestamp = 0;
+    param.getJpegGpsTimeStamp(new_gps_timestamp);
+    metadata->mGpsSetting.gpsTimeStamp = new_gps_timestamp;
+
+    // gps processing method
+    char new_gps_processing_method[MAX_NUM_GPS_PROCESSING_METHOD + 1];
+    CLEAR(new_gps_processing_method);
+    param.getJpegGpsProcessingMethod(MAX_NUM_GPS_PROCESSING_METHOD, new_gps_processing_method);
+    if (strlen(new_gps_processing_method) != 0) {
+        snprintf(metadata->mGpsSetting.gpsProcessingMethod,
+                 sizeof(metadata->mGpsSetting.gpsProcessingMethod), "%s",
+                 new_gps_processing_method);
+    }
+
+    return status;
+}
+
+status_t JpegMaker::processColoreffectSettings(const Parameters& param, ExifMetaData* metaData) {
+    LOG2("@%s:", __func__);
+    status_t status = OK;
+
+    camera_effect_mode_t new_image_effect = CAM_EFFECT_NONE;
+    param.getImageEffect(new_image_effect);
+    metaData->effectMode = new_image_effect;
+    LOG2("effect mode=%d", metaData->effectMode);
+
+    return status;
+}
+
+status_t JpegMaker::processScalerCropSettings(const Parameters& param, ExifMetaData* metaData) {
+    LOG2("@%s:", __func__);
+    status_t status = OK;
+
+    return status;
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/jpeg/JpegMaker.h b/camera/hal/intel/ipu6/src/jpeg/JpegMaker.h
new file mode 100644
index 000000000000..8f5036216c54
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/jpeg/JpegMaker.h
@@ -0,0 +1,56 @@
+/*
+ * Copyright (C) 2016-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+
+#include "EXIFMaker.h"
+#include "EXIFMetaData.h"
+#include "IJpegEncoder.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+/**
+ * \class JpegMaker
+ * Does the EXIF header creation and appending to the provided jpeg buffer
+ *
+ */
+class JpegMaker {
+ public: /* Methods */
+    explicit JpegMaker();
+    virtual ~JpegMaker();
+    status_t setupExifWithMetaData(int bufWidth, int bufHeight, const Parameters& parameter,
+                                   ExifMetaData* metaData);
+    status_t getExif(const EncodePackage& thumbnailPackage, uint8_t* exifPtr, uint32_t* exifSize);
+    void writeExifData(EncodePackage* package);
+
+ private: /* Methods */
+    // prevent copy constructor and assignment operator
+    DISALLOW_COPY_AND_ASSIGN(JpegMaker);
+
+    status_t processExifSettings(const Parameters& params, ExifMetaData* metaData);
+    status_t processJpegSettings(const Parameters& params, ExifMetaData* metaData);
+    status_t processGpsSettings(const Parameters& params, ExifMetaData* metadata);
+    status_t processColoreffectSettings(const Parameters& params, ExifMetaData* metaData);
+    status_t processScalerCropSettings(const Parameters& params, ExifMetaData* metaData);
+
+ private: /* Members */
+    std::unique_ptr<EXIFMaker> mExifMaker;
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/jpeg/chrome/JpegEncoderCore.cpp b/camera/hal/intel/ipu6/src/jpeg/chrome/JpegEncoderCore.cpp
new file mode 100644
index 000000000000..ed89d935f7fe
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/jpeg/chrome/JpegEncoderCore.cpp
@@ -0,0 +1,74 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "JpegEncoderCore"
+
+#include "JpegEncoderCore.h"
+
+#include <cros-camera/camera_buffer_manager.h>
+#include <linux/videodev2.h>
+
+#include "iutils/CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+JpegEncoderCore::JpegEncoderCore() {
+    LOG1("@%s", __func__);
+}
+
+JpegEncoderCore::~JpegEncoderCore() {
+    LOG1("@%s", __func__);
+}
+
+std::unique_ptr<IJpegEncoder> IJpegEncoder::createJpegEncoder() {
+    return std::unique_ptr<JpegEncoderCore>(new JpegEncoderCore());
+}
+
+/**
+ * doJpegEncode
+ *
+ * Do HW / SW JPEG encoding for the main buffer
+ * Do SW JPEG encoding for the thumbnail buffer
+ *
+ * \param pa [IN/OUT] Information that should be encoded
+ */
+bool JpegEncoderCore::doJpegEncode(EncodePackage* pa) {
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL1);
+    CheckError(pa == nullptr, false, "@%s, pa is nullptr", __func__);
+
+    CheckError(pa->inputWidth != pa->outputWidth || pa->inputHeight != pa->outputHeight, false,
+               "@%s, input size != output size", __func__);
+    CheckError(pa->inputWidth <= 0 || pa->outputWidth <= 0, false,
+               "@%s, inputWidth:%d, outputWidth:%d", __func__, pa->inputWidth, pa->outputWidth);
+    CheckError(pa->inputHeight <= 0 || pa->outputHeight <= 0, false,
+               "@%s, inputHeight:%d, outputHeight:%d", __func__, pa->inputHeight, pa->outputHeight);
+
+    nsecs_t startTime = CameraUtils::systemTime();
+    bool ret = mJpegCompressor->CompressImage(
+        *(reinterpret_cast<buffer_handle_t*>(pa->inputBufferHandle)), pa->inputData,
+        V4L2_PIX_FMT_NV12, *(reinterpret_cast<buffer_handle_t*>(pa->outputBufferHandle)),
+        pa->outputData, pa->outputSize, pa->inputWidth, pa->inputHeight, pa->quality, pa->exifData,
+        pa->exifDataSize, &pa->encodedDataSize);
+
+    LOG1("@%s: encoding ret:%d, %dx%d need % ms, jpeg size %u, quality %d)", __func__, ret,
+         pa->outputWidth, pa->outputHeight, (CameraUtils::systemTime() - startTime) / 1000000,
+         pa->encodedDataSize, pa->quality);
+
+    return ret && pa->encodedDataSize > 0;
+}
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/jpeg/chrome/JpegEncoderCore.h b/camera/hal/intel/ipu6/src/jpeg/chrome/JpegEncoderCore.h
new file mode 100644
index 000000000000..bb9187f0b0b7
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/jpeg/chrome/JpegEncoderCore.h
@@ -0,0 +1,41 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <cros-camera/jpeg_compressor.h>
+
+#include <memory>
+#include <mutex>
+
+#include "IJpegEncoder.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+class JpegEncoderCore : public IJpegEncoder {
+ public:
+    JpegEncoderCore();
+    ~JpegEncoderCore();
+
+    virtual bool doJpegEncode(EncodePackage* pa);
+
+ private:
+    DISALLOW_COPY_AND_ASSIGN(JpegEncoderCore);
+
+    std::unique_ptr<cros::JpegCompressor> mJpegCompressor;
+};
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/jpeg/sw/SWJpegEncoder.cpp b/camera/hal/intel/ipu6/src/jpeg/sw/SWJpegEncoder.cpp
new file mode 100644
index 000000000000..0aafef8c185f
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/jpeg/sw/SWJpegEncoder.cpp
@@ -0,0 +1,740 @@
+/*
+ * Copyright (C) 2011 The Android Open Source Project
+ * Copyright (C) 2016-2020 Intel Corporation. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#define LOG_TAG "SWJpegEncoder"
+
+#include "SWJpegEncoder.h"
+
+#include <string>
+
+#include "ImageConverter.h"
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+#define RESOLUTION_1_3MP_WIDTH 1280
+#define RESOLUTION_1_3MP_HEIGHT 960
+
+namespace icamera {
+
+SWJpegEncoder::SWJpegEncoder()
+        : mJpegSize(-1),
+          mTotalWidth(0),
+          mTotalHeight(0),
+          mDstBuf(nullptr),
+          mCPUCoresNum(1) {
+    LOG2("@%s, line:%d", __func__, __LINE__);
+}
+
+SWJpegEncoder::~SWJpegEncoder() {
+    LOG2("@%s, line:%d", __func__, __LINE__);
+}
+
+std::unique_ptr<IJpegEncoder> IJpegEncoder::createJpegEncoder() {
+    return std::unique_ptr<SWJpegEncoder>(new SWJpegEncoder());
+}
+
+/**
+ * \param package: encode package for either thumbnail or main image
+ * \return true if encoding succeeds
+ * \return false if encoding fails
+ */
+bool SWJpegEncoder::doJpegEncode(EncodePackage* package) {
+    CheckError(package == nullptr, false, "@%s, package is nullptr", __func__);
+
+    int status = 0;
+    nsecs_t startTime = CameraUtils::systemTime();
+
+    LOG2("@%s: IN = {buf:%p, w:%u, h:%u, sz:%u, stride:%d, fmt:%s}", __func__, package->inputData,
+         package->inputWidth, package->inputHeight, package->inputSize, package->inputStride,
+         icamera::CameraUtils::format2string(package->inputFormat).c_str());
+
+    LOG2("@%s: OUT = {buf:%p, w:%u, h:%u, sz:%u, q:%d}", __func__, package->outputData,
+         package->outputWidth, package->outputHeight, package->outputSize, package->quality);
+
+    if (package->inputWidth == 0 || package->inputHeight == 0 || package->inputFormat == 0) {
+        ALOGE("Invalid input received!");
+        mJpegSize = -1;
+        goto exit;
+    }
+
+    mTotalWidth = package->inputWidth;
+    mTotalHeight = package->inputHeight;
+
+    /*
+     * For encoding main buffer, need to skip the exif data and SOI header.
+     * Because the SOI(jpeg maker) is written in the beginning of jpeg data when
+     * do encode, so only skip the exif data size here, and the SOI will be moved
+     * to the head of output buffer
+     */
+    mDstBuf = reinterpret_cast<unsigned char*>(package->outputData) + package->exifDataSize;
+
+    if (useMultiThreadEncoding(package->inputWidth, package->inputHeight))
+        status = swEncodeMultiThread(*package);
+    else
+        status = swEncode(*package);
+
+    if (status < 0) goto exit;
+
+    package->encodedDataSize = mJpegSize;
+
+    LOG2("@%s encode, total consume:%ums, encoded jpeg size: %d", __func__,
+         (unsigned)((CameraUtils::systemTime() - startTime) / 1000000), mJpegSize);
+
+    return mJpegSize > 0 ? true : false;
+exit:
+    return false;
+}
+
+/**
+ *  This function will decide if we need to enable the multi thread jpeg encoding.
+ *  currently, we have two conditions to use the old single jpeg encoding.
+ *  one is that the resolution is smaller than the 1.3M
+ *  the other is that the CPU number is 1
+ *
+ *  \param width: the Jpeg width
+ *  \param height: the Jpeg height
+ *
+ *  \return false if we don't need multi thread Jpeg encoding
+ *  \return true if we need multi thread Jpeg encoding
+ */
+bool SWJpegEncoder::useMultiThreadEncoding(int width, int height) {
+    LOG2("@%s, line:%d, width:%d, height:%d", __func__, __LINE__, width, height);
+    bool ret = false;
+
+    /* more conditions could be added to here by according to the request */
+    if ((width < RESOLUTION_1_3MP_WIDTH && height < RESOLUTION_1_3MP_HEIGHT))
+        ret = false;
+    else if (width & 0xf)
+        ret = false;
+    else
+        ret = true;
+
+    LOG2("@%s, line:%d, ret:%d", __func__, __LINE__, ret);
+    return ret;
+}
+
+/**
+ * encode jpeg by calling the SWJpegEncoder which is the libjpeg wrapper
+ * single thread.
+ *
+ * \param package: jpeg encode package
+ * \return 0 if encoding was successful
+ * \return -1 if encoding failed
+ */
+int SWJpegEncoder::swEncode(const EncodePackage& package) {
+    LOG2("@%s, line:%d, use the libjpeg to do sw jpeg encoding", __func__, __LINE__);
+    int status = 0;
+    Codec encoder;
+
+    encoder.init();
+    encoder.setJpegQuality(package.quality);
+    status = encoder.configEncoding(package.inputWidth, package.inputHeight, package.inputStride,
+                                    static_cast<JSAMPLE*>(mDstBuf),
+                                    (package.outputSize - package.exifDataSize));
+    const void* uv_buf =
+        static_cast<unsigned char*>(package.inputData) + package.inputStride * package.inputHeight;
+
+    if (status) goto exit;
+
+    status = encoder.doJpegEncoding(package.inputData, uv_buf, package.inputFormat);
+    if (status) goto exit;
+
+exit:
+    if (status)
+        mJpegSize = -1;
+    else
+        encoder.getJpegSize(&mJpegSize);
+
+    encoder.deInit();
+
+    return (status ? -1 : 0);
+}
+
+/**
+ * encode jpeg by calling the SWJpegEncoder which is the libjpeg wrapper
+ * multi thread.
+ * the thread number depends on the CPU number.
+ *
+ * \param package: jpeg encode package
+ * \return 0 if encoding was successful
+ * \return -1 if encoding failed
+ */
+int SWJpegEncoder::swEncodeMultiThread(const EncodePackage& package) {
+    LOG2("@%s, line:%d, use the libjpeg to do sw jpeg encoding", __func__, __LINE__);
+    int status = 0;
+
+    init(mCPUCoresNum);
+    config(package);
+
+    status = doJpegEncodingMultiThread();
+    if (status) goto exit;
+
+exit:
+    mJpegSize = status ? -1 : mergeJpeg();
+    deInit();
+
+    return (status ? -1 : 0);
+}
+
+/**
+ * Initialize for the multi thread jpeg encoding
+ *
+ * it will create n CodecWorkerThread by according to the thread number.
+ */
+void SWJpegEncoder::init(unsigned int threadNum) {
+    unsigned int num = CLIP(threadNum, MAX_THREAD_NUM, MIN_THREAD_NUM);
+    LOG2("@%s, line:%d, thread number, pass:%d, real:%d", __func__, __LINE__, threadNum, num);
+
+    for (unsigned int i = 0; i < num; i++) {
+        std::shared_ptr<CodecWorkerThread> codecWorkerThread(new CodecWorkerThread);
+        mSwJpegEncoder.push_back(codecWorkerThread);
+    }
+}
+
+/**
+ * deInit for the multi thread jpeg encoding
+ *
+ * it will release all n CodecWorkerThread
+ */
+void SWJpegEncoder::deInit(void) {
+    LOG2("@%s, line:%d", __func__, __LINE__);
+    for (auto& encoder : mSwJpegEncoder) {
+        encoder.reset();
+    }
+
+    mSwJpegEncoder.clear();
+}
+
+/**
+ * configue every thread for multi thread jpeg
+ *
+ * \param package: jpeg encode package
+ */
+void SWJpegEncoder::config(const EncodePackage& package) {
+    LOG2("@%s, line:%d", __func__, __LINE__);
+    std::shared_ptr<CodecWorkerThread> encThread;
+    CodecWorkerThread::CodecConfig cfg;
+
+    for (unsigned int i = 0; i < mSwJpegEncoder.size(); i++) {
+        cfg.width = package.inputWidth;
+        /*
+            for example, there are 4 threads.
+            the first 3 threads must align to 16 which is NV12_MCU_SIZE
+            but for the last thread, it doesn't have this request.
+        */
+        cfg.height = ALIGN_16(package.inputHeight / mSwJpegEncoder.size());
+        cfg.stride = package.inputStride;
+        /*
+         * For NV12 format, Y and UV data are independent, total size is width*height*1.5;
+         * For YUYV format, Y and UV data are crossing, total size is width*height*2;
+         * So the inBufY and inBufUV should be distinguished base on format.
+         */
+        cfg.fourcc = package.inputFormat;
+        cfg.inBufY =
+            (cfg.fourcc == V4L2_PIX_FMT_YUYV)
+                ? static_cast<unsigned char*>(package.inputData) + cfg.stride * cfg.height * 2 * i
+                : static_cast<unsigned char*>(package.inputData) + cfg.stride * cfg.height * i;
+        cfg.inBufUV =
+            (cfg.fourcc == V4L2_PIX_FMT_NV12 || cfg.fourcc == V4L2_PIX_FMT_NV21)
+                ? (static_cast<unsigned char*>(package.inputData) +
+                   package.inputStride * package.inputHeight + cfg.stride * cfg.height * i / 2)
+                : nullptr;
+        cfg.quality = package.quality;
+        cfg.outBufSize = (package.outputSize - package.exifDataSize - DEST_BUF_OFFSET) /
+                         package.inputHeight * cfg.height;
+        cfg.outBuf = static_cast<unsigned char*>(mDstBuf) + DEST_BUF_OFFSET + cfg.outBufSize * i;
+        /* update the last thread's height */
+        if (i == mSwJpegEncoder.size() - 1) {
+            cfg.height = package.inputHeight - cfg.height * (mSwJpegEncoder.size() - 1);
+            cfg.outBufSize = package.outputSize - package.exifDataSize - DEST_BUF_OFFSET -
+                             cfg.outBufSize * (mSwJpegEncoder.size() - 1);
+        }
+
+        encThread = mSwJpegEncoder[i];
+        encThread->setConfig(cfg);
+        LOG2("@%s, line:%d, the %d picture thread cfg", __func__, __LINE__, i);
+        LOG2("@%s, line:%d, cfg.width:%d, cfg.height:%d", __func__, __LINE__, cfg.width,
+             cfg.height);
+        LOG2("@%s, line:%d, cfg.fourcc:%d, cfg.quality:%d", __func__, __LINE__, cfg.fourcc,
+             cfg.quality);
+        LOG2("@%s, line:%d, cfg.inBufY:%p, cfg.inBufUV:%p", __func__, __LINE__, cfg.inBufY,
+             cfg.inBufUV);
+        LOG2("@%s, line:%d, cfg.outBuf:%p, cfg.outBufSize:%d", __func__, __LINE__, cfg.outBuf,
+             cfg.outBufSize);
+    }
+}
+
+/**
+ * the function will trigger the multi jpeg encoding
+ *
+ * \return 0 if encoding was successful
+ * \return -1 if encoding failed
+ */
+int SWJpegEncoder::doJpegEncodingMultiThread(void) {
+    LOG2("@%s, line:%d", __func__, __LINE__);
+    std::shared_ptr<CodecWorkerThread> encThread;
+    status_t status = OK;
+    std::string threadName("CamHAL_SWEncodeMultiThread");
+
+    /* run all threads */
+    for (unsigned int i = 0; i < mSwJpegEncoder.size(); i++) {
+        threadName = threadName + ":" + std::to_string(i);
+        LOG2("@%s, new sw jpeg thread name:%s", __func__, threadName.c_str());
+        encThread = mSwJpegEncoder[i];
+        status = encThread->runThread(threadName.c_str());
+        if (status != OK) {
+            ALOGE("@%s, line:%d, start jpeg thread fail, thread name:%s", __func__, __LINE__,
+                  threadName.c_str());
+            return status;
+        }
+    }
+
+    /* wait all threads to finish */
+    for (unsigned int i = 0; i < mSwJpegEncoder.size(); i++) {
+        LOG2("@%s, the %d sw jpeg encoder thread before join!", __func__, i);
+        encThread = mSwJpegEncoder[i];
+        encThread->waitThreadFinish();
+        if (encThread->getJpegDataSize() == -1) status = UNKNOWN_ERROR;
+    }
+
+    return status;
+}
+
+/**
+ * the function will merge all jpeg pictures which are generated in multi threads
+ * to one jpeg picture
+ *
+ * \return int the merged jpeg size
+ */
+int SWJpegEncoder::mergeJpeg(void) {
+#define HEADER_TOTAL_LEN 623
+#define HEADER_SOS_LEN 14
+#define HEADER_EOI_LEN 2
+#define HEADER_HEIGHT_POS 163
+#define HEADER_WIDTH_POS 165
+#define NV12_MCU_SIZE 16
+    LOG2("@%s, line:%d", __func__, __LINE__);
+    int size = HEADER_TOTAL_LEN - HEADER_SOS_LEN;
+    CodecWorkerThread::CodecConfig cfg;
+    nsecs_t startTime;
+    std::shared_ptr<CodecWorkerThread> encThread = mSwJpegEncoder.at(0);
+    if (encThread == nullptr) {
+        ALOGE("encThread is nullptr");
+        return -1;
+    }
+    encThread->getConfig(&cfg);
+
+    /* Write the JPEG header */
+    MEMCPY_S(mDstBuf, size, cfg.outBuf, size);
+
+    /* Update the width and height info */
+    mDstBuf[HEADER_HEIGHT_POS] = (mTotalHeight >> 8) & 0xFF;
+    mDstBuf[HEADER_HEIGHT_POS + 1] = mTotalHeight & 0xFF;
+    mDstBuf[HEADER_WIDTH_POS] = (mTotalWidth >> 8) & 0xFF;
+    mDstBuf[HEADER_WIDTH_POS + 1] = mTotalWidth & 0xFF;
+
+    /* Write the restarting interval */
+    if (mSwJpegEncoder.size() > 1) {
+        unsigned int MCUs = (cfg.height / NV12_MCU_SIZE) * (cfg.width / NV12_MCU_SIZE);
+        mDstBuf[size++] = 0xFF;
+        mDstBuf[size++] = 0xDD;
+        mDstBuf[size++] = 0;
+        mDstBuf[size++] = 4;
+        mDstBuf[size++] = (MCUs >> 8) & 0xFF;
+        mDstBuf[size++] = MCUs & 0xFF;
+    }
+
+    /* Write the SOS */
+    MEMCPY_S(reinterpret_cast<void*>((unsigned long)mDstBuf + size), HEADER_SOS_LEN,
+             reinterpret_cast<void*>((unsigned long)cfg.outBuf + HEADER_TOTAL_LEN - HEADER_SOS_LEN),
+             HEADER_SOS_LEN);
+    size += HEADER_SOS_LEN;
+
+    /* Write coded segments */
+    for (unsigned int i = 0; i < mSwJpegEncoder.size(); i++) {
+        encThread = mSwJpegEncoder[i];
+        startTime = CameraUtils::systemTime();
+        encThread->getConfig(&cfg);
+        memmove(reinterpret_cast<void*>((unsigned long)mDstBuf + size),
+                reinterpret_cast<void*>((unsigned long)cfg.outBuf + HEADER_TOTAL_LEN),
+                (encThread->getJpegDataSize() - HEADER_TOTAL_LEN - HEADER_EOI_LEN));
+        LOG2("@%s, wr %d segments, size:%d, consume:%ums", __func__, i,
+             (encThread->getJpegDataSize() - HEADER_TOTAL_LEN - HEADER_EOI_LEN),
+             (unsigned)((CameraUtils::systemTime() - startTime) / 1000000));
+        size += (encThread->getJpegDataSize() - HEADER_TOTAL_LEN - HEADER_EOI_LEN);
+
+        if (i != (mSwJpegEncoder.size() - 1)) {
+            mDstBuf[size++] = 0xFF;
+            mDstBuf[size++] = (i & 0x7) | 0xD0;
+        }
+    }
+
+    /* Write EOI */
+    mDstBuf[size++] = 0xFF;
+    mDstBuf[size++] = 0xD9;
+
+    return size;
+}
+
+SWJpegEncoder::CodecWorkerThread::CodecWorkerThread() : mDataSize(-1) {
+    LOG2("@%s, line:%d", __func__, __LINE__);
+    CLEAR(mCfg);
+}
+
+SWJpegEncoder::CodecWorkerThread::~CodecWorkerThread() {
+    LOG2("@%s, line:%d", __func__, __LINE__);
+}
+
+/**
+ * run one thread for multi thread jpeg encoding
+ *
+ * \param name: the thread name
+ */
+status_t SWJpegEncoder::CodecWorkerThread::runThread(const char* name) {
+    LOG2("@%s, line:%d", __func__, __LINE__);
+    return this->run(name);
+}
+
+/**
+ * wait one thread until it has finished
+ *
+ */
+void SWJpegEncoder::CodecWorkerThread::waitThreadFinish(void) {
+    LOG2("@%s, line:%d", __func__, __LINE__);
+    this->join();
+    this->requestExitAndWait();
+}
+
+/**
+ * get jpeg size which is done in one thread
+ *
+ * \return int the coded jpeg size
+ */
+int SWJpegEncoder::CodecWorkerThread::getJpegDataSize(void) {
+    LOG2("@%s, line:%d", __func__, __LINE__);
+    return mDataSize;
+}
+
+/**
+ * the thread exe function for one jpeg thread
+ * when the encoding has been done, it will return false to terminate the thread
+ *
+ * \return false
+ */
+bool SWJpegEncoder::CodecWorkerThread::threadLoop() {
+    LOG2("@%s, line:%d, in CodecWorkerThread", __func__, __LINE__);
+    nsecs_t startTime = CameraUtils::systemTime();
+    int ret = swEncode();
+    LOG2("@%s one swEncode done!, consume:%ums, ret:%d", __func__,
+         (unsigned)((CameraUtils::systemTime() - startTime) / 1000000), ret);
+
+    return false;
+}
+
+/**
+ * this function will call the SWJpegEncoder to encode one jpeg.
+ * it's the main function of the threadLoop
+ *
+ * \return 0 if encoding was successful
+ * \return -1 if encoding failed
+ */
+int SWJpegEncoder::CodecWorkerThread::swEncode(void) {
+    LOG2("@%s, line:%d, in CodecWorkerThread", __func__, __LINE__);
+    int status = 0;
+    Codec encoder;
+
+    encoder.init();
+    encoder.setJpegQuality(mCfg.quality);
+    status = encoder.configEncoding(mCfg.width, mCfg.height, mCfg.stride,
+                                    static_cast<JSAMPLE*>(mCfg.outBuf), mCfg.outBufSize);
+    if (status) goto exit;
+
+    status = encoder.doJpegEncoding(mCfg.inBufY, mCfg.inBufUV, mCfg.fourcc);
+    if (status) goto exit;
+
+exit:
+    if (status)
+        mDataSize = -1;
+    else
+        encoder.getJpegSize(&mDataSize);
+
+    encoder.deInit();
+
+    return (status ? -1 : 0);
+}
+
+SWJpegEncoder::Codec::Codec() : mStride(-1), mJpegQuality(DEFAULT_JPEG_QUALITY) {
+    LOG2("@%s", __func__);
+    CLEAR(mCInfo);
+    CLEAR(mJErr);
+}
+
+SWJpegEncoder::Codec::~Codec() {
+    LOG2("@%s", __func__);
+}
+
+/**
+ * Init the SW jpeg encoder
+ *
+ * It will init the libjpeg library
+ */
+void SWJpegEncoder::Codec::init(void) {
+    LOG2("@%s", __func__);
+    CLEAR(mCInfo);
+    mCInfo.err = jpeg_std_error(&mJErr);
+    jpeg_create_compress(&mCInfo);
+}
+
+/**
+ * deInit the SW jpeg encoder
+ *
+ * It will deinit the libjpeg library
+ */
+void SWJpegEncoder::Codec::deInit(void) {
+    LOG2("@%s", __func__);
+    jpeg_destroy_compress(&mCInfo);
+}
+
+/**
+ * Set the jpeg quality
+ *
+ * \param quality: one value from 0 to 100
+ *
+ */
+void SWJpegEncoder::Codec::setJpegQuality(int quality) {
+    LOG2("@%s, quality:%d", __func__, quality);
+    mJpegQuality = CLIP(quality, 100, 1);
+}
+
+/**
+ * Config the SW jpeg encoder.
+ *
+ * mainly, it will set the destination buffer manager, color space, quality.
+ *
+ * \param width: the width of the jpeg dimentions.
+ * \param height: the height of the jpeg dimentions.
+ * \param jpegBuf: the dest buffer to store the jpeg data
+ * \param jpegBufSize: the size of jpegBuf buffer
+ *
+ * \return 0 if the configuration is right.
+ * \return -1 if the configuration fails.
+ */
+int SWJpegEncoder::Codec::configEncoding(int width, int height, int stride, void* jpegBuf,
+                                         int jpegBufSize) {
+    LOG2("@%s", __func__);
+
+    mStride = stride;
+    mCInfo.input_components = 3;
+    mCInfo.in_color_space = (J_COLOR_SPACE)SUPPORTED_FORMAT;
+    mCInfo.image_width = width;
+    mCInfo.image_height = height;
+
+    if (setupJpegDestMgr(&mCInfo, static_cast<JSAMPLE*>(jpegBuf), jpegBufSize) < 0) {
+        ALOGE("@%s, line:%d, setupJpegDestMgr fail", __func__, __LINE__);
+        return -1;
+    }
+
+    jpeg_set_defaults(&mCInfo);
+    jpeg_set_colorspace(&mCInfo, (J_COLOR_SPACE)SUPPORTED_FORMAT);
+    jpeg_set_quality(&mCInfo, mJpegQuality, TRUE);
+    mCInfo.raw_data_in = TRUE;
+    mCInfo.dct_method = JDCT_ISLOW;
+    mCInfo.comp_info[0].h_samp_factor = 2;
+    mCInfo.comp_info[0].v_samp_factor = 2;
+    mCInfo.comp_info[1].h_samp_factor = 1;
+    mCInfo.comp_info[1].v_samp_factor = 1;
+    mCInfo.comp_info[2].h_samp_factor = 1;
+    mCInfo.comp_info[2].v_samp_factor = 1;
+    jpeg_start_compress(&mCInfo, TRUE);
+
+    return 0;
+}
+
+/**
+ * Do the SW jpeg encoding.
+ *
+ * it will convert the YUV data to P411 and then do jpeg encoding.
+ *
+ * \param y_buf: the source buffer for Y data
+ * \param uv_buf: the source buffer for UV data,
+ * \it could be nullptr if fourcc is V4L2_PIX_FMT_YUYV
+ * \return 0 if the encoding is successful.
+ * \return -1 if the encoding fails.
+ */
+int SWJpegEncoder::Codec::doJpegEncoding(const void* y_buf, const void* uv_buf, int fourcc) {
+    LOG2("@%s", __func__);
+
+    unsigned char* srcY = nullptr;
+    unsigned char* srcUV = nullptr;
+    unsigned char* p411 = nullptr;
+    JSAMPROW y[16], u[16], v[16];
+    JSAMPARRAY data[3];
+    int i, j, width, height;
+
+    width = mCInfo.image_width;
+    height = mCInfo.image_height;
+    srcY = (unsigned char*)y_buf;
+    srcUV = (unsigned char*)uv_buf;
+    p411 = new unsigned char[width * height * 3 / 2];
+
+    switch (fourcc) {
+        case V4L2_PIX_FMT_YUYV:
+            ImageConverter::YUY2ToP411(width, height, mStride, srcY, p411);
+            break;
+        case V4L2_PIX_FMT_NV12:
+            ImageConverter::NV12ToP411Separate(width, height, mStride, srcY, srcUV, p411);
+            break;
+        case V4L2_PIX_FMT_NV21:
+            ImageConverter::NV21ToP411Separate(width, height, mStride, srcY, srcUV, p411);
+            break;
+        default:
+            ALOGE("%s Unsupported fourcc %d", __func__, fourcc);
+            delete[] p411;
+            return -1;
+    }
+
+    data[0] = y;
+    data[1] = u;
+    data[2] = v;
+    for (i = 0; i < height; i += 16) {
+        for (j = 0; j < 16 && (i + j) < height; j++) {
+            y[j] = p411 + width * (j + i);
+            if (j % 2 == 0) {
+                u[j / 2] = p411 + width * height + width / 2 * ((j + i) / 2);
+                v[j / 2] = p411 + width * height + width * height / 4 + width / 2 * ((j + i) / 2);
+            }
+        }
+        jpeg_write_raw_data(&mCInfo, data, 16);
+    }
+
+    jpeg_finish_compress(&mCInfo);
+
+    delete[] p411;
+    p411 = nullptr;
+
+    return 0;
+}
+
+/**
+ * Get the jpeg size.
+ *
+ * \param jpegSize: get the real jpeg size, it will be -1, if encoding fails
+ */
+void SWJpegEncoder::Codec::getJpegSize(int* jpegSize) {
+    LOG2("@%s", __func__);
+
+    JpegDestMgrPtr dest = (JpegDestMgrPtr)mCInfo.dest;
+
+    *jpegSize = (false == dest->encodeSuccess) ? -1 : dest->codedSize;
+}
+
+/**
+ * Setup the jpeg destination buffer manager
+ *
+ * it will convert the YUV data to P411 and then do jpeg encoding.
+ *
+ * \param cInfo: the compress pointer
+ * \param jpegBuf: the buffer pointer for jpeg data
+ * \param jpegBufSize: the jpegBuf buffer's size
+ * \return 0 if it's successful.
+ * \return -1 if it fails.
+ */
+int SWJpegEncoder::Codec::setupJpegDestMgr(j_compress_ptr cInfo, JSAMPLE* jpegBuf,
+                                           int jpegBufSize) {
+    LOG2("@%s", __func__);
+    JpegDestMgrPtr dest;
+
+    if (nullptr == jpegBuf || jpegBufSize <= 0) {
+        ALOGE("@%s, line:%d, jpegBuf:%p, jpegBufSize:%d", __func__, __LINE__, jpegBuf, jpegBufSize);
+        return -1;
+    }
+
+    if (cInfo->dest == nullptr) {
+        cInfo->dest = (struct jpeg_destination_mgr*)(*cInfo->mem->alloc_small)(
+            (j_common_ptr)cInfo, JPOOL_PERMANENT, sizeof(JpegDestMgr));
+        CLEAR(*cInfo->dest);
+    }
+    dest = (JpegDestMgrPtr)cInfo->dest;
+
+    dest->pub.init_destination = initDestination;
+    dest->pub.empty_output_buffer = emptyOutputBuffer;
+    dest->pub.term_destination = termDestination;
+    dest->outJpegBuf = jpegBuf;
+    dest->outJpegBufSize = jpegBufSize;
+
+    return 0;
+}
+
+/**
+ * Init the destination
+ *
+ * It's the first function which be called
+ * among initDestination, emptyOutputBuffer and termDestination
+ *
+ * \param cInfo: the compress pointer
+ */
+void SWJpegEncoder::Codec::initDestination(j_compress_ptr cInfo) {
+    LOG2("@%s", __func__);
+    JpegDestMgrPtr dest = (JpegDestMgrPtr)cInfo->dest;
+
+    dest->pub.next_output_byte = dest->outJpegBuf;
+    dest->pub.free_in_buffer = dest->outJpegBufSize;
+    dest->encodeSuccess = true;
+}
+
+/**
+ * Empty the output buffer
+ *
+ * The function should not be called,
+ * because we should allocate enough memory for the jpeg destination buffer
+ * If we return FALSE, the libjpeg will terminate, so return TRUE always.
+ * But when the function is called, the encoding failing will be recorded.
+ *
+ * \param cInfo: the compress pointer
+ * \return TRUE if it is successful.
+ * \return FALSE if something is wrong
+ */
+boolean SWJpegEncoder::Codec::emptyOutputBuffer(j_compress_ptr cInfo) {
+    LOG2("@%s", __func__);
+    ALOGE("@%s, line:%d, buffer overflow!", __func__, __LINE__);
+    JpegDestMgrPtr dest = (JpegDestMgrPtr)cInfo->dest;
+
+    /* re-cfg the buffer info */
+    dest->pub.next_output_byte = dest->outJpegBuf;
+    dest->pub.free_in_buffer = dest->outJpegBufSize;
+    dest->encodeSuccess = false;
+
+    return TRUE; /* if return FALSE, the total taking picture will fail */
+}
+
+/**
+ * Terminate the destination
+ *
+ * The function will be called as the last function,
+ * among initDestination, emptyOutputBuffer and termDestination.
+ * We can get the encoded jpeg size from it.
+ *
+ * \param cInfo: the compress pointer
+ */
+void SWJpegEncoder::Codec::termDestination(j_compress_ptr cInfo) {
+    LOG2("@%s", __func__);
+    JpegDestMgrPtr dest = (JpegDestMgrPtr)cInfo->dest;
+
+    dest->codedSize = dest->outJpegBufSize - dest->pub.free_in_buffer;
+    LOG2("@%s, line:%d, codedSize:%d", __func__, __LINE__, dest->codedSize);
+}
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/jpeg/sw/SWJpegEncoder.h b/camera/hal/intel/ipu6/src/jpeg/sw/SWJpegEncoder.h
new file mode 100644
index 000000000000..170dfdbb023e
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/jpeg/sw/SWJpegEncoder.h
@@ -0,0 +1,186 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ * Copyright (C) 2016-2020 Intel Corporation. All Rights Reserved.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ *\file SWJpegEncoder.h
+ *
+ * Abstracts the SW jpeg encoder
+ *
+ * This class calls the libjpeg ditectly. And libskia's performance is poor.
+ * The SW jpeg encoder is used for the thumbnail encoding mainly.
+ * But When the HW jpeg encoding fails, it will use the SW jpeg encoder also.
+ *
+ */
+
+#pragma once
+
+#include <linux/videodev2.h>
+#include <stdio.h>
+
+#include <vector>
+
+#include "IJpegEncoder.h"
+#include "iutils/Errors.h"
+#include "iutils/Thread.h"
+#include "iutils/Utils.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+#include "jpeglib.h"
+#ifdef __cplusplus
+}
+#endif
+
+namespace icamera {
+
+/**
+ * \class SWJpegEncoder
+ *
+ * This class is used for sw jpeg encoder.
+ * It will use single or multi thread to do the sw jpeg encoding
+ * It just support NV12 input currently.
+ */
+class SWJpegEncoder : public IJpegEncoder {
+ public:
+    SWJpegEncoder();
+    ~SWJpegEncoder();
+
+    virtual bool doJpegEncode(EncodePackage* package);
+
+ private:
+    // prevent copy constructor and assignment operator
+    DISALLOW_COPY_AND_ASSIGN(SWJpegEncoder);
+
+ private:
+    bool useMultiThreadEncoding(int width, int height);
+    int swEncode(const EncodePackage& package);
+    int swEncodeMultiThread(const EncodePackage& package);
+
+    int mJpegSize;             /*!< it's used to store jpeg size */
+    int mTotalWidth;           /*!< the final jpeg width */
+    int mTotalHeight;          /*!< the final jpeg height */
+    unsigned char* mDstBuf;    /*!< the dest buffer to store the final jpeg */
+    unsigned int mCPUCoresNum; /*!< use to remember the CPU Cores number */
+
+ private:
+    /**
+     * \class CodecWorkerThread
+     *
+     * This class will create one thread to do one sw jpeg encoder.
+     * It will call the SWJpegEncoderWrapper directly.
+     */
+    class CodecWorkerThread : public Thread {
+     public:
+        struct CodecConfig {
+            // input buffer configuration
+            int width;
+            int height;
+            int stride;
+            int fourcc;
+            void* inBufY;
+            void* inBufUV;
+            // output buffer configuration
+            int quality;
+            void* outBuf;
+            int outBufSize;
+        };
+
+        CodecWorkerThread();
+        ~CodecWorkerThread();
+
+        void setConfig(const CodecConfig& cfg) { mCfg = cfg; }
+        void getConfig(CodecConfig* cfg) const { *cfg = mCfg; }
+        status_t runThread(const char* name);
+        void waitThreadFinish(void);
+        int getJpegDataSize(void);
+
+     private:
+        int mDataSize;    /*!< the jpeg data size in one thread */
+        CodecConfig mCfg; /*!< the cfg in one thread */
+     private:
+        virtual bool threadLoop();
+        int swEncode(void);
+    };
+
+ private:
+    void init(unsigned int threadNum = 1);
+    void deInit(void);
+    void config(const EncodePackage& package);
+    int doJpegEncodingMultiThread(void);
+    int mergeJpeg(void);
+
+    std::vector<std::shared_ptr<CodecWorkerThread> > mSwJpegEncoder;
+    static const unsigned int MAX_THREAD_NUM = 8; /*!< the same as max jpeg restart time */
+    static const unsigned int MIN_THREAD_NUM = 1;
+
+    /*!< it's used to use one buffer to merge the multi jpeg data to one jpeg data */
+    static const unsigned int DEST_BUF_OFFSET = 1024;
+
+ private:
+    /**
+     * \class Codec
+     *
+     * This class is used for sw jpeg encoder.
+     * It will call the libjpeg directly.
+     * It just support NV12 input currently.
+     */
+    class Codec {
+     public:
+        Codec();
+        ~Codec();
+
+        void init(void);
+        void deInit(void);
+        void setJpegQuality(int quality);
+        int configEncoding(int width, int height, int stride, void* jpegBuf, int jpegBufSize);
+        /*
+            if fourcc is V4L2_PIX_FMT_NV12, y_buf and uv_buf must be passed
+            if fourcc is V4L2_PIX_FMT_YUYV, y_buf must be passed, uv_buf could be nullptr
+        */
+        int doJpegEncoding(const void* y_buf, const void* uv_buf = nullptr,
+                           int fourcc = V4L2_PIX_FMT_NV12);
+        void getJpegSize(int* jpegSize);
+
+     private:
+        // prevent copy constructor and assignment operator
+        DISALLOW_COPY_AND_ASSIGN(Codec);
+
+     private:
+        typedef struct {
+            struct jpeg_destination_mgr pub;
+            JSAMPLE* outJpegBuf; /*!< jpeg output buffer */
+            int outJpegBufSize;  /*!< jpeg output buffer size */
+            int codedSize;       /*!< the final encoded out jpeg size */
+            bool encodeSuccess;  /*!< if buffer overflow, it will be set to false */
+        } JpegDestMgr, *JpegDestMgrPtr;
+
+        int mStride;
+        struct jpeg_compress_struct mCInfo;
+        struct jpeg_error_mgr mJErr;
+        int mJpegQuality;
+        static const unsigned int SUPPORTED_FORMAT = JCS_YCbCr;
+
+        int setupJpegDestMgr(j_compress_ptr cInfo, JSAMPLE* jpegBuf, int jpegBufSize);
+        // the below three functions are for the dest buffer manager.
+        static void initDestination(j_compress_ptr cInfo);
+        static boolean emptyOutputBuffer(j_compress_ptr cInfo);
+        static void termDestination(j_compress_ptr cInfo);
+    };
+};
+
+}  // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/metadata/CameraMetadata.cpp b/camera/hal/intel/ipu6/src/metadata/CameraMetadata.cpp
new file mode 100644
index 000000000000..9aa26c9ca706
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/metadata/CameraMetadata.cpp
@@ -0,0 +1,435 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ * Copyright (C) 2015-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "CameraMetadata"
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+#include "CameraMetadata.h"
+
+namespace icamera {
+
+CameraMetadata::CameraMetadata() :
+        mBuffer(NULL), mLocked(false) {
+}
+
+CameraMetadata::CameraMetadata(size_t entryCapacity, size_t dataCapacity) :
+        mLocked(false)
+{
+    mBuffer = allocate_icamera_metadata(entryCapacity, dataCapacity);
+}
+
+CameraMetadata::CameraMetadata(const CameraMetadata &other) :
+        mLocked(false) {
+    mBuffer = clone_icamera_metadata(other.mBuffer);
+}
+
+CameraMetadata::CameraMetadata(icamera_metadata_t *buffer) :
+        mBuffer(NULL), mLocked(false) {
+    acquire(buffer);
+}
+
+CameraMetadata &CameraMetadata::operator=(const CameraMetadata &other) {
+    return operator=(other.mBuffer);
+}
+
+CameraMetadata &CameraMetadata::operator=(const icamera_metadata_t *buffer) {
+    if (mLocked) {
+        LOGE("%s: Assignment to a locked CameraMetadata!", __func__);
+        return *this;
+    }
+
+    if (buffer != mBuffer) {
+        icamera_metadata_t *newBuffer = clone_icamera_metadata(buffer);
+        clear();
+        mBuffer = newBuffer;
+    }
+    return *this;
+}
+
+CameraMetadata::~CameraMetadata() {
+    mLocked = false;
+    clear();
+}
+
+const icamera_metadata_t* CameraMetadata::getAndLock() {
+    mLocked = true;
+    return mBuffer;
+}
+
+status_t CameraMetadata::unlock(const icamera_metadata_t *buffer) {
+    if (!mLocked) {
+        LOGE("%s: Can't unlock a non-locked CameraMetadata!", __func__);
+        return INVALID_OPERATION;
+    }
+    if (buffer != mBuffer) {
+        LOGE("%s: Can't unlock CameraMetadata with wrong pointer!",
+                __func__);
+        return BAD_VALUE;
+    }
+    mLocked = false;
+    return OK;
+}
+
+icamera_metadata_t* CameraMetadata::release() {
+    if (mLocked) {
+        LOGE("%s: CameraMetadata is locked", __func__);
+        return NULL;
+    }
+    icamera_metadata_t *released = mBuffer;
+    mBuffer = NULL;
+    return released;
+}
+
+void CameraMetadata::clear() {
+    if (mLocked) {
+        LOGE("%s: CameraMetadata is locked", __func__);
+        return;
+    }
+    if (mBuffer) {
+        free_icamera_metadata(mBuffer);
+        mBuffer = NULL;
+    }
+}
+
+void CameraMetadata::acquire(icamera_metadata_t *buffer) {
+    if (mLocked) {
+        LOGE("%s: CameraMetadata is locked", __func__);
+        return;
+    }
+    clear();
+    mBuffer = buffer;
+
+    if (validate_icamera_metadata_structure(mBuffer, /*size*/NULL) != OK) {
+        LOGE("%s: Failed to validate metadata structure %p",
+                 __func__, buffer);
+    }
+}
+
+void CameraMetadata::acquire(CameraMetadata &other) {
+    if (mLocked) {
+        LOGE("%s: CameraMetadata is locked", __func__);
+        return;
+    }
+    acquire(other.release());
+}
+
+status_t CameraMetadata::append(const CameraMetadata &other) {
+    return append(other.mBuffer);
+}
+
+status_t CameraMetadata::append(const icamera_metadata_t* other) {
+    if (mLocked) {
+        LOGE("%s: CameraMetadata is locked", __func__);
+        return INVALID_OPERATION;
+    }
+    size_t extraEntries = get_icamera_metadata_entry_count(other);
+    size_t extraData = get_icamera_metadata_data_count(other);
+    resizeIfNeeded(extraEntries, extraData);
+
+    return append_icamera_metadata(mBuffer, other);
+}
+
+size_t CameraMetadata::entryCount() const {
+    return (mBuffer == NULL) ? 0 :
+            get_icamera_metadata_entry_count(mBuffer);
+}
+
+bool CameraMetadata::isEmpty() const {
+    return entryCount() == 0;
+}
+
+status_t CameraMetadata::sort() {
+    if (mLocked) {
+        LOGE("%s: CameraMetadata is locked", __func__);
+        return INVALID_OPERATION;
+    }
+    return sort_icamera_metadata(mBuffer);
+}
+
+status_t CameraMetadata::checkType(uint32_t tag, uint8_t expectedType) {
+    int tagType = get_icamera_metadata_tag_type(tag);
+    if (tagType == -1 || tagType >= ICAMERA_NUM_TYPES) {
+        LOGE("Update metadata entry: Unknown tag %d type=%d", tag, tagType);
+        return INVALID_OPERATION;
+    }
+    if (tagType != expectedType) {
+        LOGE("Mismatched tag type when updating entry %s (%d) of type %s; "
+                "got type %s data instead ",
+                get_icamera_metadata_tag_name(tag), tag,
+                icamera_metadata_type_names[tagType],
+                icamera_metadata_type_names[expectedType]);
+        return INVALID_OPERATION;
+    }
+    return OK;
+}
+
+status_t CameraMetadata::update(uint32_t tag,
+        const int32_t *data, size_t data_count) {
+    status_t res;
+    if (mLocked) {
+        LOGE("%s: CameraMetadata is locked", __func__);
+        return INVALID_OPERATION;
+    }
+    if ( (res = checkType(tag, ICAMERA_TYPE_INT32)) != OK) {
+        return res;
+    }
+    return updateImpl(tag, (const void*)data, data_count);
+}
+
+status_t CameraMetadata::update(uint32_t tag,
+        const uint8_t *data, size_t data_count) {
+    status_t res;
+    if (mLocked) {
+        LOGE("%s: CameraMetadata is locked", __func__);
+        return INVALID_OPERATION;
+    }
+    if ( (res = checkType(tag, ICAMERA_TYPE_BYTE)) != OK) {
+        return res;
+    }
+    return updateImpl(tag, (const void*)data, data_count);
+}
+
+status_t CameraMetadata::update(uint32_t tag,
+        const float *data, size_t data_count) {
+    status_t res;
+    if (mLocked) {
+        LOGE("%s: CameraMetadata is locked", __func__);
+        return INVALID_OPERATION;
+    }
+    if ( (res = checkType(tag, ICAMERA_TYPE_FLOAT)) != OK) {
+        return res;
+    }
+    return updateImpl(tag, (const void*)data, data_count);
+}
+
+status_t CameraMetadata::update(uint32_t tag,
+        const int64_t *data, size_t data_count) {
+    status_t res;
+    if (mLocked) {
+        LOGE("%s: CameraMetadata is locked", __func__);
+        return INVALID_OPERATION;
+    }
+    if ( (res = checkType(tag, ICAMERA_TYPE_INT64)) != OK) {
+        return res;
+    }
+    return updateImpl(tag, (const void*)data, data_count);
+}
+
+status_t CameraMetadata::update(uint32_t tag,
+        const double *data, size_t data_count) {
+    status_t res;
+    if (mLocked) {
+        LOGE("%s: CameraMetadata is locked", __func__);
+        return INVALID_OPERATION;
+    }
+    if ( (res = checkType(tag, ICAMERA_TYPE_DOUBLE)) != OK) {
+        return res;
+    }
+    return updateImpl(tag, (const void*)data, data_count);
+}
+
+status_t CameraMetadata::update(uint32_t tag,
+        const icamera_metadata_rational_t *data, size_t data_count) {
+    status_t res;
+    if (mLocked) {
+        LOGE("%s: CameraMetadata is locked", __func__);
+        return INVALID_OPERATION;
+    }
+    if ( (res = checkType(tag, ICAMERA_TYPE_RATIONAL)) != OK) {
+        return res;
+    }
+    return updateImpl(tag, (const void*)data, data_count);
+}
+
+status_t CameraMetadata::update(uint32_t tag,
+        const std::string &string) {
+    status_t res;
+    if (mLocked) {
+        LOGE("%s: CameraMetadata is locked", __func__);
+        return INVALID_OPERATION;
+    }
+    if ( (res = checkType(tag, ICAMERA_TYPE_BYTE)) != OK) {
+        return res;
+    }
+    // string.size() doesn't count the null termination character.
+    return updateImpl(tag, (const void*)string.c_str(), string.size() + 1);
+}
+
+status_t CameraMetadata::updateImpl(uint32_t tag, const void *data,
+        size_t data_count) {
+    status_t res;
+    if (mLocked) {
+        LOGE("%s: CameraMetadata is locked", __func__);
+        return INVALID_OPERATION;
+    }
+    int type = get_icamera_metadata_tag_type(tag);
+    if (type == -1) {
+        LOGE("%s: Tag %d not found", __func__, tag);
+        return BAD_VALUE;
+    }
+    size_t data_size = calculate_icamera_metadata_entry_data_size(type,
+            data_count);
+
+    res = resizeIfNeeded(1, data_size);
+
+    if (res == OK) {
+        icamera_metadata_entry_t entry;
+        res = find_icamera_metadata_entry(mBuffer, tag, &entry);
+        if (res == NAME_NOT_FOUND) {
+            res = add_icamera_metadata_entry(mBuffer,
+                    tag, data, data_count);
+        } else if (res == OK) {
+            res = update_icamera_metadata_entry(mBuffer,
+                    entry.index, data, data_count, NULL);
+        }
+    }
+
+    if (res != OK) {
+        LOGE("%s: Unable to update metadata entry %s.%s (%x): %s (%d)",
+                __func__, get_icamera_metadata_section_name(tag),
+                get_icamera_metadata_tag_name(tag), tag, strerror(-res), res);
+    }
+
+    if(validate_icamera_metadata_structure(mBuffer, /*size*/NULL) != OK) {
+             LOGE("%s: Failed to validate metadata structure after update %p",
+             __func__, mBuffer);
+    }
+
+    return res;
+}
+
+bool CameraMetadata::exists(uint32_t tag) const {
+    icamera_metadata_ro_entry entry;
+    return find_icamera_metadata_ro_entry(mBuffer, tag, &entry) == 0;
+}
+
+icamera_metadata_entry_t CameraMetadata::find(uint32_t tag) {
+    status_t res;
+    icamera_metadata_entry entry;
+    CLEAR(entry);
+    if (mLocked) {
+        LOGE("%s: CameraMetadata is locked", __func__);
+        entry.count = 0;
+        return entry;
+    }
+    res = find_icamera_metadata_entry(mBuffer, tag, &entry);
+    if (res != OK) {
+        entry.count = 0;
+        entry.data.u8 = NULL;
+    }
+    return entry;
+}
+
+icamera_metadata_ro_entry_t CameraMetadata::find(uint32_t tag) const {
+    status_t res;
+    icamera_metadata_ro_entry entry;
+    res = find_icamera_metadata_ro_entry(mBuffer, tag, &entry);
+    if (res != OK) {
+        entry.count = 0;
+        entry.data.u8 = NULL;
+    }
+    return entry;
+}
+
+status_t CameraMetadata::erase(uint32_t tag) {
+    icamera_metadata_entry_t entry;
+    status_t res;
+    if (mLocked) {
+        LOGE("%s: CameraMetadata is locked", __func__);
+        return INVALID_OPERATION;
+    }
+    res = find_icamera_metadata_entry(mBuffer, tag, &entry);
+    if (res == NAME_NOT_FOUND) {
+        return OK;
+    } else if (res != OK) {
+        LOGE("%s: Error looking for entry %s.%s (%x): %s %d",
+                __func__,
+                get_icamera_metadata_section_name(tag),
+                get_icamera_metadata_tag_name(tag), tag, strerror(-res), res);
+        return res;
+    }
+    res = delete_icamera_metadata_entry(mBuffer, entry.index);
+    if (res != OK) {
+        LOGE("%s: Error deleting entry %s.%s (%x): %s %d",
+                __func__,
+                get_icamera_metadata_section_name(tag),
+                get_icamera_metadata_tag_name(tag), tag, strerror(-res), res);
+    }
+    return res;
+}
+
+void CameraMetadata::dump(int fd, int verbosity, int indentation) const {
+    dump_indented_icamera_metadata(mBuffer, fd, verbosity, indentation);
+}
+
+status_t CameraMetadata::resizeIfNeeded(size_t extraEntries, size_t extraData) {
+    if (mBuffer == NULL) {
+        mBuffer = allocate_icamera_metadata(extraEntries * 2, extraData * 2);
+        if (mBuffer == NULL) {
+            LOGE("%s: Can't allocate larger metadata buffer", __func__);
+            return NO_MEMORY;
+        }
+    } else {
+        size_t currentEntryCount = get_icamera_metadata_entry_count(mBuffer);
+        size_t currentEntryCap = get_icamera_metadata_entry_capacity(mBuffer);
+        size_t newEntryCount = currentEntryCount +
+                extraEntries;
+        newEntryCount = (newEntryCount > currentEntryCap) ?
+                newEntryCount * 2 : currentEntryCap;
+
+        size_t currentDataCount = get_icamera_metadata_data_count(mBuffer);
+        size_t currentDataCap = get_icamera_metadata_data_capacity(mBuffer);
+        size_t newDataCount = currentDataCount +
+                extraData;
+        newDataCount = (newDataCount > currentDataCap) ?
+                newDataCount * 2 : currentDataCap;
+
+        if (newEntryCount > currentEntryCap ||
+                newDataCount > currentDataCap) {
+            icamera_metadata_t *oldBuffer = mBuffer;
+            mBuffer = allocate_icamera_metadata(newEntryCount,
+                    newDataCount);
+            if (mBuffer == NULL) {
+                LOGE("%s: Can't allocate larger metadata buffer", __func__);
+                return NO_MEMORY;
+            }
+            append_icamera_metadata(mBuffer, oldBuffer);
+            free_icamera_metadata(oldBuffer);
+        }
+    }
+    return OK;
+}
+
+void CameraMetadata::swap(CameraMetadata& other) {
+    if (mLocked) {
+        LOGE("%s: CameraMetadata is locked", __func__);
+        return;
+    } else if (other.mLocked) {
+        LOGE("%s: Other CameraMetadata is locked", __func__);
+        return;
+    }
+
+    icamera_metadata* thisBuf = mBuffer;
+    icamera_metadata* otherBuf = other.mBuffer;
+
+    other.mBuffer = thisBuf;
+    mBuffer = otherBuf;
+}
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/metadata/CameraMetadata.h b/camera/hal/intel/ipu6/src/metadata/CameraMetadata.h
new file mode 100644
index 000000000000..6acf03e91989
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/metadata/CameraMetadata.h
@@ -0,0 +1,200 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ * Copyright (C) 2015-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <string>
+
+#include "icamera_metadata_base.h"
+#include "iutils/Errors.h"
+
+namespace icamera {
+
+/**
+ * A convenience wrapper around the C-based icamera_metadata_t library.
+ */
+class CameraMetadata {
+  public:
+    /** Creates an empty object; best used when expecting to acquire contents
+     * from elsewhere */
+    CameraMetadata();
+    /** Creates an object with space for entryCapacity entries, with
+     * dataCapacity extra storage */
+    CameraMetadata(size_t entryCapacity, size_t dataCapacity = 10);
+
+    ~CameraMetadata();
+
+    /** Takes ownership of passed-in buffer */
+    CameraMetadata(icamera_metadata_t *buffer);
+    /** Clones the metadata */
+    CameraMetadata(const CameraMetadata &other);
+
+    /**
+     * Assignment clones metadata buffer.
+     */
+    CameraMetadata &operator=(const CameraMetadata &other);
+    CameraMetadata &operator=(const icamera_metadata_t *buffer);
+
+    /**
+     * Get reference to the underlying metadata buffer. Ownership remains with
+     * the CameraMetadata object, but non-const CameraMetadata methods will not
+     * work until unlock() is called. Note that the lock has nothing to do with
+     * thread-safety, it simply prevents the icamera_metadata_t pointer returned
+     * here from being accidentally invalidated by CameraMetadata operations.
+     */
+    const icamera_metadata_t* getAndLock();
+
+    /**
+     * Unlock the CameraMetadata for use again. After this unlock, the pointer
+     * given from getAndLock() may no longer be used. The pointer passed out
+     * from getAndLock must be provided to guarantee that the right object is
+     * being unlocked.
+     */
+    status_t unlock(const icamera_metadata_t *buffer);
+
+    /**
+     * Release a raw metadata buffer to the caller. After this call,
+     * CameraMetadata no longer references the buffer, and the caller takes
+     * responsibility for freeing the raw metadata buffer (using
+     * free_camera_metadata()), or for handing it to another CameraMetadata
+     * instance.
+     */
+    icamera_metadata_t* release();
+
+    /**
+     * Clear the metadata buffer and free all storage used by it
+     */
+    void clear();
+
+    /**
+     * Acquire a raw metadata buffer from the caller. After this call,
+     * the caller no longer owns the raw buffer, and must not free or manipulate it.
+     * If CameraMetadata already contains metadata, it is freed.
+     */
+    void acquire(icamera_metadata_t* buffer);
+
+    /**
+     * Acquires raw buffer from other CameraMetadata object. After the call, the argument
+     * object no longer has any metadata.
+     */
+    void acquire(CameraMetadata &other);
+
+    /**
+     * Append metadata from another CameraMetadata object.
+     */
+    status_t append(const CameraMetadata &other);
+
+    /**
+     * Append metadata from a raw camera_metadata buffer
+     */
+    status_t append(const icamera_metadata* other);
+
+    /**
+     * Number of metadata entries.
+     */
+    size_t entryCount() const;
+
+    /**
+     * Is the buffer empty (no entires)
+     */
+    bool isEmpty() const;
+
+    /**
+     * Sort metadata buffer for faster find
+     */
+    status_t sort();
+
+    /**
+     * Update metadata entry. Will create entry if it doesn't exist already, and
+     * will reallocate the buffer if insufficient space exists. Overloaded for
+     * the various types of valid data.
+     */
+    status_t update(uint32_t tag,
+            const uint8_t *data, size_t data_count);
+    status_t update(uint32_t tag,
+            const int32_t *data, size_t data_count);
+    status_t update(uint32_t tag,
+            const float *data, size_t data_count);
+    status_t update(uint32_t tag,
+            const int64_t *data, size_t data_count);
+    status_t update(uint32_t tag,
+            const double *data, size_t data_count);
+    status_t update(uint32_t tag,
+            const icamera_metadata_rational_t *data, size_t data_count);
+    status_t update(uint32_t tag,
+            const std::string &string);
+
+    /**
+     * Check if a metadata entry exists for a given tag id
+     *
+     */
+    bool exists(uint32_t tag) const;
+
+    /**
+     * Get metadata entry by tag id
+     */
+    icamera_metadata_entry find(uint32_t tag);
+
+    /**
+     * Get metadata entry by tag id, with no editing
+     */
+    icamera_metadata_ro_entry find(uint32_t tag) const;
+
+    /**
+     * Delete metadata entry by tag
+     */
+    status_t erase(uint32_t tag);
+
+    /**
+     * Swap the underlying camera metadata between this and the other
+     * metadata object.
+     */
+    void swap(CameraMetadata &other);
+
+    /**
+     * Dump contents into FD for debugging. The verbosity levels are
+     * 0: Tag entry information only, no data values
+     * 1: Level 0 plus at most 16 data values per entry
+     * 2: All information
+     *
+     * The indentation parameter sets the number of spaces to add to the start
+     * each line of output.
+     */
+    void dump(int fd, int verbosity = 1, int indentation = 0) const;
+
+  private:
+    icamera_metadata_t *mBuffer;
+    bool               mLocked;
+
+    /**
+     * Check if tag has a given type
+     */
+    status_t checkType(uint32_t tag, uint8_t expectedType);
+
+    /**
+     * Base update entry method
+     */
+    status_t updateImpl(uint32_t tag, const void *data, size_t data_count);
+
+    /**
+     * Resize metadata buffer if needed by reallocating it and copying it over.
+     */
+    status_t resizeIfNeeded(size_t extraEntries, size_t extraData);
+
+};
+
+} // namespace android
diff --git a/camera/hal/intel/ipu6/src/metadata/ParameterGenerator.cpp b/camera/hal/intel/ipu6/src/metadata/ParameterGenerator.cpp
new file mode 100644
index 000000000000..0dbdca736e7c
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/metadata/ParameterGenerator.cpp
@@ -0,0 +1,378 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "ParameterGenerator"
+
+#include <math.h>
+#include <memory>
+
+#include "iutils/Errors.h"
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+#include "MakerNote.h"
+#include "AiqResultStorage.h"
+#include "AiqUtils.h"
+#include "ParameterHelper.h"
+#include "ParameterGenerator.h"
+
+namespace icamera {
+
+#define MIN_TONEMAP_POINTS 64
+#define CHECK_REQUEST_ID(id) CheckError((id < 0), UNKNOWN_ERROR, "%s: error request id %ld!", __func__, id);
+#define CHECK_SEQUENCE(id) CheckError((id < 0), UNKNOWN_ERROR, "%s: error sequence %ld!", __func__, id);
+
+ParameterGenerator::ParameterGenerator(int cameraId) :
+    mCameraId(cameraId),
+    mLastSequence(-1),
+    mTonemapMaxCurvePoints(0)
+{
+    LOG1("%s, mCameraId = %d", __func__, mCameraId);
+    reset();
+
+    camera_info_t info;
+    CLEAR(info);
+    PlatformData::getCameraInfo(mCameraId, info);
+    info.capability->getTonemapMaxCurvePoints(mTonemapMaxCurvePoints);
+    if (mTonemapMaxCurvePoints > 0 && mTonemapMaxCurvePoints < MIN_TONEMAP_POINTS) {
+        LOGW("%s: wrong tonemap points", __func__);
+        mTonemapMaxCurvePoints = 0;
+    }
+    if (mTonemapMaxCurvePoints) {
+        mTonemapCurveRed = std::unique_ptr<float[]>(new float[mTonemapMaxCurvePoints * 2]);
+        mTonemapCurveBlue = std::unique_ptr<float[]>(new float[mTonemapMaxCurvePoints * 2]);
+        mTonemapCurveGreen = std::unique_ptr<float[]>(new float[mTonemapMaxCurvePoints * 2]);
+
+        // Initialize P_IN, P_OUT values [(P_IN, P_OUT), ..]
+        for (int32_t i = 0; i < mTonemapMaxCurvePoints; i++) {
+            float index = static_cast<float>(i);
+            mTonemapCurveRed[i * 2] = index / (mTonemapMaxCurvePoints - 1);
+            mTonemapCurveRed[i * 2 + 1] = index / (mTonemapMaxCurvePoints - 1);
+            mTonemapCurveBlue[i * 2] = index / (mTonemapMaxCurvePoints - 1);
+            mTonemapCurveBlue[i * 2 + 1] = index / (mTonemapMaxCurvePoints - 1);
+            mTonemapCurveGreen[i * 2] = index / (mTonemapMaxCurvePoints - 1);
+            mTonemapCurveGreen[i * 2 + 1] = index / (mTonemapMaxCurvePoints - 1);
+        }
+    }
+}
+
+ParameterGenerator::~ParameterGenerator()
+{
+    LOG1("%s, mCameraId = %d", __func__, mCameraId);
+}
+
+int ParameterGenerator::reset()
+{
+    LOG1("%s, mCameraId = %d", __func__, mCameraId);
+    AutoMutex l(mParamsLock);
+    for (int i = 0; i < kStorageSize; i++) {
+        mRequestParams[i].reset();
+    }
+
+    mLastSequence = -1;
+    return OK;
+}
+
+int ParameterGenerator::saveParameters(long sequence, long requestId, const Parameters* param)
+{
+    CHECK_REQUEST_ID(requestId);
+    CHECK_SEQUENCE(sequence);
+
+    long index = sequence % kStorageSize;
+
+    AutoMutex l(mParamsLock);
+    mLastSequence = sequence;
+    if (param)
+        mLastParam = *param;
+
+    LOG2("%s, sequence %ld", __func__, sequence);
+    mRequestParams[index].sequence = sequence;
+    mRequestParams[index].requestId = requestId;
+    mRequestParams[index].param = mLastParam;
+    return OK;
+}
+
+void ParameterGenerator::updateParameters(long sequence, const Parameters *param)
+{
+    CheckError(!param, VOID_VALUE, "The param is nullptr!");
+
+    LOG2("%s, sequence %ld", __func__, sequence);
+    long index = sequence % kStorageSize;
+
+    AutoMutex l(mParamsLock);
+    int32_t userRequestId = 0;
+    int ret = param->getUserRequestId(userRequestId);
+    if (ret == OK) {
+        mRequestParams[index].param.setUserRequestId(userRequestId);
+    }
+    // Update Jpeg related settings
+    int rotation = 0;
+    ret = param->getJpegRotation(rotation);
+    if (ret == OK) {
+        mRequestParams[index].param.setJpegRotation(rotation);
+    }
+    uint8_t quality = 0;
+    ret = param->getJpegQuality(&quality);
+    if (ret == OK) {
+        mRequestParams[index].param.setJpegQuality(quality);
+    }
+    int64_t timestamp = 0;
+    ret = param->getJpegGpsTimeStamp(timestamp);
+    if (ret == OK) {
+        mRequestParams[index].param.setJpegGpsTimeStamp(timestamp);
+    }
+    double gpsCoordinates[3] = { 0 };
+    int retLat = param->getJpegGpsLatitude(gpsCoordinates[0]);
+    int retLon = param->getJpegGpsLongitude(gpsCoordinates[1]);
+    int retAlt = param->getJpegGpsAltitude(gpsCoordinates[2]);
+    if (retLat == OK && retLon == OK && retAlt == OK) {
+        mRequestParams[index].param.setJpegGpsCoordinates(gpsCoordinates);
+    }
+    camera_resolution_t res;
+    ret = param->getJpegThumbnailSize(res);
+    if (ret == OK) {
+        mRequestParams[index].param.setJpegThumbnailSize(res);
+    }
+    quality = 0;
+    ret = param->getJpegThumbnailQuality(&quality);
+    if (ret == OK) {
+        mRequestParams[index].param.setJpegThumbnailQuality(quality);
+    }
+}
+
+int ParameterGenerator::getParameters(long sequence, Parameters *param, bool resultOnly)
+{
+    CheckError((param == nullptr), UNKNOWN_ERROR, "nullptr to get param!");
+
+    if (!resultOnly && sequence < 0) {
+        *param = mLastParam;
+    } else if (!resultOnly) {
+        long pos = -1;
+        for (long seq = sequence; (seq > (sequence - kStorageSize)) && (seq >= 0); seq--) {
+            long index = seq % kStorageSize;
+            if (seq == mRequestParams[index].sequence) {
+                pos = index;
+                break;
+            }
+        }
+
+        if (pos >= 0) {
+            *param = mRequestParams[pos].param;
+        } else {
+            LOGE("Can't find settings for seq %ld", sequence);
+        }
+    }
+
+    generateParametersL(sequence, param);
+    return OK;
+}
+
+int ParameterGenerator::getUserRequestId(long sequence, int32_t& userRequestId)
+{
+    CHECK_SEQUENCE(sequence);
+    long index = sequence % kStorageSize;
+
+    AutoMutex l(mParamsLock);
+    CheckError((sequence != mRequestParams[index].sequence), UNKNOWN_ERROR,
+               "Can't find requestId for seq %ld, saved %ld",
+               sequence, mRequestParams[index].sequence);
+
+    return mRequestParams[index].param.getUserRequestId(userRequestId);
+}
+
+int ParameterGenerator::getRequestId(long sequence, long& requestId)
+{
+    CHECK_SEQUENCE(sequence);
+    requestId = -1;
+    long index = sequence % kStorageSize;
+
+    AutoMutex l(mParamsLock);
+    CheckError((sequence != mRequestParams[index].sequence), UNKNOWN_ERROR,
+        "Can't find requestId for seq %ld, saved %ld:%ld",
+        sequence, mRequestParams[index].requestId, mRequestParams[index].sequence);
+
+    requestId = mRequestParams[index].requestId;
+    return OK;
+}
+
+int ParameterGenerator::generateParametersL(long sequence, Parameters *params)
+{
+    if (PlatformData::isEnableAIQ(mCameraId)) {
+        updateWithAiqResultsL(sequence, params);
+        updateTonemapCurve(sequence, params);
+    }
+    return OK;
+}
+
+int ParameterGenerator::updateWithAiqResultsL(long sequence, Parameters *params)
+{
+    const AiqResult *aiqResult = AiqResultStorage::getInstance(mCameraId)->getAiqResult(sequence);
+    CheckError((aiqResult == nullptr), UNKNOWN_ERROR,
+           "%s Aiq result of sequence %ld does not exist", __func__, sequence);
+
+    // Update AE related parameters
+    camera_ae_state_t aeState = aiqResult->mAeResults.exposures[0].converged ?
+            AE_STATE_CONVERGED : AE_STATE_NOT_CONVERGED;
+    params->setAeState(aeState);
+
+    if (CameraUtils::isMultiExposureCase(aiqResult->mTuningMode) &&
+        aiqResult->mAeResults.num_exposures > 1 && aiqResult->mAeResults.exposures[1].exposure) {
+        params->setExposureTime(aiqResult->mAeResults.exposures[1].exposure->exposure_time_us);
+    } else {
+        params->setExposureTime(aiqResult->mAeResults.exposures[0].exposure->exposure_time_us);
+    }
+    params->setSensitivityIso(aiqResult->mAeResults.exposures[0].exposure->iso);
+    float fps = 1000000.0 / aiqResult->mFrameDuration;
+    params->setFrameRate(fps);
+
+    // Update AWB related parameters
+    updateAwbGainsL(params, aiqResult->mAwbResults);
+    camera_color_transform_t ccm;
+    MEMCPY_S(ccm.color_transform, sizeof(ccm.color_transform),
+             aiqResult->mPaResults.color_conversion_matrix,
+             sizeof(aiqResult->mPaResults.color_conversion_matrix));
+    params->setColorTransform(ccm);
+
+    camera_color_gains_t colorGains;
+    colorGains.color_gains_rggb[0] = aiqResult->mPaResults.color_gains.r;
+    colorGains.color_gains_rggb[1] = aiqResult->mPaResults.color_gains.gr;
+    colorGains.color_gains_rggb[2] = aiqResult->mPaResults.color_gains.gb;
+    colorGains.color_gains_rggb[3] = aiqResult->mPaResults.color_gains.b;
+    params->setColorGains(colorGains);
+
+    camera_awb_state_t awbState = (fabs(aiqResult->mAwbResults.distance_from_convergence) < 0.001) ?
+            AWB_STATE_CONVERGED : AWB_STATE_NOT_CONVERGED;
+    params->setAwbState(awbState);
+
+    // Update AF related parameters
+    camera_af_state_t afState = \
+            (aiqResult->mAfResults.status == ia_aiq_af_status_local_search) ? AF_STATE_LOCAL_SEARCH
+          : (aiqResult->mAfResults.status == ia_aiq_af_status_extended_search) ? AF_STATE_EXTENDED_SEARCH
+          : (aiqResult->mAfResults.status == ia_aiq_af_status_success) ? AF_STATE_SUCCESS
+          : (aiqResult->mAfResults.status == ia_aiq_af_status_fail) ? AF_STATE_FAIL
+          : AF_STATE_IDLE;
+    params->setAfState(afState);
+    params->setFocusDistance(aiqResult->mAfDistanceDiopters);
+    params->setFocusRange(aiqResult->mFocusRange);
+
+    bool lensMoving = false;
+    if (afState == AF_STATE_LOCAL_SEARCH || afState == AF_STATE_EXTENDED_SEARCH) {
+        lensMoving = (aiqResult->mAfResults.final_lens_position_reached == false);
+    } else if (aiqResult->mAiqParam.afMode == AF_MODE_OFF) {
+        lensMoving = (aiqResult->mAfResults.lens_driver_action ? true : false);
+    }
+    params->setLensState(lensMoving);
+
+    // Update scene mode
+    camera_scene_mode_t sceneMode = SCENE_MODE_AUTO;
+    params->getSceneMode(sceneMode);
+
+    /* Use direct AE result to update sceneMode to reflect the actual mode AE want to have,
+     * Besides needed by full pipe auto-switch, this is also necessary when user want to
+     * switch pipe in user app according to AE result.
+     */
+    if (sceneMode == SCENE_MODE_AUTO) {
+        if (aiqResult->mAeResults.multiframe== ia_aiq_bracket_mode_hdr) {
+            sceneMode = SCENE_MODE_HDR;
+        } else if (aiqResult->mAeResults.multiframe == ia_aiq_bracket_mode_ull) {
+            sceneMode = SCENE_MODE_ULL;
+        }
+    }
+    LOG2("%s, sceneMode:%d", __func__, sceneMode);
+    params->setSceneMode(sceneMode);
+
+    camera_lens_shading_map_mode_type_t lensShadingMapMode = LENS_SHADING_MAP_MODE_OFF;
+    params->getLensShadingMapMode(lensShadingMapMode);
+    if (lensShadingMapMode == LENS_SHADING_MAP_MODE_ON) {
+        size_t size = aiqResult->mAiqParam.lensShadingMapSize.x *
+                      aiqResult->mAiqParam.lensShadingMapSize.y * 4;
+        params->setLensShadingMap(aiqResult->mAiqParam.lensShadingMap, size);
+    }
+
+    return updateCommonMetadata(params, aiqResult);
+}
+
+int ParameterGenerator::updateAwbGainsL(Parameters *params, const ia_aiq_awb_results &result)
+{
+    camera_awb_gains_t awbGains;
+    CLEAR(awbGains);
+    float normalizedR, normalizedG, normalizedB;
+
+    if (params->getAwbGains(awbGains) == OK) {
+        // User manual AWB gains
+        awbGains.g_gain = CLIP(awbGains.g_gain, AWB_GAIN_MAX, AWB_GAIN_MIN);
+        normalizedG = AiqUtils::normalizeAwbGain(awbGains.g_gain);
+    } else {
+        // non-manual AWB gains, try to find a proper G that makes R/G/B all in the gain range.
+        normalizedG = sqrt((AWB_GAIN_NORMALIZED_START * AWB_GAIN_NORMALIZED_END) / \
+                           (result.accurate_r_per_g * result.accurate_b_per_g));
+        awbGains.g_gain = AiqUtils::convertToUserAwbGain(normalizedG);
+    }
+
+    normalizedR = result.accurate_r_per_g * normalizedG;
+    normalizedB = result.accurate_b_per_g * normalizedG;
+
+    awbGains.r_gain = AiqUtils::convertToUserAwbGain(normalizedR);
+    awbGains.b_gain = AiqUtils::convertToUserAwbGain(normalizedB);
+
+    LOG2("awbGains [r, g, b] = [%d, %d, %d]", awbGains.r_gain, awbGains.g_gain, awbGains.b_gain);
+    params->setAwbGains(awbGains);
+
+    // Update the AWB result
+    camera_awb_result_t awbResult;
+    awbResult.r_per_g = result.accurate_r_per_g;
+    awbResult.b_per_g = result.accurate_b_per_g;
+    LOG2("awb result: %f, %f", awbResult.r_per_g, awbResult.b_per_g);
+    params->setAwbResult(&awbResult);
+
+    return OK;
+}
+
+int ParameterGenerator::updateTonemapCurve(long sequence, Parameters *params)
+{
+    if (!mTonemapMaxCurvePoints)
+        return OK;
+
+    const AiqResult *aiqResult = AiqResultStorage::getInstance(mCameraId)->getAiqResult(sequence);
+    CheckError((aiqResult == nullptr), UNKNOWN_ERROR,
+               "%s Aiq result of sequence %ld does not exist", __func__, sequence);
+    const ia_aiq_gbce_results &gbceResults = aiqResult->mGbceResults;
+
+    int multiplier = gbceResults.gamma_lut_size / mTonemapMaxCurvePoints;
+    for (int32_t i=0; i < mTonemapMaxCurvePoints; i++) {
+        mTonemapCurveRed[i * 2 + 1] = gbceResults.r_gamma_lut[i * multiplier];
+        mTonemapCurveBlue[i * 2 + 1] = gbceResults.g_gamma_lut[i * multiplier];
+        mTonemapCurveGreen[i * 2 + 1] = gbceResults.b_gamma_lut[i * multiplier];
+    }
+
+    int count = mTonemapMaxCurvePoints * 2;
+    camera_tonemap_curves_t curves =
+            {count, count, count,
+             mTonemapCurveRed.get(), mTonemapCurveBlue.get(), mTonemapCurveGreen.get()};
+    params->setTonemapCurves(curves);
+    return OK;
+}
+
+int ParameterGenerator::updateCommonMetadata(Parameters *params, const AiqResult *aiqResult) {
+    CameraMetadata metadata;
+
+    metadata.update(CAMERA_SENSOR_ROLLING_SHUTTER_SKEW, &aiqResult->mRollingShutter, 1);
+
+    ParameterHelper::merge(metadata, params);
+    return OK;
+}
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/metadata/ParameterGenerator.h b/camera/hal/intel/ipu6/src/metadata/ParameterGenerator.h
new file mode 100644
index 000000000000..91f3c427ee0d
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/metadata/ParameterGenerator.h
@@ -0,0 +1,115 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+
+#include "iutils/Thread.h"
+#include "Parameters.h"
+
+namespace icamera {
+
+/*
+ * \class ParameterGenerator
+ * This class is used to generator parameter results. It updates the parameters
+ * with AIQ results, sensor embedded metadata and 3A statistics.
+ * The parameter results are stored with the frame sequence indicating on which
+ * frame the parameters are active.
+ */
+class ParameterGenerator {
+public:
+    ParameterGenerator(int cameraId);
+    ~ParameterGenerator();
+
+    /**
+     * \brief reset the parameters data.
+     */
+    int reset();
+
+    /**
+     * \brief Save parameters with sequence id indicating the active frame.
+     *           And update the aiq result parameters as well.
+     */
+    int saveParameters(long predictSequence, long requestId, const Parameters *param = nullptr);
+
+    /**
+     * \brief Update parameters per sequence id.
+     */
+    void updateParameters(long sequence, const Parameters *param);
+    int getUserRequestId(long sequence, int32_t& userRequestId);
+
+    /**
+     * \brief Get the parameters for the frame indicated by the sequence id.
+     */
+    int getParameters(long sequence, Parameters *param, bool mergeResultOnly = true);
+    int getRequestId(long predictSequence, long& requestId);
+
+private:
+    ParameterGenerator(const ParameterGenerator& other);
+    ParameterGenerator& operator=(const ParameterGenerator& other);
+
+    int generateParametersL(long sequence, Parameters *params);
+    int updateWithAiqResultsL(long sequence, Parameters *params);
+    int updateAwbGainsL(Parameters *params, const ia_aiq_awb_results &result);
+    int updateTonemapCurve(long sequence, Parameters *params);
+
+    int updateCommonMetadata(Parameters *params, const AiqResult *aiqResult);
+
+private:
+    typedef enum {
+        RESULT_TYPE_AIQ = 1,
+        RESULT_TYPE_SENSOR_EMD = 1 << 1,
+        RESULT_TYPE_STATISTICS = 1 << 2
+    } ResultType;
+
+    class RequestParam {
+    public:
+        RequestParam() : sequence(-1), requestId(-1) {}
+
+        ~RequestParam() {}
+
+        void reset() {
+            sequence = -1;
+            requestId = -1;
+        }
+
+        long sequence;
+        long requestId;
+        Parameters param;
+
+    private:
+        RequestParam(const RequestParam& other);
+        RequestParam& operator=(const RequestParam& other);
+    };
+
+private:
+    int mCameraId;
+    static const int kStorageSize = MAX_SETTING_COUNT;
+
+    // Guard for ParameterGenerator public API.
+    Mutex mParamsLock;
+    RequestParam mRequestParams[kStorageSize];
+    long mLastSequence;
+    Parameters mLastParam;
+
+    std::unique_ptr<float[]> mTonemapCurveRed;
+    std::unique_ptr<float[]> mTonemapCurveBlue;
+    std::unique_ptr<float[]> mTonemapCurveGreen;
+    int32_t mTonemapMaxCurvePoints;
+};
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/metadata/ParameterHelper.cpp b/camera/hal/intel/ipu6/src/metadata/ParameterHelper.cpp
new file mode 100644
index 000000000000..578557f5caca
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/metadata/ParameterHelper.cpp
@@ -0,0 +1,84 @@
+/*
+ * Copyright (C) 2017-2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "ParameterHelper"
+
+#include "iutils/Utils.h"
+#include "iutils/CameraLog.h"
+
+#include "Parameters.h"
+#include "ParameterHelper.h"
+
+namespace icamera {
+
+void ParameterHelper::merge(const Parameters& src, Parameters* dst)
+{
+    AutoRLock rl(src.mData);
+    merge(getMetadata(src.mData), dst);
+}
+
+void ParameterHelper::merge(const CameraMetadata& metadata, Parameters* dst)
+{
+    if (metadata.isEmpty()) {
+        // Nothing needs to be merged
+        return;
+    }
+
+    AutoWLock wl(dst->mData);
+    const icamera_metadata_t* src = const_cast<CameraMetadata*>(&metadata)->getAndLock();
+    size_t count = metadata.entryCount();
+    icamera_metadata_ro_entry_t entry;
+    for (size_t i = 0; i < count; i++) {
+        CLEAR(entry);
+        if (get_icamera_metadata_ro_entry(src, i, &entry) != OK) {
+            continue;
+        }
+        switch (entry.type) {
+        case ICAMERA_TYPE_BYTE:
+            getMetadata(dst->mData).update(entry.tag, entry.data.u8, entry.count);
+            break;
+        case ICAMERA_TYPE_INT32:
+            getMetadata(dst->mData).update(entry.tag, entry.data.i32, entry.count);
+            break;
+        case ICAMERA_TYPE_FLOAT:
+            getMetadata(dst->mData).update(entry.tag, entry.data.f, entry.count);
+            break;
+        case ICAMERA_TYPE_INT64:
+            getMetadata(dst->mData).update(entry.tag, entry.data.i64, entry.count);
+            break;
+        case ICAMERA_TYPE_DOUBLE:
+            getMetadata(dst->mData).update(entry.tag, entry.data.d, entry.count);
+            break;
+        case ICAMERA_TYPE_RATIONAL:
+            getMetadata(dst->mData).update(entry.tag, entry.data.r, entry.count);
+            break;
+        default:
+            LOGW("Invalid entry type, should never happen");
+            break;
+        }
+    }
+    const_cast<CameraMetadata*>(&metadata)->unlock(src);
+}
+
+void ParameterHelper::copyMetadata(const Parameters& source, CameraMetadata* metadata)
+{
+    CheckError((!metadata), VOID_VALUE, "null metadata to be updated!");
+
+    AutoRLock rl(source.mData);
+    *metadata = getMetadata(source.mData);
+}
+
+} // end of namespace icamera
diff --git a/camera/hal/intel/ipu6/src/metadata/ParameterHelper.h b/camera/hal/intel/ipu6/src/metadata/ParameterHelper.h
new file mode 100644
index 000000000000..8e5cf6e25f81
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/metadata/ParameterHelper.h
@@ -0,0 +1,142 @@
+/*
+ * Copyright (C) 2017-2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "iutils/RWLock.h"
+#include "CameraMetadata.h"
+
+namespace icamera {
+
+class Parameters;
+
+/**
+ * \class ParameterHelper
+ *
+ * There are two main purposes of this class:
+ * 1. Provide some wrapper functions for the implementation of Parameters.
+ * 2. Provide some interface for HAL to access and modify Parameters internal data.
+ */
+class ParameterHelper {
+public:
+    /**
+     * \brief Merge and update dst parameter buffer with another parameter instance.
+     *
+     * \param[in] Parameters src: the source parameter.
+     * \param[out] Parameters dst: the parameter to be updated.
+     *
+     * \return void
+     */
+    static void merge(const Parameters& src, Parameters* dst);
+
+    /**
+     * \brief Merge and update dst parameter buffer by using metadata.
+     *
+     * \param[in] CameraMetadata metadata: the source metadata.
+     * \param[out] Parameters dst: the parameter to be updated.
+     *
+     * \return void
+     */
+    static void merge(const CameraMetadata& metadata, Parameters* dst);
+
+    /**
+     * \brief Copy metadata from parameter buffer.
+     *
+     * Some HAL V3 style parameters are not returned in the getParameter()
+     * because it is not used inside the HAL.
+     * Provide this function to assist upperlayer to return metadata
+     * to applications.
+     *
+     * \param[in] Parameters source: the parameter to provide metadata.
+     * \param[out] CameraMetadata metadata: the metadata to be updated.
+     *
+     * \return void
+     */
+    static void copyMetadata(const Parameters& source, CameraMetadata* metadata);
+
+private:
+    // The definitions and interfaces in this private section are only for Parameters internal
+    // use, HAL other code shouldn't and cannot access them.
+    friend class Parameters;
+
+    /**
+     * \class ParameterData
+     *
+     * \brief The definition of Parameters' internal data structure used to hide implementation
+     *        details of Parameters.
+     */
+    class ParameterData {
+    public:
+        ParameterData() {}
+        ~ParameterData() {}
+
+        ParameterData(const ParameterData& other) : mMetadata(other.mMetadata) {}
+        ParameterData& operator=(const ParameterData& other) {
+            mMetadata = other.mMetadata;
+            return *this;
+        }
+
+        CameraMetadata mMetadata; // The data structure to save all of the parameters.
+        RWLock mRwLock;           // Read-write lock to make Parameters class thread-safe
+    };
+
+    // Customized wrappers of RWLock to make the implementation of Parameters much cleaner.
+    class AutoRLock {
+    public:
+        AutoRLock(void* data) : mLock(getInternalData(data).mRwLock) { mLock.readLock(); }
+        ~AutoRLock() { mLock.unlock(); }
+    private:
+        RWLock& mLock;
+    };
+
+    class AutoWLock {
+    public:
+        AutoWLock(void* data) : mLock(getInternalData(data).mRwLock) { mLock.writeLock(); }
+        ~AutoWLock() { mLock.unlock(); }
+    private:
+        RWLock& mLock;
+    };
+
+    static ParameterData& getInternalData(void* data) {
+        return *reinterpret_cast<ParameterData*>(data);
+    }
+
+    static void* createParameterData() {
+        return new ParameterData();
+    }
+
+    static void* createParameterData(void* data) {
+        return new ParameterData(getInternalData(data));
+    }
+
+    static void releaseParameterData(void* data) {
+        delete &getInternalData(data);
+    }
+
+    static void deepCopy(void* srcData, void* dstData) {
+        getInternalData(dstData) = getInternalData(srcData);
+    }
+
+    static CameraMetadata& getMetadata(void* data) {
+        return getInternalData(data).mMetadata;
+    }
+
+    static icamera_metadata_ro_entry_t getMetadataEntry(void* data, uint32_t tag) {
+        return const_cast<const CameraMetadata*>(&getMetadata(data))->find(tag);
+    }
+};
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/metadata/Parameters.cpp b/camera/hal/intel/ipu6/src/metadata/Parameters.cpp
new file mode 100644
index 000000000000..e62c1f8fc4d9
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/metadata/Parameters.cpp
@@ -0,0 +1,2026 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "Parameters"
+
+#include "iutils/Errors.h"
+#include "iutils/CameraLog.h"
+#include "iutils/CameraDump.h"
+#include "iutils/Utils.h"
+
+#include "PlatformData.h"
+#include "Parameters.h"
+#include "ParameterHelper.h"
+
+using std::vector;
+
+namespace icamera {
+
+Parameters::Parameters() : mData(ParameterHelper::createParameterData()) {}
+
+Parameters::Parameters(const Parameters& other) :
+        mData(ParameterHelper::createParameterData(other.mData)) {}
+
+Parameters& Parameters::operator=(const Parameters& other)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    ParameterHelper::deepCopy(other.mData, mData);
+    return *this;
+}
+
+Parameters::~Parameters()
+{
+    ParameterHelper::releaseParameterData(mData);
+    mData = nullptr;
+}
+
+void Parameters::merge(const Parameters& other)
+{
+    ParameterHelper::merge(other, this);
+}
+
+int Parameters::setAeMode(camera_ae_mode_t aeMode)
+{
+    uint8_t mode = aeMode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AE_MODE, &mode, 1);
+}
+
+int Parameters::getAeMode(camera_ae_mode_t& aeMode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AE_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    aeMode = (camera_ae_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setAeState(camera_ae_state_t aeState)
+{
+    uint8_t state = aeState;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AE_STATE, &state, 1);
+}
+
+int Parameters::getAeState(camera_ae_state_t& aeState) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AE_STATE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    aeState = (camera_ae_state_t)entry.data.u8[0];
+    return OK;
+}
+
+static int setRegions(CameraMetadata& metadata, camera_window_list_t regions, int tag)
+{
+    if (regions.empty()) {
+        // Nothing to do with an empty parameter.
+        return INVALID_OPERATION;
+    }
+
+    const int ELEM_NUM = sizeof(camera_window_t) / sizeof(int);
+    int values[regions.size() * ELEM_NUM];
+    for (size_t i = 0; i < regions.size(); i++) {
+        values[i * ELEM_NUM] = regions[i].left;
+        values[i * ELEM_NUM + 1] = regions[i].top;
+        values[i * ELEM_NUM + 2] = regions[i].right;
+        values[i * ELEM_NUM + 3] = regions[i].bottom;
+        values[i * ELEM_NUM + 4] = regions[i].weight;
+    }
+
+    return metadata.update(tag, values, ARRAY_SIZE(values));
+}
+
+static int getRegions(icamera_metadata_ro_entry_t entry, camera_window_list_t& regions)
+{
+    regions.clear();
+    const int ELEM_NUM = sizeof(camera_window_t) / sizeof(int);
+    if (entry.count == 0 || entry.count % ELEM_NUM != 0) {
+        return NAME_NOT_FOUND;
+    }
+
+    camera_window_t w;
+    for (size_t i = 0; i < entry.count; i += ELEM_NUM) {
+        w.left = entry.data.i32[i];
+        w.top = entry.data.i32[i + 1];
+        w.right = entry.data.i32[i + 2];
+        w.bottom = entry.data.i32[i + 3];
+        w.weight = entry.data.i32[i + 4];
+        regions.push_back(w);
+    }
+
+    return OK;
+}
+
+int Parameters::setAeRegions(camera_window_list_t aeRegions)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return setRegions(ParameterHelper::getMetadata(mData), aeRegions, CAMERA_AE_REGIONS);
+}
+
+int Parameters::getAeRegions(camera_window_list_t& aeRegions) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    return getRegions(ParameterHelper::getMetadataEntry(mData, CAMERA_AE_REGIONS), aeRegions);
+}
+
+int Parameters::setAeLock(bool lock)
+{
+    uint8_t lockValue = lock ? 1 : 0;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AE_LOCK, &lockValue, 1);
+}
+
+int Parameters::getAeLock(bool& lock) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AE_LOCK);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    lock = entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setExposureTime(int64_t exposureTime)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_SENSOR_EXPOSURE_TIME, &exposureTime, 1);
+}
+
+int Parameters::getExposureTime(int64_t& exposureTime) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_SENSOR_EXPOSURE_TIME);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    exposureTime = entry.data.i64[0];
+    return OK;
+}
+
+int Parameters::setSensitivityGain(float gain)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_SENSITIVITY_GAIN, &gain, 1);
+}
+
+int Parameters::getSensitivityGain(float& gain) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_SENSITIVITY_GAIN);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    gain = entry.data.f[0];
+    return OK;
+}
+
+int Parameters::setSensitivityIso(int32_t iso)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_SENSOR_SENSITIVITY, &iso, 1);
+}
+
+int Parameters::getSensitivityIso(int32_t& iso) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_SENSOR_SENSITIVITY);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    iso = entry.data.i32[0];
+    return OK;
+}
+
+int Parameters::setAeCompensation(int ev)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AE_COMPENSATION, &ev, 1);
+}
+
+int Parameters::getAeCompensation(int& ev) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AE_COMPENSATION);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    ev = entry.data.i32[0];
+    return OK;
+}
+
+int Parameters::setFrameRate(float fps)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_FRAME_RATE, &fps, 1);
+}
+
+int Parameters::getFrameRate(float& fps) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_FRAME_RATE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    fps = entry.data.f[0];
+    return OK;
+}
+
+int Parameters::setAntiBandingMode(camera_antibanding_mode_t bandingMode)
+{
+    uint8_t mode = bandingMode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AE_ANTIBANDING_MODE, &mode, 1);
+}
+
+int Parameters::getAntiBandingMode(camera_antibanding_mode_t& bandingMode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AE_ANTIBANDING_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    bandingMode = (camera_antibanding_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setAwbMode(camera_awb_mode_t awbMode)
+{
+    uint8_t mode = awbMode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AWB_MODE, &mode, 1);
+}
+
+int Parameters::getAwbMode(camera_awb_mode_t& awbMode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AWB_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    awbMode = (camera_awb_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setAwbState(camera_awb_state_t awbState)
+{
+    uint8_t state = awbState;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AWB_STATE, &state, 1);
+}
+
+int Parameters::getAwbState(camera_awb_state_t& awbState) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AWB_STATE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    awbState = (camera_awb_state_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setAwbLock(bool lock)
+{
+    uint8_t lockValue = lock ? 1 : 0;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AWB_LOCK, &lockValue, 1);
+}
+
+int Parameters::getAwbLock(bool& lock) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AWB_LOCK);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    lock = entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setAwbCctRange(camera_range_t cct)
+{
+    int range[] = {(int)cct.min, (int)cct.max};
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AWB_CCT_RANGE, range, ARRAY_SIZE(range));
+}
+
+int Parameters::getAwbCctRange(camera_range_t& cct) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AWB_CCT_RANGE);
+    const size_t ELEM_NUM = sizeof(camera_range_t) / sizeof(int);
+    if (entry.count != ELEM_NUM) {
+        return NAME_NOT_FOUND;
+    }
+    cct.min = entry.data.i32[0];
+    cct.max = entry.data.i32[1];
+    return OK;
+}
+
+int Parameters::setAwbGains(camera_awb_gains_t awbGains)
+{
+    int values[] = {awbGains.r_gain, awbGains.g_gain, awbGains.b_gain};
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AWB_GAINS, values, ARRAY_SIZE(values));
+}
+
+int Parameters::getAwbGains(camera_awb_gains_t& awbGains) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AWB_GAINS);
+    const size_t ELEM_NUM = sizeof(camera_awb_gains_t) / sizeof(int);
+    if (entry.count != ELEM_NUM) {
+        return NAME_NOT_FOUND;
+    }
+    awbGains.r_gain = entry.data.i32[0];
+    awbGains.g_gain = entry.data.i32[1];
+    awbGains.b_gain = entry.data.i32[2];
+    return OK;
+}
+
+int Parameters::setAwbResult(void* data)
+{
+    uint32_t size = sizeof(camera_awb_result_t);
+    uint32_t tag = CAMERA_AWB_RESULT;
+    ParameterHelper::AutoWLock wl(mData);
+
+    if (data == NULL) {
+        return ParameterHelper::getMetadata(mData).erase(tag);
+    }
+    return ParameterHelper::getMetadata(mData).update(tag, (uint8_t*)data, size);
+}
+
+int Parameters::getAwbResult(void* data) const
+{
+    if (data == NULL) {
+        return BAD_VALUE;
+    }
+
+    uint32_t size = sizeof(camera_awb_result_t);
+    uint32_t tag = CAMERA_AWB_RESULT;
+    ParameterHelper::AutoRLock rl(mData);
+
+    auto entry = ParameterHelper::getMetadataEntry(mData, tag);
+    if (entry.count != size) {
+        return NAME_NOT_FOUND;
+    }
+
+    MEMCPY_S(data, size, entry.data.u8, size);
+
+    return OK;
+}
+
+int Parameters::setAwbGainShift(camera_awb_gains_t awbGainShift)
+{
+    int values[] = {awbGainShift.r_gain, awbGainShift.g_gain, awbGainShift.b_gain};
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AWB_GAIN_SHIFT, values, ARRAY_SIZE(values));
+}
+
+int Parameters::getAwbGainShift(camera_awb_gains_t& awbGainShift) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AWB_GAIN_SHIFT);
+    const size_t ELEM_NUM = sizeof(camera_awb_gains_t) / sizeof(int);
+    if (entry.count != ELEM_NUM) {
+        return NAME_NOT_FOUND;
+    }
+    awbGainShift.r_gain = entry.data.i32[0];
+    awbGainShift.g_gain = entry.data.i32[1];
+    awbGainShift.b_gain = entry.data.i32[2];
+    return OK;
+}
+
+int Parameters::setAwbWhitePoint(camera_coordinate_t whitePoint)
+{
+    int values[] = {whitePoint.x, whitePoint.y};
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AWB_WHITE_POINT, values, ARRAY_SIZE(values));
+}
+
+int Parameters::getAwbWhitePoint(camera_coordinate_t& whitePoint) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AWB_WHITE_POINT);
+    const size_t ELEM_NUM = sizeof(camera_coordinate_t) / sizeof(int);
+    if (entry.count != ELEM_NUM) {
+        return NAME_NOT_FOUND;
+    }
+
+    whitePoint.x = entry.data.i32[0];
+    whitePoint.y = entry.data.i32[1];
+
+    return OK;
+}
+
+int Parameters::setColorTransform(camera_color_transform_t colorTransform)
+{
+    float* transform = (float*)colorTransform.color_transform;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AWB_COLOR_TRANSFORM, transform, 3 * 3);
+}
+
+int Parameters::getColorTransform(camera_color_transform_t& colorTransform) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AWB_COLOR_TRANSFORM);
+    const size_t ELEM_NUM = 3 * 3;
+    if (entry.count != ELEM_NUM) {
+        return NAME_NOT_FOUND;
+    }
+    for (size_t i = 0; i < ELEM_NUM; i++) {
+        colorTransform.color_transform[i / 3][i % 3] = entry.data.f[i];
+    }
+
+    return OK;
+}
+
+int Parameters::setColorGains(camera_color_gains_t colorGains)
+{
+    float* gains = colorGains.color_gains_rggb;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AWB_COLOR_GAINS, gains, 4);
+}
+
+int Parameters::getColorGains(camera_color_gains_t& colorGains) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    icamera_metadata_ro_entry_t entry =
+        ParameterHelper::getMetadataEntry(mData, CAMERA_AWB_COLOR_GAINS);
+    const size_t ELEM_NUM = 4;
+    if (entry.count != ELEM_NUM) {
+        return NAME_NOT_FOUND;
+    }
+    for (size_t i = 0; i < ELEM_NUM; i++) {
+        colorGains.color_gains_rggb[i] = entry.data.f[i];
+    }
+    return OK;
+}
+
+int Parameters::setAwbRegions(camera_window_list_t awbRegions)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return setRegions(ParameterHelper::getMetadata(mData), awbRegions, CAMERA_AWB_REGIONS);
+}
+
+int Parameters::getAwbRegions(camera_window_list_t& awbRegions) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    return getRegions(ParameterHelper::getMetadataEntry(mData, CAMERA_AWB_REGIONS), awbRegions);
+}
+
+int Parameters::setNrMode(camera_nr_mode_t nrMode)
+{
+    uint8_t mode = nrMode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_NR_MODE, &mode, 1);
+}
+
+int Parameters::getNrMode(camera_nr_mode_t& nrMode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_NR_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    nrMode = (camera_nr_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setNrLevel(camera_nr_level_t level)
+{
+    int values [] = {level.overall, level.spatial, level.temporal};
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_NR_LEVEL, values, ARRAY_SIZE(values));
+}
+
+int Parameters::getNrLevel(camera_nr_level_t& level) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_NR_LEVEL);
+    const size_t ELEM_NUM = sizeof(camera_nr_level_t) / sizeof(int);
+    if (entry.count != ELEM_NUM) {
+        return NAME_NOT_FOUND;
+    }
+    level.overall = entry.data.i32[0];
+    level.spatial = entry.data.i32[1];
+    level.temporal = entry.data.i32[2];
+    return OK;
+}
+
+int Parameters::setIrisMode(camera_iris_mode_t irisMode)
+{
+    uint8_t mode = irisMode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_IRIS_MODE, &mode, 1);
+}
+
+int Parameters::getIrisMode(camera_iris_mode_t& irisMode)
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_IRIS_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    irisMode = (camera_iris_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setIrisLevel(int level)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_IRIS_LEVEL, &level, 1);
+}
+
+int Parameters::getIrisLevel(int& level)
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_IRIS_LEVEL);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    level = entry.data.i32[0];
+    return OK;
+}
+
+int Parameters::setWdrLevel(uint8_t level)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_WDR_LEVEL, &level, 1);
+}
+
+int Parameters::getWdrLevel(uint8_t& level) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_WDR_LEVEL);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    level = entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setEffectSceneMode(camera_scene_mode_t sceneMode)
+{
+    uint8_t mode = sceneMode;
+    LOGW("Effect scene mode is deprecated. Please use setSceneMode() instead.");
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_SCENE_MODE, &mode, 1);
+}
+
+int Parameters::getEffectSceneMode(camera_scene_mode_t& sceneMode) const
+{
+    LOGW("Effect scene mode is deprecated. Please use getSceneMode() instead.");
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_SCENE_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    sceneMode = (camera_scene_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setSceneMode(camera_scene_mode_t sceneMode)
+{
+    uint8_t mode = sceneMode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_SCENE_MODE, &mode, 1);
+}
+
+int Parameters::getSceneMode(camera_scene_mode_t& sceneMode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_SCENE_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    sceneMode = (camera_scene_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setWeightGridMode(camera_weight_grid_mode_t weightGridMode)
+{
+    uint8_t mode = (uint8_t)weightGridMode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_WEIGHT_GRID_MODE, &mode, 1);
+}
+
+int Parameters::getWeightGridMode(camera_weight_grid_mode_t& weightGridMode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_WEIGHT_GRID_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    weightGridMode = (camera_weight_grid_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setBlcAreaMode(camera_blc_area_mode_t blcAreaMode)
+{
+    uint8_t mode = blcAreaMode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_BLC_AREA_MODE, &mode, 1);
+}
+
+int Parameters::getBlcAreaMode(camera_blc_area_mode_t& blcAreaMode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_BLC_AREA_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    blcAreaMode = (camera_blc_area_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setFpsRange(camera_range_t fpsRange)
+{
+    float range[] = {fpsRange.min, fpsRange.max};
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AE_TARGET_FPS_RANGE, range, ARRAY_SIZE(range));
+}
+
+int Parameters::getFpsRange(camera_range_t& fpsRange) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AE_TARGET_FPS_RANGE);
+    const size_t ELEM_NUM = sizeof(camera_range_t) / sizeof(float);
+    if (entry.count != ELEM_NUM) {
+        return NAME_NOT_FOUND;
+    }
+    fpsRange.min = entry.data.f[0];
+    fpsRange.max = entry.data.f[1];
+    return OK;
+}
+
+int Parameters::setImageEnhancement(camera_image_enhancement_t effects)
+{
+    int values[] = {effects.sharpness, effects.brightness, effects.contrast, effects.hue, effects.saturation};
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_IMAGE_ENHANCEMENT, values, ARRAY_SIZE(values));
+}
+
+int Parameters::getImageEnhancement(camera_image_enhancement_t& effects) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_IMAGE_ENHANCEMENT);
+    size_t number_of_effects = sizeof(camera_image_enhancement_t) / sizeof(int);
+    if (entry.count != number_of_effects) {
+        return NAME_NOT_FOUND;
+    }
+    effects.sharpness = entry.data.i32[0];
+    effects.brightness = entry.data.i32[1];
+    effects.contrast = entry.data.i32[2];
+    effects.hue = entry.data.i32[3];
+    effects.saturation = entry.data.i32[4];
+
+    return OK;
+}
+
+int Parameters::setDeinterlaceMode(camera_deinterlace_mode_t deinterlaceMode)
+{
+    uint8_t mode = deinterlaceMode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_DEINTERLACE_MODE, &mode, 1);
+}
+
+int Parameters::getDeinterlaceMode(camera_deinterlace_mode_t &deinterlaceMode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_DEINTERLACE_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    deinterlaceMode = (camera_deinterlace_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::getSupportedVideoStabilizationMode(camera_video_stabilization_list_t &supportedModes) const
+{
+    supportedModes.clear();
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_CONTROL_AVAILABLE_VIDEO_STABILIZATION_MODES);
+    for (size_t i = 0; i < entry.count; i++) {
+        supportedModes.push_back((camera_video_stabilization_mode_t)entry.data.u8[i]);
+    }
+
+    return OK;
+}
+
+int Parameters::getSupportedAeMode(vector <camera_ae_mode_t> &supportedAeModes) const
+{
+    supportedAeModes.clear();
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AE_AVAILABLE_MODES);
+    for (size_t i = 0; i < entry.count; i++) {
+        supportedAeModes.push_back((camera_ae_mode_t)entry.data.u8[i]);
+    }
+
+    return OK;
+}
+
+int Parameters::getSupportedAwbMode(vector <camera_awb_mode_t> &supportedAwbModes) const
+{
+    supportedAwbModes.clear();
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AWB_AVAILABLE_MODES);
+    for (size_t i = 0; i < entry.count; i++) {
+        supportedAwbModes.push_back((camera_awb_mode_t)entry.data.u8[i]);
+    }
+
+    return OK;
+}
+
+int Parameters::getSupportedAfMode(vector <camera_af_mode_t> &supportedAfModes) const
+{
+    supportedAfModes.clear();
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AF_AVAILABLE_MODES);
+    for (size_t i = 0; i < entry.count; i++) {
+        supportedAfModes.push_back((camera_af_mode_t)entry.data.u8[i]);
+    }
+
+    return OK;
+}
+
+int Parameters::getSupportedSceneMode(vector <camera_scene_mode_t> &supportedSceneModes) const
+{
+    supportedSceneModes.clear();
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_CONTROL_AVAILABLE_SCENE_MODES);
+    for (size_t i = 0; i < entry.count; i++) {
+        supportedSceneModes.push_back((camera_scene_mode_t)entry.data.u8[i]);
+    }
+
+    return OK;
+}
+
+int Parameters::getSupportedAntibandingMode(vector <camera_antibanding_mode_t> &supportedAntibindingModes) const
+{
+    supportedAntibindingModes.clear();
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AE_AVAILABLE_ANTIBANDING_MODES);
+    for (size_t i = 0; i < entry.count; i++) {
+        supportedAntibindingModes.push_back((camera_antibanding_mode_t)entry.data.u8[i]);
+    }
+
+    return OK;
+}
+
+int Parameters::getSupportedFpsRange(camera_range_array_t& ranges) const
+{
+    ranges.clear();
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AE_AVAILABLE_TARGET_FPS_RANGES);
+    if (entry.count == 0 || entry.count % 2 != 0) {
+        return NAME_NOT_FOUND;
+    }
+
+    camera_range_t fps;
+    for (size_t i = 0; i < entry.count; i += 2) {
+        fps.min = entry.data.f[i];
+        fps.max = entry.data.f[i + 1];
+        ranges.push_back(fps);
+    }
+
+    return OK;
+}
+
+int Parameters::getSupportedStreamConfig(stream_array_t& config) const
+{
+    config.clear();
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_INFO_AVAILABLE_CONFIGURATIONS);
+    const int streamConfMemberNum = sizeof(stream_t) / sizeof(int);
+    if (entry.count == 0 || entry.count % streamConfMemberNum != 0) {
+        return NAME_NOT_FOUND;
+    }
+
+    stream_t cfg;
+    CLEAR(cfg);
+
+    for (size_t i = 0; i < entry.count; i += streamConfMemberNum) {
+        MEMCPY_S(&cfg, sizeof(stream_t), &entry.data.i32[i], sizeof(stream_t));
+        cfg.stride = CameraUtils::getStride(cfg.format, cfg.width);
+        cfg.size   = CameraUtils::getFrameSize(cfg.format, cfg.width, cfg.height);
+        config.push_back(cfg);
+    }
+    return OK;
+}
+
+int Parameters::getSupportedSensorExposureTimeRange(camera_range_t& range) const
+{
+    CLEAR(range);
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_SENSOR_INFO_EXPOSURE_TIME_RANGE);
+    if (entry.count != 2) {
+        return NAME_NOT_FOUND;
+    }
+
+    range.min = (float)(entry.data.i64[0]);
+    range.max = (float)(entry.data.i64[1]);
+    return OK;
+}
+
+int Parameters::getSupportedSensorSensitivityRange(camera_range_t& range) const
+{
+    CLEAR(range);
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_SENSOR_INFO_SENSITIVITY_RANGE);
+    if (entry.count != 2) {
+        return NAME_NOT_FOUND;
+    }
+
+    range.min = entry.data.i32[0];
+    range.max = entry.data.i32[1];
+    return OK;
+}
+
+int Parameters::getSupportedFeatures(camera_features_list_t& features) const
+{
+    features.clear();
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_INFO_AVAILABLE_FEATURES);
+    for (size_t i = 0; i < entry.count; i++) {
+        features.push_back((camera_features)entry.data.u8[i]);
+    }
+    return OK;
+}
+
+int Parameters::getAeCompensationRange(camera_range_t& evRange) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AE_COMPENSATION_RANGE);
+    const size_t ELEM_NUM = sizeof(camera_range_t) / sizeof(int);
+    if (entry.count != ELEM_NUM) {
+        return NAME_NOT_FOUND;
+    }
+
+    evRange.min = entry.data.i32[0];
+    evRange.max = entry.data.i32[1];
+    return OK;
+}
+
+int Parameters::getAeCompensationStep(camera_rational_t& evStep) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AE_COMPENSATION_STEP);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+
+    evStep.numerator = entry.data.r[0].numerator;
+    evStep.denominator = entry.data.r[0].denominator;
+    return OK;
+}
+
+int Parameters::getSupportedAeExposureTimeRange(std::vector<camera_ae_exposure_time_range_t> & etRanges) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+
+    const int MEMBER_COUNT = 3;
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_INFO_AE_EXPOSURE_TIME_RANGE);
+    if (entry.count == 0 || entry.count % MEMBER_COUNT != 0) {
+        return NAME_NOT_FOUND;
+    }
+
+    camera_ae_exposure_time_range_t range;
+    CLEAR(range);
+
+    for (size_t i = 0; i < entry.count; i += MEMBER_COUNT) {
+        range.scene_mode = (camera_scene_mode_t)entry.data.i32[i];
+        range.et_range.min = entry.data.i32[i + 1];
+        range.et_range.max = entry.data.i32[i + 2];
+        etRanges.push_back(range);
+    }
+    return OK;
+}
+
+int Parameters::getSupportedAeGainRange(std::vector<camera_ae_gain_range_t>& gainRanges) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+
+    const int MEMBER_COUNT = 3;
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_INFO_AE_GAIN_RANGE);
+    if (entry.count == 0 || entry.count % MEMBER_COUNT != 0) {
+        return NAME_NOT_FOUND;
+    }
+
+    camera_ae_gain_range_t range;
+    CLEAR(range);
+
+    for (size_t i = 0; i < entry.count; i += MEMBER_COUNT) {
+        range.scene_mode = (camera_scene_mode_t)entry.data.i32[i];
+        // Since we use int to store float, before storing it we multiply min and max by 100,
+        // so we need to divide 100 when giving them outside.
+        range.gain_range.min = (float)entry.data.i32[i + 1] / 100.0;
+        range.gain_range.max = (float)entry.data.i32[i + 2] / 100.0;
+        gainRanges.push_back(range);
+    }
+    return OK;
+}
+
+bool Parameters::getAeLockAvailable() const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AE_LOCK_AVAILABLE);
+    if (entry.count != 1) {
+        return false;
+    }
+
+    return (entry.data.u8[0] == CAMERA_AE_LOCK_AVAILABLE_TRUE);
+}
+
+bool Parameters::getAwbLockAvailable() const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AWB_LOCK_AVAILABLE);
+    if (entry.count != 1) {
+        return false;
+    }
+
+    return (entry.data.u8[0] == CAMERA_AWB_LOCK_AVAILABLE_TRUE);
+}
+
+int Parameters::setExposureTimeRange(camera_range_t exposureTimeRange)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    const int MEMBER_COUNT = 2;
+    int values[MEMBER_COUNT] = {(int)exposureTimeRange.min, (int)exposureTimeRange.max};
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_EXPOSURE_TIME_RANGE, values, MEMBER_COUNT);
+}
+
+int Parameters::getExposureTimeRange(camera_range_t& exposureTimeRange) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+
+    const int MEMBER_COUNT = 2;
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_EXPOSURE_TIME_RANGE);
+    if (entry.count == 0 || entry.count != MEMBER_COUNT) {
+        return NAME_NOT_FOUND;
+    }
+
+    exposureTimeRange.min = entry.data.i32[0];
+    exposureTimeRange.max = entry.data.i32[1];
+    return OK;
+}
+
+int Parameters::setSensitivityGainRange(camera_range_t sensitivityGainRange)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    float values[] = {sensitivityGainRange.min, sensitivityGainRange.max};
+
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_SENSITIVITY_GAIN_RANGE, values, ARRAY_SIZE(values));
+}
+
+int Parameters::getSensitivityGainRange(camera_range_t& sensitivityGainRange) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+
+    const int MEMBER_COUNT = 2;
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_SENSITIVITY_GAIN_RANGE);
+    if (entry.count == 0 || entry.count != MEMBER_COUNT) {
+        return NAME_NOT_FOUND;
+    }
+
+    sensitivityGainRange.min = entry.data.f[0];
+    sensitivityGainRange.max = entry.data.f[1];
+    return OK;
+}
+
+int Parameters::setAeConvergeSpeed(camera_converge_speed_t speed)
+{
+    uint8_t aeSpeed = speed;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_AE_CONVERGE_SPEED, &aeSpeed, 1);
+}
+
+int Parameters::getAeConvergeSpeed(camera_converge_speed_t& speed) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_AE_CONVERGE_SPEED);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+
+    speed = (camera_converge_speed_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setAwbConvergeSpeed(camera_converge_speed_t speed)
+{
+    uint8_t awbSpeed = speed;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AWB_CONVERGE_SPEED, &awbSpeed, 1);
+}
+
+int Parameters::getAwbConvergeSpeed(camera_converge_speed_t& speed) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AWB_CONVERGE_SPEED);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+
+    speed = (camera_converge_speed_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setAeConvergeSpeedMode(camera_converge_speed_mode_t mode)
+{
+    uint8_t speedMode = mode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_AE_CONVERGE_SPEED_MODE, &speedMode, 1);
+}
+
+int Parameters::getAeConvergeSpeedMode(camera_converge_speed_mode_t& mode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_AE_CONVERGE_SPEED_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+
+    mode = (camera_converge_speed_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setAwbConvergeSpeedMode(camera_converge_speed_mode_t mode)
+{
+    uint8_t speedMode = mode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AWB_CONVERGE_SPEED_MODE, &speedMode, 1);
+}
+
+int Parameters::getAwbConvergeSpeedMode(camera_converge_speed_mode_t& mode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AWB_CONVERGE_SPEED_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+
+    mode = (camera_converge_speed_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setMakernoteData(const void* data, unsigned int size)
+{
+    CheckError(!data || size == 0, BAD_VALUE, "%s, invalid parameters", __func__);
+    ParameterHelper::AutoWLock wl(mData);
+
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_MAKERNOTE_DATA, (uint8_t*)data, size);
+}
+
+int Parameters::getMakernoteData(void* data, unsigned int* size) const
+{
+    CheckError(!data || !size, BAD_VALUE, "%s, invalid parameters", __func__);
+    ParameterHelper::AutoRLock rl(mData);
+
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_MAKERNOTE_DATA);
+    if (entry.count > 0) {
+        MEMCPY_S(data, *size, entry.data.u8, entry.count);
+        *size = entry.count;
+    } else {
+        return NAME_NOT_FOUND;
+    }
+
+    return OK;
+}
+
+int Parameters::setCustomAicParam(const void* data, unsigned int length)
+{
+    CheckError(!data, BAD_VALUE, "%s, invalid parameters", __func__);
+    ParameterHelper::AutoWLock wl(mData);
+
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_CUSTOM_AIC_PARAM, (uint8_t*)data, length);
+}
+
+int Parameters::getCustomAicParam(void* data, unsigned int* length) const
+{
+    CheckError(!data || !length, BAD_VALUE, "%s, invalid parameters", __func__);
+    ParameterHelper::AutoRLock rl(mData);
+
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_CUSTOM_AIC_PARAM);
+    if (entry.count > 0) {
+        MEMCPY_S(data, *length, entry.data.u8, entry.count);
+        *length = entry.count;
+    } else {
+        return NAME_NOT_FOUND;
+    }
+
+    return OK;
+}
+
+int Parameters::setMakernoteMode(camera_makernote_mode_t mode)
+{
+    uint8_t mknMode = mode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_MAKERNOTE_MODE, &mknMode, 1);
+}
+
+int Parameters::getMakernoteMode(camera_makernote_mode_t &mode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_MAKERNOTE_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+
+    mode = (camera_makernote_mode_t)entry.data.u8[0];
+
+    return OK;
+}
+
+int Parameters::setDigitalZoomRatio(float ratio)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_DIGITAL_ZOOM_RATIO, &ratio, 1);
+}
+
+int Parameters::getDigitalZoomRatio(float& ratio) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_DIGITAL_ZOOM_RATIO);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    ratio = entry.data.f[0];
+    return OK;
+}
+
+int Parameters::setLdcMode(camera_ldc_mode_t mode)
+{
+    uint8_t ldcMode = mode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_LDC_MODE, &ldcMode, 1);
+}
+
+int Parameters::getLdcMode(camera_ldc_mode_t &mode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_LDC_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    mode = (camera_ldc_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setRscMode(camera_rsc_mode_t mode)
+{
+    uint8_t rscMode = mode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_RSC_MODE, &rscMode, 1);
+}
+
+int Parameters::getRscMode(camera_rsc_mode_t &mode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_RSC_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    mode = (camera_rsc_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setFlipMode(camera_flip_mode_t mode)
+{
+    uint8_t flipMode = mode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_FLIP_MODE, &flipMode, 1);
+}
+
+int Parameters::getFlipMode(camera_flip_mode_t &mode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_FLIP_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    mode = (camera_flip_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setMonoDsMode(camera_mono_downscale_mode_t mode)
+{
+    uint8_t monoDsMode = mode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_MONO_DOWNSCALE, &monoDsMode, 1);
+}
+
+int Parameters::getMonoDsMode(camera_mono_downscale_mode_t &mode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_MONO_DOWNSCALE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    mode = (camera_mono_downscale_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setRun3ACadence(int cadence)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_RUN3_A_CADENCE, &cadence, 1);
+}
+
+int Parameters::getRun3ACadence(int &cadence) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_RUN3_A_CADENCE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    cadence = entry.data.i32[0];
+    return OK;
+}
+
+int Parameters::setFisheyeDewarpingMode(camera_fisheye_dewarping_mode_t mode)
+{
+    uint8_t dewarpingMode = mode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_FISHEYE_DEWARPING_MODE, &dewarpingMode, 1);
+}
+
+int Parameters::getFisheyeDewarpingMode(camera_fisheye_dewarping_mode_t &mode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_FISHEYE_DEWARPING_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    mode = (camera_fisheye_dewarping_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setAeDistributionPriority(camera_ae_distribution_priority_t priority)
+{
+    uint8_t distributionPriority = priority;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_AE_DISTRIBUTION_PRIORITY, &distributionPriority, 1);
+}
+
+int Parameters::getAeDistributionPriority(camera_ae_distribution_priority_t& priority) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_AE_DISTRIBUTION_PRIORITY);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+
+    priority = (camera_ae_distribution_priority_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setYuvColorRangeMode(camera_yuv_color_range_mode_t colorRange)
+{
+    uint8_t mode = colorRange;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(INTEL_CONTROL_YUV_COLOR_RANGE, &mode, 1);
+}
+
+int Parameters::getYuvColorRangeMode(camera_yuv_color_range_mode_t& colorRange) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_CONTROL_YUV_COLOR_RANGE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+
+    colorRange = (camera_yuv_color_range_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setJpegQuality(uint8_t quality)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_JPEG_QUALITY, &quality, 1);
+}
+
+int Parameters::getJpegQuality(uint8_t *quality) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_JPEG_QUALITY);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    *quality = entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setJpegThumbnailQuality(uint8_t quality)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_JPEG_THUMBNAIL_QUALITY, &quality, 1);
+}
+
+int Parameters::getJpegThumbnailQuality(uint8_t *quality) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_JPEG_THUMBNAIL_QUALITY);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    *quality = entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setJpegThumbnailSize(const camera_resolution_t& res)
+{
+    int size[2] = {res.width, res.height};
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_JPEG_THUMBNAIL_SIZE, size, 2);
+}
+
+int Parameters::getJpegThumbnailSize(camera_resolution_t& res) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_JPEG_THUMBNAIL_SIZE);
+    if (entry.count != 2) {
+        return NAME_NOT_FOUND;
+    }
+    res.width  = entry.data.i32[0];
+    res.height = entry.data.i32[1];
+    return OK;
+}
+
+int Parameters::setJpegRotation(int rotation)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_JPEG_ORIENTATION, &rotation, 1);
+}
+
+int Parameters::getJpegRotation(int &rotation) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_JPEG_ORIENTATION);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    rotation = entry.data.i32[0];
+    return OK;
+}
+
+int Parameters::setJpegGpsCoordinates(const double *coordinates)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_JPEG_GPS_COORDINATES, coordinates, 3);
+}
+
+int Parameters::getJpegGpsLatitude(double &latitude) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_JPEG_GPS_COORDINATES);
+    if (entry.count != 3) {
+        return NAME_NOT_FOUND;
+    }
+    latitude = entry.data.d[0];
+    return OK;
+}
+
+int Parameters::getJpegGpsLongitude(double &longitude) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_JPEG_GPS_COORDINATES);
+    if (entry.count != 3) {
+        return NAME_NOT_FOUND;
+    }
+    longitude = entry.data.d[1];
+    return OK;
+}
+
+int Parameters::getJpegGpsAltitude(double &altitude) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_JPEG_GPS_COORDINATES);
+    if (entry.count != 3) {
+        return NAME_NOT_FOUND;
+    }
+    altitude = entry.data.d[2];
+    return OK;
+}
+
+int Parameters::setJpegGpsTimeStamp(int64_t  timestamp)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_JPEG_GPS_TIMESTAMP, &timestamp, 1);
+}
+
+int Parameters::getJpegGpsTimeStamp(int64_t &timestamp) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_JPEG_GPS_TIMESTAMP);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    timestamp = entry.data.i32[0];
+    return OK;
+}
+
+int Parameters::setJpegGpsProcessingMethod(int processMethod)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_JPEG_GPS_PROCESSING_METHOD, &processMethod, 1);
+}
+
+int Parameters::getJpegGpsProcessingMethod(int &processMethod) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_JPEG_GPS_PROCESSING_METHOD);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    processMethod = entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setJpegGpsProcessingMethod(const char* processMethod)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_JPEG_GPS_PROCESSING_METHOD, (const uint8_t*)processMethod, strlen(processMethod) + 1);
+}
+
+int Parameters::getJpegGpsProcessingMethod(int size, char* processMethod) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_JPEG_GPS_PROCESSING_METHOD);
+    if (entry.count <= 0) {
+        return NAME_NOT_FOUND;
+    }
+    MEMCPY_S(processMethod, size, entry.data.u8, entry.count);
+    return OK;
+}
+
+int Parameters::setImageEffect(camera_effect_mode_t  effect)
+{
+    uint8_t effectmode = effect;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_CONTROL_EFFECT_MODE, &effectmode, 1);
+}
+
+int Parameters::getImageEffect(camera_effect_mode_t &effect) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_CONTROL_EFFECT_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    effect = (camera_effect_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setVideoStabilizationMode(camera_video_stabilization_mode_t mode)
+{
+    uint8_t dvsMode = mode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_CONTROL_VIDEO_STABILIZATION_MODE, &dvsMode, 1);
+}
+
+int Parameters::getVideoStabilizationMode(camera_video_stabilization_mode_t &mode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_CONTROL_VIDEO_STABILIZATION_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    mode = (camera_video_stabilization_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::getFocalLength(float &focal) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_LENS_FOCAL_LENGTH);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    focal = (float)entry.data.f[0];
+    return OK;
+}
+
+int Parameters::setFocalLength(float focal)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_LENS_FOCAL_LENGTH, &focal, 1);
+}
+
+int Parameters::getAperture(float &aperture) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_LENS_APERTURE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    aperture = entry.data.f[0];
+    return OK;
+}
+
+int Parameters::setAperture(float aperture)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_LENS_APERTURE, &aperture, 1);
+}
+
+int Parameters::getFocusDistance(float &distance) const {
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_LENS_FOCUS_DISTANCE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    distance = entry.data.f[0];
+    return OK;
+}
+
+int Parameters::setFocusDistance(float distance) {
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_LENS_FOCUS_DISTANCE, &distance, 1);
+}
+
+int Parameters::setFocusRange(const camera_range_t &focusRange)
+{
+    float range[] = {focusRange.min, focusRange.max};
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_LENS_FOCUS_RANGE,
+                                                      range, ARRAY_SIZE(range));
+}
+
+int Parameters::getFocusRange(camera_range_t& focusRange) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_LENS_FOCUS_RANGE);
+    if (entry.count != (sizeof(camera_range_t) / sizeof(float))) {
+        return NAME_NOT_FOUND;
+    }
+    focusRange.min = entry.data.f[0];
+    focusRange.max = entry.data.f[1];
+    return OK;
+}
+
+int Parameters::setAfMode(camera_af_mode_t afMode)
+{
+    uint8_t mode = afMode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AF_MODE, &mode, 1);
+}
+
+int Parameters::getAfMode(camera_af_mode_t& afMode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AF_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    afMode = (camera_af_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setAfTrigger(camera_af_trigger_t afTrigger)
+{
+    uint8_t trigger = afTrigger;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AF_TRIGGER, &trigger, 1);
+}
+
+int Parameters::getAfTrigger(camera_af_trigger_t& afTrigger) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AF_TRIGGER);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    afTrigger = (camera_af_trigger_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setAfRegions(camera_window_list_t afRegions)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return setRegions(ParameterHelper::getMetadata(mData), afRegions, CAMERA_AF_REGIONS);
+}
+
+int Parameters::getAfRegions(camera_window_list_t& afRegions) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    return getRegions(ParameterHelper::getMetadataEntry(mData, CAMERA_AF_REGIONS), afRegions);
+}
+
+int Parameters::setAfState(camera_af_state_t afState)
+{
+    uint8_t state = afState;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_AF_STATE, &state, 1);
+}
+
+int Parameters::getAfState(camera_af_state_t& afState) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_AF_STATE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    afState = (camera_af_state_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setLensState(bool lensMoving)
+{
+    uint8_t state = (lensMoving) ? 1 : 0;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_LENS_STATE, &state, 1);
+}
+
+int Parameters::getLensState(bool& lensMoving) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_LENS_STATE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    lensMoving = (entry.data.u8[0] > 0);
+    return OK;
+}
+
+int Parameters::getLensAperture(float &aperture) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_LENS_INFO_AVAILABLE_APERTURES);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    aperture = entry.data.f[0];
+    return OK;
+}
+
+int Parameters::getLensFilterDensity(float &filterDensity) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData,
+                                                   CAMERA_LENS_INFO_AVAILABLE_FILTER_DENSITIES);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    filterDensity = entry.data.f[0];
+    return OK;
+}
+
+int Parameters::getLensMinFocusDistance(float &minFocusDistance) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    minFocusDistance = entry.data.f[0];
+    return OK;
+}
+
+int Parameters::getLensHyperfocalDistance(float &hyperfocalDistance) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_LENS_INFO_HYPERFOCAL_DISTANCE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    hyperfocalDistance = entry.data.f[0];
+    return OK;
+}
+
+int Parameters::getSensorMountType(camera_mount_type_t& sensorMountType) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, INTEL_INFO_SENSOR_MOUNT_TYPE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+
+    sensorMountType = (camera_mount_type_t)entry.data.u8[0];
+    return OK;
+}
+
+// User can set envrionment and then call api to update the debug level.
+int Parameters::updateDebugLevel()
+{
+    Log::setDebugLevel();
+    CameraDump::setDumpLevel();
+    return OK;
+}
+
+int Parameters::setTestPatternMode(camera_test_pattern_mode_t mode)
+{
+    int32_t testPatterMode = mode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_SENSOR_TEST_PATTERN_MODE, &testPatterMode, 1);
+}
+
+int Parameters::getTestPatternMode(camera_test_pattern_mode_t& mode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_SENSOR_TEST_PATTERN_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    mode = (camera_test_pattern_mode_t)entry.data.i32[0];
+    return OK;
+}
+
+int Parameters::setCropRegion(camera_crop_region_t cropRegion)
+{
+    int values[] = {cropRegion.flag, cropRegion.x, cropRegion.y};
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_SCALER_CROP_REGION, values, ARRAY_SIZE(values));
+}
+
+int Parameters::getCropRegion(camera_crop_region_t& cropRegion) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_SCALER_CROP_REGION);
+    if (entry.count <= 0) {
+        return NAME_NOT_FOUND;
+    }
+    cropRegion.flag = entry.data.i32[0];
+    cropRegion.x = entry.data.i32[1];
+    cropRegion.y = entry.data.i32[2];
+    return OK;
+}
+
+int Parameters::setControlSceneMode(uint8_t sceneModeValue)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_CONTROL_SCENE_MODE, &sceneModeValue, 1);
+}
+
+int Parameters::setFaceDetectMode(uint8_t faceDetectMode)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_STATISTICS_FACE_DETECT_MODE, &faceDetectMode, 1);
+}
+
+int Parameters::getFaceDetectMode(uint8_t& faceDetectMode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_STATISTICS_FACE_DETECT_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    faceDetectMode = entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setFaceIds(int *faceIds, int faceNum)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_STATISTICS_FACE_IDS, faceIds, faceNum);
+}
+
+int Parameters::getSensorActiveArraySize(camera_coordinate_system_t& arraySize) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE);
+    if (entry.count <= 0) {
+        return NAME_NOT_FOUND;
+    }
+    arraySize.left = entry.data.i32[0];
+    arraySize.top = entry.data.i32[1];
+    arraySize.right = arraySize.left + entry.data.i32[2]; //width
+    arraySize.bottom = arraySize.top + entry.data.i32[3]; //height
+    return OK;
+}
+
+int Parameters::setShadingMode(camera_shading_mode_t shadingMode)
+{
+    uint8_t mode = shadingMode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_SHADING_MODE, &mode, 1);
+}
+
+int Parameters::getShadingMode(camera_shading_mode_t& shadingMode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_SHADING_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    shadingMode = (camera_shading_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setLensShadingMapMode(camera_lens_shading_map_mode_type_t lensShadingMapMode)
+{
+    uint8_t mode = lensShadingMapMode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_STATISTICS_LENS_SHADING_MAP_MODE,
+                                                      &mode, 1);
+}
+
+int Parameters::getLensShadingMapMode(
+                                  camera_lens_shading_map_mode_type_t &lensShadingMapMode) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_STATISTICS_LENS_SHADING_MAP_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    lensShadingMapMode = (camera_lens_shading_map_mode_type_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setLensShadingMap(const float *lensShadingMap, size_t lensShadingMapSize)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_STATISTICS_LENS_SHADING_MAP,
+                                                      lensShadingMap, lensShadingMapSize);
+}
+
+int Parameters::getLensShadingMap(float **lensShadingMap, size_t &lensShadingMapSize) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_STATISTICS_LENS_SHADING_MAP);
+
+    camera_coordinate_t shadingMapSize;
+    getLensInfoShadingMapSize(shadingMapSize);
+    const size_t lsmSize = shadingMapSize.x * shadingMapSize.y * 4;
+    if (entry.count < lsmSize || !entry.data.f || !lensShadingMap) {
+        return NAME_NOT_FOUND;
+    }
+
+    *lensShadingMap = const_cast<float*>(entry.data.f);
+    lensShadingMapSize = lsmSize;
+    return OK;
+}
+
+int Parameters::getLensInfoShadingMapSize(camera_coordinate_t &shadingMapSize) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_LENS_INFO_SHADING_MAP_SIZE);
+    if (entry.count <= 0) {
+        return NAME_NOT_FOUND;
+    }
+
+    shadingMapSize.x = entry.data.i32[0];  // width
+    shadingMapSize.y = entry.data.i32[1];  // height
+    return OK;
+}
+
+int Parameters::setTonemapMode(camera_tonemap_mode_t mode) {
+    uint8_t tMode = mode;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_TONEMAP_MODE, &tMode, 1);
+}
+
+int Parameters::getTonemapMode(camera_tonemap_mode_t& mode) const {
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_TONEMAP_MODE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    mode = (camera_tonemap_mode_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::getSupportedTonemapMode(vector<camera_tonemap_mode_t>& tonemapModes) const {
+    ParameterHelper::AutoRLock rl(mData);
+
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_TONEMAP_AVAILABLE_TONE_MAP_MODES);
+    for (size_t i = 0; i < entry.count; i++) {
+        tonemapModes.push_back((camera_tonemap_mode_t)entry.data.u8[i]);
+    }
+    return OK;
+}
+
+int Parameters::setTonemapPresetCurve(camera_tonemap_preset_curve_t type) {
+    uint8_t cType = type;
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_TONEMAP_PRESET_CURVE, &cType, 1);
+}
+
+int Parameters::getTonemapPresetCurve(camera_tonemap_preset_curve_t& type) const {
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_TONEMAP_PRESET_CURVE);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    type = (camera_tonemap_preset_curve_t)entry.data.u8[0];
+    return OK;
+}
+
+int Parameters::setTonemapGamma(float gamma) {
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_TONEMAP_GAMMA, &gamma, 1);
+}
+
+int Parameters::getTonemapGamma(float& gamma) const {
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_TONEMAP_GAMMA);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    gamma = entry.data.f[0];
+    return OK;
+}
+
+int Parameters::getTonemapMaxCurvePoints(int32_t& number) const {
+    ParameterHelper::AutoRLock rl(mData);
+
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_TONEMAP_MAX_CURVE_POINTS);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    number = entry.data.i32[0];
+    return OK;
+}
+
+int Parameters::setTonemapCurves(const camera_tonemap_curves_t& curves) {
+    ParameterHelper::AutoWLock wl(mData);
+    int ret = ParameterHelper::getMetadata(mData).update(CAMERA_TONEMAP_CURVE_RED,
+                                                         curves.rCurve, curves.rSize);
+    ret |= ParameterHelper::getMetadata(mData).update(CAMERA_TONEMAP_CURVE_BLUE,
+                                                      curves.bCurve, curves.bSize);
+    ret |= ParameterHelper::getMetadata(mData).update(CAMERA_TONEMAP_CURVE_GREEN,
+                                                      curves.gCurve, curves.gSize);
+    return ret;
+}
+
+int Parameters::getTonemapCurves(camera_tonemap_curves_t& curves) const {
+    curves.rSize = 0;
+    curves.bSize = 0;
+    curves.gSize = 0;
+
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_TONEMAP_CURVE_RED);
+    if (entry.count > 0) {
+        curves.rSize = entry.count;
+        curves.rCurve = entry.data.f;
+    }
+    entry = ParameterHelper::getMetadataEntry(mData, CAMERA_TONEMAP_CURVE_BLUE);
+    if (entry.count > 0) {
+        curves.bSize = entry.count;
+        curves.bCurve = entry.data.f;
+    }
+    entry = ParameterHelper::getMetadataEntry(mData, CAMERA_TONEMAP_CURVE_GREEN);
+    if (entry.count > 0) {
+        curves.gSize = entry.count;
+        curves.gCurve = entry.data.f;
+    }
+    return (curves.rSize && curves.bSize && curves.gSize) ? OK : NAME_NOT_FOUND;
+}
+
+int Parameters::setUserRequestId(int32_t userRequestId) {
+    ParameterHelper::AutoWLock wl(mData);
+
+    return ParameterHelper::getMetadata(mData).update(CAMERA_REQUEST_ID, &userRequestId, 1);
+}
+
+int Parameters::getUserRequestId(int32_t& userRequestId) const {
+    ParameterHelper::AutoRLock rl(mData);
+
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_REQUEST_ID);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+
+    userRequestId = entry.data.i32[0];
+    return OK;
+}
+
+int Parameters::setCaptureIntent(uint8_t captureIntent)
+{
+    ParameterHelper::AutoWLock wl(mData);
+    return ParameterHelper::getMetadata(mData).update(CAMERA_CONTROL_CAPTUREINTENT,
+                                                      &captureIntent, 1);
+}
+
+int Parameters::getCaptureIntent(uint8_t& captureIntent) const
+{
+    ParameterHelper::AutoRLock rl(mData);
+    auto entry = ParameterHelper::getMetadataEntry(mData, CAMERA_CONTROL_CAPTUREINTENT);
+    if (entry.count != 1) {
+        return NAME_NOT_FOUND;
+    }
+    captureIntent = entry.data.u8[0];
+    return OK;
+}
+
+} // end of namespace icamera
diff --git a/camera/hal/intel/ipu6/src/metadata/icamera_metadata_base.cpp b/camera/hal/intel/ipu6/src/metadata/icamera_metadata_base.cpp
new file mode 100644
index 000000000000..52ffd86ded71
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/metadata/icamera_metadata_base.cpp
@@ -0,0 +1,976 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ * Copyright (C) 2015-2018 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "camera_metadata"
+
+#include <inttypes.h>
+#include "icamera_metadata_base.h"
+#include "iutils/Utils.h"
+#include <assert.h>
+#include <stdlib.h>
+#include <errno.h>
+
+// TODO need to find out the correct print function
+#include <stdio.h>
+#define ALOGE printf
+
+#define OK         0
+#define ERROR      1
+#define NOT_FOUND (-ENOENT)
+
+/**
+ * A single metadata entry, storing an array of values of a given type. If the
+ * array is no larger than 4 bytes in size, it is stored in the data.value[]
+ * array; otherwise, it can found in the parent's data array at index
+ * data.offset.
+ */
+#define ENTRY_ALIGNMENT ((size_t) 4)
+typedef struct camera_metadata_buffer_entry {
+    uint32_t tag;
+    uint32_t count;
+    union {
+        uint32_t offset;
+        uint8_t  value[4];
+    } data;
+    uint8_t  type;
+    uint8_t  reserved[3];
+} camera_metadata_buffer_entry_t;
+
+typedef uint32_t metadata_uptrdiff_t;
+typedef uint32_t metadata_size_t;
+
+/**
+ * A packet of metadata. This is a list of entries, each of which may point to
+ * its values stored at an offset in data.
+ *
+ * It is assumed by the utility functions that the memory layout of the packet
+ * is as follows:
+ *
+ *   |-----------------------------------------------|
+ *   | icamera_metadata_t                             |
+ *   |                                               |
+ *   |-----------------------------------------------|
+ *   | reserved for future expansion                 |
+ *   |-----------------------------------------------|
+ *   | camera_metadata_buffer_entry_t #0             |
+ *   |-----------------------------------------------|
+ *   | ....                                          |
+ *   |-----------------------------------------------|
+ *   | camera_metadata_buffer_entry_t #entry_count-1 |
+ *   |-----------------------------------------------|
+ *   | free space for                                |
+ *   | (entry_capacity-entry_count) entries          |
+ *   |-----------------------------------------------|
+ *   | start of camera_metadata.data                 |
+ *   |                                               |
+ *   |-----------------------------------------------|
+ *   | free space for                                |
+ *   | (data_capacity-data_count) bytes              |
+ *   |-----------------------------------------------|
+ *
+ * With the total length of the whole packet being camera_metadata.size bytes.
+ *
+ * In short, the entries and data are contiguous in memory after the metadata
+ * header.
+ */
+#define METADATA_ALIGNMENT ((size_t) 4)
+struct icamera_metadata {
+    metadata_size_t          size;
+    uint32_t                 version;
+    uint32_t                 flags;
+    metadata_size_t          entry_count;
+    metadata_size_t          entry_capacity;
+    metadata_uptrdiff_t      entries_start; // Offset from camera_metadata
+    metadata_size_t          data_count;
+    metadata_size_t          data_capacity;
+    metadata_uptrdiff_t      data_start; // Offset from camera_metadata
+    uint8_t                  reserved[];
+};
+
+/**
+ * A datum of metadata. This corresponds to icamera_metadata_entry_t::data
+ * with the difference that each element is not a pointer. We need to have a
+ * non-pointer type description in order to figure out the largest alignment
+ * requirement for data (DATA_ALIGNMENT).
+ */
+#define DATA_ALIGNMENT ((size_t) 8)
+typedef union camera_metadata_data {
+    uint8_t u8;
+    int32_t i32;
+    float   f;
+    int64_t i64;
+    double  d;
+    icamera_metadata_rational_t r;
+} camera_metadata_data_t;
+
+/**
+ * The preferred alignment of a packet of camera metadata. In general,
+ * this is the lowest common multiple of the constituents of a metadata
+ * package, i.e, of DATA_ALIGNMENT and ENTRY_ALIGNMENT.
+ */
+#define MAX_ALIGNMENT(A, B) (((A) > (B)) ? (A) : (B))
+#define METADATA_PACKET_ALIGNMENT \
+    MAX_ALIGNMENT(MAX_ALIGNMENT(DATA_ALIGNMENT, METADATA_ALIGNMENT), ENTRY_ALIGNMENT);
+
+/** Versioning information */
+#define CURRENT_METADATA_VERSION 1
+
+/** Flag definitions */
+#define FLAG_SORTED 0x00000001
+
+/** Tag information */
+
+typedef struct tag_info {
+    const char *tag_name;
+    uint8_t     tag_type;
+} tag_info_t;
+
+#include "icamera_metadata_tag_info.c"
+
+const size_t icamera_metadata_type_size[ICAMERA_NUM_TYPES] = {
+    sizeof(uint8_t),                    // ICAMERA_TYPE_BYTE
+    sizeof(int32_t),                    // ICAMERA_TYPE_INT32
+    sizeof(float),                      // ICAMERA_TYPE_FLOAT
+    sizeof(int64_t),                    // ICAMERA_TYPE_INT64
+    sizeof(double),                     // ICAMERA_TYPE_DOUBLE
+    sizeof(icamera_metadata_rational_t) // ICAMERA_TYPE_RATIONAL
+};
+
+const char *icamera_metadata_type_names[ICAMERA_NUM_TYPES] = {
+    "byte",                             // ICAMERA_TYPE_BYTE
+    "int32",                            // ICAMERA_TYPE_INT32
+    "float",                            // ICAMERA_TYPE_FLOAT
+    "int64",                            // ICAMERA_TYPE_INT64
+    "double",                           // ICAMERA_TYPE_DOUBLE
+    "rational"                          // ICAMERA_TYPE_RATIONAL
+};
+
+static camera_metadata_buffer_entry_t *get_entries(
+        const icamera_metadata_t *metadata) {
+    return (camera_metadata_buffer_entry_t*)
+            ((uint8_t*)metadata + metadata->entries_start);
+}
+
+static uint8_t *get_data(const icamera_metadata_t *metadata) {
+    return (uint8_t*)metadata + metadata->data_start;
+}
+
+size_t get_icamera_metadata_alignment() {
+    return METADATA_PACKET_ALIGNMENT;
+}
+
+icamera_metadata_t *allocate_copy_icamera_metadata_checked(
+        const icamera_metadata_t *src,
+        size_t src_size) {
+
+    if (src == NULL) {
+        return NULL;
+    }
+
+    void *buffer = malloc(src_size);
+    MEMCPY_S(buffer, src_size, src, src_size);
+
+    icamera_metadata_t *metadata = (icamera_metadata_t*) buffer;
+    if (validate_icamera_metadata_structure(metadata, &src_size) != OK) {
+        free(buffer);
+        return NULL;
+    }
+
+    return metadata;
+}
+
+icamera_metadata_t *allocate_icamera_metadata(size_t entry_capacity,
+                                              size_t data_capacity) {
+
+    size_t memory_needed = calculate_icamera_metadata_size(entry_capacity,
+                                                          data_capacity);
+    void *buffer = malloc(memory_needed);
+    return place_icamera_metadata(buffer, memory_needed,
+                                 entry_capacity,
+                                 data_capacity);
+}
+
+icamera_metadata_t *place_icamera_metadata(void *dst,
+                                           size_t dst_size,
+                                           size_t entry_capacity,
+                                           size_t data_capacity) {
+    if (dst == NULL) return NULL;
+
+    size_t memory_needed = calculate_icamera_metadata_size(entry_capacity,
+                                                          data_capacity);
+    if (memory_needed > dst_size) return NULL;
+
+    icamera_metadata_t *metadata = (icamera_metadata_t*)dst;
+    metadata->version = CURRENT_METADATA_VERSION;
+    metadata->flags = 0;
+    metadata->entry_count = 0;
+    metadata->entry_capacity = entry_capacity;
+    metadata->entries_start =
+            ALIGN_TO(sizeof(icamera_metadata_t), ENTRY_ALIGNMENT);
+    metadata->data_count = 0;
+    metadata->data_capacity = data_capacity;
+    metadata->size = memory_needed;
+    size_t data_unaligned = (uint8_t*)(get_entries(metadata) +
+            metadata->entry_capacity) - (uint8_t*)metadata;
+    metadata->data_start = ALIGN_TO(data_unaligned, DATA_ALIGNMENT);
+
+    assert(validate_icamera_metadata_structure(metadata, NULL) == OK);
+    return metadata;
+}
+void free_icamera_metadata(icamera_metadata_t *metadata) {
+    free(metadata);
+}
+
+size_t calculate_icamera_metadata_size(size_t entry_count,
+                                       size_t data_count) {
+    size_t memory_needed = sizeof(icamera_metadata_t);
+    // Start entry list at aligned boundary
+    memory_needed = ALIGN_TO(memory_needed, ENTRY_ALIGNMENT);
+    memory_needed += sizeof(camera_metadata_buffer_entry_t[entry_count]);
+    // Start buffer list at aligned boundary
+    memory_needed = ALIGN_TO(memory_needed, DATA_ALIGNMENT);
+    memory_needed += sizeof(uint8_t[data_count]);
+    return memory_needed;
+}
+
+size_t get_icamera_metadata_size(const icamera_metadata_t *metadata) {
+    if (metadata == NULL) return ERROR;
+
+    return metadata->size;
+}
+
+size_t get_icamera_metadata_compact_size(const icamera_metadata_t *metadata) {
+    if (metadata == NULL) return ERROR;
+
+    return calculate_icamera_metadata_size(metadata->entry_count,
+                                          metadata->data_count);
+}
+
+size_t get_icamera_metadata_entry_count(const icamera_metadata_t *metadata) {
+    return metadata->entry_count;
+}
+
+size_t get_icamera_metadata_entry_capacity(const icamera_metadata_t *metadata) {
+    return metadata->entry_capacity;
+}
+
+size_t get_icamera_metadata_data_count(const icamera_metadata_t *metadata) {
+    return metadata->data_count;
+}
+
+size_t get_icamera_metadata_data_capacity(const icamera_metadata_t *metadata) {
+    return metadata->data_capacity;
+}
+
+icamera_metadata_t* copy_icamera_metadata(void *dst, size_t dst_size,
+        const icamera_metadata_t *src) {
+    size_t memory_needed = get_icamera_metadata_compact_size(src);
+
+    if (dst == NULL) return NULL;
+    if (dst_size < memory_needed) return NULL;
+
+    icamera_metadata_t *metadata =
+        place_icamera_metadata(dst, dst_size, src->entry_count, src->data_count);
+
+    if (metadata == NULL) {
+        ALOGE("%s: metadata is null!", __func__);
+        return NULL;
+    }
+    metadata->flags = src->flags;
+    metadata->entry_count = src->entry_count;
+    metadata->data_count = src->data_count;
+
+    MEMCPY_S(get_entries(metadata),sizeof(camera_metadata_buffer_entry_t[metadata->entry_count]),
+             get_entries(src), sizeof(camera_metadata_buffer_entry_t[metadata->entry_count]));
+    MEMCPY_S(get_data(metadata), sizeof(uint8_t[metadata->data_count]),
+             get_data(src), sizeof(uint8_t[metadata->data_count]));
+
+    assert(validate_icamera_metadata_structure(metadata, NULL) == OK);
+    return metadata;
+}
+
+int validate_icamera_metadata_structure(const icamera_metadata_t *metadata,
+                                        const size_t *expected_size) {
+
+    if (metadata == NULL) {
+        ALOGE("%s: metadata is null!", __func__);
+        return ERROR;
+    }
+
+    // Check that the metadata pointer is well-aligned first.
+    {
+        static const struct {
+            const char *name;
+            size_t alignment;
+        } alignments[] = {
+            {
+                .name = "icamera_metadata",
+                .alignment = METADATA_ALIGNMENT
+            },
+            {
+                .name = "camera_metadata_buffer_entry",
+                .alignment = ENTRY_ALIGNMENT
+            },
+            {
+                .name = "camera_metadata_data",
+                .alignment = DATA_ALIGNMENT
+            },
+        };
+
+        size_t i = 0;
+        for (i = 0; i < sizeof(alignments)/sizeof(alignments[0]); ++i) {
+            uintptr_t aligned_ptr = ALIGN_TO(metadata, alignments[i].alignment);
+
+            if ((uintptr_t)metadata != aligned_ptr) {
+                ALOGE("%s: Metadata pointer is not aligned (actual %p, "
+                      "expected %p) to type %s",
+                      __func__, metadata,
+                      (void*)aligned_ptr, alignments[i].name);
+                return ERROR;
+            }
+        }
+    }
+
+    /**
+     * Check that the metadata contents are correct
+     */
+
+    if (expected_size != NULL && metadata->size > *expected_size) {
+        ALOGE("%s: Metadata size (%" PRIu32 ") should be <= expected size (%zu)",
+              __func__, metadata->size, *expected_size);
+        return ERROR;
+    }
+
+    if (metadata->entry_count > metadata->entry_capacity) {
+        ALOGE("%s: Entry count (%" PRIu32 ") should be <= entry capacity "
+              "(%" PRIu32 ")",
+              __func__, metadata->entry_count, metadata->entry_capacity);
+        return ERROR;
+    }
+
+    const metadata_uptrdiff_t entries_end =
+        metadata->entries_start + metadata->entry_capacity;
+    if (entries_end < metadata->entries_start || // overflow check
+        entries_end > metadata->data_start) {
+
+        ALOGE("%s: Entry start + capacity (%" PRIu32 ") should be <= data start "
+              "(%" PRIu32 ")",
+               __func__,
+              (metadata->entries_start + metadata->entry_capacity),
+              metadata->data_start);
+        return ERROR;
+    }
+
+    const metadata_uptrdiff_t data_end =
+        metadata->data_start + metadata->data_capacity;
+    if (data_end < metadata->data_start || // overflow check
+        data_end > metadata->size) {
+
+        ALOGE("%s: Data start + capacity (%" PRIu32 ") should be <= total size "
+              "(%" PRIu32 ")",
+               __func__,
+              (metadata->data_start + metadata->data_capacity),
+              metadata->size);
+        return ERROR;
+    }
+
+    // Validate each entry
+    const metadata_size_t entry_count = metadata->entry_count;
+    camera_metadata_buffer_entry_t *entries = get_entries(metadata);
+
+    size_t i = 0;
+    for (i = 0; i < entry_count; ++i) {
+
+        if ((uintptr_t)&entries[i] != ALIGN_TO(&entries[i], ENTRY_ALIGNMENT)) {
+            ALOGE("%s: Entry index %zu had bad alignment (address %p),"
+                  " expected alignment %zu",
+                  __func__, i, &entries[i], ENTRY_ALIGNMENT);
+            return ERROR;
+        }
+
+        camera_metadata_buffer_entry_t entry = entries[i];
+
+        if (entry.type >= ICAMERA_NUM_TYPES) {
+            ALOGE("%s: Entry index %zu had a bad type %d",
+                  __func__, i, entry.type);
+            return ERROR;
+        }
+
+        int tag_type = get_icamera_metadata_tag_type(entry.tag);
+        if (tag_type != (int)entry.type) {
+            ALOGE("%s: Entry index %zu had tag type %d, but the type was %d",
+                  __func__, i, tag_type, entry.type);
+            return ERROR;
+        }
+
+        size_t data_size =
+                calculate_icamera_metadata_entry_data_size(entry.type,
+                                                          entry.count);
+
+        if (data_size != 0) {
+            camera_metadata_data_t *data =
+                    (camera_metadata_data_t*) (get_data(metadata) +
+                                               entry.data.offset);
+
+            if ((uintptr_t)data != ALIGN_TO(data, DATA_ALIGNMENT)) {
+                ALOGE("%s: Entry index %zu had bad data alignment (address %p),"
+                      " expected align %zu, (tag name %s, data size %zu)",
+                      __func__, i, data, DATA_ALIGNMENT,
+                      get_icamera_metadata_tag_name(entry.tag) ?: "unknown",
+                      data_size);
+                return ERROR;
+            }
+
+            size_t data_entry_end = entry.data.offset + data_size;
+            if (data_entry_end < entry.data.offset || // overflow check
+                data_entry_end > metadata->data_capacity) {
+
+                ALOGE("%s: Entry index %zu data ends (%zu) beyond the capacity "
+                      "%" PRIu32, __func__, i, data_entry_end,
+                      metadata->data_capacity);
+                return ERROR;
+            }
+
+        } else if (entry.count == 0) {
+            if (entry.data.offset != 0) {
+                ALOGE("%s: Entry index %zu had 0 items, but offset was non-0 "
+                     "(%" PRIu32 "), tag name: %s", __func__, i, entry.data.offset,
+                        get_icamera_metadata_tag_name(entry.tag) ?: "unknown");
+                return ERROR;
+            }
+        } // else data stored inline, so we look at value which can be anything.
+    }
+
+    return OK;
+}
+
+int append_icamera_metadata(icamera_metadata_t *dst,
+        const icamera_metadata_t *src) {
+    if (dst == NULL || src == NULL ) return ERROR;
+
+    if (dst->entry_capacity < src->entry_count + dst->entry_count) return ERROR;
+    if (dst->data_capacity < src->data_count + dst->data_count) return ERROR;
+
+    if (dst->entry_capacity - dst->entry_count < src->entry_count) {
+        ALOGE("%s: Dst available buffer size for entry is smaller than src needed.", __func__);
+    }
+    MEMCPY_S(get_entries(dst) + dst->entry_count,
+             sizeof(camera_metadata_buffer_entry_t) * (dst->entry_capacity - dst->entry_count),
+             get_entries(src), sizeof(camera_metadata_buffer_entry_t[src->entry_count]));
+
+    if (dst->data_capacity - dst->data_count < src->data_count) {
+        ALOGE("%s: Dst available buffer size for data is smaller than src needed.", __func__);
+    }
+    MEMCPY_S(get_data(dst) + dst->data_count, sizeof(uint8_t[dst->data_capacity - dst->data_count]),
+             get_data(src), sizeof(uint8_t[src->data_count]));
+    if (dst->data_count != 0) {
+        camera_metadata_buffer_entry_t *entry = get_entries(dst) + dst->entry_count;
+        size_t i = 0;
+        for (i = 0; i < src->entry_count; i++, entry++) {
+            if ( calculate_icamera_metadata_entry_data_size(entry->type,
+                            entry->count) > 0 ) {
+                entry->data.offset += dst->data_count;
+            }
+        }
+    }
+    if (dst->entry_count == 0) {
+        // Appending onto empty buffer, keep sorted state
+        dst->flags |= src->flags & FLAG_SORTED;
+    } else if (src->entry_count != 0) {
+        // Both src, dst are nonempty, cannot assume sort remains
+        dst->flags &= ~FLAG_SORTED;
+    } else {
+        // Src is empty, keep dst sorted state
+    }
+    dst->entry_count += src->entry_count;
+    dst->data_count += src->data_count;
+
+    assert(validate_icamera_metadata_structure(dst, NULL) == OK);
+    return OK;
+}
+
+icamera_metadata_t *clone_icamera_metadata(const icamera_metadata_t *src) {
+    if (src == NULL) return NULL;
+    icamera_metadata_t *clone = allocate_icamera_metadata(
+        get_icamera_metadata_entry_count(src),
+        get_icamera_metadata_data_count(src));
+    if (clone != NULL) {
+        int res = append_icamera_metadata(clone, src);
+        if (res != OK) {
+            free_icamera_metadata(clone);
+            clone = NULL;
+        }
+    }
+    assert(validate_icamera_metadata_structure(clone, NULL) == OK);
+    return clone;
+}
+
+size_t calculate_icamera_metadata_entry_data_size(uint8_t type,
+        size_t data_count) {
+    if (type >= ICAMERA_NUM_TYPES) return 0;
+    size_t data_bytes = data_count *
+            icamera_metadata_type_size[type];
+    return data_bytes <= 4 ? 0 : ALIGN_TO(data_bytes, DATA_ALIGNMENT);
+}
+
+static int add_camera_metadata_entry_raw(icamera_metadata_t *dst,
+        uint32_t tag,
+        uint8_t  type,
+        const void *data,
+        size_t data_count) {
+
+    if (dst == NULL) return ERROR;
+    if (dst->entry_count == dst->entry_capacity) return ERROR;
+    if (data == NULL) return ERROR;
+
+    size_t data_bytes =
+            calculate_icamera_metadata_entry_data_size(type, data_count);
+    if (data_bytes + dst->data_count > dst->data_capacity) return ERROR;
+
+    if (type >= ICAMERA_NUM_TYPES) {
+        ALOGE("%s: Bad type %d", __func__, type);
+        return ERROR;
+    }
+    size_t data_payload_bytes =
+            data_count * icamera_metadata_type_size[type];
+    camera_metadata_buffer_entry_t *entry = get_entries(dst) + dst->entry_count;
+    memset(entry, 0, sizeof(camera_metadata_buffer_entry_t));
+    entry->tag = tag;
+    entry->type = type;
+    entry->count = data_count;
+
+    if (data_bytes == 0) {
+        MEMCPY_S(entry->data.value, data_payload_bytes, data, data_payload_bytes);
+    } else {
+        entry->data.offset = dst->data_count;
+        MEMCPY_S(get_data(dst) + entry->data.offset, data_payload_bytes, data, data_payload_bytes);
+        dst->data_count += data_bytes;
+    }
+    dst->entry_count++;
+    dst->flags &= ~FLAG_SORTED;
+    assert(validate_icamera_metadata_structure(dst, NULL) == OK);
+    return OK;
+}
+
+int add_icamera_metadata_entry(icamera_metadata_t *dst,
+        uint32_t tag,
+        const void *data,
+        size_t data_count) {
+
+    int type = get_icamera_metadata_tag_type(tag);
+    if (type == -1) {
+        ALOGE("%s: Unknown tag %04x.", __func__, tag);
+        return ERROR;
+    }
+
+    return add_camera_metadata_entry_raw(dst,
+            tag,
+            type,
+            data,
+            data_count);
+}
+
+static int compare_entry_tags(const void *p1, const void *p2) {
+    uint32_t tag1 = ((camera_metadata_buffer_entry_t*)p1)->tag;
+    uint32_t tag2 = ((camera_metadata_buffer_entry_t*)p2)->tag;
+    return  tag1 < tag2 ? -1 :
+            tag1 == tag2 ? 0 :
+            1;
+}
+
+int sort_icamera_metadata(icamera_metadata_t *dst) {
+    if (dst == NULL) return ERROR;
+    if (dst->flags & FLAG_SORTED) return OK;
+
+    qsort(get_entries(dst), dst->entry_count,
+            sizeof(camera_metadata_buffer_entry_t),
+            compare_entry_tags);
+    dst->flags |= FLAG_SORTED;
+
+    assert(validate_icamera_metadata_structure(dst, NULL) == OK);
+    return OK;
+}
+
+int get_icamera_metadata_entry(icamera_metadata_t *src,
+        size_t index,
+        icamera_metadata_entry_t *entry) {
+    if (src == NULL || entry == NULL) return ERROR;
+    if (index >= src->entry_count) return ERROR;
+
+    camera_metadata_buffer_entry_t *buffer_entry = get_entries(src) + index;
+
+    entry->index = index;
+    entry->tag = buffer_entry->tag;
+    entry->type = buffer_entry->type;
+    entry->count = buffer_entry->count;
+    if (buffer_entry->count *
+            icamera_metadata_type_size[buffer_entry->type] > 4) {
+        entry->data.u8 = get_data(src) + buffer_entry->data.offset;
+    } else {
+        entry->data.u8 = buffer_entry->data.value;
+    }
+    return OK;
+}
+
+int get_icamera_metadata_ro_entry(const icamera_metadata_t *src,
+        size_t index,
+        icamera_metadata_ro_entry_t *entry) {
+    return get_icamera_metadata_entry((icamera_metadata_t*)src, index,
+            (icamera_metadata_entry_t*)entry);
+}
+
+int find_icamera_metadata_entry(icamera_metadata_t *src,
+        uint32_t tag,
+        icamera_metadata_entry_t *entry) {
+    if (src == NULL) return ERROR;
+
+    uint32_t index;
+    if (src->flags & FLAG_SORTED) {
+        // Sorted entries, do a binary search
+        camera_metadata_buffer_entry_t *search_entry = NULL;
+        camera_metadata_buffer_entry_t key;
+        key.tag = tag;
+        search_entry = (camera_metadata_buffer_entry_t *)bsearch(&key,
+                get_entries(src),
+                src->entry_count,
+                sizeof(camera_metadata_buffer_entry_t),
+                compare_entry_tags);
+        if (search_entry == NULL) return NOT_FOUND;
+        index = search_entry - get_entries(src);
+    } else {
+        // Not sorted, linear search
+        camera_metadata_buffer_entry_t *search_entry = get_entries(src);
+        for (index = 0; index < src->entry_count; index++, search_entry++) {
+            if (search_entry->tag == tag) {
+                break;
+            }
+        }
+        if (index == src->entry_count) return NOT_FOUND;
+    }
+
+    return get_icamera_metadata_entry(src, index,
+            entry);
+}
+
+int find_icamera_metadata_ro_entry(const icamera_metadata_t *src,
+        uint32_t tag,
+        icamera_metadata_ro_entry_t *entry) {
+    return find_icamera_metadata_entry((icamera_metadata_t*)src, tag,
+            (icamera_metadata_entry_t*)entry);
+}
+
+int delete_icamera_metadata_entry(icamera_metadata_t *dst,
+        size_t index) {
+    if (dst == NULL) return ERROR;
+    if (index >= dst->entry_count) return ERROR;
+
+    camera_metadata_buffer_entry_t *entry = get_entries(dst) + index;
+    size_t data_bytes = calculate_icamera_metadata_entry_data_size(entry->type,
+            entry->count);
+
+    if (data_bytes > 0) {
+        // Shift data buffer to overwrite deleted data
+        uint8_t *start = get_data(dst) + entry->data.offset;
+        uint8_t *end = start + data_bytes;
+        size_t length = dst->data_count - entry->data.offset - data_bytes;
+        memmove(start, end, length);
+
+        // Update all entry indices to account for shift
+        camera_metadata_buffer_entry_t *e = get_entries(dst);
+        size_t i;
+        for (i = 0; i < dst->entry_count; i++) {
+            if (calculate_icamera_metadata_entry_data_size(
+                    e->type, e->count) > 0 &&
+                    e->data.offset > entry->data.offset) {
+                e->data.offset -= data_bytes;
+            }
+            ++e;
+        }
+        dst->data_count -= data_bytes;
+    }
+    // Shift entry array
+    memmove(entry, entry + 1,
+            sizeof(camera_metadata_buffer_entry_t) *
+            (dst->entry_count - index - 1) );
+    dst->entry_count -= 1;
+
+    assert(validate_icamera_metadata_structure(dst, NULL) == OK);
+    return OK;
+}
+
+int update_icamera_metadata_entry(icamera_metadata_t *dst,
+        size_t index,
+        const void *data,
+        size_t data_count,
+        icamera_metadata_entry_t *updated_entry) {
+    if (dst == NULL) return ERROR;
+    if (index >= dst->entry_count) return ERROR;
+
+    camera_metadata_buffer_entry_t *entry = get_entries(dst) + index;
+    if (entry->type >= ICAMERA_NUM_TYPES) return ERROR;
+
+    size_t data_bytes =
+            calculate_icamera_metadata_entry_data_size(entry->type,
+                    data_count);
+    size_t data_payload_bytes =
+            data_count * icamera_metadata_type_size[entry->type];
+
+    size_t entry_bytes =
+            calculate_icamera_metadata_entry_data_size(entry->type,
+                    entry->count);
+    if (data_bytes != entry_bytes) {
+        // May need to shift/add to data array
+        if (dst->data_capacity < dst->data_count + data_bytes - entry_bytes) {
+            // No room
+            return ERROR;
+        }
+        if (entry_bytes != 0) {
+            // Remove old data
+            uint8_t *start = get_data(dst) + entry->data.offset;
+            uint8_t *end = start + entry_bytes;
+            size_t length = dst->data_count - entry->data.offset - entry_bytes;
+            memmove(start, end, length);
+            dst->data_count -= entry_bytes;
+
+            // Update all entry indices to account for shift
+            camera_metadata_buffer_entry_t *e = get_entries(dst);
+            size_t i;
+            for (i = 0; i < dst->entry_count; i++) {
+                if (calculate_icamera_metadata_entry_data_size(
+                        e->type, e->count) > 0 &&
+                        e->data.offset > entry->data.offset) {
+                    e->data.offset -= entry_bytes;
+                }
+                ++e;
+            }
+        }
+
+        if (data_bytes != 0) {
+            // Append new data
+            entry->data.offset = dst->data_count;
+
+            MEMCPY_S(get_data(dst) + entry->data.offset, data_payload_bytes, data, data_payload_bytes);
+            dst->data_count += data_bytes;
+        }
+    } else if (data_bytes != 0) {
+        // data size unchanged, reuse same data location
+        MEMCPY_S(get_data(dst) + entry->data.offset, data_payload_bytes, data, data_payload_bytes);
+    }
+
+    if (data_bytes == 0) {
+        // Data fits into entry
+        MEMCPY_S(entry->data.value, data_payload_bytes, data, data_payload_bytes);
+    }
+
+    entry->count = data_count;
+
+    if (updated_entry != NULL) {
+        get_icamera_metadata_entry(dst,
+                index,
+                updated_entry);
+    }
+
+    assert(validate_icamera_metadata_structure(dst, NULL) == OK);
+    return OK;
+}
+
+const char *get_icamera_metadata_section_name(uint32_t tag) {
+    uint32_t tag_section = tag >> 16;
+    if (tag_section >= CAMERA_SECTION_COUNT) {
+        return NULL;
+    }
+    return icamera_metadata_section_names[tag_section];
+}
+
+const char *get_icamera_metadata_tag_name(uint32_t tag) {
+    uint32_t tag_section = tag >> 16;
+    if (tag_section >= CAMERA_SECTION_COUNT ||
+        tag >= icamera_metadata_section_bounds[tag_section][1] ) {
+        return NULL;
+    }
+    uint32_t tag_index = tag & 0xFFFF;
+    return icamera_tag_info[tag_section][tag_index].tag_name;
+}
+
+int get_icamera_metadata_tag_type(uint32_t tag) {
+    uint32_t tag_section = tag >> 16;
+    if (tag_section >= CAMERA_SECTION_COUNT ||
+            tag >= icamera_metadata_section_bounds[tag_section][1] ) {
+        return -1;
+    }
+    uint32_t tag_index = tag & 0xFFFF;
+    return icamera_tag_info[tag_section][tag_index].tag_type;
+}
+
+static void print_data(int fd, const uint8_t *data_ptr, uint32_t tag, int type,
+        int count,
+        int indentation);
+
+void dump_icamera_metadata(const icamera_metadata_t *metadata,
+        int fd,
+        int verbosity) {
+    dump_indented_icamera_metadata(metadata, fd, verbosity, 0);
+}
+
+void dump_indented_icamera_metadata(const icamera_metadata_t *metadata,
+        int fd,
+        int verbosity,
+        int indentation) {
+    if (metadata == NULL) {
+        dprintf(fd, "%*sDumping camera metadata array: Not allocated\n",
+                indentation, "");
+        return;
+    }
+    unsigned int i;
+    dprintf(fd,
+            "%*sDumping camera metadata array: %" PRIu32 " / %" PRIu32 " entries, "
+            "%" PRIu32 " / %" PRIu32 " bytes of extra data.\n", indentation, "",
+            metadata->entry_count, metadata->entry_capacity,
+            metadata->data_count, metadata->data_capacity);
+    dprintf(fd, "%*sVersion: %d, Flags: %08x\n",
+            indentation + 2, "",
+            metadata->version, metadata->flags);
+    camera_metadata_buffer_entry_t *entry = get_entries(metadata);
+    for (i=0; i < metadata->entry_count; i++, entry++) {
+
+        const char *tag_name, *tag_section;
+        tag_section = get_icamera_metadata_section_name(entry->tag);
+        if (tag_section == NULL) {
+            tag_section = "unknownSection";
+        }
+        tag_name = get_icamera_metadata_tag_name(entry->tag);
+        if (tag_name == NULL) {
+            tag_name = "unknownTag";
+        }
+        const char *type_name;
+        if (entry->type >= ICAMERA_NUM_TYPES) {
+            type_name = "unknown";
+        } else {
+            type_name = icamera_metadata_type_names[entry->type];
+        }
+        dprintf(fd, "%*s%s.%s (%05x): %s[%" PRIu32 "]\n",
+             indentation + 2, "",
+             tag_section,
+             tag_name,
+             entry->tag,
+             type_name,
+             entry->count);
+
+        if (verbosity < 1) continue;
+
+        if (entry->type >= ICAMERA_NUM_TYPES) continue;
+
+        size_t type_size = icamera_metadata_type_size[entry->type];
+        uint8_t *data_ptr;
+        if ( type_size * entry->count > 4 ) {
+            if (entry->data.offset >= metadata->data_count) {
+                ALOGE("%s: Malformed entry data offset: %" PRIu32 " (max %" PRIu32 ")",
+                        __func__,
+                        entry->data.offset,
+                        metadata->data_count);
+                continue;
+            }
+            data_ptr = get_data(metadata) + entry->data.offset;
+        } else {
+            data_ptr = entry->data.value;
+        }
+        int count = entry->count;
+        if (verbosity < 2 && count > 16) count = 16;
+
+        print_data(fd, data_ptr, entry->tag, entry->type, count, indentation);
+    }
+}
+
+static void print_data(int fd, const uint8_t *data_ptr, uint32_t tag,
+        int type, int count, int indentation) {
+    static int values_per_line[ICAMERA_NUM_TYPES] = {
+        16,                   // ICAMERA_TYPE_BYTE
+        4,                    // ICAMERA_TYPE_INT32
+        8,                    // ICAMERA_TYPE_FLOAT
+        2,                    // ICAMERA_TYPE_INT64
+        4,                    // ICAMERA_TYPE_DOUBLE
+        2,                    // ICAMERA_TYPE_RATIONAL
+    };
+    size_t type_size = icamera_metadata_type_size[type];
+    char value_string_tmp[ICAMERA_METADATA_ENUM_STRING_MAX_SIZE];
+    int32_t value;
+
+    int lines = count / values_per_line[type];
+    if (count % values_per_line[type] != 0) lines++;
+
+    int index = 0;
+    int j, k;
+    for (j = 0; j < lines; j++) {
+        dprintf(fd, "%*s[", indentation + 4, "");
+        for (k = 0;
+             k < values_per_line[type] && count > 0;
+             k++, count--, index += type_size) {
+
+            switch (type) {
+                case ICAMERA_TYPE_BYTE:
+                    value = *(data_ptr + index);
+                    if (icamera_metadata_enum_snprint(tag,
+                                                      value,
+                                                      value_string_tmp,
+                                                      sizeof(value_string_tmp))
+                        == OK) {
+                        dprintf(fd, "%s ", value_string_tmp);
+                    } else {
+                        dprintf(fd, "%hhu ",
+                                *(data_ptr + index));
+                    }
+                    break;
+                case ICAMERA_TYPE_INT32:
+                    value =
+                            *(int32_t*)(data_ptr + index);
+                    if (icamera_metadata_enum_snprint(tag,
+                                                      value,
+                                                      value_string_tmp,
+                                                      sizeof(value_string_tmp))
+                        == OK) {
+                        dprintf(fd, "%s ", value_string_tmp);
+                    } else {
+                        dprintf(fd, "%" PRId32 " ",
+                                *(int32_t*)(data_ptr + index));
+                    }
+                    break;
+                case ICAMERA_TYPE_FLOAT:
+                    dprintf(fd, "%0.8f ",
+                            *(float*)(data_ptr + index));
+                    break;
+                case ICAMERA_TYPE_INT64:
+                    dprintf(fd, "%" PRId64 " ",
+                            *(int64_t*)(data_ptr + index));
+                    break;
+                case ICAMERA_TYPE_DOUBLE:
+                    dprintf(fd, "%0.8f ",
+                            *(double*)(data_ptr + index));
+                    break;
+                case ICAMERA_TYPE_RATIONAL: {
+                    int32_t numerator = *(int32_t*)(data_ptr + index);
+                    int32_t denominator = *(int32_t*)(data_ptr + index + 4);
+                    dprintf(fd, "(%d / %d) ",
+                            numerator, denominator);
+                    break;
+                }
+                default:
+                    dprintf(fd, "??? ");
+            }
+        }
+        dprintf(fd, "]\n");
+    }
+}
diff --git a/camera/hal/intel/ipu6/src/metadata/icamera_metadata_base.h b/camera/hal/intel/ipu6/src/metadata/icamera_metadata_base.h
new file mode 100644
index 000000000000..77f02d0ae038
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/metadata/icamera_metadata_base.h
@@ -0,0 +1,445 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ * Copyright (C) 2015-2018 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <string.h>
+#include <stdint.h>
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#define ALIGN_TO(val, alignment) \
+    (((uintptr_t)(val) + ((alignment) - 1)) & ~((alignment) - 1))
+
+/**
+ * Tag hierarchy and enum definitions for camera_metadata_entry
+ * =============================================================================
+ */
+
+/**
+ * Main enum definitions are in a separate file to make it easy to
+ * maintain
+ */
+#include "icamera_metadata_tags.h"
+
+/**
+ * Enum range for each top-level category
+ */
+
+extern unsigned int icamera_metadata_section_bounds[CAMERA_SECTION_COUNT][2];
+
+extern const char *icamera_metadata_section_names[CAMERA_SECTION_COUNT];
+
+/**
+ * Type definitions for camera_metadata_entry
+ * =============================================================================
+ */
+enum {
+    // Unsigned 8-bit integer (uint8_t)
+    ICAMERA_TYPE_BYTE = 0,
+    // Signed 32-bit integer (int32_t)
+    ICAMERA_TYPE_INT32 = 1,
+    // 32-bit float (float)
+    ICAMERA_TYPE_FLOAT = 2,
+    // Signed 64-bit integer (int64_t)
+    ICAMERA_TYPE_INT64 = 3,
+    // 64-bit float (double)
+    ICAMERA_TYPE_DOUBLE = 4,
+    // A 64-bit fraction (camera_metadata_rational_t)
+    ICAMERA_TYPE_RATIONAL = 5,
+    // Number of type fields
+    ICAMERA_NUM_TYPES
+};
+
+typedef struct icamera_metadata_rational {
+    int32_t numerator;
+    int32_t denominator;
+} icamera_metadata_rational_t;
+
+/**
+ * A reference to a metadata entry in a buffer.
+ *
+ * The data union pointers point to the real data in the buffer, and can be
+ * modified in-place if the count does not need to change. The count is the
+ * number of entries in data of the entry's type, not a count of bytes.
+ */
+typedef struct icamera_metadata_entry {
+    size_t   index;
+    uint32_t tag;
+    uint8_t  type;
+    size_t   count;
+    union {
+        uint8_t *u8;
+        int32_t *i32;
+        float   *f;
+        int64_t *i64;
+        double  *d;
+        icamera_metadata_rational_t *r;
+    } data;
+} icamera_metadata_entry_t;
+
+/**
+ * A read-only reference to a metadata entry in a buffer. Identical to
+ * camera_metadata_entry in layout
+ */
+typedef struct icamera_metadata_ro_entry {
+    size_t   index;
+    uint32_t tag;
+    uint8_t  type;
+    size_t   count;
+    union {
+        const uint8_t *u8;
+        const int32_t *i32;
+        const float   *f;
+        const int64_t *i64;
+        const double  *d;
+        const icamera_metadata_rational_t *r;
+    } data;
+} icamera_metadata_ro_entry_t;
+
+/**
+ * Size in bytes of each entry type
+ */
+extern const size_t icamera_metadata_type_size[ICAMERA_NUM_TYPES];
+
+/**
+ * Human-readable name of each entry type
+ */
+extern const char* icamera_metadata_type_names[ICAMERA_NUM_TYPES];
+
+/**
+ * Main definitions for the metadata entry and array structures
+ * =============================================================================
+ */
+
+/**
+ * A packet of metadata. This is a list of metadata entries, each of which has
+ * an integer tag to identify its meaning, 'type' and 'count' field, and the
+ * data, which contains a 'count' number of entries of type 'type'. The packet
+ * has a fixed capacity for entries and for extra data.  A new entry uses up one
+ * entry slot, and possibly some amount of data capacity; the function
+ * calculate_camera_metadata_entry_data_size() provides the amount of data
+ * capacity that would be used up by an entry.
+ *
+ * Entries are not sorted by default, and are not forced to be unique - multiple
+ * entries with the same tag are allowed. The packet will not dynamically resize
+ * when full.
+ *
+ * The packet is contiguous in memory, with size in bytes given by
+ * get_camera_metadata_size(). Therefore, it can be copied safely with memcpy()
+ * to a buffer of sufficient size. The copy_camera_metadata() function is
+ * intended for eliminating unused capacity in the destination packet.
+ */
+struct icamera_metadata;
+typedef struct icamera_metadata icamera_metadata_t;
+
+/**
+ * Functions for manipulating camera metadata
+ * =============================================================================
+ *
+ * NOTE: Unless otherwise specified, functions that return type "int"
+ * return 0 on success, and non-0 value on error.
+ */
+
+/**
+ * Allocate a new camera_metadata structure, with some initial space for entries
+ * and extra data. The entry_capacity is measured in entry counts, and
+ * data_capacity in bytes. The resulting structure is all contiguous in memory,
+ * and can be freed with free_camera_metadata().
+ */
+icamera_metadata_t *allocate_icamera_metadata(size_t entry_capacity,
+        size_t data_capacity);
+
+/**
+ * Get the required alignment of a packet of camera metadata, which is the
+ * maximal alignment of the embedded camera_metadata, camera_metadata_buffer_entry,
+ * and camera_metadata_data.
+ */
+size_t get_icamera_metadata_alignment();
+
+/**
+ * Allocate a new camera_metadata structure of size src_size. Copy the data,
+ * ignoring alignment, and then attempt validation. If validation
+ * fails, free the memory and return NULL. Otherwise return the pointer.
+ *
+ * The resulting pointer can be freed with free_camera_metadata().
+ */
+icamera_metadata_t *allocate_copy_icamera_metadata_checked(
+        const icamera_metadata_t *src,
+        size_t src_size);
+
+/**
+ * Place a camera metadata structure into an existing buffer. Returns NULL if
+ * the buffer is too small for the requested number of reserved entries and
+ * bytes of data. The entry_capacity is measured in entry counts, and
+ * data_capacity in bytes. If the buffer is larger than the required space,
+ * unused space will be left at the end. If successful, returns a pointer to the
+ * metadata header placed at the start of the buffer. It is the caller's
+ * responsibility to free the original buffer; do not call
+ * free_camera_metadata() with the returned pointer.
+ */
+icamera_metadata_t *place_icamera_metadata(void *dst, size_t dst_size,
+        size_t entry_capacity,
+        size_t data_capacity);
+
+/**
+ * Free a camera_metadata structure. Should only be used with structures
+ * allocated with allocate_camera_metadata().
+ */
+void free_icamera_metadata(icamera_metadata_t *metadata);
+
+/**
+ * Calculate the buffer size needed for a metadata structure of entry_count
+ * metadata entries, needing a total of data_count bytes of extra data storage.
+ */
+size_t calculate_icamera_metadata_size(size_t entry_count,
+        size_t data_count);
+
+/**
+ * Get current size of entire metadata structure in bytes, including reserved
+ * but unused space.
+ */
+size_t get_icamera_metadata_size(const icamera_metadata_t *metadata);
+
+/**
+ * Get size of entire metadata buffer in bytes, not including reserved but
+ * unused space. This is the amount of space needed by copy_camera_metadata for
+ * its dst buffer.
+ */
+size_t get_icamera_metadata_compact_size(const icamera_metadata_t *metadata);
+
+/**
+ * Get the current number of entries in the metadata packet.
+ *
+ * metadata packet must be valid, which can be checked before the call with
+ * validate_camera_metadata_structure().
+ */
+size_t get_icamera_metadata_entry_count(const icamera_metadata_t *metadata);
+
+/**
+ * Get the maximum number of entries that could fit in the metadata packet.
+ */
+size_t get_icamera_metadata_entry_capacity(const icamera_metadata_t *metadata);
+
+/**
+ * Get the current count of bytes used for value storage in the metadata packet.
+ */
+size_t get_icamera_metadata_data_count(const icamera_metadata_t *metadata);
+
+/**
+ * Get the maximum count of bytes that could be used for value storage in the
+ * metadata packet.
+ */
+size_t get_icamera_metadata_data_capacity(const icamera_metadata_t *metadata);
+
+/**
+ * Copy a metadata structure to a memory buffer, compacting it along the
+ * way. That is, in the copied structure, entry_count == entry_capacity, and
+ * data_count == data_capacity.
+ *
+ * If dst_size > get_camera_metadata_compact_size(), the unused bytes are at the
+ * end of the buffer. If dst_size < get_camera_metadata_compact_size(), returns
+ * NULL. Otherwise returns a pointer to the metadata structure header placed at
+ * the start of dst.
+ *
+ * Since the buffer was not allocated by allocate_camera_metadata, the caller is
+ * responsible for freeing the underlying buffer when needed; do not call
+ * free_camera_metadata.
+ */
+icamera_metadata_t *copy_icamera_metadata(void *dst, size_t dst_size,
+        const icamera_metadata_t *src);
+
+/**
+ * Validate that a metadata is structurally sane. That is, its internal
+ * state is such that we won't get buffer overflows or run into other
+ * 'impossible' issues when calling the other API functions.
+ *
+ * This is useful in particular after copying the binary metadata blob
+ * from an untrusted source, since passing this check means the data is at least
+ * consistent.
+ *
+ * The expected_size argument is optional.
+ *
+ * Returns 0 on success. A non-0 value is returned on error.
+ */
+int validate_icamera_metadata_structure(const icamera_metadata_t *metadata,
+                                        const size_t *expected_size);
+
+/**
+ * Append camera metadata in src to an existing metadata structure in dst.  This
+ * does not resize the destination structure, so if it is too small, a non-zero
+ * value is returned. On success, 0 is returned. Appending onto a sorted
+ * structure results in a non-sorted combined structure.
+ */
+int append_icamera_metadata(icamera_metadata_t *dst, const icamera_metadata_t *src);
+
+/**
+ * Clone an existing metadata buffer, compacting along the way. This is
+ * equivalent to allocating a new buffer of the minimum needed size, then
+ * appending the buffer to be cloned into the new buffer. The resulting buffer
+ * can be freed with free_camera_metadata(). Returns NULL if cloning failed.
+ */
+
+icamera_metadata_t *clone_icamera_metadata(const icamera_metadata_t *src);
+/**
+ * Calculate the number of bytes of extra data a given metadata entry will take
+ * up. That is, if entry of 'type' with a payload of 'data_count' values is
+ * added, how much will the value returned by get_camera_metadata_data_count()
+ * be increased? This value may be zero, if no extra data storage is needed.
+ */
+size_t calculate_icamera_metadata_entry_data_size(uint8_t type,
+        size_t data_count);
+
+/**
+ * Add a metadata entry to a metadata structure. Returns 0 if the addition
+ * succeeded. Returns a non-zero value if there is insufficient reserved space
+ * left to add the entry, or if the tag is unknown.  data_count is the number of
+ * entries in the data array of the tag's type, not a count of
+ * bytes. Entries are always added to the end of the structure (highest index),
+ * so after addition, a previously-sorted array will be marked as unsorted.
+ *
+ * Returns 0 on success. A non-0 value is returned on error.
+ */
+int add_icamera_metadata_entry(icamera_metadata_t *dst,
+        uint32_t tag,
+        const void *data,
+        size_t data_count);
+
+/**
+ * Sort the metadata buffer for fast searching. If already marked as sorted,
+ * does nothing. Adding or appending entries to the buffer will place the buffer
+ * back into an unsorted state.
+ *
+ * Returns 0 on success. A non-0 value is returned on error.
+ */
+int sort_icamera_metadata(icamera_metadata_t *dst);
+
+/**
+ * Get metadata entry at position index in the metadata buffer.
+ * Index must be less than entry count, which is returned by
+ * get_icamera_metadata_entry_count().
+ *
+ * src and index are inputs; the passed-in entry is updated with the details of
+ * the entry. The data pointer points to the real data in the buffer, and can be
+ * updated as long as the data count does not change.
+ *
+ * Returns 0 on success. A non-0 value is returned on error.
+ */
+int get_icamera_metadata_entry(icamera_metadata_t *src,
+        size_t index,
+        icamera_metadata_entry_t *entry);
+
+/**
+ * Get metadata entry at position index, but disallow editing the data.
+ */
+int get_icamera_metadata_ro_entry(const icamera_metadata_t *src,
+        size_t index,
+        icamera_metadata_ro_entry_t *entry);
+
+/**
+ * Find an entry with given tag value. If not found, returns -ENOENT. Otherwise,
+ * returns entry contents like get_camera_metadata_entry.
+ *
+ * If multiple entries with the same tag exist, does not have any guarantees on
+ * which is returned. To speed up searching for tags, sort the metadata
+ * structure first by calling sort_camera_metadata().
+ */
+int find_icamera_metadata_entry(icamera_metadata_t *src,
+        uint32_t tag,
+        icamera_metadata_entry_t *entry);
+
+/**
+ * Find an entry with given tag value, but disallow editing the data
+ */
+int find_icamera_metadata_ro_entry(const icamera_metadata_t *src,
+        uint32_t tag,
+        icamera_metadata_ro_entry_t *entry);
+
+/**
+ * Delete an entry at given index. This is an expensive operation, since it
+ * requires repacking entries and possibly entry data. This also invalidates any
+ * existing camera_metadata_entry.data pointers to this buffer. Sorting is
+ * maintained.
+ */
+int delete_icamera_metadata_entry(icamera_metadata_t *dst,
+        size_t index);
+
+/**
+ * Updates a metadata entry with new data. If the data size is changing, may
+ * need to adjust the data array, making this an O(N) operation. If the data
+ * size is the same or still fits in the entry space, this is O(1). Maintains
+ * sorting, but invalidates camera_metadata_entry instances that point to the
+ * updated entry. If a non-NULL value is passed in to entry, the entry structure
+ * is updated to match the new buffer state.  Returns a non-zero value if there
+ * is no room for the new data in the buffer.
+ */
+int update_icamera_metadata_entry(icamera_metadata_t *dst,
+        size_t index,
+        const void *data,
+        size_t data_count,
+        icamera_metadata_entry_t *updated_entry);
+
+/**
+ * Retrieve human-readable name of section the tag is in. Returns NULL if
+ * no such tag is defined.
+ */
+const char *get_icamera_metadata_section_name(uint32_t tag);
+
+/**
+ * Retrieve human-readable name of tag (not including section). Returns NULL if
+ * no such tag is defined.
+ */
+const char *get_icamera_metadata_tag_name(uint32_t tag);
+
+/**
+ * Retrieve the type of a tag. Returns -1 if no such tag is defined.
+ */
+int get_icamera_metadata_tag_type(uint32_t tag);
+
+/**
+ * Print fields in the metadata to the log.
+ * verbosity = 0: Only tag entry information
+ * verbosity = 1: Tag entry information plus at most 16 data values
+ * verbosity = 2: All information
+ */
+void dump_icamera_metadata(const icamera_metadata_t *metadata,
+        int fd,
+        int verbosity);
+
+/**
+ * Print fields in the metadata to the log; adds indentation parameter, which
+ * specifies the number of spaces to insert before each line of the dump
+ */
+void dump_indented_icamera_metadata(const icamera_metadata_t *metadata,
+        int fd,
+        int verbosity,
+        int indentation);
+
+/**
+ * Prints the specified tag value as a string. Only works for enum tags.
+ * Returns 0 on success, -1 on failure.
+ */
+int icamera_metadata_enum_snprint(uint32_t tag,
+                                  int32_t value,
+                                  char *dst,
+                                  size_t size);
+
+#ifdef __cplusplus
+}
+#endif
diff --git a/camera/hal/intel/ipu6/src/metadata/icamera_metadata_tag_info.c b/camera/hal/intel/ipu6/src/metadata/icamera_metadata_tag_info.c
new file mode 100644
index 000000000000..491348aba397
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/metadata/icamera_metadata_tag_info.c
@@ -0,0 +1,2698 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ * Copyright (C) 2015-2018 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * !! Do not reference this file directly !!
+ *
+ * It is logically a part of camera_metadata_base.cpp.  It is broken out for
+ * ease of maintaining the tag info.
+ *
+ * Array assignments are done using specified-index syntax to keep things in
+ * sync with icamera_metadata_tags.h
+ */
+
+/**
+ * ! Do not edit this file directly !
+ *
+ * Generated automatically from icamera_metadata_tag_info.mako
+ */
+
+const char *icamera_metadata_section_names[CAMERA_SECTION_COUNT] = {
+    "camera.ae", /* [CAMERA_AE] */
+    "camera.awb", /* [CAMERA_AWB] */
+    "camera.af", /* [CAMERA_AF] */
+    "camera.control", /* [CAMERA_CONTROL] */
+    "camera.demosaic", /* [CAMERA_DEMOSAIC] */
+    "camera.edge", /* [CAMERA_EDGE] */
+    "camera.flash", /* [CAMERA_FLASH] */
+    "camera.flash.info", /* [CAMERA_FLASH_INFO] */
+    "camera.hotPixel", /* [CAMERA_HOT_PIXEL] */
+    "camera.jpeg", /* [CAMERA_JPEG] */
+    "camera.lens", /* [CAMERA_LENS] */
+    "camera.lens.info", /* [CAMERA_LENS_INFO] */
+    "camera.noiseReduction", /* [CAMERA_NOISE_REDUCTION] */
+    "camera.request", /* [CAMERA_REQUEST] */
+    "camera.scaler", /* [CAMERA_SCALER] */
+    "camera.sensor", /* [CAMERA_SENSOR] */
+    "camera.sensor.info", /* [CAMERA_SENSOR_INFO] */
+    "camera.shading", /* [CAMERA_SHADING] */
+    "camera.statistics", /* [CAMERA_STATISTICS] */
+    "camera.statistics.info", /* [CAMERA_STATISTICS_INFO] */
+    "camera.tonemap", /* [CAMERA_TONEMAP] */
+    "camera.led", /* [CAMERA_LED] */
+    "camera.info", /* [CAMERA_INFO] */
+    "camera.blackLevel", /* [CAMERA_BLACK_LEVEL] */
+    "camera.sync", /* [CAMERA_SYNC] */
+    "camera.reprocess", /* [CAMERA_REPROCESS] */
+    "intel.info", /* [INTEL_INFO] */
+    "intel.control", /* [INTEL_CONTROL] */
+    "intel.control_isp", /* [INTEL_CONTROL_ISP] */
+};
+
+unsigned int icamera_metadata_section_bounds[CAMERA_SECTION_COUNT][2] = {
+    /* [CAMERA_AE] */
+    { CAMERA_AE_START, CAMERA_AE_END },
+    /* [CAMERA_AWB] */
+    { CAMERA_AWB_START, CAMERA_AWB_END },
+    /* [CAMERA_AF] */
+    { CAMERA_AF_START, CAMERA_AF_END },
+    /* [CAMERA_CONTROL] */
+    { CAMERA_CONTROL_START, CAMERA_CONTROL_END },
+    /* [CAMERA_DEMOSAIC] */
+    { CAMERA_DEMOSAIC_START, CAMERA_DEMOSAIC_END },
+    /* [CAMERA_EDGE] */
+    { CAMERA_EDGE_START, CAMERA_EDGE_END },
+    /* [CAMERA_FLASH] */
+    { CAMERA_FLASH_START, CAMERA_FLASH_END },
+    /* [CAMERA_FLASH_INFO] */
+    { CAMERA_FLASH_INFO_START, CAMERA_FLASH_INFO_END },
+    /* [CAMERA_HOT_PIXEL] */
+    { CAMERA_HOT_PIXEL_START, CAMERA_HOT_PIXEL_END },
+    /* [CAMERA_JPEG] */
+    { CAMERA_JPEG_START, CAMERA_JPEG_END },
+    /* [CAMERA_LENS] */
+    { CAMERA_LENS_START, CAMERA_LENS_END },
+    /* [CAMERA_LENS_INFO] */
+    { CAMERA_LENS_INFO_START, CAMERA_LENS_INFO_END },
+    /* [CAMERA_NOISE_REDUCTION] */
+    { CAMERA_NOISE_REDUCTION_START, CAMERA_NOISE_REDUCTION_END },
+    /* [CAMERA_REQUEST] */
+    { CAMERA_REQUEST_START, CAMERA_REQUEST_END },
+    /* [CAMERA_SCALER] */
+    { CAMERA_SCALER_START, CAMERA_SCALER_END },
+    /* [CAMERA_SENSOR] */
+    { CAMERA_SENSOR_START, CAMERA_SENSOR_END },
+    /* [CAMERA_SENSOR_INFO] */
+    { CAMERA_SENSOR_INFO_START, CAMERA_SENSOR_INFO_END },
+    /* [CAMERA_SHADING] */
+    { CAMERA_SHADING_START, CAMERA_SHADING_END },
+    /* [CAMERA_STATISTICS] */
+    { CAMERA_STATISTICS_START, CAMERA_STATISTICS_END },
+    /* [CAMERA_STATISTICS_INFO] */
+    { CAMERA_STATISTICS_INFO_START, CAMERA_STATISTICS_INFO_END },
+    /* [CAMERA_TONEMAP] */
+    { CAMERA_TONEMAP_START, CAMERA_TONEMAP_END },
+    /* [CAMERA_LED] */
+    { CAMERA_LED_START, CAMERA_LED_END },
+    /* [CAMERA_INFO] */
+    { CAMERA_INFO_START, CAMERA_INFO_END },
+    /* [CAMERA_BLACK_LEVEL] */
+    { CAMERA_BLACK_LEVEL_START, CAMERA_BLACK_LEVEL_END },
+    /* [CAMERA_SYNC] */
+    { CAMERA_SYNC_START, CAMERA_SYNC_END },
+    /* [CAMERA_REPROCESS] */
+    { CAMERA_REPROCESS_START, CAMERA_REPROCESS_END },
+    /* [INTEL_INFO] */
+    { INTEL_INFO_START, INTEL_INFO_END },
+    /* [INTEL_CONTROL] */
+    { INTEL_CONTROL_START, INTEL_CONTROL_END },
+    /* [INTEL_CONTROL_ISP] */
+    { INTEL_CONTROL_ISP_START, INTEL_CONTROL_ISP_END },
+};
+
+static tag_info_t camera_ae[CAMERA_AE_END -
+        CAMERA_AE_START] = {
+    { "mode",                          ICAMERA_TYPE_BYTE   },
+    { "lock",                          ICAMERA_TYPE_BYTE   },
+    { "regions",                       ICAMERA_TYPE_INT32  },
+    { "antibandingMode",               ICAMERA_TYPE_BYTE   },
+    { "compensation",                  ICAMERA_TYPE_INT32  },
+    { "targetFpsRange",                ICAMERA_TYPE_FLOAT  },
+    { "precaptureTrigger",             ICAMERA_TYPE_BYTE   },
+    { "state",                         ICAMERA_TYPE_BYTE   },
+    { "availableModes",                ICAMERA_TYPE_BYTE   },
+    { "availableAntibandingModes",     ICAMERA_TYPE_BYTE   },
+    { "compensationStep",              ICAMERA_TYPE_RATIONAL
+                        },
+    { "compensationRange",             ICAMERA_TYPE_INT32  },
+    { "availableTargetFpsRanges",      ICAMERA_TYPE_FLOAT  },
+    { "lockAvailable",                 ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_awb[CAMERA_AWB_END -
+        CAMERA_AWB_START] = {
+    { "mode",                          ICAMERA_TYPE_BYTE   },
+    { "colorTransform",                ICAMERA_TYPE_FLOAT  },
+    { "colorGains",                    ICAMERA_TYPE_FLOAT  },
+    { "lock",                          ICAMERA_TYPE_BYTE   },
+    { "regions",                       ICAMERA_TYPE_INT32  },
+    { "cctRange",                      ICAMERA_TYPE_INT32  },
+    { "gains",                         ICAMERA_TYPE_INT32  },
+    { "gainShift",                     ICAMERA_TYPE_INT32  },
+    { "whitePoint",                    ICAMERA_TYPE_INT32  },
+    { "convergeSpeed",                 ICAMERA_TYPE_BYTE   },
+    { "convergeSpeedMode",             ICAMERA_TYPE_BYTE   },
+    { "state",                         ICAMERA_TYPE_BYTE   },
+    { "result",                        ICAMERA_TYPE_BYTE   },
+    { "availableModes",                ICAMERA_TYPE_BYTE   },
+    { "lockAvailable",                 ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_af[CAMERA_AF_END -
+        CAMERA_AF_START] = {
+    { "mode",                          ICAMERA_TYPE_BYTE   },
+    { "regions",                       ICAMERA_TYPE_INT32  },
+    { "trigger",                       ICAMERA_TYPE_BYTE   },
+    { "availableModes",                ICAMERA_TYPE_BYTE   },
+    { "state",                         ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_control[CAMERA_CONTROL_END -
+        CAMERA_CONTROL_START] = {
+    { "captureintent",                 ICAMERA_TYPE_BYTE   },
+    { "effectMode",                    ICAMERA_TYPE_BYTE   },
+    { "mode",                          ICAMERA_TYPE_BYTE   },
+    { "sceneMode",                     ICAMERA_TYPE_BYTE   },
+    { "videoStabilizationMode",        ICAMERA_TYPE_BYTE   },
+    { "availableEffects",              ICAMERA_TYPE_BYTE   },
+    { "availableModes",                ICAMERA_TYPE_BYTE   },
+    { "availableSceneModes",           ICAMERA_TYPE_BYTE   },
+    { "availableVideoStabilizationModes",
+                                        ICAMERA_TYPE_BYTE   },
+    { "maxRegions",                    ICAMERA_TYPE_INT32  },
+    { "sceneModeOverrides",            ICAMERA_TYPE_BYTE   },
+    { "availableHighSpeedVideoConfigurations",
+                                        ICAMERA_TYPE_INT32  },
+};
+
+static tag_info_t camera_demosaic[CAMERA_DEMOSAIC_END -
+        CAMERA_DEMOSAIC_START] = {
+    { "mode",                          ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_edge[CAMERA_EDGE_END -
+        CAMERA_EDGE_START] = {
+    { "mode",                          ICAMERA_TYPE_BYTE   },
+    { "strength",                      ICAMERA_TYPE_BYTE   },
+    { "availableEdgeModes",            ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_flash[CAMERA_FLASH_END -
+        CAMERA_FLASH_START] = {
+    { "firingPower",                   ICAMERA_TYPE_BYTE   },
+    { "firingTime",                    ICAMERA_TYPE_INT64  },
+    { "mode",                          ICAMERA_TYPE_BYTE   },
+    { "colorTemperature",              ICAMERA_TYPE_BYTE   },
+    { "maxEnergy",                     ICAMERA_TYPE_BYTE   },
+    { "state",                         ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_flash_info[CAMERA_FLASH_INFO_END -
+        CAMERA_FLASH_INFO_START] = {
+    { "available",                     ICAMERA_TYPE_BYTE   },
+    { "chargeDuration",                ICAMERA_TYPE_INT64  },
+};
+
+static tag_info_t camera_hot_pixel[CAMERA_HOT_PIXEL_END -
+        CAMERA_HOT_PIXEL_START] = {
+    { "mode",                          ICAMERA_TYPE_BYTE   },
+    { "availableHotPixelModes",        ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_jpeg[CAMERA_JPEG_END -
+        CAMERA_JPEG_START] = {
+    { "gpsCoordinates",                ICAMERA_TYPE_DOUBLE },
+    { "gpsProcessingMethod",           ICAMERA_TYPE_BYTE   },
+    { "gpsTimestamp",                  ICAMERA_TYPE_INT64  },
+    { "orientation",                   ICAMERA_TYPE_INT32  },
+    { "quality",                       ICAMERA_TYPE_BYTE   },
+    { "thumbnailQuality",              ICAMERA_TYPE_BYTE   },
+    { "thumbnailSize",                 ICAMERA_TYPE_INT32  },
+    { "availableThumbnailSizes",       ICAMERA_TYPE_INT32  },
+    { "maxSize",                       ICAMERA_TYPE_INT32  },
+    { "size",                          ICAMERA_TYPE_INT32  },
+};
+
+static tag_info_t camera_lens[CAMERA_LENS_END -
+        CAMERA_LENS_START] = {
+    { "aperture",                      ICAMERA_TYPE_FLOAT  },
+    { "filterDensity",                 ICAMERA_TYPE_FLOAT  },
+    { "focalLength",                   ICAMERA_TYPE_FLOAT  },
+    { "focusDistance",                 ICAMERA_TYPE_FLOAT  },
+    { "opticalStabilizationMode",      ICAMERA_TYPE_BYTE   },
+    { "facing",                        ICAMERA_TYPE_BYTE   },
+    { "focusRange",                    ICAMERA_TYPE_FLOAT  },
+    { "state",                         ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_lens_info[CAMERA_LENS_INFO_END -
+        CAMERA_LENS_INFO_START] = {
+    { "availableApertures",            ICAMERA_TYPE_FLOAT  },
+    { "availableFilterDensities",      ICAMERA_TYPE_FLOAT  },
+    { "availableFocalLengths",         ICAMERA_TYPE_FLOAT  },
+    { "availableOpticalStabilization", ICAMERA_TYPE_BYTE   },
+    { "hyperfocalDistance",            ICAMERA_TYPE_FLOAT  },
+    { "minimumFocusDistance",          ICAMERA_TYPE_FLOAT  },
+    { "shadingMapSize",                ICAMERA_TYPE_INT32  },
+    { "focusDistanceCalibration",      ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_noise_reduction[CAMERA_NOISE_REDUCTION_END -
+        CAMERA_NOISE_REDUCTION_START] = {
+    { "mode",                          ICAMERA_TYPE_BYTE   },
+    { "strength",                      ICAMERA_TYPE_BYTE   },
+    { "availableNoiseReductionModes",  ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_request[CAMERA_REQUEST_END -
+        CAMERA_REQUEST_START] = {
+    { "id",                            ICAMERA_TYPE_INT32  },
+    { "metadataMode",                  ICAMERA_TYPE_BYTE   },
+    { "maxNumOutputStreams",           ICAMERA_TYPE_INT32  },
+    { "maxNumInputStreams",            ICAMERA_TYPE_INT32  },
+    { "pipelineDepth",                 ICAMERA_TYPE_BYTE   },
+    { "pipelineMaxDepth",              ICAMERA_TYPE_BYTE   },
+    { "partialResultCount",            ICAMERA_TYPE_INT32  },
+    { "availableCapabilities",         ICAMERA_TYPE_BYTE   },
+    { "availableRequestKeys",          ICAMERA_TYPE_INT32  },
+    { "availableResultKeys",           ICAMERA_TYPE_INT32  },
+    { "availableCharacteristicsKeys",  ICAMERA_TYPE_INT32  },
+};
+
+static tag_info_t camera_scaler[CAMERA_SCALER_END -
+        CAMERA_SCALER_START] = {
+    { "cropRegion",                    ICAMERA_TYPE_INT32  },
+    { "availableJpegSizes",            ICAMERA_TYPE_INT32  },
+    { "availableMaxDigitalZoom",       ICAMERA_TYPE_FLOAT  },
+    { "availableInputOutputFormatsMap",
+                                        ICAMERA_TYPE_INT32  },
+    { "availableStreamConfigurations", ICAMERA_TYPE_INT32  },
+    { "availableMinFrameDurations",    ICAMERA_TYPE_INT64  },
+    { "availableStallDurations",       ICAMERA_TYPE_INT64  },
+    { "croppingType",                  ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_sensor[CAMERA_SENSOR_END -
+        CAMERA_SENSOR_START] = {
+    { "exposureTime",                  ICAMERA_TYPE_INT64  },
+    { "frameDuration",                 ICAMERA_TYPE_INT64  },
+    { "sensitivity",                   ICAMERA_TYPE_INT32  },
+    { "referenceIlluminant1",          ICAMERA_TYPE_BYTE   },
+    { "referenceIlluminant2",          ICAMERA_TYPE_BYTE   },
+    { "calibrationTransform1",         ICAMERA_TYPE_RATIONAL
+                        },
+    { "calibrationTransform2",         ICAMERA_TYPE_RATIONAL
+                        },
+    { "colorTransform1",               ICAMERA_TYPE_RATIONAL
+                        },
+    { "colorTransform2",               ICAMERA_TYPE_RATIONAL
+                        },
+    { "forwardMatrix1",                ICAMERA_TYPE_RATIONAL
+                        },
+    { "forwardMatrix2",                ICAMERA_TYPE_RATIONAL
+                        },
+    { "baseGainFactor",                ICAMERA_TYPE_RATIONAL
+                        },
+    { "blackLevelPattern",             ICAMERA_TYPE_INT32  },
+    { "maxAnalogSensitivity",          ICAMERA_TYPE_INT32  },
+    { "orientation",                   ICAMERA_TYPE_INT32  },
+    { "profileHueSatMapDimensions",    ICAMERA_TYPE_INT32  },
+    { "timestamp",                     ICAMERA_TYPE_INT64  },
+    { "temperature",                   ICAMERA_TYPE_FLOAT  },
+    { "neutralColorPoint",             ICAMERA_TYPE_RATIONAL
+                        },
+    { "noiseProfile",                  ICAMERA_TYPE_DOUBLE },
+    { "profileHueSatMap",              ICAMERA_TYPE_FLOAT  },
+    { "profileToneCurve",              ICAMERA_TYPE_FLOAT  },
+    { "greenSplit",                    ICAMERA_TYPE_FLOAT  },
+    { "testPatternData",               ICAMERA_TYPE_INT32  },
+    { "testPatternMode",               ICAMERA_TYPE_INT32  },
+    { "availableTestPatternModes",     ICAMERA_TYPE_INT32  },
+    { "opaqueRawSize",                 ICAMERA_TYPE_INT32  },
+    { "rollingShutterSkew",            ICAMERA_TYPE_INT64  },
+};
+
+static tag_info_t camera_sensor_info[CAMERA_SENSOR_INFO_END -
+        CAMERA_SENSOR_INFO_START] = {
+    { "activeArraySize",               ICAMERA_TYPE_INT32  },
+    { "sensitivityRange",              ICAMERA_TYPE_INT32  },
+    { "colorFilterArrangement",        ICAMERA_TYPE_BYTE   },
+    { "exposureTimeRange",             ICAMERA_TYPE_INT64  },
+    { "maxFrameDuration",              ICAMERA_TYPE_INT64  },
+    { "physicalSize",                  ICAMERA_TYPE_FLOAT  },
+    { "pixelArraySize",                ICAMERA_TYPE_INT32  },
+    { "whiteLevel",                    ICAMERA_TYPE_INT32  },
+    { "timestampSource",               ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_shading[CAMERA_SHADING_END -
+        CAMERA_SHADING_START] = {
+    { "mode",                          ICAMERA_TYPE_BYTE   },
+    { "strength",                      ICAMERA_TYPE_BYTE   },
+    { "availableModes",                ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_statistics[CAMERA_STATISTICS_END -
+        CAMERA_STATISTICS_START] = {
+    { "faceDetectMode",                ICAMERA_TYPE_BYTE   },
+    { "histogramMode",                 ICAMERA_TYPE_BYTE   },
+    { "sharpnessMapMode",              ICAMERA_TYPE_BYTE   },
+    { "hotPixelMapMode",               ICAMERA_TYPE_BYTE   },
+    { "faceIds",                       ICAMERA_TYPE_INT32  },
+    { "faceLandmarks",                 ICAMERA_TYPE_INT32  },
+    { "faceRectangles",                ICAMERA_TYPE_INT32  },
+    { "faceScores",                    ICAMERA_TYPE_BYTE   },
+    { "histogram",                     ICAMERA_TYPE_INT32  },
+    { "sharpnessMap",                  ICAMERA_TYPE_INT32  },
+    { "lensShadingCorrectionMap",      ICAMERA_TYPE_BYTE   },
+    { "lensShadingMap",                ICAMERA_TYPE_FLOAT  },
+    { "predictedColorGains",           ICAMERA_TYPE_FLOAT  },
+    { "predictedColorTransform",       ICAMERA_TYPE_RATIONAL
+                        },
+    { "sceneFlicker",                  ICAMERA_TYPE_BYTE   },
+    { "hotPixelMap",                   ICAMERA_TYPE_INT32  },
+    { "lensShadingMapMode",            ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_statistics_info[CAMERA_STATISTICS_INFO_END -
+        CAMERA_STATISTICS_INFO_START] = {
+    { "availableFaceDetectModes",      ICAMERA_TYPE_BYTE   },
+    { "histogramBucketCount",          ICAMERA_TYPE_INT32  },
+    { "maxFaceCount",                  ICAMERA_TYPE_INT32  },
+    { "maxHistogramCount",             ICAMERA_TYPE_INT32  },
+    { "maxSharpnessMapValue",          ICAMERA_TYPE_INT32  },
+    { "sharpnessMapSize",              ICAMERA_TYPE_INT32  },
+    { "availableHotPixelMapModes",     ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_tonemap[CAMERA_TONEMAP_END -
+        CAMERA_TONEMAP_START] = {
+    { "curveBlue",                     ICAMERA_TYPE_FLOAT  },
+    { "curveGreen",                    ICAMERA_TYPE_FLOAT  },
+    { "curveRed",                      ICAMERA_TYPE_FLOAT  },
+    { "mode",                          ICAMERA_TYPE_BYTE   },
+    { "maxCurvePoints",                ICAMERA_TYPE_INT32  },
+    { "availableToneMapModes",         ICAMERA_TYPE_BYTE   },
+    { "gamma",                         ICAMERA_TYPE_FLOAT  },
+    { "presetCurve",                   ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_led[CAMERA_LED_END -
+        CAMERA_LED_START] = {
+    { "transmit",                      ICAMERA_TYPE_BYTE   },
+    { "availableLeds",                 ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_info[CAMERA_INFO_END -
+        CAMERA_INFO_START] = {
+    { "supportedHardwareLevel",        ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_black_level[CAMERA_BLACK_LEVEL_END -
+        CAMERA_BLACK_LEVEL_START] = {
+    { "lock",                          ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t camera_sync[CAMERA_SYNC_END -
+        CAMERA_SYNC_START] = {
+    { "frameNumber",                   ICAMERA_TYPE_INT64  },
+    { "maxLatency",                    ICAMERA_TYPE_INT32  },
+};
+
+static tag_info_t camera_reprocess[CAMERA_REPROCESS_END -
+        CAMERA_REPROCESS_START] = {
+    { "maxCaptureStall",               ICAMERA_TYPE_INT32  },
+};
+
+static tag_info_t intel_info[INTEL_INFO_END -
+        INTEL_INFO_START] = {
+    { "availableConfigurations",       ICAMERA_TYPE_INT32  },
+    { "availableFeatures",             ICAMERA_TYPE_BYTE   },
+    { "aeExposureTimeRange",           ICAMERA_TYPE_INT32  },
+    { "aeGainRange",                   ICAMERA_TYPE_INT32  },
+    { "wfov",                          ICAMERA_TYPE_BYTE   },
+    { "sensorMountType",               ICAMERA_TYPE_BYTE   },
+};
+
+static tag_info_t intel_control[INTEL_CONTROL_END -
+        INTEL_CONTROL_START] = {
+    { "imageEnhancement",              ICAMERA_TYPE_INT32  },
+    { "sensitivityGain",               ICAMERA_TYPE_FLOAT  },
+    { "frameRate",                     ICAMERA_TYPE_FLOAT  },
+    { "aeConvergeSpeed",               ICAMERA_TYPE_BYTE   },
+    { "nrMode",                        ICAMERA_TYPE_BYTE   },
+    { "nrLevel",                       ICAMERA_TYPE_INT32  },
+    { "irisMode",                      ICAMERA_TYPE_BYTE   },
+    { "aeDistributionPriority",        ICAMERA_TYPE_BYTE   },
+    { "irisLevel",                     ICAMERA_TYPE_INT32  },
+    { "wdrMode",                       ICAMERA_TYPE_BYTE   },
+    { "wdrLevel",                      ICAMERA_TYPE_BYTE   },
+    { "blcAreaMode",                   ICAMERA_TYPE_BYTE   },
+    { "sceneMode",                     ICAMERA_TYPE_BYTE   },
+    { "weightGridMode",                ICAMERA_TYPE_BYTE   },
+    { "aeConvergeSpeedMode",           ICAMERA_TYPE_BYTE   },
+    { "deinterlaceMode",               ICAMERA_TYPE_BYTE   },
+    { "makernoteData",                 ICAMERA_TYPE_BYTE   },
+    { "customAicParam",                ICAMERA_TYPE_BYTE   },
+    { "makernoteMode",                 ICAMERA_TYPE_BYTE   },
+    { "yuvColorRange",                 ICAMERA_TYPE_BYTE   },
+    { "sensitivityGainRange",          ICAMERA_TYPE_FLOAT  },
+    { "exposureTimeRange",             ICAMERA_TYPE_INT32  },
+    { "fisheyeDewarpingMode",          ICAMERA_TYPE_BYTE   },
+    { "ltmTuningData",                 ICAMERA_TYPE_BYTE   },
+    { "digitalZoomRatio",              ICAMERA_TYPE_FLOAT  },
+    { "ldcMode",                       ICAMERA_TYPE_BYTE   },
+    { "rscMode",                       ICAMERA_TYPE_BYTE   },
+    { "flipMode",                      ICAMERA_TYPE_BYTE   },
+    { "monoDownscale",                 ICAMERA_TYPE_BYTE   },
+    { "run3ACadence",                  ICAMERA_TYPE_INT32  },
+    { "viewProjection",                ICAMERA_TYPE_BYTE   },
+    { "viewRotation",                  ICAMERA_TYPE_BYTE   },
+    { "viewFineAdjustments",           ICAMERA_TYPE_BYTE   },
+    { "cameraRotation",                ICAMERA_TYPE_BYTE   },
+    { "scalerCropRegion",              ICAMERA_TYPE_INT32  },
+};
+
+static tag_info_t intel_control_isp[INTEL_CONTROL_ISP_END -
+        INTEL_CONTROL_ISP_START] = {
+    { "supportedCtrlIds",              ICAMERA_TYPE_INT32  },
+    { "enabledCtrlIds",                ICAMERA_TYPE_INT32  },
+    { "wb_gains",                      ICAMERA_TYPE_BYTE   },
+    { "color_correction_matrix",       ICAMERA_TYPE_BYTE   },
+    { "advanced_color_correction_matrix",
+                                        ICAMERA_TYPE_BYTE   },
+    { "bxt_csc",                       ICAMERA_TYPE_BYTE   },
+    { "bxt_demosaic",                  ICAMERA_TYPE_BYTE   },
+    { "sc_iefd",                       ICAMERA_TYPE_BYTE   },
+    { "see",                           ICAMERA_TYPE_BYTE   },
+    { "bnlm",                          ICAMERA_TYPE_BYTE   },
+    { "tnr5_21",                       ICAMERA_TYPE_BYTE   },
+    { "xnr_dss",                       ICAMERA_TYPE_BYTE   },
+    { "gamma_tone_map",                ICAMERA_TYPE_BYTE   },
+    { "tnr5_22",                       ICAMERA_TYPE_BYTE   },
+    { "tnr5_25",                       ICAMERA_TYPE_BYTE   },
+};
+
+tag_info_t *icamera_tag_info[CAMERA_SECTION_COUNT] = {
+    camera_ae,
+    camera_awb,
+    camera_af,
+    camera_control,
+    camera_demosaic,
+    camera_edge,
+    camera_flash,
+    camera_flash_info,
+    camera_hot_pixel,
+    camera_jpeg,
+    camera_lens,
+    camera_lens_info,
+    camera_noise_reduction,
+    camera_request,
+    camera_scaler,
+    camera_sensor,
+    camera_sensor_info,
+    camera_shading,
+    camera_statistics,
+    camera_statistics_info,
+    camera_tonemap,
+    camera_led,
+    camera_info,
+    camera_black_level,
+    camera_sync,
+    camera_reprocess,
+    intel_info,
+    intel_control,
+    intel_control_isp,
+};
+
+int icamera_metadata_enum_snprint(uint32_t tag,
+                                  int32_t value,
+                                  char *dst,
+                                  size_t size) {
+    const char *msg = "error: not an enum";
+    int ret = -1;
+
+    switch(tag) {
+        case CAMERA_AE_MODE: {
+            switch (value) {
+                case CAMERA_AE_MODE_MANUAL:
+                    msg = "MANUAL";
+                    ret = 0;
+                    break;
+                case CAMERA_AE_MODE_AUTO:
+                    msg = "AUTO";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_AE_LOCK: {
+            switch (value) {
+                case CAMERA_AE_LOCK_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_AE_LOCK_ON:
+                    msg = "ON";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_AE_REGIONS: {
+            break;
+        }
+        case CAMERA_AE_ANTIBANDING_MODE: {
+            switch (value) {
+                case CAMERA_AE_ANTIBANDING_MODE_AUTO:
+                    msg = "AUTO";
+                    ret = 0;
+                    break;
+                case CAMERA_AE_ANTIBANDING_MODE_50HZ:
+                    msg = "50HZ";
+                    ret = 0;
+                    break;
+                case CAMERA_AE_ANTIBANDING_MODE_60HZ:
+                    msg = "60HZ";
+                    ret = 0;
+                    break;
+                case CAMERA_AE_ANTIBANDING_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_AE_COMPENSATION: {
+            break;
+        }
+        case CAMERA_AE_TARGET_FPS_RANGE: {
+            break;
+        }
+        case CAMERA_AE_PRECAPTURE_TRIGGER: {
+            switch (value) {
+                case CAMERA_AE_PRECAPTURE_TRIGGER_IDLE:
+                    msg = "IDLE";
+                    ret = 0;
+                    break;
+                case CAMERA_AE_PRECAPTURE_TRIGGER_START:
+                    msg = "START";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_AE_STATE: {
+            switch (value) {
+                case CAMERA_AE_STATE_INACTIVE:
+                    msg = "INACTIVE";
+                    ret = 0;
+                    break;
+                case CAMERA_AE_STATE_SEARCHING:
+                    msg = "SEARCHING";
+                    ret = 0;
+                    break;
+                case CAMERA_AE_STATE_CONVERGED:
+                    msg = "CONVERGED";
+                    ret = 0;
+                    break;
+                case CAMERA_AE_STATE_LOCKED:
+                    msg = "LOCKED";
+                    ret = 0;
+                    break;
+                case CAMERA_AE_STATE_FLASH_REQUIRED:
+                    msg = "FLASH_REQUIRED";
+                    ret = 0;
+                    break;
+                case CAMERA_AE_STATE_PRECAPTURE:
+                    msg = "PRECAPTURE";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_AE_AVAILABLE_MODES: {
+            break;
+        }
+        case CAMERA_AE_AVAILABLE_ANTIBANDING_MODES: {
+            break;
+        }
+        case CAMERA_AE_COMPENSATION_STEP: {
+            break;
+        }
+        case CAMERA_AE_COMPENSATION_RANGE: {
+            break;
+        }
+        case CAMERA_AE_AVAILABLE_TARGET_FPS_RANGES: {
+            break;
+        }
+        case CAMERA_AE_LOCK_AVAILABLE: {
+            switch (value) {
+                case CAMERA_AE_LOCK_AVAILABLE_FALSE:
+                    msg = "FALSE";
+                    ret = 0;
+                    break;
+                case CAMERA_AE_LOCK_AVAILABLE_TRUE:
+                    msg = "TRUE";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+
+        case CAMERA_AWB_MODE: {
+            switch (value) {
+                case CAMERA_AWB_MODE_AUTO:
+                    msg = "AUTO";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_MODE_INCANDESCENT:
+                    msg = "INCANDESCENT";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_MODE_FLUORESCENT:
+                    msg = "FLUORESCENT";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_MODE_DAYLIGHT:
+                    msg = "DAYLIGHT";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_MODE_FULL_OVERCAST:
+                    msg = "FULL_OVERCAST";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_MODE_PARTLY_OVERCAST:
+                    msg = "PARTLY_OVERCAST";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_MODE_SUNSET:
+                    msg = "SUNSET";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_MODE_VIDEO_CONFERENCE:
+                    msg = "VIDEO_CONFERENCE";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_MODE_MANUAL_CCT_RANGE:
+                    msg = "MANUAL_CCT_RANGE";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_MODE_MANUAL_WHITE_POINT:
+                    msg = "MANUAL_WHITE_POINT";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_MODE_MANUAL_GAIN:
+                    msg = "MANUAL_GAIN";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_MODE_MANUAL_COLOR_TRANSFORM:
+                    msg = "MANUAL_COLOR_TRANSFORM";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_AWB_COLOR_TRANSFORM: {
+            break;
+        }
+        case CAMERA_AWB_COLOR_GAINS: {
+            break;
+        }
+        case CAMERA_AWB_LOCK: {
+            switch (value) {
+                case CAMERA_AWB_LOCK_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_LOCK_ON:
+                    msg = "ON";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_AWB_REGIONS: {
+            break;
+        }
+        case CAMERA_AWB_CCT_RANGE: {
+            break;
+        }
+        case CAMERA_AWB_GAINS: {
+            break;
+        }
+        case CAMERA_AWB_GAIN_SHIFT: {
+            break;
+        }
+        case CAMERA_AWB_WHITE_POINT: {
+            break;
+        }
+        case CAMERA_AWB_CONVERGE_SPEED: {
+            switch (value) {
+                case CAMERA_AWB_CONVERGE_SPEED_NORMAL:
+                    msg = "NORMAL";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_CONVERGE_SPEED_MID:
+                    msg = "MID";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_CONVERGE_SPEED_LOW:
+                    msg = "LOW";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_AWB_CONVERGE_SPEED_MODE: {
+            switch (value) {
+                case CAMERA_AWB_CONVERGE_SPEED_MODE_HAL:
+                    msg = "HAL";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_CONVERGE_SPEED_MODE_AIQ:
+                    msg = "AIQ";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_AWB_STATE: {
+            switch (value) {
+                case CAMERA_AWB_STATE_INACTIVE:
+                    msg = "INACTIVE";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_STATE_SEARCHING:
+                    msg = "SEARCHING";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_STATE_CONVERGED:
+                    msg = "CONVERGED";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_STATE_LOCKED:
+                    msg = "LOCKED";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_AWB_RESULT: {
+            break;
+        }
+        case CAMERA_AWB_AVAILABLE_MODES: {
+            break;
+        }
+        case CAMERA_AWB_LOCK_AVAILABLE: {
+            switch (value) {
+                case CAMERA_AWB_LOCK_AVAILABLE_FALSE:
+                    msg = "FALSE";
+                    ret = 0;
+                    break;
+                case CAMERA_AWB_LOCK_AVAILABLE_TRUE:
+                    msg = "TRUE";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+
+        case CAMERA_AF_MODE: {
+            switch (value) {
+                case CAMERA_AF_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_AF_MODE_AUTO:
+                    msg = "AUTO";
+                    ret = 0;
+                    break;
+                case CAMERA_AF_MODE_MACRO:
+                    msg = "MACRO";
+                    ret = 0;
+                    break;
+                case CAMERA_AF_MODE_CONTINUOUS_VIDEO:
+                    msg = "CONTINUOUS_VIDEO";
+                    ret = 0;
+                    break;
+                case CAMERA_AF_MODE_CONTINUOUS_PICTURE:
+                    msg = "CONTINUOUS_PICTURE";
+                    ret = 0;
+                    break;
+                case CAMERA_AF_MODE_EDOF:
+                    msg = "EDOF";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_AF_REGIONS: {
+            break;
+        }
+        case CAMERA_AF_TRIGGER: {
+            switch (value) {
+                case CAMERA_AF_TRIGGER_IDLE:
+                    msg = "IDLE";
+                    ret = 0;
+                    break;
+                case CAMERA_AF_TRIGGER_START:
+                    msg = "START";
+                    ret = 0;
+                    break;
+                case CAMERA_AF_TRIGGER_CANCEL:
+                    msg = "CANCEL";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_AF_AVAILABLE_MODES: {
+            break;
+        }
+        case CAMERA_AF_STATE: {
+            switch (value) {
+                case CAMERA_AF_STATE_INACTIVE:
+                    msg = "INACTIVE";
+                    ret = 0;
+                    break;
+                case CAMERA_AF_STATE_PASSIVE_SCAN:
+                    msg = "PASSIVE_SCAN";
+                    ret = 0;
+                    break;
+                case CAMERA_AF_STATE_PASSIVE_FOCUSED:
+                    msg = "PASSIVE_FOCUSED";
+                    ret = 0;
+                    break;
+                case CAMERA_AF_STATE_ACTIVE_SCAN:
+                    msg = "ACTIVE_SCAN";
+                    ret = 0;
+                    break;
+                case CAMERA_AF_STATE_FOCUSED_LOCKED:
+                    msg = "FOCUSED_LOCKED";
+                    ret = 0;
+                    break;
+                case CAMERA_AF_STATE_NOT_FOCUSED_LOCKED:
+                    msg = "NOT_FOCUSED_LOCKED";
+                    ret = 0;
+                    break;
+                case CAMERA_AF_STATE_PASSIVE_UNFOCUSED:
+                    msg = "PASSIVE_UNFOCUSED";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+
+        case CAMERA_CONTROL_CAPTUREINTENT: {
+            switch (value) {
+                case CAMERA_CONTROL_CAPTUREINTENT_CUSTOM:
+                    msg = "CUSTOM";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_CAPTUREINTENT_PREVIEW:
+                    msg = "PREVIEW";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_CAPTUREINTENT_STILL_CAPTURE:
+                    msg = "STILL_CAPTURE";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_CAPTUREINTENT_VIDEO_RECORD:
+                    msg = "VIDEO_RECORD";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_CAPTUREINTENT_VIDEO_SNAPSHOT:
+                    msg = "VIDEO_SNAPSHOT";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_CAPTUREINTENT_ZERO_SHUTTER_LAG:
+                    msg = "ZERO_SHUTTER_LAG";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_CAPTUREINTENT_MANUAL:
+                    msg = "MANUAL";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_CAPTUREINTENT_MOTION_TRACKING:
+                    msg = "MOTION_TRACKING";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_CONTROL_EFFECT_MODE: {
+            switch (value) {
+                case CAMERA_CONTROL_EFFECT_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_EFFECT_MODE_MONO:
+                    msg = "MONO";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_EFFECT_MODE_NEGATIVE:
+                    msg = "NEGATIVE";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_EFFECT_MODE_SOLARIZE:
+                    msg = "SOLARIZE";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_EFFECT_MODE_SEPIA:
+                    msg = "SEPIA";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_EFFECT_MODE_POSTERIZE:
+                    msg = "POSTERIZE";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_EFFECT_MODE_WHITEBOARD:
+                    msg = "WHITEBOARD";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_EFFECT_MODE_BLACKBOARD:
+                    msg = "BLACKBOARD";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_EFFECT_MODE_AQUA:
+                    msg = "AQUA";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_CONTROL_MODE: {
+            switch (value) {
+                case CAMERA_CONTROL_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_MODE_AUTO:
+                    msg = "AUTO";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_MODE_USE_SCENE_MODE:
+                    msg = "USE_SCENE_MODE";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_MODE_OFF_KEEP_STATE:
+                    msg = "OFF_KEEP_STATE";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_CONTROL_SCENE_MODE: {
+            switch (value) {
+                case CAMERA_CONTROL_SCENE_MODE_DISABLED:
+                    msg = "DISABLED";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_FACE_PRIORITY:
+                    msg = "FACE_PRIORITY";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_ACTION:
+                    msg = "ACTION";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_PORTRAIT:
+                    msg = "PORTRAIT";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_LANDSCAPE:
+                    msg = "LANDSCAPE";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_NIGHT:
+                    msg = "NIGHT";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_NIGHT_PORTRAIT:
+                    msg = "NIGHT_PORTRAIT";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_THEATRE:
+                    msg = "THEATRE";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_BEACH:
+                    msg = "BEACH";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_SNOW:
+                    msg = "SNOW";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_SUNSET:
+                    msg = "SUNSET";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_STEADYPHOTO:
+                    msg = "STEADYPHOTO";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_FIREWORKS:
+                    msg = "FIREWORKS";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_SPORTS:
+                    msg = "SPORTS";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_PARTY:
+                    msg = "PARTY";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_CANDLELIGHT:
+                    msg = "CANDLELIGHT";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_BARCODE:
+                    msg = "BARCODE";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_HIGH_SPEED_VIDEO:
+                    msg = "HIGH_SPEED_VIDEO";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_SCENE_MODE_HDR:
+                    msg = "HDR";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_CONTROL_VIDEO_STABILIZATION_MODE: {
+            switch (value) {
+                case CAMERA_CONTROL_VIDEO_STABILIZATION_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_CONTROL_VIDEO_STABILIZATION_MODE_ON:
+                    msg = "ON";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_CONTROL_AVAILABLE_EFFECTS: {
+            break;
+        }
+        case CAMERA_CONTROL_AVAILABLE_MODES: {
+            break;
+        }
+        case CAMERA_CONTROL_AVAILABLE_SCENE_MODES: {
+            break;
+        }
+        case CAMERA_CONTROL_AVAILABLE_VIDEO_STABILIZATION_MODES: {
+            break;
+        }
+        case CAMERA_CONTROL_MAX_REGIONS: {
+            break;
+        }
+        case CAMERA_CONTROL_SCENE_MODE_OVERRIDES: {
+            break;
+        }
+        case CAMERA_CONTROL_AVAILABLE_HIGH_SPEED_VIDEO_CONFIGURATIONS: {
+            break;
+        }
+
+        case CAMERA_DEMOSAIC_MODE: {
+            switch (value) {
+                case CAMERA_DEMOSAIC_MODE_FAST:
+                    msg = "FAST";
+                    ret = 0;
+                    break;
+                case CAMERA_DEMOSAIC_MODE_HIGH_QUALITY:
+                    msg = "HIGH_QUALITY";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+
+        case CAMERA_EDGE_MODE: {
+            switch (value) {
+                case CAMERA_EDGE_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_EDGE_MODE_FAST:
+                    msg = "FAST";
+                    ret = 0;
+                    break;
+                case CAMERA_EDGE_MODE_HIGH_QUALITY:
+                    msg = "HIGH_QUALITY";
+                    ret = 0;
+                    break;
+                case CAMERA_EDGE_MODE_ZERO_SHUTTER_LAG:
+                    msg = "ZERO_SHUTTER_LAG";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_EDGE_STRENGTH: {
+            break;
+        }
+        case CAMERA_EDGE_AVAILABLE_EDGE_MODES: {
+            break;
+        }
+
+        case CAMERA_FLASH_FIRING_POWER: {
+            break;
+        }
+        case CAMERA_FLASH_FIRING_TIME: {
+            break;
+        }
+        case CAMERA_FLASH_MODE: {
+            switch (value) {
+                case CAMERA_FLASH_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_FLASH_MODE_SINGLE:
+                    msg = "SINGLE";
+                    ret = 0;
+                    break;
+                case CAMERA_FLASH_MODE_TORCH:
+                    msg = "TORCH";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_FLASH_COLOR_TEMPERATURE: {
+            break;
+        }
+        case CAMERA_FLASH_MAX_ENERGY: {
+            break;
+        }
+        case CAMERA_FLASH_STATE: {
+            switch (value) {
+                case CAMERA_FLASH_STATE_UNAVAILABLE:
+                    msg = "UNAVAILABLE";
+                    ret = 0;
+                    break;
+                case CAMERA_FLASH_STATE_CHARGING:
+                    msg = "CHARGING";
+                    ret = 0;
+                    break;
+                case CAMERA_FLASH_STATE_READY:
+                    msg = "READY";
+                    ret = 0;
+                    break;
+                case CAMERA_FLASH_STATE_FIRED:
+                    msg = "FIRED";
+                    ret = 0;
+                    break;
+                case CAMERA_FLASH_STATE_PARTIAL:
+                    msg = "PARTIAL";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+
+        case CAMERA_FLASH_INFO_AVAILABLE: {
+            switch (value) {
+                case CAMERA_FLASH_INFO_AVAILABLE_FALSE:
+                    msg = "FALSE";
+                    ret = 0;
+                    break;
+                case CAMERA_FLASH_INFO_AVAILABLE_TRUE:
+                    msg = "TRUE";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_FLASH_INFO_CHARGE_DURATION: {
+            break;
+        }
+
+        case CAMERA_HOT_PIXEL_MODE: {
+            switch (value) {
+                case CAMERA_HOT_PIXEL_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_HOT_PIXEL_MODE_FAST:
+                    msg = "FAST";
+                    ret = 0;
+                    break;
+                case CAMERA_HOT_PIXEL_MODE_HIGH_QUALITY:
+                    msg = "HIGH_QUALITY";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES: {
+            break;
+        }
+
+        case CAMERA_JPEG_GPS_COORDINATES: {
+            break;
+        }
+        case CAMERA_JPEG_GPS_PROCESSING_METHOD: {
+            break;
+        }
+        case CAMERA_JPEG_GPS_TIMESTAMP: {
+            break;
+        }
+        case CAMERA_JPEG_ORIENTATION: {
+            break;
+        }
+        case CAMERA_JPEG_QUALITY: {
+            break;
+        }
+        case CAMERA_JPEG_THUMBNAIL_QUALITY: {
+            break;
+        }
+        case CAMERA_JPEG_THUMBNAIL_SIZE: {
+            break;
+        }
+        case CAMERA_JPEG_AVAILABLE_THUMBNAIL_SIZES: {
+            break;
+        }
+        case CAMERA_JPEG_MAX_SIZE: {
+            break;
+        }
+        case CAMERA_JPEG_SIZE: {
+            break;
+        }
+
+        case CAMERA_LENS_APERTURE: {
+            break;
+        }
+        case CAMERA_LENS_FILTER_DENSITY: {
+            break;
+        }
+        case CAMERA_LENS_FOCAL_LENGTH: {
+            break;
+        }
+        case CAMERA_LENS_FOCUS_DISTANCE: {
+            break;
+        }
+        case CAMERA_LENS_OPTICAL_STABILIZATION_MODE: {
+            switch (value) {
+                case CAMERA_LENS_OPTICAL_STABILIZATION_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_LENS_OPTICAL_STABILIZATION_MODE_ON:
+                    msg = "ON";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_LENS_FACING: {
+            switch (value) {
+                case CAMERA_LENS_FACING_FRONT:
+                    msg = "FRONT";
+                    ret = 0;
+                    break;
+                case CAMERA_LENS_FACING_BACK:
+                    msg = "BACK";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_LENS_FOCUS_RANGE: {
+            break;
+        }
+        case CAMERA_LENS_STATE: {
+            switch (value) {
+                case CAMERA_LENS_STATE_STATIONARY:
+                    msg = "STATIONARY";
+                    ret = 0;
+                    break;
+                case CAMERA_LENS_STATE_MOVING:
+                    msg = "MOVING";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+
+        case CAMERA_LENS_INFO_AVAILABLE_APERTURES: {
+            break;
+        }
+        case CAMERA_LENS_INFO_AVAILABLE_FILTER_DENSITIES: {
+            break;
+        }
+        case CAMERA_LENS_INFO_AVAILABLE_FOCAL_LENGTHS: {
+            break;
+        }
+        case CAMERA_LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION: {
+            break;
+        }
+        case CAMERA_LENS_INFO_HYPERFOCAL_DISTANCE: {
+            break;
+        }
+        case CAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE: {
+            break;
+        }
+        case CAMERA_LENS_INFO_SHADING_MAP_SIZE: {
+            break;
+        }
+        case CAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION: {
+            switch (value) {
+                case CAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION_UNCALIBRATED:
+                    msg = "UNCALIBRATED";
+                    ret = 0;
+                    break;
+                case CAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION_APPROXIMATE:
+                    msg = "APPROXIMATE";
+                    ret = 0;
+                    break;
+                case CAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION_CALIBRATED:
+                    msg = "CALIBRATED";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+
+        case CAMERA_NOISE_REDUCTION_MODE: {
+            switch (value) {
+                case CAMERA_NOISE_REDUCTION_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_NOISE_REDUCTION_MODE_FAST:
+                    msg = "FAST";
+                    ret = 0;
+                    break;
+                case CAMERA_NOISE_REDUCTION_MODE_HIGH_QUALITY:
+                    msg = "HIGH_QUALITY";
+                    ret = 0;
+                    break;
+                case CAMERA_NOISE_REDUCTION_MODE_MINIMAL:
+                    msg = "MINIMAL";
+                    ret = 0;
+                    break;
+                case CAMERA_NOISE_REDUCTION_MODE_ZERO_SHUTTER_LAG:
+                    msg = "ZERO_SHUTTER_LAG";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_NOISE_REDUCTION_STRENGTH: {
+            break;
+        }
+        case CAMERA_NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES: {
+            break;
+        }
+
+        case CAMERA_REQUEST_ID: {
+            break;
+        }
+        case CAMERA_REQUEST_METADATA_MODE: {
+            switch (value) {
+                case CAMERA_REQUEST_METADATA_MODE_NONE:
+                    msg = "NONE";
+                    ret = 0;
+                    break;
+                case CAMERA_REQUEST_METADATA_MODE_FULL:
+                    msg = "FULL";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_REQUEST_MAX_NUM_OUTPUT_STREAMS: {
+            break;
+        }
+        case CAMERA_REQUEST_MAX_NUM_INPUT_STREAMS: {
+            break;
+        }
+        case CAMERA_REQUEST_PIPELINE_DEPTH: {
+            break;
+        }
+        case CAMERA_REQUEST_PIPELINE_MAX_DEPTH: {
+            break;
+        }
+        case CAMERA_REQUEST_PARTIAL_RESULT_COUNT: {
+            break;
+        }
+        case CAMERA_REQUEST_AVAILABLE_CAPABILITIES: {
+            switch (value) {
+                case CAMERA_REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE:
+                    msg = "BACKWARD_COMPATIBLE";
+                    ret = 0;
+                    break;
+                case CAMERA_REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR:
+                    msg = "MANUAL_SENSOR";
+                    ret = 0;
+                    break;
+                case CAMERA_REQUEST_AVAILABLE_CAPABILITIES_MANUAL_POST_PROCESSING:
+                    msg = "MANUAL_POST_PROCESSING";
+                    ret = 0;
+                    break;
+                case CAMERA_REQUEST_AVAILABLE_CAPABILITIES_RAW:
+                    msg = "RAW";
+                    ret = 0;
+                    break;
+                case CAMERA_REQUEST_AVAILABLE_CAPABILITIES_ZSL:
+                    msg = "ZSL";
+                    ret = 0;
+                    break;
+                case CAMERA_REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS:
+                    msg = "READ_SENSOR_SETTINGS";
+                    ret = 0;
+                    break;
+                case CAMERA_REQUEST_AVAILABLE_CAPABILITIES_BURST_CAPTURE:
+                    msg = "BURST_CAPTURE";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_REQUEST_AVAILABLE_REQUEST_KEYS: {
+            break;
+        }
+        case CAMERA_REQUEST_AVAILABLE_RESULT_KEYS: {
+            break;
+        }
+        case CAMERA_REQUEST_AVAILABLE_CHARACTERISTICS_KEYS: {
+            break;
+        }
+
+        case CAMERA_SCALER_CROP_REGION: {
+            break;
+        }
+        case CAMERA_SCALER_AVAILABLE_JPEG_SIZES: {
+            break;
+        }
+        case CAMERA_SCALER_AVAILABLE_MAX_DIGITAL_ZOOM: {
+            break;
+        }
+        case CAMERA_SCALER_AVAILABLE_INPUT_OUTPUT_FORMATS_MAP: {
+            break;
+        }
+        case CAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS: {
+            break;
+        }
+        case CAMERA_SCALER_AVAILABLE_MIN_FRAME_DURATIONS: {
+            break;
+        }
+        case CAMERA_SCALER_AVAILABLE_STALL_DURATIONS: {
+            break;
+        }
+        case CAMERA_SCALER_CROPPING_TYPE: {
+            switch (value) {
+                case CAMERA_SCALER_CROPPING_TYPE_CENTER_ONLY:
+                    msg = "CENTER_ONLY";
+                    ret = 0;
+                    break;
+                case CAMERA_SCALER_CROPPING_TYPE_FREEFORM:
+                    msg = "FREEFORM";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+
+        case CAMERA_SENSOR_EXPOSURE_TIME: {
+            break;
+        }
+        case CAMERA_SENSOR_FRAME_DURATION: {
+            break;
+        }
+        case CAMERA_SENSOR_SENSITIVITY: {
+            break;
+        }
+        case CAMERA_SENSOR_REFERENCE_ILLUMINANT1: {
+            switch (value) {
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_DAYLIGHT:
+                    msg = "DAYLIGHT";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_FLUORESCENT:
+                    msg = "FLUORESCENT";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_TUNGSTEN:
+                    msg = "TUNGSTEN";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_FLASH:
+                    msg = "FLASH";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_FINE_WEATHER:
+                    msg = "FINE_WEATHER";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_CLOUDY_WEATHER:
+                    msg = "CLOUDY_WEATHER";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_SHADE:
+                    msg = "SHADE";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_DAYLIGHT_FLUORESCENT:
+                    msg = "DAYLIGHT_FLUORESCENT";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_DAY_WHITE_FLUORESCENT:
+                    msg = "DAY_WHITE_FLUORESCENT";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_COOL_WHITE_FLUORESCENT:
+                    msg = "COOL_WHITE_FLUORESCENT";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_WHITE_FLUORESCENT:
+                    msg = "WHITE_FLUORESCENT";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_STANDARD_A:
+                    msg = "STANDARD_A";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_STANDARD_B:
+                    msg = "STANDARD_B";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_STANDARD_C:
+                    msg = "STANDARD_C";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_D55:
+                    msg = "D55";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_D65:
+                    msg = "D65";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_D75:
+                    msg = "D75";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_D50:
+                    msg = "D50";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_REFERENCE_ILLUMINANT1_ISO_STUDIO_TUNGSTEN:
+                    msg = "ISO_STUDIO_TUNGSTEN";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_SENSOR_REFERENCE_ILLUMINANT2: {
+            break;
+        }
+        case CAMERA_SENSOR_CALIBRATION_TRANSFORM1: {
+            break;
+        }
+        case CAMERA_SENSOR_CALIBRATION_TRANSFORM2: {
+            break;
+        }
+        case CAMERA_SENSOR_COLOR_TRANSFORM1: {
+            break;
+        }
+        case CAMERA_SENSOR_COLOR_TRANSFORM2: {
+            break;
+        }
+        case CAMERA_SENSOR_FORWARD_MATRIX1: {
+            break;
+        }
+        case CAMERA_SENSOR_FORWARD_MATRIX2: {
+            break;
+        }
+        case CAMERA_SENSOR_BASE_GAIN_FACTOR: {
+            break;
+        }
+        case CAMERA_SENSOR_BLACK_LEVEL_PATTERN: {
+            break;
+        }
+        case CAMERA_SENSOR_MAX_ANALOG_SENSITIVITY: {
+            break;
+        }
+        case CAMERA_SENSOR_ORIENTATION: {
+            break;
+        }
+        case CAMERA_SENSOR_PROFILE_HUE_SAT_MAP_DIMENSIONS: {
+            break;
+        }
+        case CAMERA_SENSOR_TIMESTAMP: {
+            break;
+        }
+        case CAMERA_SENSOR_TEMPERATURE: {
+            break;
+        }
+        case CAMERA_SENSOR_NEUTRAL_COLOR_POINT: {
+            break;
+        }
+        case CAMERA_SENSOR_NOISE_PROFILE: {
+            break;
+        }
+        case CAMERA_SENSOR_PROFILE_HUE_SAT_MAP: {
+            break;
+        }
+        case CAMERA_SENSOR_PROFILE_TONE_CURVE: {
+            break;
+        }
+        case CAMERA_SENSOR_GREEN_SPLIT: {
+            break;
+        }
+        case CAMERA_SENSOR_TEST_PATTERN_DATA: {
+            break;
+        }
+        case CAMERA_SENSOR_TEST_PATTERN_MODE: {
+            switch (value) {
+                case CAMERA_SENSOR_TEST_PATTERN_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_TEST_PATTERN_MODE_SOLID_COLOR:
+                    msg = "SOLID_COLOR";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_TEST_PATTERN_MODE_COLOR_BARS:
+                    msg = "COLOR_BARS";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_TEST_PATTERN_MODE_COLOR_BARS_FADE_TO_GRAY:
+                    msg = "COLOR_BARS_FADE_TO_GRAY";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_TEST_PATTERN_MODE_PN9:
+                    msg = "PN9";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_TEST_PATTERN_MODE_CUSTOM1:
+                    msg = "CUSTOM1";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_SENSOR_AVAILABLE_TEST_PATTERN_MODES: {
+            break;
+        }
+        case CAMERA_SENSOR_OPAQUE_RAW_SIZE: {
+            break;
+        }
+        case CAMERA_SENSOR_ROLLING_SHUTTER_SKEW: {
+            break;
+        }
+
+        case CAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE: {
+            break;
+        }
+        case CAMERA_SENSOR_INFO_SENSITIVITY_RANGE: {
+            break;
+        }
+        case CAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT: {
+            switch (value) {
+                case CAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_RGGB:
+                    msg = "RGGB";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_GRBG:
+                    msg = "GRBG";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_GBRG:
+                    msg = "GBRG";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_BGGR:
+                    msg = "BGGR";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_RGB:
+                    msg = "RGB";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_SENSOR_INFO_EXPOSURE_TIME_RANGE: {
+            break;
+        }
+        case CAMERA_SENSOR_INFO_MAX_FRAME_DURATION: {
+            break;
+        }
+        case CAMERA_SENSOR_INFO_PHYSICAL_SIZE: {
+            break;
+        }
+        case CAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE: {
+            break;
+        }
+        case CAMERA_SENSOR_INFO_WHITE_LEVEL: {
+            break;
+        }
+        case CAMERA_SENSOR_INFO_TIMESTAMP_SOURCE: {
+            switch (value) {
+                case CAMERA_SENSOR_INFO_TIMESTAMP_SOURCE_UNKNOWN:
+                    msg = "UNKNOWN";
+                    ret = 0;
+                    break;
+                case CAMERA_SENSOR_INFO_TIMESTAMP_SOURCE_REALTIME:
+                    msg = "REALTIME";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+
+        case CAMERA_SHADING_MODE: {
+            switch (value) {
+                case CAMERA_SHADING_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_SHADING_MODE_FAST:
+                    msg = "FAST";
+                    ret = 0;
+                    break;
+                case CAMERA_SHADING_MODE_HIGH_QUALITY:
+                    msg = "HIGH_QUALITY";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_SHADING_STRENGTH: {
+            break;
+        }
+        case CAMERA_SHADING_AVAILABLE_MODES: {
+            break;
+        }
+
+        case CAMERA_STATISTICS_FACE_DETECT_MODE: {
+            switch (value) {
+                case CAMERA_STATISTICS_FACE_DETECT_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_STATISTICS_FACE_DETECT_MODE_SIMPLE:
+                    msg = "SIMPLE";
+                    ret = 0;
+                    break;
+                case CAMERA_STATISTICS_FACE_DETECT_MODE_FULL:
+                    msg = "FULL";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_STATISTICS_HISTOGRAM_MODE: {
+            switch (value) {
+                case CAMERA_STATISTICS_HISTOGRAM_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_STATISTICS_HISTOGRAM_MODE_ON:
+                    msg = "ON";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_STATISTICS_SHARPNESS_MAP_MODE: {
+            switch (value) {
+                case CAMERA_STATISTICS_SHARPNESS_MAP_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_STATISTICS_SHARPNESS_MAP_MODE_ON:
+                    msg = "ON";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_STATISTICS_HOT_PIXEL_MAP_MODE: {
+            switch (value) {
+                case CAMERA_STATISTICS_HOT_PIXEL_MAP_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_STATISTICS_HOT_PIXEL_MAP_MODE_ON:
+                    msg = "ON";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_STATISTICS_FACE_IDS: {
+            break;
+        }
+        case CAMERA_STATISTICS_FACE_LANDMARKS: {
+            break;
+        }
+        case CAMERA_STATISTICS_FACE_RECTANGLES: {
+            break;
+        }
+        case CAMERA_STATISTICS_FACE_SCORES: {
+            break;
+        }
+        case CAMERA_STATISTICS_HISTOGRAM: {
+            break;
+        }
+        case CAMERA_STATISTICS_SHARPNESS_MAP: {
+            break;
+        }
+        case CAMERA_STATISTICS_LENS_SHADING_CORRECTION_MAP: {
+            break;
+        }
+        case CAMERA_STATISTICS_LENS_SHADING_MAP: {
+            break;
+        }
+        case CAMERA_STATISTICS_PREDICTED_COLOR_GAINS: {
+            break;
+        }
+        case CAMERA_STATISTICS_PREDICTED_COLOR_TRANSFORM: {
+            break;
+        }
+        case CAMERA_STATISTICS_SCENE_FLICKER: {
+            switch (value) {
+                case CAMERA_STATISTICS_SCENE_FLICKER_NONE:
+                    msg = "NONE";
+                    ret = 0;
+                    break;
+                case CAMERA_STATISTICS_SCENE_FLICKER_50HZ:
+                    msg = "50HZ";
+                    ret = 0;
+                    break;
+                case CAMERA_STATISTICS_SCENE_FLICKER_60HZ:
+                    msg = "60HZ";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_STATISTICS_HOT_PIXEL_MAP: {
+            break;
+        }
+        case CAMERA_STATISTICS_LENS_SHADING_MAP_MODE: {
+            switch (value) {
+                case CAMERA_STATISTICS_LENS_SHADING_MAP_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_STATISTICS_LENS_SHADING_MAP_MODE_ON:
+                    msg = "ON";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+
+        case CAMERA_STATISTICS_INFO_AVAILABLE_FACE_DETECT_MODES: {
+            break;
+        }
+        case CAMERA_STATISTICS_INFO_HISTOGRAM_BUCKET_COUNT: {
+            break;
+        }
+        case CAMERA_STATISTICS_INFO_MAX_FACE_COUNT: {
+            break;
+        }
+        case CAMERA_STATISTICS_INFO_MAX_HISTOGRAM_COUNT: {
+            break;
+        }
+        case CAMERA_STATISTICS_INFO_MAX_SHARPNESS_MAP_VALUE: {
+            break;
+        }
+        case CAMERA_STATISTICS_INFO_SHARPNESS_MAP_SIZE: {
+            break;
+        }
+        case CAMERA_STATISTICS_INFO_AVAILABLE_HOT_PIXEL_MAP_MODES: {
+            break;
+        }
+
+        case CAMERA_TONEMAP_CURVE_BLUE: {
+            break;
+        }
+        case CAMERA_TONEMAP_CURVE_GREEN: {
+            break;
+        }
+        case CAMERA_TONEMAP_CURVE_RED: {
+            break;
+        }
+        case CAMERA_TONEMAP_MODE: {
+            switch (value) {
+                case CAMERA_TONEMAP_MODE_CONTRAST_CURVE:
+                    msg = "CONTRAST_CURVE";
+                    ret = 0;
+                    break;
+                case CAMERA_TONEMAP_MODE_FAST:
+                    msg = "FAST";
+                    ret = 0;
+                    break;
+                case CAMERA_TONEMAP_MODE_HIGH_QUALITY:
+                    msg = "HIGH_QUALITY";
+                    ret = 0;
+                    break;
+                case CAMERA_TONEMAP_MODE_GAMMA_VALUE:
+                    msg = "GAMMA_VALUE";
+                    ret = 0;
+                    break;
+                case CAMERA_TONEMAP_MODE_PRESET_CURVE:
+                    msg = "PRESET_CURVE";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_TONEMAP_MAX_CURVE_POINTS: {
+            break;
+        }
+        case CAMERA_TONEMAP_AVAILABLE_TONE_MAP_MODES: {
+            break;
+        }
+        case CAMERA_TONEMAP_GAMMA: {
+            break;
+        }
+        case CAMERA_TONEMAP_PRESET_CURVE: {
+            switch (value) {
+                case CAMERA_TONEMAP_PRESET_CURVE_SRGB:
+                    msg = "SRGB";
+                    ret = 0;
+                    break;
+                case CAMERA_TONEMAP_PRESET_CURVE_REC709:
+                    msg = "REC709";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+
+        case CAMERA_LED_TRANSMIT: {
+            switch (value) {
+                case CAMERA_LED_TRANSMIT_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_LED_TRANSMIT_ON:
+                    msg = "ON";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_LED_AVAILABLE_LEDS: {
+            switch (value) {
+                case CAMERA_LED_AVAILABLE_LEDS_TRANSMIT:
+                    msg = "TRANSMIT";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+
+        case CAMERA_INFO_SUPPORTED_HARDWARE_LEVEL: {
+            switch (value) {
+                case CAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED:
+                    msg = "LIMITED";
+                    ret = 0;
+                    break;
+                case CAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_FULL:
+                    msg = "FULL";
+                    ret = 0;
+                    break;
+                case CAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY:
+                    msg = "LEGACY";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+
+        case CAMERA_BLACK_LEVEL_LOCK: {
+            switch (value) {
+                case CAMERA_BLACK_LEVEL_LOCK_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case CAMERA_BLACK_LEVEL_LOCK_ON:
+                    msg = "ON";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+
+        case CAMERA_SYNC_FRAME_NUMBER: {
+            switch (value) {
+                case CAMERA_SYNC_FRAME_NUMBER_CONVERGING:
+                    msg = "CONVERGING";
+                    ret = 0;
+                    break;
+                case CAMERA_SYNC_FRAME_NUMBER_UNKNOWN:
+                    msg = "UNKNOWN";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case CAMERA_SYNC_MAX_LATENCY: {
+            switch (value) {
+                case CAMERA_SYNC_MAX_LATENCY_PER_FRAME_CONTROL:
+                    msg = "PER_FRAME_CONTROL";
+                    ret = 0;
+                    break;
+                case CAMERA_SYNC_MAX_LATENCY_UNKNOWN:
+                    msg = "UNKNOWN";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+
+        case CAMERA_REPROCESS_MAX_CAPTURE_STALL: {
+            break;
+        }
+
+        case INTEL_INFO_AVAILABLE_CONFIGURATIONS: {
+            break;
+        }
+        case INTEL_INFO_AVAILABLE_FEATURES: {
+            switch (value) {
+                case INTEL_INFO_AVAILABLE_FEATURES_MANUAL_EXPOSURE:
+                    msg = "MANUAL_EXPOSURE";
+                    ret = 0;
+                    break;
+                case INTEL_INFO_AVAILABLE_FEATURES_MANUAL_WHITE_BALANCE:
+                    msg = "MANUAL_WHITE_BALANCE";
+                    ret = 0;
+                    break;
+                case INTEL_INFO_AVAILABLE_FEATURES_IMAGE_ENHANCEMENT:
+                    msg = "IMAGE_ENHANCEMENT";
+                    ret = 0;
+                    break;
+                case INTEL_INFO_AVAILABLE_FEATURES_NOISE_REDUCTION:
+                    msg = "NOISE_REDUCTION";
+                    ret = 0;
+                    break;
+                case INTEL_INFO_AVAILABLE_FEATURES_SCENE_MODE:
+                    msg = "SCENE_MODE";
+                    ret = 0;
+                    break;
+                case INTEL_INFO_AVAILABLE_FEATURES_WEIGHT_GRID_MODE:
+                    msg = "WEIGHT_GRID_MODE";
+                    ret = 0;
+                    break;
+                case INTEL_INFO_AVAILABLE_FEATURES_PER_FRAME_CONTROL:
+                    msg = "PER_FRAME_CONTROL";
+                    ret = 0;
+                    break;
+                case INTEL_INFO_AVAILABLE_FEATURES_ISP_CONTROL:
+                    msg = "ISP_CONTROL";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_INFO_AE_EXPOSURE_TIME_RANGE: {
+            break;
+        }
+        case INTEL_INFO_AE_GAIN_RANGE: {
+            break;
+        }
+        case INTEL_INFO_WFOV: {
+            switch (value) {
+                case INTEL_INFO_WFOV_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case INTEL_INFO_WFOV_ON:
+                    msg = "ON";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_INFO_SENSOR_MOUNT_TYPE: {
+            switch (value) {
+                case INTEL_INFO_SENSOR_MOUNT_TYPE_WALL_MOUNTED:
+                    msg = "WALL_MOUNTED";
+                    ret = 0;
+                    break;
+                case INTEL_INFO_SENSOR_MOUNT_TYPE_CEILING_MOUNTER:
+                    msg = "CEILING_MOUNTER";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+
+        case INTEL_CONTROL_IMAGE_ENHANCEMENT: {
+            break;
+        }
+        case INTEL_CONTROL_SENSITIVITY_GAIN: {
+            break;
+        }
+        case INTEL_CONTROL_FRAME_RATE: {
+            break;
+        }
+        case INTEL_CONTROL_AE_CONVERGE_SPEED: {
+            switch (value) {
+                case INTEL_CONTROL_AE_CONVERGE_SPEED_NORMAL:
+                    msg = "NORMAL";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_AE_CONVERGE_SPEED_MID:
+                    msg = "MID";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_AE_CONVERGE_SPEED_LOW:
+                    msg = "LOW";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_CONTROL_NR_MODE: {
+            switch (value) {
+                case INTEL_CONTROL_NR_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_NR_MODE_AUTO:
+                    msg = "AUTO";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_NR_MODE_MANUAL_NORMAL:
+                    msg = "MANUAL_NORMAL";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_NR_MODE_MANUAL_EXPERT:
+                    msg = "MANUAL_EXPERT";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_CONTROL_NR_LEVEL: {
+            break;
+        }
+        case INTEL_CONTROL_IRIS_MODE: {
+            switch (value) {
+                case INTEL_CONTROL_IRIS_MODE_AUTO:
+                    msg = "AUTO";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_IRIS_MODE_MANUAL:
+                    msg = "MANUAL";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_IRIS_MODE_CUSTOMIZED:
+                    msg = "CUSTOMIZED";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_CONTROL_AE_DISTRIBUTION_PRIORITY: {
+            switch (value) {
+                case INTEL_CONTROL_AE_DISTRIBUTION_PRIORITY_AUTO:
+                    msg = "AUTO";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_AE_DISTRIBUTION_PRIORITY_SHUTTER:
+                    msg = "SHUTTER";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_AE_DISTRIBUTION_PRIORITY_ISO:
+                    msg = "ISO";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_AE_DISTRIBUTION_PRIORITY_APERTURE:
+                    msg = "APERTURE";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_CONTROL_IRIS_LEVEL: {
+            break;
+        }
+        case INTEL_CONTROL_WDR_MODE: {
+            switch (value) {
+                case INTEL_CONTROL_WDR_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_WDR_MODE_ON:
+                    msg = "ON";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_WDR_MODE_AUTO:
+                    msg = "AUTO";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_CONTROL_WDR_LEVEL: {
+            break;
+        }
+        case INTEL_CONTROL_BLC_AREA_MODE: {
+            switch (value) {
+                case INTEL_CONTROL_BLC_AREA_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_BLC_AREA_MODE_ON:
+                    msg = "ON";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_CONTROL_SCENE_MODE: {
+            switch (value) {
+                case INTEL_CONTROL_SCENE_MODE_AUTO:
+                    msg = "AUTO";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_SCENE_MODE_HDR:
+                    msg = "HDR";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_SCENE_MODE_ULL:
+                    msg = "ULL";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_SCENE_MODE_VIDEO_LL:
+                    msg = "VIDEO_LL";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_SCENE_MODE_HDR2:
+                    msg = "HDR2";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_CONTROL_WEIGHT_GRID_MODE: {
+            switch (value) {
+                case INTEL_CONTROL_WEIGHT_GRID_MODE_AUTO:
+                    msg = "AUTO";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_WEIGHT_GRID_MODE_CUSTOM_WEIGHT_GRID1:
+                    msg = "CUSTOM_WEIGHT_GRID1";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_WEIGHT_GRID_MODE_CUSTOM_WEIGHT_GRID2:
+                    msg = "CUSTOM_WEIGHT_GRID2";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_WEIGHT_GRID_MODE_CUSTOM_WEIGHT_GRID3:
+                    msg = "CUSTOM_WEIGHT_GRID3";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_CONTROL_AE_CONVERGE_SPEED_MODE: {
+            switch (value) {
+                case INTEL_CONTROL_AE_CONVERGE_SPEED_MODE_HAL:
+                    msg = "HAL";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_AE_CONVERGE_SPEED_MODE_AIQ:
+                    msg = "AIQ";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_CONTROL_DEINTERLACE_MODE: {
+            switch (value) {
+                case INTEL_CONTROL_DEINTERLACE_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_DEINTERLACE_MODE_WEAVING:
+                    msg = "WEAVING";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_CONTROL_MAKERNOTE_DATA: {
+            break;
+        }
+        case INTEL_CONTROL_CUSTOM_AIC_PARAM: {
+            break;
+        }
+        case INTEL_CONTROL_MAKERNOTE_MODE: {
+            switch (value) {
+                case INTEL_CONTROL_MAKERNOTE_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_MAKERNOTE_MODE_JPEG:
+                    msg = "JPEG";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_MAKERNOTE_MODE_RAW:
+                    msg = "RAW";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_CONTROL_YUV_COLOR_RANGE: {
+            switch (value) {
+                case INTEL_CONTROL_YUV_COLOR_RANGE_FULL:
+                    msg = "FULL";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_YUV_COLOR_RANGE_REDUCED:
+                    msg = "REDUCED";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_CONTROL_SENSITIVITY_GAIN_RANGE: {
+            break;
+        }
+        case INTEL_CONTROL_EXPOSURE_TIME_RANGE: {
+            break;
+        }
+        case INTEL_CONTROL_FISHEYE_DEWARPING_MODE: {
+            switch (value) {
+                case INTEL_CONTROL_FISHEYE_DEWARPING_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_FISHEYE_DEWARPING_MODE_REARVIEW:
+                    msg = "REARVIEW";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_FISHEYE_DEWARPING_MODE_HITCHVIEW:
+                    msg = "HITCHVIEW";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_CONTROL_LTM_TUNING_DATA: {
+            break;
+        }
+        case INTEL_CONTROL_DIGITAL_ZOOM_RATIO: {
+            break;
+        }
+        case INTEL_CONTROL_LDC_MODE: {
+            switch (value) {
+                case INTEL_CONTROL_LDC_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_LDC_MODE_ON:
+                    msg = "ON";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_CONTROL_RSC_MODE: {
+            switch (value) {
+                case INTEL_CONTROL_RSC_MODE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_RSC_MODE_ON:
+                    msg = "ON";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_CONTROL_FLIP_MODE: {
+            switch (value) {
+                case INTEL_CONTROL_FLIP_MODE_NONE:
+                    msg = "NONE";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_FLIP_MODE_VFLIP:
+                    msg = "VFLIP";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_FLIP_MODE_HFLIP:
+                    msg = "HFLIP";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_FLIP_MODE_VHFLIP:
+                    msg = "VHFLIP";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_CONTROL_MONO_DOWNSCALE: {
+            switch (value) {
+                case INTEL_CONTROL_MONO_DOWNSCALE_OFF:
+                    msg = "OFF";
+                    ret = 0;
+                    break;
+                case INTEL_CONTROL_MONO_DOWNSCALE_ON:
+                    msg = "ON";
+                    ret = 0;
+                    break;
+                default:
+                    msg = "error: enum value out of range";
+            }
+            break;
+        }
+        case INTEL_CONTROL_RUN3_A_CADENCE: {
+            break;
+        }
+        case INTEL_CONTROL_VIEW_PROJECTION: {
+            break;
+        }
+        case INTEL_CONTROL_VIEW_ROTATION: {
+            break;
+        }
+        case INTEL_CONTROL_VIEW_FINE_ADJUSTMENTS: {
+            break;
+        }
+        case INTEL_CONTROL_CAMERA_ROTATION: {
+            break;
+        }
+        case INTEL_CONTROL_SCALER_CROP_REGION: {
+            break;
+        }
+
+        case INTEL_CONTROL_ISP_SUPPORTED_CTRL_IDS: {
+            break;
+        }
+        case INTEL_CONTROL_ISP_ENABLED_CTRL_IDS: {
+            break;
+        }
+        case INTEL_CONTROL_ISP_WB_GAINS: {
+            break;
+        }
+        case INTEL_CONTROL_ISP_COLOR_CORRECTION_MATRIX: {
+            break;
+        }
+        case INTEL_CONTROL_ISP_ADVANCED_COLOR_CORRECTION_MATRIX: {
+            break;
+        }
+        case INTEL_CONTROL_ISP_BXT_CSC: {
+            break;
+        }
+        case INTEL_CONTROL_ISP_BXT_DEMOSAIC: {
+            break;
+        }
+        case INTEL_CONTROL_ISP_SC_IEFD: {
+            break;
+        }
+        case INTEL_CONTROL_ISP_SEE: {
+            break;
+        }
+        case INTEL_CONTROL_ISP_BNLM: {
+            break;
+        }
+        case INTEL_CONTROL_ISP_TNR5_21: {
+            break;
+        }
+        case INTEL_CONTROL_ISP_XNR_DSS: {
+            break;
+        }
+        case INTEL_CONTROL_ISP_GAMMA_TONE_MAP: {
+            break;
+        }
+        case INTEL_CONTROL_ISP_TNR5_22: {
+            break;
+        }
+        case INTEL_CONTROL_ISP_TNR5_25: {
+            break;
+        }
+
+    }
+
+    snprintf(dst, size, "%s", msg);
+    dst[size - 1] = '\0';
+
+    return ret;
+}
+
+#define ICAMERA_METADATA_ENUM_STRING_MAX_SIZE 24
diff --git a/camera/hal/intel/ipu6/src/metadata/icamera_metadata_tags.h b/camera/hal/intel/ipu6/src/metadata/icamera_metadata_tags.h
new file mode 100644
index 000000000000..f8223c898db9
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/metadata/icamera_metadata_tags.h
@@ -0,0 +1,1019 @@
+/*
+ * Copyright (C) 2012 The Android Open Source Project
+ * Copyright (C) 2015-2018 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * !! Do not include this file directly !!
+ *
+ * Include icamera_metadata_base.h instead.
+ */
+
+/**
+ * ! Do not edit this file directly !
+ *
+ * Generated automatically from icamera_metadata_tags.mako
+ */
+
+/**
+ * Top level hierarchy definitions for camera metadata. *_INFO sections are for
+ * the static metadata that can be retrived without opening the camera device.
+ * New sections must be added right before CAMERA_SECTION_COUNT to maintain
+ * existing enumerations.
+ */
+typedef enum icamera_metadata_section {
+    CAMERA_AE,
+    CAMERA_AWB,
+    CAMERA_AF,
+    CAMERA_CONTROL,
+    CAMERA_DEMOSAIC,
+    CAMERA_EDGE,
+    CAMERA_FLASH,
+    CAMERA_FLASH_INFO,
+    CAMERA_HOT_PIXEL,
+    CAMERA_JPEG,
+    CAMERA_LENS,
+    CAMERA_LENS_INFO,
+    CAMERA_NOISE_REDUCTION,
+    CAMERA_REQUEST,
+    CAMERA_SCALER,
+    CAMERA_SENSOR,
+    CAMERA_SENSOR_INFO,
+    CAMERA_SHADING,
+    CAMERA_STATISTICS,
+    CAMERA_STATISTICS_INFO,
+    CAMERA_TONEMAP,
+    CAMERA_LED,
+    CAMERA_INFO,
+    CAMERA_BLACK_LEVEL,
+    CAMERA_SYNC,
+    CAMERA_REPROCESS,
+    INTEL_INFO,
+    INTEL_CONTROL,
+    INTEL_CONTROL_ISP,
+    CAMERA_SECTION_COUNT
+} icamera_metadata_section_t;
+
+/**
+ * Hierarchy positions in enum space.
+ */
+typedef enum icamera_metadata_section_start {
+    CAMERA_AE_START                = CAMERA_AE                 << 16,
+    CAMERA_AWB_START               = CAMERA_AWB                << 16,
+    CAMERA_AF_START                = CAMERA_AF                 << 16,
+    CAMERA_CONTROL_START           = CAMERA_CONTROL            << 16,
+    CAMERA_DEMOSAIC_START          = CAMERA_DEMOSAIC           << 16,
+    CAMERA_EDGE_START              = CAMERA_EDGE               << 16,
+    CAMERA_FLASH_START             = CAMERA_FLASH              << 16,
+    CAMERA_FLASH_INFO_START        = CAMERA_FLASH_INFO         << 16,
+    CAMERA_HOT_PIXEL_START         = CAMERA_HOT_PIXEL          << 16,
+    CAMERA_JPEG_START              = CAMERA_JPEG               << 16,
+    CAMERA_LENS_START              = CAMERA_LENS               << 16,
+    CAMERA_LENS_INFO_START         = CAMERA_LENS_INFO          << 16,
+    CAMERA_NOISE_REDUCTION_START   = CAMERA_NOISE_REDUCTION    << 16,
+    CAMERA_REQUEST_START           = CAMERA_REQUEST            << 16,
+    CAMERA_SCALER_START            = CAMERA_SCALER             << 16,
+    CAMERA_SENSOR_START            = CAMERA_SENSOR             << 16,
+    CAMERA_SENSOR_INFO_START       = CAMERA_SENSOR_INFO        << 16,
+    CAMERA_SHADING_START           = CAMERA_SHADING            << 16,
+    CAMERA_STATISTICS_START        = CAMERA_STATISTICS         << 16,
+    CAMERA_STATISTICS_INFO_START   = CAMERA_STATISTICS_INFO    << 16,
+    CAMERA_TONEMAP_START           = CAMERA_TONEMAP            << 16,
+    CAMERA_LED_START               = CAMERA_LED                << 16,
+    CAMERA_INFO_START              = CAMERA_INFO               << 16,
+    CAMERA_BLACK_LEVEL_START       = CAMERA_BLACK_LEVEL        << 16,
+    CAMERA_SYNC_START              = CAMERA_SYNC               << 16,
+    CAMERA_REPROCESS_START         = CAMERA_REPROCESS          << 16,
+    INTEL_INFO_START               = INTEL_INFO                << 16,
+    INTEL_CONTROL_START            = INTEL_CONTROL             << 16,
+    INTEL_CONTROL_ISP_START        = INTEL_CONTROL_ISP         << 16,
+} icamera_metadata_section_start_t;
+
+/**
+ * Main enum for defining camera metadata tags.  New entries must always go
+ * before the section _END tag to preserve existing enumeration values.  In
+ * addition, the name and type of the tag needs to be added to
+ * src/metadata/icamera_metadata_tag_info.c
+ */
+typedef enum icamera_metadata_tag {
+    CAMERA_AE_MODE =                                  // enum         | public
+            CAMERA_AE_START,
+    CAMERA_AE_LOCK,                                   // enum         | public
+    CAMERA_AE_REGIONS,                                // int32[]      | public
+    CAMERA_AE_ANTIBANDING_MODE,                       // enum         | public
+    CAMERA_AE_COMPENSATION,                           // int32        | public
+    CAMERA_AE_TARGET_FPS_RANGE,                       // float[]      | public
+    CAMERA_AE_PRECAPTURE_TRIGGER,                     // enum         | public
+    CAMERA_AE_STATE,                                  // enum         | public
+    CAMERA_AE_AVAILABLE_MODES,                        // byte[]       | public
+    CAMERA_AE_AVAILABLE_ANTIBANDING_MODES,            // byte[]       | public
+    CAMERA_AE_COMPENSATION_STEP,                      // rational     | public
+    CAMERA_AE_COMPENSATION_RANGE,                     // int32[]      | public
+    CAMERA_AE_AVAILABLE_TARGET_FPS_RANGES,            // float[]      | public
+    CAMERA_AE_LOCK_AVAILABLE,                         // enum         | public
+    CAMERA_AE_END,
+
+    CAMERA_AWB_MODE =                                 // enum         | public
+            CAMERA_AWB_START,
+    CAMERA_AWB_COLOR_TRANSFORM,                       // float[]      | public
+    CAMERA_AWB_COLOR_GAINS,                           // float[]      | public
+    CAMERA_AWB_LOCK,                                  // enum         | public
+    CAMERA_AWB_REGIONS,                               // int32[]      | public
+    CAMERA_AWB_CCT_RANGE,                             // int32[]      | public
+    CAMERA_AWB_GAINS,                                 // int32[]      | public
+    CAMERA_AWB_GAIN_SHIFT,                            // int32[]      | public
+    CAMERA_AWB_WHITE_POINT,                           // int32[]      | public
+    CAMERA_AWB_CONVERGE_SPEED,                        // enum         | public
+    CAMERA_AWB_CONVERGE_SPEED_MODE,                   // enum         | public
+    CAMERA_AWB_STATE,                                 // enum         | public
+    CAMERA_AWB_RESULT,                                // byte[]       | public
+    CAMERA_AWB_AVAILABLE_MODES,                       // byte[]       | public
+    CAMERA_AWB_LOCK_AVAILABLE,                        // enum         | public
+    CAMERA_AWB_END,
+
+    CAMERA_AF_MODE =                                  // enum         | public
+            CAMERA_AF_START,
+    CAMERA_AF_REGIONS,                                // int32[]      | public
+    CAMERA_AF_TRIGGER,                                // enum         | public
+    CAMERA_AF_AVAILABLE_MODES,                        // byte[]       | public
+    CAMERA_AF_STATE,                                  // enum         | public
+    CAMERA_AF_END,
+
+    CAMERA_CONTROL_CAPTUREINTENT =                    // enum         | public
+            CAMERA_CONTROL_START,
+    CAMERA_CONTROL_EFFECT_MODE,                       // enum         | public
+    CAMERA_CONTROL_MODE,                              // enum         | public
+    CAMERA_CONTROL_SCENE_MODE,                        // enum         | public
+    CAMERA_CONTROL_VIDEO_STABILIZATION_MODE,          // enum         | public
+    CAMERA_CONTROL_AVAILABLE_EFFECTS,                 // byte[]       | public
+    CAMERA_CONTROL_AVAILABLE_MODES,                   // byte[]       | public
+    CAMERA_CONTROL_AVAILABLE_SCENE_MODES,             // byte[]       | public
+    CAMERA_CONTROL_AVAILABLE_VIDEO_STABILIZATION_MODES,
+                                                      // byte[]       | public
+    CAMERA_CONTROL_MAX_REGIONS,                       // int32[]      | hidden
+    CAMERA_CONTROL_SCENE_MODE_OVERRIDES,              // byte[]       | system
+    CAMERA_CONTROL_AVAILABLE_HIGH_SPEED_VIDEO_CONFIGURATIONS,
+                                                      // int32[]      | hidden
+    CAMERA_CONTROL_END,
+
+    CAMERA_DEMOSAIC_MODE =                            // enum         | system
+            CAMERA_DEMOSAIC_START,
+    CAMERA_DEMOSAIC_END,
+
+    CAMERA_EDGE_MODE =                                // enum         | public
+            CAMERA_EDGE_START,
+    CAMERA_EDGE_STRENGTH,                             // byte         | system
+    CAMERA_EDGE_AVAILABLE_EDGE_MODES,                 // byte[]       | public
+    CAMERA_EDGE_END,
+
+    CAMERA_FLASH_FIRING_POWER =                       // byte         | system
+            CAMERA_FLASH_START,
+    CAMERA_FLASH_FIRING_TIME,                         // int64        | system
+    CAMERA_FLASH_MODE,                                // enum         | public
+    CAMERA_FLASH_COLOR_TEMPERATURE,                   // byte         | system
+    CAMERA_FLASH_MAX_ENERGY,                          // byte         | system
+    CAMERA_FLASH_STATE,                               // enum         | public
+    CAMERA_FLASH_END,
+
+    CAMERA_FLASH_INFO_AVAILABLE =                     // enum         | public
+            CAMERA_FLASH_INFO_START,
+    CAMERA_FLASH_INFO_CHARGE_DURATION,                // int64        | system
+    CAMERA_FLASH_INFO_END,
+
+    CAMERA_HOT_PIXEL_MODE =                           // enum         | public
+            CAMERA_HOT_PIXEL_START,
+    CAMERA_HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES,       // byte[]       | public
+    CAMERA_HOT_PIXEL_END,
+
+    CAMERA_JPEG_GPS_COORDINATES =                     // double[]     | hidden
+            CAMERA_JPEG_START,
+    CAMERA_JPEG_GPS_PROCESSING_METHOD,                // byte         | hidden
+    CAMERA_JPEG_GPS_TIMESTAMP,                        // int64        | hidden
+    CAMERA_JPEG_ORIENTATION,                          // int32        | public
+    CAMERA_JPEG_QUALITY,                              // byte         | public
+    CAMERA_JPEG_THUMBNAIL_QUALITY,                    // byte         | public
+    CAMERA_JPEG_THUMBNAIL_SIZE,                       // int32[]      | public
+    CAMERA_JPEG_AVAILABLE_THUMBNAIL_SIZES,            // int32[]      | public
+    CAMERA_JPEG_MAX_SIZE,                             // int32        | system
+    CAMERA_JPEG_SIZE,                                 // int32        | system
+    CAMERA_JPEG_END,
+
+    CAMERA_LENS_APERTURE =                            // float        | public
+            CAMERA_LENS_START,
+    CAMERA_LENS_FILTER_DENSITY,                       // float        | public
+    CAMERA_LENS_FOCAL_LENGTH,                         // float        | public
+    CAMERA_LENS_FOCUS_DISTANCE,                       // float        | public
+    CAMERA_LENS_OPTICAL_STABILIZATION_MODE,           // enum         | public
+    CAMERA_LENS_FACING,                               // enum         | public
+    CAMERA_LENS_FOCUS_RANGE,                          // float[]      | public
+    CAMERA_LENS_STATE,                                // enum         | public
+    CAMERA_LENS_END,
+
+    CAMERA_LENS_INFO_AVAILABLE_APERTURES =            // float[]      | public
+            CAMERA_LENS_INFO_START,
+    CAMERA_LENS_INFO_AVAILABLE_FILTER_DENSITIES,      // float[]      | public
+    CAMERA_LENS_INFO_AVAILABLE_FOCAL_LENGTHS,         // float[]      | public
+    CAMERA_LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION, // byte[]       | public
+    CAMERA_LENS_INFO_HYPERFOCAL_DISTANCE,             // float        | public
+    CAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE,          // float        | public
+    CAMERA_LENS_INFO_SHADING_MAP_SIZE,                // int32[]      | hidden
+    CAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION,      // enum         | public
+    CAMERA_LENS_INFO_END,
+
+    CAMERA_NOISE_REDUCTION_MODE =                     // enum         | public
+            CAMERA_NOISE_REDUCTION_START,
+    CAMERA_NOISE_REDUCTION_STRENGTH,                  // byte         | system
+    CAMERA_NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES,
+                                                      // byte[]       | public
+    CAMERA_NOISE_REDUCTION_END,
+
+    CAMERA_REQUEST_ID =                               // int32        | hidden
+            CAMERA_REQUEST_START,
+    CAMERA_REQUEST_METADATA_MODE,                     // enum         | system
+    CAMERA_REQUEST_MAX_NUM_OUTPUT_STREAMS,            // int32[]      | hidden
+    CAMERA_REQUEST_MAX_NUM_INPUT_STREAMS,             // int32        | hidden
+    CAMERA_REQUEST_PIPELINE_DEPTH,                    // byte         | public
+    CAMERA_REQUEST_PIPELINE_MAX_DEPTH,                // byte         | public
+    CAMERA_REQUEST_PARTIAL_RESULT_COUNT,              // int32        | public
+    CAMERA_REQUEST_AVAILABLE_CAPABILITIES,            // enum[]       | public
+    CAMERA_REQUEST_AVAILABLE_REQUEST_KEYS,            // int32[]      | hidden
+    CAMERA_REQUEST_AVAILABLE_RESULT_KEYS,             // int32[]      | hidden
+    CAMERA_REQUEST_AVAILABLE_CHARACTERISTICS_KEYS,    // int32[]      | hidden
+    CAMERA_REQUEST_END,
+
+    CAMERA_SCALER_CROP_REGION =                       // int32[]      | public
+            CAMERA_SCALER_START,
+    CAMERA_SCALER_AVAILABLE_JPEG_SIZES,               // int32[]      | hidden
+    CAMERA_SCALER_AVAILABLE_MAX_DIGITAL_ZOOM,         // float        | public
+    CAMERA_SCALER_AVAILABLE_INPUT_OUTPUT_FORMATS_MAP, // int32[]      | hidden
+    CAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS,    // int32[]      | hidden
+    CAMERA_SCALER_AVAILABLE_MIN_FRAME_DURATIONS,      // int64[]      | hidden
+    CAMERA_SCALER_AVAILABLE_STALL_DURATIONS,          // int64[]      | hidden
+    CAMERA_SCALER_CROPPING_TYPE,                      // enum         | public
+    CAMERA_SCALER_END,
+
+    CAMERA_SENSOR_EXPOSURE_TIME =                     // int64        | public
+            CAMERA_SENSOR_START,
+    CAMERA_SENSOR_FRAME_DURATION,                     // int64        | public
+    CAMERA_SENSOR_SENSITIVITY,                        // int32        | public
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1,              // enum         | public
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT2,              // byte         | public
+    CAMERA_SENSOR_CALIBRATION_TRANSFORM1,             // rational[]   | public
+    CAMERA_SENSOR_CALIBRATION_TRANSFORM2,             // rational[]   | public
+    CAMERA_SENSOR_COLOR_TRANSFORM1,                   // rational[]   | public
+    CAMERA_SENSOR_COLOR_TRANSFORM2,                   // rational[]   | public
+    CAMERA_SENSOR_FORWARD_MATRIX1,                    // rational[]   | public
+    CAMERA_SENSOR_FORWARD_MATRIX2,                    // rational[]   | public
+    CAMERA_SENSOR_BASE_GAIN_FACTOR,                   // rational     | system
+    CAMERA_SENSOR_BLACK_LEVEL_PATTERN,                // int32[]      | public
+    CAMERA_SENSOR_MAX_ANALOG_SENSITIVITY,             // int32        | public
+    CAMERA_SENSOR_ORIENTATION,                        // int32        | public
+    CAMERA_SENSOR_PROFILE_HUE_SAT_MAP_DIMENSIONS,     // int32[]      | system
+    CAMERA_SENSOR_TIMESTAMP,                          // int64        | public
+    CAMERA_SENSOR_TEMPERATURE,                        // float        | system
+    CAMERA_SENSOR_NEUTRAL_COLOR_POINT,                // rational[]   | public
+    CAMERA_SENSOR_NOISE_PROFILE,                      // double[]     | public
+    CAMERA_SENSOR_PROFILE_HUE_SAT_MAP,                // float[]      | system
+    CAMERA_SENSOR_PROFILE_TONE_CURVE,                 // float[]      | system
+    CAMERA_SENSOR_GREEN_SPLIT,                        // float        | public
+    CAMERA_SENSOR_TEST_PATTERN_DATA,                  // int32[]      | public
+    CAMERA_SENSOR_TEST_PATTERN_MODE,                  // enum         | public
+    CAMERA_SENSOR_AVAILABLE_TEST_PATTERN_MODES,       // int32[]      | public
+    CAMERA_SENSOR_OPAQUE_RAW_SIZE,                    // int32[]      | system
+    CAMERA_SENSOR_ROLLING_SHUTTER_SKEW,               // int64        | public
+    CAMERA_SENSOR_END,
+
+    CAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE =            // int32[]      | public
+            CAMERA_SENSOR_INFO_START,
+    CAMERA_SENSOR_INFO_SENSITIVITY_RANGE,             // int32[]      | public
+    CAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT,      // enum         | public
+    CAMERA_SENSOR_INFO_EXPOSURE_TIME_RANGE,           // int64[]      | public
+    CAMERA_SENSOR_INFO_MAX_FRAME_DURATION,            // int64        | public
+    CAMERA_SENSOR_INFO_PHYSICAL_SIZE,                 // float[]      | public
+    CAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE,              // int32[]      | public
+    CAMERA_SENSOR_INFO_WHITE_LEVEL,                   // int32        | public
+    CAMERA_SENSOR_INFO_TIMESTAMP_SOURCE,              // enum         | public
+    CAMERA_SENSOR_INFO_END,
+
+    CAMERA_SHADING_MODE =                             // enum         | public
+            CAMERA_SHADING_START,
+    CAMERA_SHADING_STRENGTH,                          // byte         | system
+    CAMERA_SHADING_AVAILABLE_MODES,                   // byte[]       | public
+    CAMERA_SHADING_END,
+
+    CAMERA_STATISTICS_FACE_DETECT_MODE =              // enum         | public
+            CAMERA_STATISTICS_START,
+    CAMERA_STATISTICS_HISTOGRAM_MODE,                 // enum         | system
+    CAMERA_STATISTICS_SHARPNESS_MAP_MODE,             // enum         | system
+    CAMERA_STATISTICS_HOT_PIXEL_MAP_MODE,             // enum         | public
+    CAMERA_STATISTICS_FACE_IDS,                       // int32[]      | hidden
+    CAMERA_STATISTICS_FACE_LANDMARKS,                 // int32[]      | hidden
+    CAMERA_STATISTICS_FACE_RECTANGLES,                // int32[]      | hidden
+    CAMERA_STATISTICS_FACE_SCORES,                    // byte[]       | hidden
+    CAMERA_STATISTICS_HISTOGRAM,                      // int32[]      | system
+    CAMERA_STATISTICS_SHARPNESS_MAP,                  // int32[]      | system
+    CAMERA_STATISTICS_LENS_SHADING_CORRECTION_MAP,    // byte         | public
+    CAMERA_STATISTICS_LENS_SHADING_MAP,               // float[]      | hidden
+    CAMERA_STATISTICS_PREDICTED_COLOR_GAINS,          // float[]      | hidden
+    CAMERA_STATISTICS_PREDICTED_COLOR_TRANSFORM,      // rational[]   | hidden
+    CAMERA_STATISTICS_SCENE_FLICKER,                  // enum         | public
+    CAMERA_STATISTICS_HOT_PIXEL_MAP,                  // int32[]      | public
+    CAMERA_STATISTICS_LENS_SHADING_MAP_MODE,          // enum         | public
+    CAMERA_STATISTICS_END,
+
+    CAMERA_STATISTICS_INFO_AVAILABLE_FACE_DETECT_MODES = 
+                                                      // byte[]       | public
+            CAMERA_STATISTICS_INFO_START,
+    CAMERA_STATISTICS_INFO_HISTOGRAM_BUCKET_COUNT,    // int32        | system
+    CAMERA_STATISTICS_INFO_MAX_FACE_COUNT,            // int32        | public
+    CAMERA_STATISTICS_INFO_MAX_HISTOGRAM_COUNT,       // int32        | system
+    CAMERA_STATISTICS_INFO_MAX_SHARPNESS_MAP_VALUE,   // int32        | system
+    CAMERA_STATISTICS_INFO_SHARPNESS_MAP_SIZE,        // int32[]      | system
+    CAMERA_STATISTICS_INFO_AVAILABLE_HOT_PIXEL_MAP_MODES,
+                                                      // byte[]       | public
+    CAMERA_STATISTICS_INFO_END,
+
+    CAMERA_TONEMAP_CURVE_BLUE =                       // float[]      | public
+            CAMERA_TONEMAP_START,
+    CAMERA_TONEMAP_CURVE_GREEN,                       // float[]      | public
+    CAMERA_TONEMAP_CURVE_RED,                         // float[]      | public
+    CAMERA_TONEMAP_MODE,                              // enum         | public
+    CAMERA_TONEMAP_MAX_CURVE_POINTS,                  // int32        | public
+    CAMERA_TONEMAP_AVAILABLE_TONE_MAP_MODES,          // byte[]       | public
+    CAMERA_TONEMAP_GAMMA,                             // float        | public
+    CAMERA_TONEMAP_PRESET_CURVE,                      // enum         | public
+    CAMERA_TONEMAP_END,
+
+    CAMERA_LED_TRANSMIT =                             // enum         | hidden
+            CAMERA_LED_START,
+    CAMERA_LED_AVAILABLE_LEDS,                        // enum[]       | hidden
+    CAMERA_LED_END,
+
+    CAMERA_INFO_SUPPORTED_HARDWARE_LEVEL =            // enum         | public
+            CAMERA_INFO_START,
+    CAMERA_INFO_END,
+
+    CAMERA_BLACK_LEVEL_LOCK =                         // enum         | public
+            CAMERA_BLACK_LEVEL_START,
+    CAMERA_BLACK_LEVEL_END,
+
+    CAMERA_SYNC_FRAME_NUMBER =                        // enum         | hidden
+            CAMERA_SYNC_START,
+    CAMERA_SYNC_MAX_LATENCY,                          // enum         | public
+    CAMERA_SYNC_END,
+
+    CAMERA_REPROCESS_MAX_CAPTURE_STALL =              // int32        | public
+            CAMERA_REPROCESS_START,
+    CAMERA_REPROCESS_END,
+
+    INTEL_INFO_AVAILABLE_CONFIGURATIONS =             // int32[]      | hidden
+            INTEL_INFO_START,
+    INTEL_INFO_AVAILABLE_FEATURES,                    // enum[]       | public
+    INTEL_INFO_AE_EXPOSURE_TIME_RANGE,                // int32[]      | public
+    INTEL_INFO_AE_GAIN_RANGE,                         // int32[]      | public
+    INTEL_INFO_WFOV,                                  // enum         | public
+    INTEL_INFO_SENSOR_MOUNT_TYPE,                     // enum         | public
+    INTEL_INFO_END,
+
+    INTEL_CONTROL_IMAGE_ENHANCEMENT =                 // int32        | public
+            INTEL_CONTROL_START,
+    INTEL_CONTROL_SENSITIVITY_GAIN,                   // float        | public
+    INTEL_CONTROL_FRAME_RATE,                         // float        | public
+    INTEL_CONTROL_AE_CONVERGE_SPEED,                  // enum         | public
+    INTEL_CONTROL_NR_MODE,                            // enum         | public
+    INTEL_CONTROL_NR_LEVEL,                           // int32[]      | public
+    INTEL_CONTROL_IRIS_MODE,                          // enum         | public
+    INTEL_CONTROL_AE_DISTRIBUTION_PRIORITY,           // enum         | public
+    INTEL_CONTROL_IRIS_LEVEL,                         // int32        | public
+    INTEL_CONTROL_WDR_MODE,                           // enum         | public
+    INTEL_CONTROL_WDR_LEVEL,                          // byte         | public
+    INTEL_CONTROL_BLC_AREA_MODE,                      // enum         | public
+    INTEL_CONTROL_SCENE_MODE,                         // enum         | public
+    INTEL_CONTROL_WEIGHT_GRID_MODE,                   // enum         | public
+    INTEL_CONTROL_AE_CONVERGE_SPEED_MODE,             // enum         | public
+    INTEL_CONTROL_DEINTERLACE_MODE,                   // enum         | public
+    INTEL_CONTROL_MAKERNOTE_DATA,                     // byte         | public
+    INTEL_CONTROL_CUSTOM_AIC_PARAM,                   // byte         | public
+    INTEL_CONTROL_MAKERNOTE_MODE,                     // enum         | public
+    INTEL_CONTROL_YUV_COLOR_RANGE,                    // enum         | public
+    INTEL_CONTROL_SENSITIVITY_GAIN_RANGE,             // float[]      | public
+    INTEL_CONTROL_EXPOSURE_TIME_RANGE,                // int32[]      | public
+    INTEL_CONTROL_FISHEYE_DEWARPING_MODE,             // enum         | public
+    INTEL_CONTROL_LTM_TUNING_DATA,                    // byte[]       | public
+    INTEL_CONTROL_DIGITAL_ZOOM_RATIO,                 // float        | public
+    INTEL_CONTROL_LDC_MODE,                           // enum         | public
+    INTEL_CONTROL_RSC_MODE,                           // enum         | public
+    INTEL_CONTROL_FLIP_MODE,                          // enum         | public
+    INTEL_CONTROL_MONO_DOWNSCALE,                     // enum         | public
+    INTEL_CONTROL_RUN3_A_CADENCE,                     // int32        | public
+    INTEL_CONTROL_VIEW_PROJECTION,                    // byte[]       | public
+    INTEL_CONTROL_VIEW_ROTATION,                      // byte[]       | public
+    INTEL_CONTROL_VIEW_FINE_ADJUSTMENTS,              // byte[]       | public
+    INTEL_CONTROL_CAMERA_ROTATION,                    // byte[]       | public
+    INTEL_CONTROL_SCALER_CROP_REGION,                 // int32[]      | public
+    INTEL_CONTROL_END,
+
+    INTEL_CONTROL_ISP_SUPPORTED_CTRL_IDS =            // int32[]      | public
+            INTEL_CONTROL_ISP_START,
+    INTEL_CONTROL_ISP_ENABLED_CTRL_IDS,               // int32[]      | public
+    INTEL_CONTROL_ISP_WB_GAINS,                       // byte[]       | public
+    INTEL_CONTROL_ISP_COLOR_CORRECTION_MATRIX,        // byte[]       | public
+    INTEL_CONTROL_ISP_ADVANCED_COLOR_CORRECTION_MATRIX,
+                                                      // byte[]       | public
+    INTEL_CONTROL_ISP_BXT_CSC,                        // byte[]       | public
+    INTEL_CONTROL_ISP_BXT_DEMOSAIC,                   // byte[]       | public
+    INTEL_CONTROL_ISP_SC_IEFD,                        // byte[]       | public
+    INTEL_CONTROL_ISP_SEE,                            // byte[]       | public
+    INTEL_CONTROL_ISP_BNLM,                           // byte[]       | public
+    INTEL_CONTROL_ISP_TNR5_21,                        // byte[]       | public
+    INTEL_CONTROL_ISP_XNR_DSS,                        // byte[]       | public
+    INTEL_CONTROL_ISP_GAMMA_TONE_MAP,                 // byte[]       | public
+    INTEL_CONTROL_ISP_TNR5_22,                        // byte[]       | public
+    INTEL_CONTROL_ISP_TNR5_25,                        // byte[]       | public
+    INTEL_CONTROL_ISP_END,
+
+} icamera_metadata_tag_t;
+
+/**
+ * Enumeration definitions for the various entries that need them
+ */
+
+// CAMERA_AE_MODE
+typedef enum icamera_metadata_enum_camera_ae_mode {
+    CAMERA_AE_MODE_MANUAL,
+    CAMERA_AE_MODE_AUTO,
+} icamera_metadata_enum_camera_ae_mode_t;
+
+// CAMERA_AE_LOCK
+typedef enum icamera_metadata_enum_camera_ae_lock {
+    CAMERA_AE_LOCK_OFF,
+    CAMERA_AE_LOCK_ON,
+} icamera_metadata_enum_camera_ae_lock_t;
+
+// CAMERA_AE_ANTIBANDING_MODE
+typedef enum icamera_metadata_enum_camera_ae_antibanding_mode {
+    CAMERA_AE_ANTIBANDING_MODE_AUTO,
+    CAMERA_AE_ANTIBANDING_MODE_50HZ,
+    CAMERA_AE_ANTIBANDING_MODE_60HZ,
+    CAMERA_AE_ANTIBANDING_MODE_OFF,
+} icamera_metadata_enum_camera_ae_antibanding_mode_t;
+
+// CAMERA_AE_PRECAPTURE_TRIGGER
+typedef enum icamera_metadata_enum_camera_ae_precapture_trigger {
+    CAMERA_AE_PRECAPTURE_TRIGGER_IDLE,
+    CAMERA_AE_PRECAPTURE_TRIGGER_START,
+} icamera_metadata_enum_camera_ae_precapture_trigger_t;
+
+// CAMERA_AE_STATE
+typedef enum icamera_metadata_enum_camera_ae_state {
+    CAMERA_AE_STATE_INACTIVE,
+    CAMERA_AE_STATE_SEARCHING,
+    CAMERA_AE_STATE_CONVERGED,
+    CAMERA_AE_STATE_LOCKED,
+    CAMERA_AE_STATE_FLASH_REQUIRED,
+    CAMERA_AE_STATE_PRECAPTURE,
+} icamera_metadata_enum_camera_ae_state_t;
+
+// CAMERA_AE_LOCK_AVAILABLE
+typedef enum icamera_metadata_enum_camera_ae_lock_available {
+    CAMERA_AE_LOCK_AVAILABLE_FALSE,
+    CAMERA_AE_LOCK_AVAILABLE_TRUE,
+} icamera_metadata_enum_camera_ae_lock_available_t;
+
+// CAMERA_AWB_MODE
+typedef enum icamera_metadata_enum_camera_awb_mode {
+    CAMERA_AWB_MODE_AUTO,
+    CAMERA_AWB_MODE_INCANDESCENT,
+    CAMERA_AWB_MODE_FLUORESCENT,
+    CAMERA_AWB_MODE_DAYLIGHT,
+    CAMERA_AWB_MODE_FULL_OVERCAST,
+    CAMERA_AWB_MODE_PARTLY_OVERCAST,
+    CAMERA_AWB_MODE_SUNSET,
+    CAMERA_AWB_MODE_VIDEO_CONFERENCE,
+    CAMERA_AWB_MODE_MANUAL_CCT_RANGE,
+    CAMERA_AWB_MODE_MANUAL_WHITE_POINT,
+    CAMERA_AWB_MODE_MANUAL_GAIN,
+    CAMERA_AWB_MODE_MANUAL_COLOR_TRANSFORM,
+} icamera_metadata_enum_camera_awb_mode_t;
+
+// CAMERA_AWB_LOCK
+typedef enum icamera_metadata_enum_camera_awb_lock {
+    CAMERA_AWB_LOCK_OFF,
+    CAMERA_AWB_LOCK_ON,
+} icamera_metadata_enum_camera_awb_lock_t;
+
+// CAMERA_AWB_CONVERGE_SPEED
+typedef enum icamera_metadata_enum_camera_awb_converge_speed {
+    CAMERA_AWB_CONVERGE_SPEED_NORMAL,
+    CAMERA_AWB_CONVERGE_SPEED_MID,
+    CAMERA_AWB_CONVERGE_SPEED_LOW,
+} icamera_metadata_enum_camera_awb_converge_speed_t;
+
+// CAMERA_AWB_CONVERGE_SPEED_MODE
+typedef enum icamera_metadata_enum_camera_awb_converge_speed_mode {
+    CAMERA_AWB_CONVERGE_SPEED_MODE_HAL,
+    CAMERA_AWB_CONVERGE_SPEED_MODE_AIQ,
+} icamera_metadata_enum_camera_awb_converge_speed_mode_t;
+
+// CAMERA_AWB_STATE
+typedef enum icamera_metadata_enum_camera_awb_state {
+    CAMERA_AWB_STATE_INACTIVE,
+    CAMERA_AWB_STATE_SEARCHING,
+    CAMERA_AWB_STATE_CONVERGED,
+    CAMERA_AWB_STATE_LOCKED,
+} icamera_metadata_enum_camera_awb_state_t;
+
+// CAMERA_AWB_LOCK_AVAILABLE
+typedef enum icamera_metadata_enum_camera_awb_lock_available {
+    CAMERA_AWB_LOCK_AVAILABLE_FALSE,
+    CAMERA_AWB_LOCK_AVAILABLE_TRUE,
+} icamera_metadata_enum_camera_awb_lock_available_t;
+
+// CAMERA_AF_MODE
+typedef enum icamera_metadata_enum_camera_af_mode {
+    CAMERA_AF_MODE_OFF,
+    CAMERA_AF_MODE_AUTO,
+    CAMERA_AF_MODE_MACRO,
+    CAMERA_AF_MODE_CONTINUOUS_VIDEO,
+    CAMERA_AF_MODE_CONTINUOUS_PICTURE,
+    CAMERA_AF_MODE_EDOF,
+} icamera_metadata_enum_camera_af_mode_t;
+
+// CAMERA_AF_TRIGGER
+typedef enum icamera_metadata_enum_camera_af_trigger {
+    CAMERA_AF_TRIGGER_IDLE,
+    CAMERA_AF_TRIGGER_START,
+    CAMERA_AF_TRIGGER_CANCEL,
+} icamera_metadata_enum_camera_af_trigger_t;
+
+// CAMERA_AF_STATE
+typedef enum icamera_metadata_enum_camera_af_state {
+    CAMERA_AF_STATE_INACTIVE,
+    CAMERA_AF_STATE_PASSIVE_SCAN,
+    CAMERA_AF_STATE_PASSIVE_FOCUSED,
+    CAMERA_AF_STATE_ACTIVE_SCAN,
+    CAMERA_AF_STATE_FOCUSED_LOCKED,
+    CAMERA_AF_STATE_NOT_FOCUSED_LOCKED,
+    CAMERA_AF_STATE_PASSIVE_UNFOCUSED,
+} icamera_metadata_enum_camera_af_state_t;
+
+// CAMERA_CONTROL_CAPTUREINTENT
+typedef enum icamera_metadata_enum_camera_control_captureintent {
+    CAMERA_CONTROL_CAPTUREINTENT_CUSTOM,
+    CAMERA_CONTROL_CAPTUREINTENT_PREVIEW,
+    CAMERA_CONTROL_CAPTUREINTENT_STILL_CAPTURE,
+    CAMERA_CONTROL_CAPTUREINTENT_VIDEO_RECORD,
+    CAMERA_CONTROL_CAPTUREINTENT_VIDEO_SNAPSHOT,
+    CAMERA_CONTROL_CAPTUREINTENT_ZERO_SHUTTER_LAG,
+    CAMERA_CONTROL_CAPTUREINTENT_MANUAL,
+    CAMERA_CONTROL_CAPTUREINTENT_MOTION_TRACKING,
+} icamera_metadata_enum_camera_control_captureintent_t;
+
+// CAMERA_CONTROL_EFFECT_MODE
+typedef enum icamera_metadata_enum_camera_control_effect_mode {
+    CAMERA_CONTROL_EFFECT_MODE_OFF,
+    CAMERA_CONTROL_EFFECT_MODE_MONO,
+    CAMERA_CONTROL_EFFECT_MODE_NEGATIVE,
+    CAMERA_CONTROL_EFFECT_MODE_SOLARIZE,
+    CAMERA_CONTROL_EFFECT_MODE_SEPIA,
+    CAMERA_CONTROL_EFFECT_MODE_POSTERIZE,
+    CAMERA_CONTROL_EFFECT_MODE_WHITEBOARD,
+    CAMERA_CONTROL_EFFECT_MODE_BLACKBOARD,
+    CAMERA_CONTROL_EFFECT_MODE_AQUA,
+} icamera_metadata_enum_camera_control_effect_mode_t;
+
+// CAMERA_CONTROL_MODE
+typedef enum icamera_metadata_enum_camera_control_mode {
+    CAMERA_CONTROL_MODE_OFF,
+    CAMERA_CONTROL_MODE_AUTO,
+    CAMERA_CONTROL_MODE_USE_SCENE_MODE,
+    CAMERA_CONTROL_MODE_OFF_KEEP_STATE,
+} icamera_metadata_enum_camera_control_mode_t;
+
+// CAMERA_CONTROL_SCENE_MODE
+typedef enum icamera_metadata_enum_camera_control_scene_mode {
+    CAMERA_CONTROL_SCENE_MODE_DISABLED                          = 0,
+    CAMERA_CONTROL_SCENE_MODE_FACE_PRIORITY,
+    CAMERA_CONTROL_SCENE_MODE_ACTION,
+    CAMERA_CONTROL_SCENE_MODE_PORTRAIT,
+    CAMERA_CONTROL_SCENE_MODE_LANDSCAPE,
+    CAMERA_CONTROL_SCENE_MODE_NIGHT,
+    CAMERA_CONTROL_SCENE_MODE_NIGHT_PORTRAIT,
+    CAMERA_CONTROL_SCENE_MODE_THEATRE,
+    CAMERA_CONTROL_SCENE_MODE_BEACH,
+    CAMERA_CONTROL_SCENE_MODE_SNOW,
+    CAMERA_CONTROL_SCENE_MODE_SUNSET,
+    CAMERA_CONTROL_SCENE_MODE_STEADYPHOTO,
+    CAMERA_CONTROL_SCENE_MODE_FIREWORKS,
+    CAMERA_CONTROL_SCENE_MODE_SPORTS,
+    CAMERA_CONTROL_SCENE_MODE_PARTY,
+    CAMERA_CONTROL_SCENE_MODE_CANDLELIGHT,
+    CAMERA_CONTROL_SCENE_MODE_BARCODE,
+    CAMERA_CONTROL_SCENE_MODE_HIGH_SPEED_VIDEO,
+    CAMERA_CONTROL_SCENE_MODE_HDR,
+} icamera_metadata_enum_camera_control_scene_mode_t;
+
+// CAMERA_CONTROL_VIDEO_STABILIZATION_MODE
+typedef enum icamera_metadata_enum_camera_control_video_stabilization_mode {
+    CAMERA_CONTROL_VIDEO_STABILIZATION_MODE_OFF,
+    CAMERA_CONTROL_VIDEO_STABILIZATION_MODE_ON,
+} icamera_metadata_enum_camera_control_video_stabilization_mode_t;
+
+// CAMERA_DEMOSAIC_MODE
+typedef enum icamera_metadata_enum_camera_demosaic_mode {
+    CAMERA_DEMOSAIC_MODE_FAST,
+    CAMERA_DEMOSAIC_MODE_HIGH_QUALITY,
+} icamera_metadata_enum_camera_demosaic_mode_t;
+
+// CAMERA_EDGE_MODE
+typedef enum icamera_metadata_enum_camera_edge_mode {
+    CAMERA_EDGE_MODE_OFF,
+    CAMERA_EDGE_MODE_FAST,
+    CAMERA_EDGE_MODE_HIGH_QUALITY,
+    CAMERA_EDGE_MODE_ZERO_SHUTTER_LAG,
+} icamera_metadata_enum_camera_edge_mode_t;
+
+// CAMERA_FLASH_MODE
+typedef enum icamera_metadata_enum_camera_flash_mode {
+    CAMERA_FLASH_MODE_OFF,
+    CAMERA_FLASH_MODE_SINGLE,
+    CAMERA_FLASH_MODE_TORCH,
+} icamera_metadata_enum_camera_flash_mode_t;
+
+// CAMERA_FLASH_STATE
+typedef enum icamera_metadata_enum_camera_flash_state {
+    CAMERA_FLASH_STATE_UNAVAILABLE,
+    CAMERA_FLASH_STATE_CHARGING,
+    CAMERA_FLASH_STATE_READY,
+    CAMERA_FLASH_STATE_FIRED,
+    CAMERA_FLASH_STATE_PARTIAL,
+} icamera_metadata_enum_camera_flash_state_t;
+
+// CAMERA_FLASH_INFO_AVAILABLE
+typedef enum icamera_metadata_enum_camera_flash_info_available {
+    CAMERA_FLASH_INFO_AVAILABLE_FALSE,
+    CAMERA_FLASH_INFO_AVAILABLE_TRUE,
+} icamera_metadata_enum_camera_flash_info_available_t;
+
+// CAMERA_HOT_PIXEL_MODE
+typedef enum icamera_metadata_enum_camera_hot_pixel_mode {
+    CAMERA_HOT_PIXEL_MODE_OFF,
+    CAMERA_HOT_PIXEL_MODE_FAST,
+    CAMERA_HOT_PIXEL_MODE_HIGH_QUALITY,
+} icamera_metadata_enum_camera_hot_pixel_mode_t;
+
+// CAMERA_LENS_OPTICAL_STABILIZATION_MODE
+typedef enum icamera_metadata_enum_camera_lens_optical_stabilization_mode {
+    CAMERA_LENS_OPTICAL_STABILIZATION_MODE_OFF,
+    CAMERA_LENS_OPTICAL_STABILIZATION_MODE_ON,
+} icamera_metadata_enum_camera_lens_optical_stabilization_mode_t;
+
+// CAMERA_LENS_FACING
+typedef enum icamera_metadata_enum_camera_lens_facing {
+    CAMERA_LENS_FACING_FRONT,
+    CAMERA_LENS_FACING_BACK,
+} icamera_metadata_enum_camera_lens_facing_t;
+
+// CAMERA_LENS_STATE
+typedef enum icamera_metadata_enum_camera_lens_state {
+    CAMERA_LENS_STATE_STATIONARY,
+    CAMERA_LENS_STATE_MOVING,
+} icamera_metadata_enum_camera_lens_state_t;
+
+// CAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION
+typedef enum icamera_metadata_enum_camera_lens_info_focus_distance_calibration {
+    CAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION_UNCALIBRATED,
+    CAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION_APPROXIMATE,
+    CAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION_CALIBRATED,
+} icamera_metadata_enum_camera_lens_info_focus_distance_calibration_t;
+
+// CAMERA_NOISE_REDUCTION_MODE
+typedef enum icamera_metadata_enum_camera_noise_reduction_mode {
+    CAMERA_NOISE_REDUCTION_MODE_OFF,
+    CAMERA_NOISE_REDUCTION_MODE_FAST,
+    CAMERA_NOISE_REDUCTION_MODE_HIGH_QUALITY,
+    CAMERA_NOISE_REDUCTION_MODE_MINIMAL,
+    CAMERA_NOISE_REDUCTION_MODE_ZERO_SHUTTER_LAG,
+} icamera_metadata_enum_camera_noise_reduction_mode_t;
+
+// CAMERA_REQUEST_METADATA_MODE
+typedef enum icamera_metadata_enum_camera_request_metadata_mode {
+    CAMERA_REQUEST_METADATA_MODE_NONE,
+    CAMERA_REQUEST_METADATA_MODE_FULL,
+} icamera_metadata_enum_camera_request_metadata_mode_t;
+
+// CAMERA_REQUEST_AVAILABLE_CAPABILITIES
+typedef enum icamera_metadata_enum_camera_request_available_capabilities {
+    CAMERA_REQUEST_AVAILABLE_CAPABILITIES_BACKWARD_COMPATIBLE,
+    CAMERA_REQUEST_AVAILABLE_CAPABILITIES_MANUAL_SENSOR,
+    CAMERA_REQUEST_AVAILABLE_CAPABILITIES_MANUAL_POST_PROCESSING,
+    CAMERA_REQUEST_AVAILABLE_CAPABILITIES_RAW,
+    CAMERA_REQUEST_AVAILABLE_CAPABILITIES_ZSL,
+    CAMERA_REQUEST_AVAILABLE_CAPABILITIES_READ_SENSOR_SETTINGS,
+    CAMERA_REQUEST_AVAILABLE_CAPABILITIES_BURST_CAPTURE,
+} icamera_metadata_enum_camera_request_available_capabilities_t;
+
+// CAMERA_SCALER_CROPPING_TYPE
+typedef enum icamera_metadata_enum_camera_scaler_cropping_type {
+    CAMERA_SCALER_CROPPING_TYPE_CENTER_ONLY,
+    CAMERA_SCALER_CROPPING_TYPE_FREEFORM,
+} icamera_metadata_enum_camera_scaler_cropping_type_t;
+
+// CAMERA_SENSOR_REFERENCE_ILLUMINANT1
+typedef enum icamera_metadata_enum_camera_sensor_reference_illuminant1 {
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_DAYLIGHT                = 1,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_FLUORESCENT             = 2,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_TUNGSTEN                = 3,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_FLASH                   = 4,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_FINE_WEATHER            = 9,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_CLOUDY_WEATHER          = 10,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_SHADE                   = 11,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_DAYLIGHT_FLUORESCENT    = 12,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_DAY_WHITE_FLUORESCENT   = 13,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_COOL_WHITE_FLUORESCENT  = 14,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_WHITE_FLUORESCENT       = 15,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_STANDARD_A              = 17,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_STANDARD_B              = 18,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_STANDARD_C              = 19,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_D55                     = 20,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_D65                     = 21,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_D75                     = 22,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_D50                     = 23,
+    CAMERA_SENSOR_REFERENCE_ILLUMINANT1_ISO_STUDIO_TUNGSTEN     = 24,
+} icamera_metadata_enum_camera_sensor_reference_illuminant1_t;
+
+// CAMERA_SENSOR_TEST_PATTERN_MODE
+typedef enum icamera_metadata_enum_camera_sensor_test_pattern_mode {
+    CAMERA_SENSOR_TEST_PATTERN_MODE_OFF,
+    CAMERA_SENSOR_TEST_PATTERN_MODE_SOLID_COLOR,
+    CAMERA_SENSOR_TEST_PATTERN_MODE_COLOR_BARS,
+    CAMERA_SENSOR_TEST_PATTERN_MODE_COLOR_BARS_FADE_TO_GRAY,
+    CAMERA_SENSOR_TEST_PATTERN_MODE_PN9,
+    CAMERA_SENSOR_TEST_PATTERN_MODE_CUSTOM1                     = 256,
+} icamera_metadata_enum_camera_sensor_test_pattern_mode_t;
+
+// CAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT
+typedef enum icamera_metadata_enum_camera_sensor_info_color_filter_arrangement {
+    CAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_RGGB,
+    CAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_GRBG,
+    CAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_GBRG,
+    CAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_BGGR,
+    CAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT_RGB,
+} icamera_metadata_enum_camera_sensor_info_color_filter_arrangement_t;
+
+// CAMERA_SENSOR_INFO_TIMESTAMP_SOURCE
+typedef enum icamera_metadata_enum_camera_sensor_info_timestamp_source {
+    CAMERA_SENSOR_INFO_TIMESTAMP_SOURCE_UNKNOWN,
+    CAMERA_SENSOR_INFO_TIMESTAMP_SOURCE_REALTIME,
+} icamera_metadata_enum_camera_sensor_info_timestamp_source_t;
+
+// CAMERA_SHADING_MODE
+typedef enum icamera_metadata_enum_camera_shading_mode {
+    CAMERA_SHADING_MODE_OFF,
+    CAMERA_SHADING_MODE_FAST,
+    CAMERA_SHADING_MODE_HIGH_QUALITY,
+} icamera_metadata_enum_camera_shading_mode_t;
+
+// CAMERA_STATISTICS_FACE_DETECT_MODE
+typedef enum icamera_metadata_enum_camera_statistics_face_detect_mode {
+    CAMERA_STATISTICS_FACE_DETECT_MODE_OFF,
+    CAMERA_STATISTICS_FACE_DETECT_MODE_SIMPLE,
+    CAMERA_STATISTICS_FACE_DETECT_MODE_FULL,
+} icamera_metadata_enum_camera_statistics_face_detect_mode_t;
+
+// CAMERA_STATISTICS_HISTOGRAM_MODE
+typedef enum icamera_metadata_enum_camera_statistics_histogram_mode {
+    CAMERA_STATISTICS_HISTOGRAM_MODE_OFF,
+    CAMERA_STATISTICS_HISTOGRAM_MODE_ON,
+} icamera_metadata_enum_camera_statistics_histogram_mode_t;
+
+// CAMERA_STATISTICS_SHARPNESS_MAP_MODE
+typedef enum icamera_metadata_enum_camera_statistics_sharpness_map_mode {
+    CAMERA_STATISTICS_SHARPNESS_MAP_MODE_OFF,
+    CAMERA_STATISTICS_SHARPNESS_MAP_MODE_ON,
+} icamera_metadata_enum_camera_statistics_sharpness_map_mode_t;
+
+// CAMERA_STATISTICS_HOT_PIXEL_MAP_MODE
+typedef enum icamera_metadata_enum_camera_statistics_hot_pixel_map_mode {
+    CAMERA_STATISTICS_HOT_PIXEL_MAP_MODE_OFF,
+    CAMERA_STATISTICS_HOT_PIXEL_MAP_MODE_ON,
+} icamera_metadata_enum_camera_statistics_hot_pixel_map_mode_t;
+
+// CAMERA_STATISTICS_SCENE_FLICKER
+typedef enum icamera_metadata_enum_camera_statistics_scene_flicker {
+    CAMERA_STATISTICS_SCENE_FLICKER_NONE,
+    CAMERA_STATISTICS_SCENE_FLICKER_50HZ,
+    CAMERA_STATISTICS_SCENE_FLICKER_60HZ,
+} icamera_metadata_enum_camera_statistics_scene_flicker_t;
+
+// CAMERA_STATISTICS_LENS_SHADING_MAP_MODE
+typedef enum icamera_metadata_enum_camera_statistics_lens_shading_map_mode {
+    CAMERA_STATISTICS_LENS_SHADING_MAP_MODE_OFF,
+    CAMERA_STATISTICS_LENS_SHADING_MAP_MODE_ON,
+} icamera_metadata_enum_camera_statistics_lens_shading_map_mode_t;
+
+// CAMERA_TONEMAP_MODE
+typedef enum icamera_metadata_enum_camera_tonemap_mode {
+    CAMERA_TONEMAP_MODE_CONTRAST_CURVE,
+    CAMERA_TONEMAP_MODE_FAST,
+    CAMERA_TONEMAP_MODE_HIGH_QUALITY,
+    CAMERA_TONEMAP_MODE_GAMMA_VALUE,
+    CAMERA_TONEMAP_MODE_PRESET_CURVE,
+} icamera_metadata_enum_camera_tonemap_mode_t;
+
+// CAMERA_TONEMAP_PRESET_CURVE
+typedef enum icamera_metadata_enum_camera_tonemap_preset_curve {
+    CAMERA_TONEMAP_PRESET_CURVE_SRGB,
+    CAMERA_TONEMAP_PRESET_CURVE_REC709,
+} icamera_metadata_enum_camera_tonemap_preset_curve_t;
+
+// CAMERA_LED_TRANSMIT
+typedef enum icamera_metadata_enum_camera_led_transmit {
+    CAMERA_LED_TRANSMIT_OFF,
+    CAMERA_LED_TRANSMIT_ON,
+} icamera_metadata_enum_camera_led_transmit_t;
+
+// CAMERA_LED_AVAILABLE_LEDS
+typedef enum icamera_metadata_enum_camera_led_available_leds {
+    CAMERA_LED_AVAILABLE_LEDS_TRANSMIT,
+} icamera_metadata_enum_camera_led_available_leds_t;
+
+// CAMERA_INFO_SUPPORTED_HARDWARE_LEVEL
+typedef enum icamera_metadata_enum_camera_info_supported_hardware_level {
+    CAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED,
+    CAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_FULL,
+    CAMERA_INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY,
+} icamera_metadata_enum_camera_info_supported_hardware_level_t;
+
+// CAMERA_BLACK_LEVEL_LOCK
+typedef enum icamera_metadata_enum_camera_black_level_lock {
+    CAMERA_BLACK_LEVEL_LOCK_OFF,
+    CAMERA_BLACK_LEVEL_LOCK_ON,
+} icamera_metadata_enum_camera_black_level_lock_t;
+
+// CAMERA_SYNC_FRAME_NUMBER
+typedef enum icamera_metadata_enum_camera_sync_frame_number {
+    CAMERA_SYNC_FRAME_NUMBER_CONVERGING                         = -1,
+    CAMERA_SYNC_FRAME_NUMBER_UNKNOWN                            = -2,
+} icamera_metadata_enum_camera_sync_frame_number_t;
+
+// CAMERA_SYNC_MAX_LATENCY
+typedef enum icamera_metadata_enum_camera_sync_max_latency {
+    CAMERA_SYNC_MAX_LATENCY_PER_FRAME_CONTROL                   = 0,
+    CAMERA_SYNC_MAX_LATENCY_UNKNOWN                             = -1,
+} icamera_metadata_enum_camera_sync_max_latency_t;
+
+// INTEL_INFO_AVAILABLE_FEATURES
+typedef enum icamera_metadata_enum_intel_info_available_features {
+    INTEL_INFO_AVAILABLE_FEATURES_MANUAL_EXPOSURE               = 0,
+    INTEL_INFO_AVAILABLE_FEATURES_MANUAL_WHITE_BALANCE          = 1,
+    INTEL_INFO_AVAILABLE_FEATURES_IMAGE_ENHANCEMENT             = 2,
+    INTEL_INFO_AVAILABLE_FEATURES_NOISE_REDUCTION               = 3,
+    INTEL_INFO_AVAILABLE_FEATURES_SCENE_MODE                    = 4,
+    INTEL_INFO_AVAILABLE_FEATURES_WEIGHT_GRID_MODE              = 5,
+    INTEL_INFO_AVAILABLE_FEATURES_PER_FRAME_CONTROL             = 6,
+    INTEL_INFO_AVAILABLE_FEATURES_ISP_CONTROL                   = 7,
+} icamera_metadata_enum_intel_info_available_features_t;
+
+// INTEL_INFO_WFOV
+typedef enum icamera_metadata_enum_intel_info_wfov {
+    INTEL_INFO_WFOV_OFF,
+    INTEL_INFO_WFOV_ON,
+} icamera_metadata_enum_intel_info_wfov_t;
+
+// INTEL_INFO_SENSOR_MOUNT_TYPE
+typedef enum icamera_metadata_enum_intel_info_sensor_mount_type {
+    INTEL_INFO_SENSOR_MOUNT_TYPE_WALL_MOUNTED,
+    INTEL_INFO_SENSOR_MOUNT_TYPE_CEILING_MOUNTER,
+} icamera_metadata_enum_intel_info_sensor_mount_type_t;
+
+// INTEL_CONTROL_AE_CONVERGE_SPEED
+typedef enum icamera_metadata_enum_intel_control_ae_converge_speed {
+    INTEL_CONTROL_AE_CONVERGE_SPEED_NORMAL,
+    INTEL_CONTROL_AE_CONVERGE_SPEED_MID,
+    INTEL_CONTROL_AE_CONVERGE_SPEED_LOW,
+} icamera_metadata_enum_intel_control_ae_converge_speed_t;
+
+// INTEL_CONTROL_NR_MODE
+typedef enum icamera_metadata_enum_intel_control_nr_mode {
+    INTEL_CONTROL_NR_MODE_OFF,
+    INTEL_CONTROL_NR_MODE_AUTO,
+    INTEL_CONTROL_NR_MODE_MANUAL_NORMAL,
+    INTEL_CONTROL_NR_MODE_MANUAL_EXPERT,
+} icamera_metadata_enum_intel_control_nr_mode_t;
+
+// INTEL_CONTROL_IRIS_MODE
+typedef enum icamera_metadata_enum_intel_control_iris_mode {
+    INTEL_CONTROL_IRIS_MODE_AUTO,
+    INTEL_CONTROL_IRIS_MODE_MANUAL,
+    INTEL_CONTROL_IRIS_MODE_CUSTOMIZED,
+} icamera_metadata_enum_intel_control_iris_mode_t;
+
+// INTEL_CONTROL_AE_DISTRIBUTION_PRIORITY
+typedef enum icamera_metadata_enum_intel_control_ae_distribution_priority {
+    INTEL_CONTROL_AE_DISTRIBUTION_PRIORITY_AUTO,
+    INTEL_CONTROL_AE_DISTRIBUTION_PRIORITY_SHUTTER,
+    INTEL_CONTROL_AE_DISTRIBUTION_PRIORITY_ISO,
+    INTEL_CONTROL_AE_DISTRIBUTION_PRIORITY_APERTURE,
+} icamera_metadata_enum_intel_control_ae_distribution_priority_t;
+
+// INTEL_CONTROL_WDR_MODE
+typedef enum icamera_metadata_enum_intel_control_wdr_mode {
+    INTEL_CONTROL_WDR_MODE_OFF,
+    INTEL_CONTROL_WDR_MODE_ON,
+    INTEL_CONTROL_WDR_MODE_AUTO,
+} icamera_metadata_enum_intel_control_wdr_mode_t;
+
+// INTEL_CONTROL_BLC_AREA_MODE
+typedef enum icamera_metadata_enum_intel_control_blc_area_mode {
+    INTEL_CONTROL_BLC_AREA_MODE_OFF,
+    INTEL_CONTROL_BLC_AREA_MODE_ON,
+} icamera_metadata_enum_intel_control_blc_area_mode_t;
+
+// INTEL_CONTROL_SCENE_MODE
+typedef enum icamera_metadata_enum_intel_control_scene_mode {
+    INTEL_CONTROL_SCENE_MODE_AUTO,
+    INTEL_CONTROL_SCENE_MODE_HDR,
+    INTEL_CONTROL_SCENE_MODE_ULL,
+    INTEL_CONTROL_SCENE_MODE_VIDEO_LL,
+    INTEL_CONTROL_SCENE_MODE_HDR2,
+} icamera_metadata_enum_intel_control_scene_mode_t;
+
+// INTEL_CONTROL_WEIGHT_GRID_MODE
+typedef enum icamera_metadata_enum_intel_control_weight_grid_mode {
+    INTEL_CONTROL_WEIGHT_GRID_MODE_AUTO,
+    INTEL_CONTROL_WEIGHT_GRID_MODE_CUSTOM_WEIGHT_GRID1,
+    INTEL_CONTROL_WEIGHT_GRID_MODE_CUSTOM_WEIGHT_GRID2,
+    INTEL_CONTROL_WEIGHT_GRID_MODE_CUSTOM_WEIGHT_GRID3,
+} icamera_metadata_enum_intel_control_weight_grid_mode_t;
+
+// INTEL_CONTROL_AE_CONVERGE_SPEED_MODE
+typedef enum icamera_metadata_enum_intel_control_ae_converge_speed_mode {
+    INTEL_CONTROL_AE_CONVERGE_SPEED_MODE_HAL,
+    INTEL_CONTROL_AE_CONVERGE_SPEED_MODE_AIQ,
+} icamera_metadata_enum_intel_control_ae_converge_speed_mode_t;
+
+// INTEL_CONTROL_DEINTERLACE_MODE
+typedef enum icamera_metadata_enum_intel_control_deinterlace_mode {
+    INTEL_CONTROL_DEINTERLACE_MODE_OFF,
+    INTEL_CONTROL_DEINTERLACE_MODE_WEAVING,
+} icamera_metadata_enum_intel_control_deinterlace_mode_t;
+
+// INTEL_CONTROL_MAKERNOTE_MODE
+typedef enum icamera_metadata_enum_intel_control_makernote_mode {
+    INTEL_CONTROL_MAKERNOTE_MODE_OFF,
+    INTEL_CONTROL_MAKERNOTE_MODE_JPEG,
+    INTEL_CONTROL_MAKERNOTE_MODE_RAW,
+} icamera_metadata_enum_intel_control_makernote_mode_t;
+
+// INTEL_CONTROL_YUV_COLOR_RANGE
+typedef enum icamera_metadata_enum_intel_control_yuv_color_range {
+    INTEL_CONTROL_YUV_COLOR_RANGE_FULL,
+    INTEL_CONTROL_YUV_COLOR_RANGE_REDUCED,
+} icamera_metadata_enum_intel_control_yuv_color_range_t;
+
+// INTEL_CONTROL_FISHEYE_DEWARPING_MODE
+typedef enum icamera_metadata_enum_intel_control_fisheye_dewarping_mode {
+    INTEL_CONTROL_FISHEYE_DEWARPING_MODE_OFF,
+    INTEL_CONTROL_FISHEYE_DEWARPING_MODE_REARVIEW,
+    INTEL_CONTROL_FISHEYE_DEWARPING_MODE_HITCHVIEW,
+} icamera_metadata_enum_intel_control_fisheye_dewarping_mode_t;
+
+// INTEL_CONTROL_LDC_MODE
+typedef enum icamera_metadata_enum_intel_control_ldc_mode {
+    INTEL_CONTROL_LDC_MODE_OFF,
+    INTEL_CONTROL_LDC_MODE_ON,
+} icamera_metadata_enum_intel_control_ldc_mode_t;
+
+// INTEL_CONTROL_RSC_MODE
+typedef enum icamera_metadata_enum_intel_control_rsc_mode {
+    INTEL_CONTROL_RSC_MODE_OFF,
+    INTEL_CONTROL_RSC_MODE_ON,
+} icamera_metadata_enum_intel_control_rsc_mode_t;
+
+// INTEL_CONTROL_FLIP_MODE
+typedef enum icamera_metadata_enum_intel_control_flip_mode {
+    INTEL_CONTROL_FLIP_MODE_NONE,
+    INTEL_CONTROL_FLIP_MODE_VFLIP,
+    INTEL_CONTROL_FLIP_MODE_HFLIP,
+    INTEL_CONTROL_FLIP_MODE_VHFLIP,
+} icamera_metadata_enum_intel_control_flip_mode_t;
+
+// INTEL_CONTROL_MONO_DOWNSCALE
+typedef enum icamera_metadata_enum_intel_control_mono_downscale {
+    INTEL_CONTROL_MONO_DOWNSCALE_OFF,
+    INTEL_CONTROL_MONO_DOWNSCALE_ON,
+} icamera_metadata_enum_intel_control_mono_downscale_t;
+
diff --git a/camera/hal/intel/ipu6/src/platformdata/AiqInitData.cpp b/camera/hal/intel/ipu6/src/platformdata/AiqInitData.cpp
new file mode 100644
index 000000000000..909fd9e59120
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/AiqInitData.cpp
@@ -0,0 +1,583 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "AiqInitData"
+
+#include <sys/stat.h>
+
+#include "iutils/CameraLog.h"
+#include "AiqInitData.h"
+#include "AiqUtils.h"
+#include "PlatformData.h"
+#include "ia_types.h"
+
+using std::string;
+
+namespace icamera {
+
+AiqdData::AiqdData(TuningMode tuningMode, const string& sensorName) :
+    mDataPtr(nullptr)
+{
+    CLEAR(mBinaryData);
+
+    mAiqdFileName.append(CAMERA_CACHE_DIR);
+    mAiqdFileName.append(sensorName);
+    mAiqdFileName.append("_");
+    mAiqdFileName.append(CameraUtils::tuningMode2String(tuningMode));
+    mAiqdFileName.append(".aiqd");
+
+    LOG1("%s, aiqd file name %s", __func__, mAiqdFileName.c_str());
+    loadAiqdFromFile();
+};
+
+AiqdData::~AiqdData()
+{
+    LOG1("%s, aiqd file name %s", __func__, mAiqdFileName.c_str());
+}
+
+ia_binary_data* AiqdData::getAiqd()
+{
+    return mDataPtr ? &mBinaryData : nullptr;
+}
+
+void AiqdData::saveAiqd(const ia_binary_data& data)
+{
+    LOG1("%s", __func__);
+
+    if (!mDataPtr || data.size != mBinaryData.size) {
+        mDataPtr.reset(new char[data.size]);
+        mBinaryData.size = data.size;
+        mBinaryData.data = mDataPtr.get();
+    }
+    MEMCPY_S(mBinaryData.data, mBinaryData.size, data.data, data.size);
+
+    saveAiqdToFile();
+}
+
+void AiqdData::loadAiqdFromFile()
+{
+    LOG1("%s", __func__);
+
+    // Get file size
+    struct stat fileStat;
+    CLEAR(fileStat);
+    int ret = stat(mAiqdFileName.c_str(), &fileStat);
+    if (ret != 0) {
+        LOG1("There is no aiqd file %s", mAiqdFileName.c_str());
+        return;
+    }
+
+    // Opem aiqd file
+    FILE* fp = fopen(mAiqdFileName.c_str(), "rb");
+    CheckWarning(fp == nullptr, VOID_VALUE, "Failed to open aiqd file %s, error %s",
+                 mAiqdFileName.c_str(), strerror(errno));
+
+    std::unique_ptr<char[]> dataPtr(new char[fileStat.st_size]);
+
+    // Read aiqd data
+    size_t readSize = fread(dataPtr.get(), sizeof(char), fileStat.st_size, fp);
+    fclose(fp);
+
+    CheckWarning(readSize != (size_t)fileStat.st_size, VOID_VALUE,
+                 "Failed to read aiqd %s, error %s",
+                 mAiqdFileName.c_str(), strerror(errno));
+
+    mDataPtr = move(dataPtr);
+    mBinaryData.data = mDataPtr.get();
+    mBinaryData.size = fileStat.st_size;
+    LOG1("%s, aiqd file %s, size %d", __func__, mAiqdFileName.c_str(), mBinaryData.size);
+}
+
+void AiqdData::saveAiqdToFile()
+{
+    LOG1("%s", __func__);
+
+    // Open aiqd file
+    FILE* fp = fopen(mAiqdFileName.c_str(), "wb");
+    CheckWarning(fp == nullptr, VOID_VALUE, "Failed to open aiqd file %s, error %s",
+                 mAiqdFileName.c_str(), strerror(errno));
+
+    // Write aiqd data to file
+    size_t writeSize = fwrite(mBinaryData.data, 1, mBinaryData.size, fp);
+    if (writeSize != mBinaryData.size) {
+        LOGW("Failed to write aiqd data %s, error %s", mAiqdFileName.c_str(), strerror(errno));
+        fclose(fp);
+        return;
+    }
+
+    fflush(fp);
+    fclose(fp);
+
+    LOG1("%s, aiqd file %s, size %d", __func__, mAiqdFileName.c_str(), mBinaryData.size);
+}
+
+CpfConf::CpfConf()
+{
+    mLard = new IntelLard();
+    mCmc = std::unique_ptr<IntelCmc>(new IntelCmc());
+    CLEAR(mAiq);
+    CLEAR(mIsp);
+    CLEAR(mOthers);
+}
+
+CpfConf::~CpfConf()
+{
+    delete mLard;
+    LOG1("@%s", __func__);
+}
+
+int CpfConf::init(const ia_binary_data& cpfData, const LardTagConfig* lardTagCfg)
+{
+    LOG1("@%s", __func__);
+
+    CheckWarning(mCmc->getCmc(), OK, "cmc has already been init before!");
+    CheckError((cpfData.data == nullptr), BAD_VALUE, "Error Initializing CPF configure");
+
+    bool cmcRet = false;
+    ia_lard *iaLard = mLard->init(&cpfData);
+    if (iaLard) {
+        LOG1("AIQB file supported by lard.");
+        ia_lard_input_params lardInputParams;
+        initLardInputParam(*iaLard, lardTagCfg, &lardInputParams);
+
+        ia_lard_results* lardResults;
+        // Run ia_lard, result is nullptr if aiqb file is not supported
+        ia_err iaErr = mLard->run(iaLard, &lardInputParams, &lardResults);
+        if (lardResults != nullptr) {
+            LOG1("ia_lard_run success, using lard to get cmc mode and tuning.");
+            cmcRet = mCmc->init(&lardResults->aiqb_cmc_data, nullptr);
+            mAiq = lardResults->aiqb_aiq_data;
+            mIsp = lardResults->aiqb_isp_data;
+            mOthers = lardResults->aiqb_other_data;
+        } else {
+            LOGE("Fail to run ia_lard, iaErr = %d", iaErr);
+        }
+        mLard->deinit(iaLard);
+    } else {
+        LOG1("Lard not supported. The AIQB file may be in old CPF format");
+        cmcRet = mCmc->init(&cpfData, nullptr);
+        mAiq = cpfData;
+        mIsp = cpfData;
+        mOthers = cpfData;
+    }
+    CheckError(!cmcRet, FAILED_TRANSACTION, "Error cmc parser init!");
+
+    return OK;
+}
+
+ia_cmc_t* CpfConf::getCmc() const
+{
+    return mCmc->getCmc();
+}
+
+uintptr_t CpfConf::getCmcHandle() const
+{
+    return mCmc->getCmcHandle();
+}
+
+void CpfConf::getIspData(ia_binary_data* ispData)
+{
+    ispData->data = mIsp.data;
+    ispData->size = mIsp.size;
+}
+
+void CpfConf::getAiqData(ia_binary_data* aiqData)
+{
+    aiqData->data = mAiq.data;
+    aiqData->size = mAiq.size;
+}
+
+void CpfConf::getOtherData(ia_binary_data* otherData)
+{
+    otherData->data = mOthers.data;
+    otherData->size = mOthers.size;
+}
+
+void CpfConf::deinit()
+{
+    mCmc->deinit();
+}
+
+void CpfConf::initLardInputParam(const ia_lard& iaLard,
+                                 const LardTagConfig* lardTagCfg,
+                                 ia_lard_input_params* lardInputParam)
+{
+    if (!lardTagCfg) {
+        lardInputParam->cmc_mode_tag = FOURCC_TO_UL('D','F','L','T');
+        lardInputParam->aiq_mode_tag = FOURCC_TO_UL('D','F','L','T');
+        lardInputParam->isp_mode_index = FOURCC_TO_UL('D','F','L','T');
+        lardInputParam->others_mode_tag = FOURCC_TO_UL('D','F','L','T');
+        return;
+    }
+
+    unsigned int count = 0;
+    const unsigned int *tags = nullptr;
+
+    mLard->getTagList(const_cast<ia_lard*>(&iaLard), FOURCC_TO_UL('L','C','M','C'), &count, &tags);
+    lardInputParam->cmc_mode_tag = isTagValid(lardTagCfg->cmcTag, count, tags) ? \
+                                   lardTagCfg->cmcTag : FOURCC_TO_UL('D','F','L','T');
+
+    mLard->getTagList(const_cast<ia_lard*>(&iaLard), FOURCC_TO_UL('L','A','I','Q'), &count, &tags);
+    lardInputParam->aiq_mode_tag = isTagValid(lardTagCfg->aiqTag, count, tags) ? \
+                                   lardTagCfg->aiqTag : FOURCC_TO_UL('D','F','L','T');
+
+    mLard->getTagList(const_cast<ia_lard*>(&iaLard), FOURCC_TO_UL('L','I','S','P'), &count, &tags);
+    lardInputParam->isp_mode_index = isTagValid(lardTagCfg->ispTag, count, tags) ? \
+                                     lardTagCfg->ispTag : FOURCC_TO_UL('D','F','L','T');
+
+    mLard->getTagList(const_cast<ia_lard*>(&iaLard), FOURCC_TO_UL('L','T','H','R'), &count, &tags);
+    lardInputParam->others_mode_tag = isTagValid(lardTagCfg->othersTag, count, tags) ? \
+                                      lardTagCfg->othersTag : FOURCC_TO_UL('D','F','L','T');
+
+    LOG1("@%s: The lard tags are: aiq-0x%x, isp-0x%x, cmc-0x%x, others-0x%x", __func__,
+        lardInputParam->aiq_mode_tag, lardInputParam->isp_mode_index,
+        lardInputParam->cmc_mode_tag, lardInputParam->others_mode_tag);
+}
+
+bool CpfConf::isTagValid(unsigned int tag, unsigned int count, const unsigned int* tags)
+{
+    if (tags != nullptr) {
+        for (unsigned int i = 0; i < count; i++) {
+            if (tags[i] == tag) return true;
+        }
+    }
+    LOG1("@%s: Tag 0x%x is not valid. Will use DFLT instead.", __func__, tag);
+    return false;
+}
+
+CpfStore::CpfStore(const std::string& sensorName,
+                   const std::string& camCfgDir,
+                   const std::vector<TuningConfig>& tuningCfg,
+                   const std::vector<LardTagConfig>& lardTagCfg)
+{
+    LOG1("@%s:Sensor Name = %s", __func__, sensorName.c_str());
+
+    LardTagConfig* oneLardTagCfg = nullptr;
+
+    CLEAR(mCpfConfig);
+    for (auto &cfg : tuningCfg) {
+        if (mCpfConfig[cfg.tuningMode] != nullptr) {
+            continue;
+        }
+
+        if (cfg.aiqbName.empty()) {
+            LOGE("aiqb name is empty, sensor name %s", sensorName.c_str());
+            continue;
+        }
+
+        if (mCpfData.find(cfg.aiqbName) == mCpfData.end()) {
+            // Obtain the configurations
+            if (loadConf(camCfgDir, cfg.aiqbName) != OK) {
+                LOGE("load file %s failed, sensor %s", cfg.aiqbName.c_str(), sensorName.c_str());
+                continue;
+            }
+        }
+
+        oneLardTagCfg = nullptr;
+        for (size_t i = 0; i < lardTagCfg.size(); i++) {
+            if (cfg.tuningMode == lardTagCfg[i].tuningMode) {
+                oneLardTagCfg = const_cast<LardTagConfig*>(&lardTagCfg[i]);
+                break;
+            }
+        }
+
+        CpfConf* cpfConf = new CpfConf();
+
+        cpfConf->init(mCpfData[cfg.aiqbName], oneLardTagCfg);
+        mCpfConfig[cfg.tuningMode] = cpfConf;
+    }
+}
+
+CpfStore::~CpfStore()
+{
+    LOG1("@%s", __func__);
+    for (int mode=0; mode<TUNING_MODE_MAX; mode++) {
+        if (mCpfConfig[mode]) {
+            mCpfConfig[mode]->deinit();
+            delete mCpfConfig[mode];
+        }
+    }
+    for (auto &cpfData : mCpfData) {
+        if (cpfData.second.data) {
+            free(cpfData.second.data);
+        }
+    }
+    mCpfData.clear();
+}
+
+/**
+ * findConfigFile
+ *
+ * Search the path where CPF files are stored
+*/
+int CpfStore::findConfigFile(const std::string& camCfgDir, std::string* cpfPathName)
+{
+    LOG1("@%s, cpfPathName:%p", __func__, cpfPathName);
+    CheckError(!cpfPathName, BAD_VALUE, "@%s, cpfPathName is nullptr", __func__);
+
+    std::vector<string> configFilePath;
+    configFilePath.push_back("./");
+    configFilePath.push_back(camCfgDir);
+    int configFileCount = configFilePath.size();
+
+    string cpfFile;
+    for (int i = 0; i < configFileCount; i++) {
+        cpfFile.append(configFilePath.at(i));
+        cpfFile.append(*cpfPathName);
+        struct stat st;
+        if (!stat(cpfFile.c_str(), &st))
+            break;
+        cpfFile.clear();
+    }
+
+    if (cpfFile.empty()) {//CPF file not found
+        LOG1("@%s:No CPF file found for %s", __func__,cpfPathName->c_str());
+        return NAME_NOT_FOUND;
+    }
+
+    *cpfPathName = cpfFile;
+    LOG1("@%s:CPF file found %s", __func__,cpfPathName->c_str());
+    return OK;
+}
+
+/**
+ * loadConf
+ *
+ * load the CPF file
+*/
+int CpfStore::loadConf(const std::string& camCfgDir, const std::string& aiqbName)
+{
+    LOG1("@%s", __func__);
+    int ret = OK;
+    const char *suffix = ".aiqb";
+
+    string cpfPathName = aiqbName;
+    cpfPathName.append(suffix);
+    LOG1("aiqb file name %s", cpfPathName.c_str());
+
+    if (findConfigFile(camCfgDir, &cpfPathName) != OK) {
+        LOGE("CpfStore no aiqb file:%s", aiqbName.c_str());
+        return NAME_NOT_FOUND;
+    }
+
+    LOG1("Opening CPF file \"%s\"", cpfPathName.c_str());
+    FILE *file = fopen(cpfPathName.c_str(), "rb");
+    CheckError((file == nullptr), NAME_NOT_FOUND, "ERROR in opening CPF file \"%s\": %s!", cpfPathName.c_str(), strerror(errno));
+    do {
+        int fileSize;
+        if ((fseek(file, 0, SEEK_END) < 0) || ((fileSize = ftell(file)) < 0) || (fseek(file, 0, SEEK_SET) < 0)) {
+            LOGE("ERROR querying properties of CPF file \"%s\": %s!", cpfPathName.c_str(), strerror(errno));
+            ret = BAD_VALUE;
+            break;
+        }
+
+        mCpfData[aiqbName].data = malloc(fileSize);
+        if (!mCpfData[aiqbName].data) {
+            LOGE("ERROR no memory in %s!", __func__);
+            ret = NO_MEMORY;
+            break;
+        }
+
+        if (fread(mCpfData[aiqbName].data, fileSize, 1, file) < 1) {
+            LOGE("ERROR reading CPF file \"%s\"!", cpfPathName.c_str());
+            ret = INVALID_OPERATION;
+            break;
+        }
+        mCpfData[aiqbName].size = fileSize;
+    } while (0);
+
+    if (fclose(file)) {
+        LOGE("ERROR in closing CPF file \"%s\": %s!", cpfPathName.c_str(), strerror(errno));
+    }
+
+    return ret;
+}
+
+/**
+ * convenience getter for Isp data, Aiq data, cmc data and other data.
+ */
+int CpfStore::getCpfAndCmc(ia_binary_data* ispData,
+                           ia_binary_data* aiqData,
+                           ia_binary_data* otherData,
+                           uintptr_t* cmcHandle,
+                           TuningMode mode,
+                           ia_cmc_t** cmcData)
+{
+    LOG1("@%s mode = %d", __func__, mode);
+    CheckError((mCpfConfig[mode] == nullptr), NO_INIT, "@%s, No aiqb init, mode = %d", __func__, mode);
+    if (ispData != nullptr)
+        mCpfConfig[mode]->getIspData(ispData);
+    if (aiqData != nullptr)
+        mCpfConfig[mode]->getAiqData(aiqData);
+    if (otherData != nullptr)
+        mCpfConfig[mode]->getOtherData(otherData);
+    if (cmcData) {
+        *cmcData = mCpfConfig[mode]->getCmc();
+    }
+    if (cmcHandle) {
+        *cmcHandle = mCpfConfig[mode]->getCmcHandle();
+    }
+
+    if (mode == TUNING_MODE_VIDEO_ULL) {
+        LOG2("@%s ULL mode, ULL cpf file is used", __func__);
+    } else if (mode == TUNING_MODE_VIDEO_CUSTOM_AIC) {
+        LOG2("@%s CUSTOM AIC mode, CUSTOM AIC cpf file is used", __func__);
+    } else if (mode == TUNING_MODE_VIDEO_LL) {
+        LOG2("@%s VIDEO LL mode, VIDEO LL cpf file is used", __func__);
+    } else if (mode == TUNING_MODE_VIDEO_REAR_VIEW) {
+        LOG2("@%s VIDEO Rear View mode, VIDEO REAR VIEW cpf file is used", __func__);
+    } else if (mode == TUNING_MODE_VIDEO_HITCH_VIEW) {
+        LOG2("@%s VIDEO Hitch View mode, VIDEO HITCH VIEW cpf file is used", __func__);
+    } else {
+        LOG2("@%s VIDEO mode, default cpf file is used", __func__);
+    }
+
+    return OK;
+}
+
+AiqInitData::AiqInitData(const std::string& sensorName,
+                         const std::string& camCfgDir,
+                         const std::vector<TuningConfig>& tuningCfg,
+                         const std::vector<LardTagConfig>& lardTagCfg,
+                         const std::string& nvmDir,
+                         int maxNvmSize) :
+    mSensorName(sensorName),
+    mCamCfgDir(camCfgDir),
+    mNvmDir(nvmDir),
+    mMaxNvmSize(maxNvmSize),
+    mTuningCfg(tuningCfg),
+    mLardTagCfg(lardTagCfg),
+    mCpfStore(nullptr),
+    mNvmDataBuf(nullptr),
+    mMakerNote(nullptr)
+{
+    CLEAR(mNvmData);
+    mMakerNote = unique_ptr<MakerNote>(new MakerNote);
+}
+
+AiqInitData::~AiqInitData()
+{
+    delete mCpfStore;
+    for (auto aiqd : mAiqdDataMap) {
+        delete aiqd.second;
+    }
+}
+
+int AiqInitData::getCpfAndCmc(ia_binary_data* ispData,
+                              ia_binary_data* aiqData,
+                              ia_binary_data* otherData,
+                              uintptr_t* cmcHandle,
+                              TuningMode mode,
+                              ia_cmc_t** cmcData)
+{
+    if (!mCpfStore) {
+        mCpfStore = new CpfStore(mSensorName, mCamCfgDir, mTuningCfg, mLardTagCfg);
+    }
+    return mCpfStore->getCpfAndCmc(ispData, aiqData, otherData, cmcHandle, mode, cmcData);
+}
+
+status_t AiqInitData::loadNvm()
+{
+    LOG1("@%s", __func__);
+
+    if (mNvmDir.length() == 0) {
+        LOG1("NVM dirctory from config is null");
+        return UNKNOWN_ERROR;
+    }
+
+    string nvmDataPath(NVM_DATA_PATH);
+    if (nvmDataPath.back() != '/')
+        nvmDataPath.append("/");
+
+    nvmDataPath.append(mNvmDir);
+    if (nvmDataPath.back() != '/')
+        nvmDataPath.append("/");
+
+    nvmDataPath.append("eeprom");
+    LOG2("NVM data for %s is located in %s", mSensorName.c_str(), nvmDataPath.c_str());
+
+    FILE* nvmFile = fopen(nvmDataPath.c_str(), "rb");
+    CheckError(!nvmFile, UNKNOWN_ERROR, "Failed to open NVM file: %s", nvmDataPath.c_str());
+
+    fseek(nvmFile, 0, SEEK_END);
+    int nvmDataSize = std::min(static_cast<int>(ftell(nvmFile)), mMaxNvmSize);
+    fseek(nvmFile, 0, SEEK_SET);
+
+    std::unique_ptr<char[]> nvmData(new char[nvmDataSize]);
+    LOG2("NVM data size: %d bytes", nvmDataSize);
+
+    int ret = fread(nvmData.get(), nvmDataSize, 1, nvmFile);
+    fclose(nvmFile);
+    CheckError(ret == 0, UNKNOWN_ERROR, "Cannot read nvm data");
+
+    mNvmDataBuf = std::move(nvmData);
+    mNvmData.data = mNvmDataBuf.get();
+    mNvmData.size = nvmDataSize;
+
+    return OK;
+}
+
+ia_binary_data* AiqInitData::getNvm()
+{
+    if (!mNvmData.data || mNvmData.size == 0) {
+        loadNvm();
+    }
+
+    return mNvmData.data ? &mNvmData : nullptr;
+}
+
+ia_binary_data* AiqInitData::getAiqd(TuningMode mode) {
+    if (mAiqdDataMap.find(mode) == mAiqdDataMap.end()) {
+        mAiqdDataMap[mode] = new AiqdData(mode, mSensorName);
+    }
+    AiqdData* aiqd = mAiqdDataMap[mode];
+    CheckError(!aiqd, nullptr, "@%s, aiqd is nullptr", __func__);
+
+    return aiqd->getAiqd();
+}
+
+void AiqInitData::saveAiqd(TuningMode mode, const ia_binary_data& data) {
+    if (mAiqdDataMap.find(mode) == mAiqdDataMap.end()) {
+        mAiqdDataMap[mode] = new AiqdData(mode, mSensorName);
+    }
+
+    AiqdData* aiqd = mAiqdDataMap[mode];
+    CheckError(!aiqd, VOID_VALUE, "@%s, aiqd is nullptr", __func__);
+
+    aiqd->saveAiqd(data);
+}
+
+void* AiqInitData::getMknHandle(void)
+{
+    return mMakerNote->getMknHandle();
+}
+
+int AiqInitData::saveMakernoteData(camera_makernote_mode_t makernoteMode, int64_t sequence)
+{
+    return mMakerNote->saveMakernoteData(makernoteMode, sequence);
+}
+
+void AiqInitData::updateMakernoteTimeStamp(int64_t sequence, uint64_t timestamp)
+{
+    mMakerNote->updateTimestamp(sequence, timestamp);
+}
+
+void AiqInitData::acquireMakernoteData(uint64_t timestamp, Parameters *param)
+{
+    mMakerNote->acquireMakernoteData(timestamp, param);
+}
+
+}
diff --git a/camera/hal/intel/ipu6/src/platformdata/AiqInitData.h b/camera/hal/intel/ipu6/src/platformdata/AiqInitData.h
new file mode 100644
index 000000000000..f6d090a847a9
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/AiqInitData.h
@@ -0,0 +1,235 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <map>
+#include <memory>
+#include <string>
+#include <vector>
+
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+#include "CameraMetadata.h"
+
+#include "MakerNote.h"
+#ifdef ENABLE_SANDBOXING
+#include "modules/sandboxing/client/IntelLard.h"
+#include "modules/sandboxing/client/IntelCmc.h"
+#else
+#include "modules/algowrapper/IntelLard.h"
+#include "modules/algowrapper/IntelCmc.h"
+#endif
+
+namespace icamera {
+
+#define NVM_DATA_PATH "/sys/bus/i2c/devices/"
+
+/**
+ * This class is intended to save/load AIQD data.
+ */
+class AiqdData
+{
+public:
+    AiqdData(TuningMode tuningMode, const std::string& sensorName);
+    ~AiqdData();
+
+    ia_binary_data* getAiqd();
+    void saveAiqd(const ia_binary_data& data);
+
+private:
+    void loadAiqdFromFile();
+    void saveAiqdToFile();
+
+private:
+    std::string mAiqdFileName;
+    ia_binary_data mBinaryData;
+    std::unique_ptr<char[]> mDataPtr;
+};
+
+/**
+  * The IA data stored
+*/
+class CpfConf
+{
+public:
+    CpfConf();
+    virtual ~CpfConf();
+
+    /**
+     * \brief get CMC pointer
+     *
+     */
+    ia_cmc_t* getCmc() const;
+
+    /**
+     * \brief get CMC uintptr_t
+     *
+     */
+    uintptr_t getCmcHandle() const;
+
+    /**
+     * \brief get ISP data from CPF file
+     *
+     * \param[out] ia_binary_data* IspData: ISP data
+     */
+    void getIspData(ia_binary_data* IspData);
+
+    /**
+     * \brief get AIQ data from CPF file
+     *
+     * \param[out] ia_binary_data* AiqData: AIQ data
+     */
+    void getAiqData(ia_binary_data* AiqData);
+
+    /**
+     * \brief get others data from CPF file, including LTM data
+     *
+     * \param[out] ia_binary_data* otherData: others data
+     */
+    void getOtherData(ia_binary_data* otherData);
+
+    /**
+     * \brief parse CMC/ISP/AIQ/Others from the CPF data
+     *
+     * Parse the CMC/ISP/AIQ/Others data according to the tuning mode, and init
+     * the CMC handler.
+     *
+     * \param[in] ia_binary_data: CPF data loaded from the AIQB file
+     * \param[in] LardTagConfig: lard tag cfg
+     *
+     * \return OK if init successfully; otherwise non-0 value is returned.
+     */
+    int init(const ia_binary_data& cpfData, const LardTagConfig* lardTagCfg);
+
+    /**
+      * \brief deinit CMC handler.
+    */
+    void deinit();
+
+private:
+    DISALLOW_COPY_AND_ASSIGN(CpfConf);
+
+    void initLardInputParam(const ia_lard& iaLard,
+                            const LardTagConfig* lardTagCfg,
+                            ia_lard_input_params* lardInputParam);
+    bool isTagValid(unsigned int tag, unsigned int count, const unsigned int* tags);
+
+private:
+    IntelLard* mLard;
+    std::unique_ptr<IntelCmc> mCmc;
+    ia_binary_data mAiq;
+    ia_binary_data mIsp;
+    ia_binary_data mOthers;
+};//end CpfConf
+
+/**
+  * CPF file operation class
+*/
+class CpfStore
+{
+public:
+    CpfStore(const std::string& sensorName,
+             const std::string& camCfgDir,
+             const std::vector<TuningConfig>& tuningCfg,
+             const std::vector<LardTagConfig>& lardTagCfg);
+    virtual ~CpfStore();
+
+    /**
+     * get Isp and Aiq data info
+     *
+     * \param ispData: return isp data of struct ia_binary_data
+     * \param aiqData: return aiq data of struct ia_binary_data
+     * \param otherData: return other data of struct ia_binary_data, such as tuning data for LTM
+     * \param cmcHandle: return cmc uintptr_t
+     * \param mode: Camera Mode
+     * \param cmcData: return cmc pointer
+     * \return NO_INIT if data not found, return OK if success.
+     */
+    int getCpfAndCmc(ia_binary_data* ispData,
+                     ia_binary_data* aiqData,
+                     ia_binary_data* otherData,
+                     uintptr_t* cmcHandle,
+                     TuningMode mode = TUNING_MODE_VIDEO,
+                     ia_cmc_t** cmcData = nullptr);
+private:
+    DISALLOW_COPY_AND_ASSIGN(CpfStore);
+
+    int findConfigFile(const std::string& camCfgDir, std::string* cpfPathName);
+    int loadConf(const std::string& camCfgDir, const std::string& aiqbName);
+
+public:
+    CpfConf* mCpfConfig[TUNING_MODE_MAX];
+    std::map<std::string, ia_binary_data> mCpfData;
+
+};//end CpfStore
+
+/**
+ * This class ia a wrapper class which includes CPF data, AIQD data and NVM data.
+ */
+class AiqInitData {
+ public:
+    AiqInitData(const std::string& sensorName,
+                const std::string& camCfgDir,
+                const std::vector<TuningConfig>& tuningCfg,
+                const std::vector<LardTagConfig>& lardTagCfg,
+                const std::string& nvmDir,
+                int maxNvmSize);
+    ~AiqInitData();
+
+    // cpf and cmc
+    int getCpfAndCmc(ia_binary_data* ispData,
+                     ia_binary_data* aiqData,
+                     ia_binary_data* otherData,
+                     uintptr_t* cmcHandle,
+                     TuningMode mode = TUNING_MODE_VIDEO,
+                     ia_cmc_t** cmcData = nullptr);
+
+    // aiqd
+    ia_binary_data* getAiqd(TuningMode mode);
+    void saveAiqd(TuningMode mode, const ia_binary_data& data);
+
+    // nvm
+    ia_binary_data* getNvm();
+
+    // maker note
+    void* getMknHandle(void);
+    int saveMakernoteData(camera_makernote_mode_t makernoteMode, int64_t sequence);
+    void updateMakernoteTimeStamp(int64_t sequence, uint64_t timestamp);
+    void acquireMakernoteData(uint64_t timestamp, Parameters *param);
+
+ private:
+    status_t loadNvm();
+
+ private:
+    std::string mSensorName;
+    std::string mCamCfgDir;
+    std::string mNvmDir;
+    int mMaxNvmSize;
+    std::vector<TuningConfig> mTuningCfg;
+    std::vector<LardTagConfig> mLardTagCfg;
+    CpfStore* mCpfStore;
+
+    // NVM data
+    std::unique_ptr <char[]> mNvmDataBuf;
+    ia_binary_data mNvmData;
+
+    std::unique_ptr<MakerNote> mMakerNote;
+
+    std::map<TuningMode, AiqdData*> mAiqdDataMap;
+};
+
+}
diff --git a/camera/hal/intel/ipu6/src/platformdata/CameraParser.cpp b/camera/hal/intel/ipu6/src/platformdata/CameraParser.cpp
new file mode 100644
index 000000000000..303bb8d44301
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/CameraParser.cpp
@@ -0,0 +1,1903 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation
+ * Copyright 2008-2017, The Android Open Source Project
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#define LOG_TAG "CameraParser"
+
+#include <string.h>
+#include <expat.h>
+#include <memory>
+
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+#include "metadata/ParameterHelper.h"
+
+#include "PlatformData.h"
+#include "CameraParser.h"
+
+using std::string;
+using std::vector;
+
+#include "v4l2/NodeInfo.h"
+
+namespace icamera {
+#define  LIBCAMHAL_PROFILE_NAME "libcamhal_profile.xml"
+CameraParser::CameraParser(MediaControl *mc, PlatformData::StaticCfg *cfg) :
+    mStaticCfg(cfg),
+    mCurrentDataField(FIELD_INVALID),
+    mSensorNum(0),
+    mCurrentSensor(0),
+    pCurrentCam(nullptr),
+    mInMediaCtlCfg(false),
+    mInStaticMetadata(false),
+    mMC(mc),
+    mMetadataCache(nullptr) {
+    LOGXML("@%s", __func__);
+    CheckError(mc == nullptr || cfg == nullptr, VOID_VALUE,
+               "@%s, passed parameters are wrong, mc:%p, data:%p", __func__, mc, cfg);
+
+    mMetadataCache = new long[mMetadataCacheSize];
+    getProfileDataFromXmlFile();
+
+    if(gLogLevel & CAMERA_DEBUG_LOG_LEVEL2)
+        dumpSensorInfo();
+}
+
+CameraParser::~CameraParser()
+{
+    delete []mMetadataCache;
+}
+
+/**
+ * Replacing $I2CBUS with the real mI2CBus if the value contains the string "$I2CBUS"
+ * one example: "imx319 $I2CBUS"
+ * Replacing $CSI_PORT with the real mCsiPort if the value contains the string "$CSI_PORT"
+ * one example: "Intel IPU6 CSI-2 $CSI_PORT"
+ *
+ * \param profiles: the pointer of the CameraParser.
+ * \param value: camera information.
+ * \return: if the value contains the string, it will be replaced.
+ */
+string CameraParser::replaceStringInXml(CameraParser *profiles, const char *value)
+{
+    string valueTmp;
+    CheckError(value == nullptr, valueTmp, "value is nullptr");
+
+    valueTmp = value;
+    string::size_type found = string::npos;
+    if ((found = valueTmp.find("$I2CBUS")) != string::npos) {
+        valueTmp.replace(found, sizeof("$I2CBUS"), profiles->mI2CBus);
+        LOGXML("@%s, sensor full name is %s", __func__, valueTmp.c_str());
+    } else if ((found = valueTmp.find("$CSI_PORT")) != string::npos) {
+        valueTmp.replace(found, sizeof("$CSI_PORT"), profiles->mCsiPort);
+        LOGXML("@%s, csi entity full name is %s", __func__, valueTmp.c_str());
+    }
+
+    return valueTmp;
+}
+
+/**
+ * This function will check which field that the parser parses to.
+ *
+ * The field is set to 3 types.
+ * FIELD_INVALID FIELD_SENSOR and FIELD_COMMON
+ *
+ * \param profiles: the pointer of the CameraParser.
+ * \param name: the element's name.
+ * \param atts: the element's attribute.
+ */
+void CameraParser::checkField(CameraParser *profiles, const char *name, const char **atts)
+{
+    LOGXML("@%s, name:%s", __func__, name);
+    if (strcmp(name, "CameraSettings") == 0) {
+        profiles->mCurrentDataField = FIELD_INVALID;
+        return;
+    } else if (strcmp(name, "Sensor") == 0) {
+        profiles->mSensorNum++;
+        profiles->mCurrentSensor = profiles->mSensorNum - 1;
+        if (profiles->mCurrentSensor >= 0 && profiles->mCurrentSensor < MAX_CAMERA_NUMBER) {
+            profiles->pCurrentCam = new PlatformData::StaticCfg::CameraInfo;
+
+            int idx = 0;
+            string sensorEntityName;
+            string sinkEntityName;
+            while (atts[idx]) {
+                const char* key = atts[idx];
+                const char* val = atts[idx + 1];
+                LOGXML("@%s, name:%s, atts[%d]:%s, atts[%d]:%s", __func__, name, idx, key, idx+1, val);
+                if (strcmp(key, "name") == 0) {
+                    profiles->pCurrentCam->sensorName = val;
+                } else if (strcmp(key, "description") == 0) {
+                    profiles->pCurrentCam->sensorDescription = val;
+                }
+                idx += 2;
+            }
+
+            if (!profiles->pCurrentCam->sensorName.empty() &&
+                (profiles->mAvailableSensor.find(profiles->pCurrentCam->sensorName) !=
+                 profiles->mAvailableSensor.end())) {
+                /* parameters information format example:
+                   sinkEntityName is "Intel IPU6 CSI-2 1"
+                   profiles->pCurrentCam->sensorName is "ov8856-wf"
+                   sensorName is "ov8856"
+                */
+                string sinkEntityName = profiles->mAvailableSensor[profiles->pCurrentCam->sensorName];
+                profiles->mCsiPort = sinkEntityName.substr(sinkEntityName.find_last_of(' ') + 1);
+                string sensorName = profiles->pCurrentCam->sensorName;
+                sensorName = sensorName.substr(0, (sensorName.find_last_of('-')));
+                profiles->mMC->getI2CBusAddress(sensorName, sinkEntityName, &profiles->mI2CBus);
+
+                LOGXML("@%s, mI2CBus:%s, cisPort:%s", __func__,
+                       profiles->mI2CBus.c_str(), profiles->mCsiPort.c_str());
+             }
+
+            profiles->mMetadata.clear();
+            profiles->mCurrentDataField = FIELD_SENSOR;
+
+            return;
+        }
+    } else if (strcmp(name, "Common") == 0) {
+        profiles->mCurrentDataField = FIELD_COMMON;
+        return;
+    }
+
+    LOGE("@%s, name:%s, atts[0]:%s, xml format wrong", __func__, name, atts[0]);
+    return;
+}
+
+/**
+ * This function will handle all the common related elements.
+ *
+ * It will be called in the function startElement
+ *
+ * \param profiles: the pointer of the CameraParser.
+ * \param name: the element's name.
+ * \param atts: the element's attribute.
+ */
+void CameraParser::handleCommon(CameraParser *profiles, const char *name, const char **atts)
+{
+    CheckError(strcmp(atts[0], "value") != 0 || (atts[1] == nullptr), VOID_VALUE
+         ,"@%s, name:%s, atts[0]:%s or atts[1] is nullptr, xml format wrong", __func__, name, atts[0]);
+
+    LOGXML("@%s, name:%s, atts[0]:%s, atts[1]: %s", __func__, name, atts[0], atts[1]);
+    CommonConfig *cfg = &profiles->mStaticCfg->mCommonConfig;
+    if (strcmp(name, "version") == 0) {
+        cfg->xmlVersion = atof(atts[1]);
+    } else if (strcmp(name, "platform") == 0) {
+        cfg->ipuName = atts[1];
+    } else if (strcmp(name, "availableSensors") == 0) {
+        parseXmlConvertStrings(atts[1], cfg->availableSensors, convertCharToString);
+    } else if (strcmp(name, "useGpuTnr") == 0) {
+       cfg->isGpuTnrEnabled = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "cameraNumber") == 0) {
+       cfg->cameraNumber = atoi(atts[1]);
+    }
+}
+
+/**
+ * This function will handle all the sensor related elements.
+ *
+ * It will be called in the function startElement
+ *
+ * \param profiles: the pointer of the CameraParser.
+ * \param name: the element's name.
+ * \param atts: the element's attribute.
+ */
+void CameraParser::handleSensor(CameraParser *profiles, const char *name, const char **atts)
+{
+    LOGXML("@%s, name:%s, profiles->mCurrentSensor:%d", __func__, name, profiles->mCurrentSensor);
+    CheckError(strcmp(atts[0], "value") != 0 || (atts[1] == nullptr), VOID_VALUE
+        ,"@%s, name:%s, atts[0]:%s or atts[1] is nullptr, xml format wrong", __func__, name, atts[0]);
+
+    LOGXML("@%s, name:%s, atts[0]:%s, atts[1]:%s", __func__, name, atts[0], atts[1]);
+    if (strcmp(name, "supportedISysSizes") == 0) {
+        parseSizesList(atts[1], pCurrentCam->mSupportedISysSizes);
+        for (const auto &s : pCurrentCam->mSupportedISysSizes)
+            LOGXML("@%s, mSupportedISysSizes: width:%d, height:%d", __func__,
+                s.width, s.height);
+    } else if (strcmp(name, "supportedISysFormat") == 0) {
+        getSupportedFormat(atts[1], pCurrentCam->mSupportedISysFormat);
+    } else if (strcmp(name, "iSysRawFormat") == 0) {
+        pCurrentCam->mISysRawFormat = CameraUtils::string2PixelCode(atts[1]);
+    } else if (strcmp(name, "configModeToStreamId") == 0) {
+        char* srcDup = strdup(atts[1]);
+        CheckError(!srcDup, VOID_VALUE, "Create a copy of source string failed.");
+
+        char* endPtr = (char*)strchr(srcDup, ',');
+        if (endPtr) {
+            *endPtr = 0;
+            ConfigMode configMode = CameraUtils::getConfigModeByName(srcDup);
+            int streamId = atoi(endPtr + 1);
+            pCurrentCam->mConfigModeToStreamId[configMode] = streamId;
+        }
+        free(srcDup);
+    } else if (strcmp(name, "pSysFormat") == 0) {
+        getSupportedFormat(atts[1], pCurrentCam->mPSysFormat);
+    } else if (strcmp(name, "enableAIQ") == 0) {
+        pCurrentCam->mEnableAIQ = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "useCrlModule") == 0) {
+        pCurrentCam->mUseCrlModule = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "skipFrameV4L2Error") == 0) {
+        pCurrentCam->mSkipFrameV4L2Error = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "useSensorDigitalGain") == 0) {
+        pCurrentCam->mUseSensorDigitalGain = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "useIspDigitalGain") == 0) {
+        pCurrentCam->mUseIspDigitalGain = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "preRegisterBuffer") == 0) {
+        pCurrentCam->mNeedPreRegisterBuffers = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "enableFrameSyncCheck") == 0) {
+        pCurrentCam->mFrameSyncCheckEnabled = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "lensName") == 0) {
+        profiles->mMC->getVCMI2CAddr(atts[1], &pCurrentCam->mLensName);
+    } else if (strcmp(name, "lensHwType") == 0) {
+        if (strcmp(atts[1], "LENS_VCM_HW") == 0) {
+            pCurrentCam->mLensHwType = LENS_VCM_HW;
+        } else {
+            LOGE("unknown Lens HW type %s, set to LENS_NONE_HW", atts[1]);
+            pCurrentCam->mLensHwType = LENS_NONE_HW;
+        }
+    } else if (strcmp(name, "autoSwitchType") == 0) {
+        if (strcmp(atts[1], "full") == 0) {
+            pCurrentCam->mAutoSwitchType = AUTO_SWITCH_FULL;
+        } else {
+            pCurrentCam->mAutoSwitchType = AUTO_SWITCH_PSYS;
+        }
+    } else if (strcmp(name, "lensCloseCode") == 0) {
+        pCurrentCam->mLensCloseCode = atoi(atts[1]);
+    } else if (strcmp(name, "cITMaxMargin") == 0) {
+        pCurrentCam->mCITMaxMargin = atoi(atts[1]);
+    } else if (strcmp(name, "ltmGainLag") == 0) {
+        pCurrentCam->mLtmGainLag = atoi(atts[1]);
+    } else if (strcmp(name, "enableLtmThread") == 0) {
+        pCurrentCam->mEnableLtmThread = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "enableLtmDefog") == 0) {
+        pCurrentCam->mEnableLtmDefog = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "enableLtm") == 0) {
+        pCurrentCam->mLtmEnabled = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "maxSensorDg") == 0) {
+        pCurrentCam->mMaxSensorDigitalGain = atoi(atts[1]);
+    } else if (strcmp(name, "sensorDgType") == 0) {
+        if (strcmp(atts[1], "type_2_x") == 0) {
+            pCurrentCam->mSensorDgType = SENSOR_DG_TYPE_2_X;
+        } else if (strcmp(atts[1], "type_x") == 0) {
+            pCurrentCam->mSensorDgType = SENSOR_DG_TYPE_X;
+        } else {
+            LOGE("unknown sensor digital gain type:%s, set to SENSOR_DG_TYPE_NONE", atts[1]);
+            pCurrentCam->mSensorDgType = SENSOR_DG_TYPE_NONE;
+        }
+    } else if (strcmp(name, "exposureLag") == 0) {
+        pCurrentCam->mExposureLag = atoi(atts[1]);
+    } else if (strcmp(name, "graphSettingsFile") == 0) {
+        pCurrentCam->mGraphSettingsFile = atts[1];
+    } else if (strcmp(name, "graphSettingsType") == 0) {
+        if (strcmp(atts[1], "coupled") == 0) {
+            pCurrentCam->mGraphSettingsType = COUPLED;
+        } else if (strcmp(atts[1], "dispersed") == 0) {
+            pCurrentCam->mGraphSettingsType = DISPERSED;
+        } else {
+            LOGW("unknown graph settings type %s, set to COUPLED", atts[1]);
+            pCurrentCam->mGraphSettingsType = COUPLED;
+        }
+    } else if (strcmp(name, "gainLag") == 0) {
+        pCurrentCam->mGainLag = atoi(atts[1]);
+    } else if (strcmp(name, "customAicLibraryName") == 0) {
+        pCurrentCam->mCustomAicLibraryName = atts[1];
+    } else if (strcmp(name, "custom3ALibraryName") == 0){
+        pCurrentCam->mCustom3ALibraryName = atts[1];
+    } else if (strcmp(name, "yuvColorRangeMode") == 0) {
+        if (strcmp(atts[1],"full") == 0) {
+            pCurrentCam->mYuvColorRangeMode = CAMERA_FULL_MODE_YUV_COLOR_RANGE;
+        } else if (strcmp(atts[1],"reduced") == 0) {
+            pCurrentCam->mYuvColorRangeMode = CAMERA_REDUCED_MODE_YUV_COLOR_RANGE;
+        }
+    } else if (strcmp(name, "initialSkipFrame") == 0) {
+        pCurrentCam->mInitialSkipFrame = atoi(atts[1]);
+    } else if (strcmp(name, "maxRawDataNum") == 0) {
+        pCurrentCam->mMaxRawDataNum = atoi(atts[1]);
+    }  else if (strcmp(name, "topBottomReverse") == 0) {
+        pCurrentCam->mTopBottomReverse = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "maxRequestsInflight") == 0) {
+        pCurrentCam->mMaxRequestsInflight = atoi(atts[1]);
+    } else if (strcmp(name, "psysContinueStats") == 0) {
+        pCurrentCam->mPsysContinueStats = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "preferredBufQSize") == 0) {
+        pCurrentCam->mPreferredBufQSize = atoi(atts[1]);
+    } else if (strcmp(name, "pipeSwitchDelayFrame") == 0) {
+        pCurrentCam->mPipeSwitchDelayFrame = atoi(atts[1]);
+    } else if (strcmp(name, "supportedTuningConfig") == 0) {
+        parseSupportedTuningConfig(atts[1], pCurrentCam->mSupportedTuningConfig);
+    } else if (strcmp(name, "enableAiqd") == 0) {
+        pCurrentCam->mEnableAiqd = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "testPatternMap") == 0) {
+        int size = strlen(atts[1]);
+        char src[size + 1];
+        MEMCPY_S(src, size, atts[1], size);
+        src[size] = '\0';
+        int32_t mode = TEST_PATTERN_OFF;
+        char* savePtr = nullptr;
+
+        char* tablePtr = strtok_r(src, ",", &savePtr);
+        while (tablePtr) {
+            if (strcmp(tablePtr, "Off") == 0) {
+                mode = TEST_PATTERN_OFF;
+            } else if (strcmp(tablePtr, "ColorBars") == 0) {
+                mode = COLOR_BARS;
+            } else if (strcmp(tablePtr, "SolidColor") == 0) {
+                mode = SOLID_COLOR;
+            } else if (strcmp(tablePtr, "ColorBarsFadeToGray") == 0) {
+                mode = COLOR_BARS_FADE_TO_GRAY;
+            } else if (strcmp(tablePtr, "PN9") == 0) {
+                mode = PN9;
+            } else if (strcmp(tablePtr, "CUSTOM1") == 0) {
+                mode = TEST_PATTERN_CUSTOM1;
+            } else {
+                LOGE("Test pattern string %s is unknown, please check", tablePtr);
+                return;
+            }
+
+            tablePtr = strtok_r(nullptr, ",", &savePtr);
+            CheckError(tablePtr == nullptr, VOID_VALUE, "Driver test pattern is nullptr");
+
+            pCurrentCam->mTestPatternMap[mode] = atoi(tablePtr);
+
+            tablePtr = strtok_r(nullptr, ",", &savePtr);
+        }
+    } else if (strcmp(name, "lardTags") == 0) {
+        parseLardTags(atts[1], pCurrentCam->mLardTagsConfig);
+    } else if (strcmp(name, "availableConfigModeForAuto") == 0) {
+        parseXmlConvertStrings(atts[1], pCurrentCam->mConfigModesForAuto, CameraUtils::getConfigModeByName);
+    } else if (strcmp(name, "supportedAeMultiExpRange") == 0) {
+        parseMultiExpRange(atts[1]);
+    } else if (strcmp(name, "dvsType") == 0) {
+        if (strcmp(atts[1], "MORPH_TABLE") == 0) {
+            pCurrentCam->mDVSType = MORPH_TABLE;
+        } else if (strcmp(atts[1], "IMG_TRANS") == 0) {
+            pCurrentCam->mDVSType = IMG_TRANS;
+        }
+    } else if (strcmp(name, "pslOutputMapForRotation") == 0) {
+        parseOutputMap(atts[1], pCurrentCam->mOutputMap);
+    } else if (strcmp(name, "maxNvmDataSize") == 0) {
+        pCurrentCam->mMaxNvmDataSize = atoi(atts[1]);
+    } else if (strcmp(name, "nvmDirectory") == 0) {
+        pCurrentCam->mNvmDirectory = atts[1];
+    } else if (strcmp(name, "isISYSCompression") == 0) {
+        pCurrentCam->mISYSCompression = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "isPSACompression") == 0) {
+        pCurrentCam->mPSACompression = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "isOFSCompression") == 0) {
+        pCurrentCam->mOFSCompression = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "faceAeEnabled") == 0) {
+        pCurrentCam->mFaceAeEnabled = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "psysAlignWithSof") == 0) {
+        pCurrentCam->mPsysAlignWithSof = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "psysBundleWithAic") == 0) {
+        pCurrentCam->mPsysBundleWithAic = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "swProcessingAlignWithIsp") == 0) {
+        pCurrentCam->mSwProcessingAlignWithIsp = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "faceEngineRunningInterval") == 0) {
+        int val = atoi(atts[1]);
+        pCurrentCam->mFaceEngineRunningInterval =
+            val > 0 ? val : FACE_ENGINE_DEFAULT_RUNNING_INTERVAL;
+    } else if (strcmp(name, "faceEngineRunningIntervalNoFace") == 0) {
+        int val = atoi(atts[1]);
+        pCurrentCam->mFaceEngineRunningIntervalNoFace =
+            val > 0 ? val : FACE_ENGINE_DEFAULT_RUNNING_INTERVAL;
+    } else if (strcmp(name, "faceEngineRunningSync") == 0) {
+        pCurrentCam->mFaceEngineRunningSync = strcmp(atts[1], "true") == 0;
+    } else if (strcmp(name, "videoStreamNum") == 0) {
+        int val = atoi(atts[1]);
+        pCurrentCam->mVideoStreamNum = val > 0 ? val : DEFAULT_VIDEO_STREAM_NUM;
+    }
+}
+
+int CameraParser::parseSupportedTuningConfig(const char *str, vector <TuningConfig> &config)
+{
+    CheckError(str == nullptr, -1, "@%s, str is nullptr", __func__);
+    LOGXML("@%s, str = %s", __func__, str);
+
+    int sz = strlen(str);
+    char src[sz + 1];
+    MEMCPY_S(src, sz, str, sz);
+    src[sz] = '\0';
+    char *savePtr;
+    char *configMode = strtok_r(src, ",", &savePtr);
+    TuningConfig cfg;
+    while (configMode) {
+        char* tuningMode = strtok_r(nullptr, ",", &savePtr);
+        char* aiqb = strtok_r(nullptr, ",", &savePtr);
+        CheckError(configMode == nullptr || tuningMode == nullptr
+              || aiqb == nullptr, -1, "@%s, wrong str %s", __func__, str);
+
+        LOGXML("@%s, configMode %s, tuningMode %s, aiqb name %s",
+                __func__, configMode, tuningMode, aiqb);
+        cfg.configMode = CameraUtils::getConfigModeByName(configMode);
+        cfg.tuningMode = CameraUtils::string2TuningMode(tuningMode);
+        cfg.aiqbName = aiqb;
+        config.push_back(cfg);
+        if (savePtr != nullptr)
+            savePtr = const_cast<char*>(skipWhiteSpace(savePtr));
+        configMode = strtok_r(nullptr, ",", &savePtr);
+    }
+    return 0;
+}
+
+int CameraParser::parseLardTags(const char *str, vector <LardTagConfig> &lardTags)
+{
+    CheckError(str == nullptr, -1, "@%s, str is nullptr", __func__);
+    LOGXML("@%s, str = %s", __func__, str);
+
+    int sz = strlen(str);
+    char src[sz + 1];
+    MEMCPY_S(src, sz, str, sz);
+    src[sz] = '\0';
+
+    char *savePtr;
+    char *tuningMode = strtok_r(src, ",", &savePtr);
+    LardTagConfig cfg;
+    while (tuningMode) {
+        char* cmcTag = strtok_r(nullptr, ",", &savePtr);
+        char* aiqTag = strtok_r(nullptr, ",", &savePtr);
+        char* ispTag = strtok_r(nullptr, ",", &savePtr);
+        char* othersTag = strtok_r(nullptr, ",", &savePtr);
+
+        cfg.tuningMode = CameraUtils::string2TuningMode(tuningMode);
+        cfg.cmcTag = CameraUtils::fourcc2UL(cmcTag);
+        cfg.aiqTag = CameraUtils::fourcc2UL(aiqTag);
+        cfg.ispTag = CameraUtils::fourcc2UL(ispTag);
+        cfg.othersTag = CameraUtils::fourcc2UL(othersTag);
+        CheckError(cfg.cmcTag == 0 || cfg.aiqTag == 0 || cfg.ispTag == 0
+              || cfg.othersTag == 0, -1, "@%s, wrong str %s", __func__, str);
+
+        lardTags.push_back(cfg);
+        LOGXML("@%s, tuningMode %s, cmc %s, aiq %s, isp %s, others %s",
+                __func__, tuningMode, cmcTag, aiqTag, ispTag, othersTag);
+
+        if (savePtr != nullptr)
+            savePtr = const_cast<char*>(skipWhiteSpace(savePtr));
+        tuningMode = strtok_r(nullptr, ",", &savePtr);
+    }
+
+    return 0;
+}
+
+void CameraParser::parseMediaCtlConfigElement(CameraParser *profiles, const char *name, const char **atts)
+{
+    MediaCtlConf mc;
+    int idx = 0;
+
+    while (atts[idx]) {
+        const char *key = atts[idx];
+        LOGXML("%s: name: %s, value: %s", __func__, atts[idx], atts[idx + 1]);
+        if (strcmp(key, "id") == 0) {
+            mc.mcId = strtol(atts[idx + 1], nullptr, 10);
+        } else if (strcmp(key, "ConfigMode") == 0) {
+            parseXmlConvertStrings(atts[idx + 1], mc.configMode, CameraUtils::getConfigModeByName);
+        } else if (strcmp(key, "outputWidth") == 0) {
+            mc.outputWidth = strtoul(atts[idx + 1], nullptr, 10);
+        } else if (strcmp(key, "outputHeight") == 0) {
+            mc.outputHeight = strtoul(atts[idx + 1], nullptr, 10);
+        } else if (strcmp(key, "format") == 0) {
+            mc.format = CameraUtils::string2PixelCode(atts[idx + 1]);
+        }
+        idx += 2;
+    }
+
+    LOGXML("@%s, name:%s, atts[0]:%s, id: %d", __func__, name, atts[0], mc.mcId);
+    //Add a new empty MediaControl Configuration
+    profiles->pCurrentCam->mMediaCtlConfs.push_back(mc);
+}
+
+#define V4L2_CID_WATERMARK  0x00982901
+#define V4L2_CID_WATERMARK2 0x00982902
+void CameraParser::parseControlElement(CameraParser *profiles, const char *name, const char **atts)
+{
+    McCtl ctl;
+    MediaCtlConf &mc = profiles->pCurrentCam->mMediaCtlConfs.back();
+    LOGXML("@%s, name:%s", __func__, name);
+
+    int idx = 0;
+    while (atts[idx]) {
+        const char* key = atts[idx];
+        const char* val = atts[idx + 1];
+        LOGXML("@%s, name:%s, atts[%d]:%s, atts[%d]:%s", __func__, name, idx, key, idx + 1, val);
+        if (strcmp(key, "name") == 0) {
+            ctl.entityName = replaceStringInXml(profiles, val);
+            ctl.entity = profiles->mMC->getEntityIdByName(ctl.entityName.c_str());
+        } else if (strcmp(key, "ctrlId") == 0) {
+            if (!strcmp(val, "V4L2_CID_LINK_FREQ")) {
+                ctl.ctlCmd = V4L2_CID_LINK_FREQ;
+            } else if (!strcmp(val, "V4L2_CID_VBLANK")) {
+                ctl.ctlCmd = V4L2_CID_VBLANK;
+            } else if (!strcmp(val, "V4L2_CID_HBLANK")) {
+                ctl.ctlCmd = V4L2_CID_HBLANK;
+            } else if (!strcmp(val, "V4L2_CID_EXPOSURE")) {
+                ctl.ctlCmd = V4L2_CID_EXPOSURE;
+            } else if (!strcmp(val, "V4L2_CID_ANALOGUE_GAIN")) {
+                ctl.ctlCmd = V4L2_CID_ANALOGUE_GAIN;
+            } else if (!strcmp(val, "V4L2_CID_HFLIP")) {
+                ctl.ctlCmd = V4L2_CID_HFLIP;
+            } else if (!strcmp(val, "V4L2_CID_VFLIP")) {
+                ctl.ctlCmd = V4L2_CID_VFLIP;
+            } else if (!strcmp(val, "V4L2_CID_WATERMARK")) {
+                ctl.ctlCmd = V4L2_CID_WATERMARK;
+            } else if (!strcmp(val, "V4L2_CID_WATERMARK2")) {
+                ctl.ctlCmd = V4L2_CID_WATERMARK2;
+            } else if (!strcmp(val, "V4L2_CID_TEST_PATTERN")) {
+                ctl.ctlCmd = V4L2_CID_TEST_PATTERN;
+            } else {
+                LOGE("Unknow ioctl command %s", val);
+                ctl.ctlCmd = -1;
+            }
+        } else if (strcmp(key, "value") == 0) {
+            ctl.ctlValue = strtoul(val, nullptr, 10);
+        } else if (strcmp(key, "ctrlName") == 0) {
+            ctl.ctlName = val;
+        }
+        idx += 2;
+    }
+
+    mc.ctls.push_back(ctl);
+}
+
+void CameraParser::parseSelectionElement(CameraParser *profiles, const char *name, const char **atts)
+{
+    McFormat sel;
+    MediaCtlConf &mc = profiles->pCurrentCam->mMediaCtlConfs.back();
+    LOGXML("@%s, name:%s", __func__, name);
+
+    sel.top = -1; //top is not specified, need to be calc later.
+    sel.left = -1; //left is not specified, need to be calc later.
+    sel.width = 0; //width is not specified, need to be calc later.
+    sel.height = 0; //height is not specified, need to be calc later.
+    sel.formatType = FC_SELECTION;
+
+    int idx = 0;
+    while (atts[idx]) {
+        const char* key = atts[idx];
+        const char* val = atts[idx + 1];
+        LOGXML("@%s, name:%s, atts[%d]:%s, atts[%d]:%s", __func__, name, idx, key, idx+1, val);
+        if (strcmp(key, "name") == 0) {
+            sel.entityName = replaceStringInXml(profiles, val);
+            sel.entity = profiles->mMC->getEntityIdByName(sel.entityName.c_str());
+        } else if (strcmp(key, "pad") == 0) {
+            sel.pad = strtoul(val, nullptr, 10);
+        } else if (strcmp(key, "target") == 0) {
+            if (!strcmp(val, "V4L2_SEL_TGT_COMPOSE")) {
+                sel.selCmd = V4L2_SEL_TGT_COMPOSE;
+            } else if (!strcmp(val, "V4L2_SEL_TGT_CROP")) {
+                sel.selCmd = V4L2_SEL_TGT_CROP;
+            }
+        } else if (strcmp(key, "top") == 0) {
+            sel.top = strtoul(val, nullptr, 10);
+        } else if (strcmp(key, "left") == 0) {
+            sel.left = strtoul(val, nullptr, 10);
+        } else if (strcmp(key, "width") == 0) {
+            sel.width = strtoul(val, nullptr, 10);
+        } else if (strcmp(key, "height") == 0) {
+            sel.height = strtoul(val, nullptr, 10);
+        }
+        idx += 2;
+    }
+
+    mc.formats.push_back(sel);
+}
+
+/**
+ * Store the MediaCtlConf mapping table for supportedStreamConfig by id.
+ * Then we can select the MediaCtlConf through this table and configured stream.
+ */
+void CameraParser::storeMcMappForConfig(int mcId, stream_t streamCfg)
+{
+    //We need to insert new one if mcId isn't in mStreamToMcMap.
+    if (pCurrentCam->mStreamToMcMap.find(mcId) == pCurrentCam->mStreamToMcMap.end()) {
+        pCurrentCam->mStreamToMcMap.insert(std::pair<int, stream_array_t>(mcId, stream_array_t()));
+    }
+
+    stream_array_t &streamVector = pCurrentCam->mStreamToMcMap[mcId];
+    streamVector.push_back(streamCfg);
+}
+
+/**
+ * \brief Parses the string with the supported stream configurations
+ * a stream configuration is made of 4 necessary elements
+ * - Format
+ * - Resolution
+ * - Field (Interlaced field)
+ * - Media config ID
+ * we parse the string in 4 steps
+ * example of valid stream configuration is: V4L2_PIX_FMT_NV12,1920x1080,0,0
+
+ * the following elements are optional:
+ * - Max fps, for continuous streaming and high quality capture. (optional)
+ * example: V4L2_PIX_FMT_NV12,1920x1080,0,0,(30/15)
+ *
+ * \param src: string to be parsed
+ * \param configs: Stream config array needs to be filled in
+ *
+ */
+void CameraParser::parseStreamConfig(const char* src, stream_array_t& configs)
+{
+    HAL_TRACE_CALL(1);
+
+    int mcId = -1;
+    char* endPtr = nullptr;
+    char* separatorPtr = nullptr;
+    int parseStep = 0;
+    stream_t config;
+    CLEAR(config);
+
+#define NUM_ELEMENTS_NECESSARY 4
+// Has optional element
+#define NUM_ELEMENTS (NUM_ELEMENTS_NECESSARY + 1)
+
+    bool lastElement = false; // the last one?
+    do {
+        parseStep++;
+
+        // Get the next segement for necessary element
+        // Get the next segement for optional element if it exist
+        if (parseStep <= NUM_ELEMENTS_NECESSARY
+            || (!lastElement && (*src == '('))) {
+
+            separatorPtr = (char *)strchr(src, ',');
+            if (separatorPtr) {
+                *separatorPtr = 0;
+            } else {
+                lastElement = true;
+            }
+        }
+
+        switch (parseStep) {
+            case 1: // Step 1: Parse format
+                LOGXML("stream format is %s", src);
+                config.format = CameraUtils::string2PixelCode(src);
+                CheckError(config.format == -1, VOID_VALUE, "@%s, format fails", __func__);
+                break;
+            case 2: // Step 2: Parse the resolution
+                config.width = strtol(src, &endPtr, 10);
+                CheckError(!endPtr || *endPtr != 'x', VOID_VALUE, "@%s, width fails", __func__);
+                src = endPtr + 1;
+                config.height = strtol(src, &endPtr, 10);
+                LOGXML("(%dx%d)", config.width, config.height);
+                break;
+            case 3: // Step 3: Parse field
+                config.field = strtol(src, &endPtr, 10);
+                LOGXML("stream field is %d", config.field);
+                break;
+            case 4: // Step 4: Parse MediaCtlConf id.
+                mcId = strtol(src, &endPtr, 10);
+                CheckError(mcId < 0, VOID_VALUE, "@%s, mcId fails", __func__);
+                LOGXML("the mcId for supported stream config is %d", mcId);
+                break;
+        }
+
+        if (!lastElement) {
+            // Move to the next element
+            src = separatorPtr + 1;
+            src = skipWhiteSpace(src);
+        } else if (parseStep < NUM_ELEMENTS_NECESSARY ){
+            LOGE("Malformed stream configuration, only finish step %d", parseStep);
+            return;
+        }
+
+        // Finish all elements for one config
+        if (parseStep >= NUM_ELEMENTS) {
+            configs.push_back(config);
+            storeMcMappForConfig(mcId, config);
+            CLEAR(config);
+            mcId = -1;
+            parseStep = 0;
+            LOGXML("Stream Configuration found");
+            if (lastElement) {
+                break;
+            }
+        }
+    } while (true);
+}
+
+void CameraParser::parseSupportedFeatures(const char* src, camera_features_list_t& features)
+{
+    HAL_TRACE_CALL(1);
+
+    char * endPtr = nullptr;
+    camera_features feature = INVALID_FEATURE;
+    do {
+        endPtr = (char *)strchr(src, ',');
+        if (endPtr) {
+            *endPtr = 0;
+        }
+        if (strcmp(src, "MANUAL_EXPOSURE") == 0) {
+            feature = MANUAL_EXPOSURE;
+        } else if (strcmp(src, "MANUAL_WHITE_BALANCE") == 0) {
+            feature = MANUAL_WHITE_BALANCE;
+        } else if (strcmp(src, "IMAGE_ENHANCEMENT") == 0) {
+            feature = IMAGE_ENHANCEMENT;
+        } else if (strcmp(src, "NOISE_REDUCTION") == 0) {
+            feature = NOISE_REDUCTION;
+        } else if (strcmp(src, "SCENE_MODE") == 0) {
+            feature = SCENE_MODE;
+        } else if (strcmp(src, "WEIGHT_GRID_MODE") == 0) {
+            feature = WEIGHT_GRID_MODE;
+        } else if (strcmp(src, "PER_FRAME_CONTROL") == 0) {
+            feature = PER_FRAME_CONTROL;
+        } else if (strcmp(src, "ISP_CONTROL") == 0) {
+            feature = ISP_CONTROL;
+        } else {
+            feature = INVALID_FEATURE;
+        }
+
+        if (feature != INVALID_FEATURE) {
+            features.push_back(feature);
+        }
+
+        if (endPtr) {
+            src = endPtr + 1;
+            src = skipWhiteSpace(src);
+        }
+    } while (endPtr);
+}
+
+int CameraParser::parseSupportedVideoStabilizationMode(const char* str, camera_video_stabilization_list_t &supportedModes)
+{
+    HAL_TRACE_CALL(1);
+    CheckError(str == nullptr, -1, "@%s, str is nullptr", __func__);
+
+    char *savePtr, *tablePtr;
+    int sz = strlen(str);
+    char src[sz + 1];
+    MEMCPY_S(src, sz, str, sz);
+    src[sz] = '\0';
+    camera_video_stabilization_mode_t mode = VIDEO_STABILIZATION_MODE_OFF;
+
+    tablePtr = strtok_r(src, ",", &savePtr);
+    while (tablePtr) {
+        if (strcmp(tablePtr, "ON") == 0) {
+            mode = VIDEO_STABILIZATION_MODE_ON;
+        } else if (strcmp(tablePtr, "OFF") == 0) {
+            mode = VIDEO_STABILIZATION_MODE_OFF;
+        }
+        supportedModes.push_back(mode);
+
+        if (savePtr != nullptr)
+            savePtr = const_cast<char*>(skipWhiteSpace(savePtr));
+        tablePtr = strtok_r(nullptr, ",", &savePtr);
+    }
+
+    return OK;
+}
+
+int CameraParser::parseSupportedAeMode(const char* str, vector <camera_ae_mode_t> &supportedModes)
+{
+    HAL_TRACE_CALL(1);
+    CheckError(str == nullptr, -1, "@%s, str is nullptr", __func__);
+
+    char *savePtr, *tablePtr;
+    int sz = strlen(str);
+    char src[sz + 1];
+    MEMCPY_S(src, sz, str, sz);
+    src[sz] = '\0';
+    camera_ae_mode_t aeMode = AE_MODE_AUTO;
+
+    tablePtr = strtok_r(src, ",", &savePtr);
+    while (tablePtr) {
+        if (strcmp(tablePtr, "AUTO") == 0) {
+            aeMode = AE_MODE_AUTO;
+        } else if (strcmp(tablePtr, "MANUAL") == 0) {
+            aeMode = AE_MODE_MANUAL;
+        }
+        supportedModes.push_back(aeMode);
+        if (savePtr != nullptr)
+            savePtr = const_cast<char*>(skipWhiteSpace(savePtr));
+        tablePtr = strtok_r(nullptr, ",", &savePtr);
+    }
+
+    return OK;
+}
+
+int CameraParser::parseSupportedAfMode(const char* str, vector <camera_af_mode_t> &supportedModes)
+{
+    HAL_TRACE_CALL(1);
+    CheckError(str == NULL, -1, "@%s, str is NULL", __func__);
+
+    char *savePtr, *tablePtr;
+    int sz = strlen(str);
+    char src[sz + 1];
+    MEMCPY_S(src, sz, str, sz);
+    src[sz] = '\0';
+    camera_af_mode_t afMode = AF_MODE_AUTO;
+
+    tablePtr = strtok_r(src, ",", &savePtr);
+    while (tablePtr) {
+        if (strcmp(tablePtr, "AUTO") == 0) {
+            afMode = AF_MODE_AUTO;
+        } else if (strcmp(tablePtr, "MACRO") == 0) {
+            afMode = AF_MODE_MACRO;
+        } else if (strcmp(tablePtr, "CONTINUOUS_VIDEO") == 0) {
+            afMode = AF_MODE_CONTINUOUS_VIDEO;
+        } else if (strcmp(tablePtr, "CONTINUOUS_PICTURE") == 0) {
+            afMode = AF_MODE_CONTINUOUS_PICTURE;
+        } else if (strcmp(tablePtr, "OFF") == 0) {
+            afMode = AF_MODE_OFF;
+        }
+        supportedModes.push_back(afMode);
+        if (savePtr != NULL)
+            savePtr = const_cast<char*>(skipWhiteSpace(savePtr));
+        tablePtr = strtok_r(NULL, ",", &savePtr);
+    }
+
+    return OK;
+}
+
+int CameraParser::parseSupportedAntibandingMode(const char* str, vector <camera_antibanding_mode_t> &supportedModes)
+{
+    HAL_TRACE_CALL(1);
+    CheckError(str == nullptr, -1, "@%s, str is nullptr", __func__);
+
+    char *savePtr, *tablePtr;
+    int sz = strlen(str);
+    char src[sz + 1];
+    MEMCPY_S(src, sz, str, sz);
+    src[sz] = '\0';
+    camera_antibanding_mode_t antibandingMode = ANTIBANDING_MODE_OFF;
+
+    tablePtr = strtok_r(src, ",", &savePtr);
+    while (tablePtr) {
+        if (strcmp(tablePtr, "AUTO") == 0) {
+            antibandingMode = ANTIBANDING_MODE_AUTO;
+        } else if (strcmp(tablePtr, "50Hz") == 0) {
+            antibandingMode = ANTIBANDING_MODE_50HZ;
+        } else if (strcmp(tablePtr, "60Hz") == 0) {
+            antibandingMode = ANTIBANDING_MODE_60HZ;
+        } else if (strcmp(tablePtr, "OFF") == 0) {
+            antibandingMode = ANTIBANDING_MODE_OFF;
+        }
+        supportedModes.push_back(antibandingMode);
+        if (savePtr != nullptr)
+            savePtr = const_cast<char*>(skipWhiteSpace(savePtr));
+        tablePtr = strtok_r(nullptr, ",", &savePtr);
+    }
+
+    return OK;
+}
+
+int CameraParser::parseSupportedAeParamRange(const char* src, vector<int>& scenes,
+        vector<float>& minValues, vector<float>& maxValues)
+{
+    HAL_TRACE_CALL(1);
+    char* srcDup = strdup(src);
+    CheckError((srcDup == nullptr), NO_MEMORY, "Create a copy of source string failed.");
+
+    char* srcTmp = srcDup;
+    char* endPtr = nullptr;
+    while ((endPtr = (char *)strchr(srcTmp, ','))) {
+        if (endPtr) *endPtr = 0;
+
+        camera_scene_mode_t scene = CameraUtils::getSceneModeByName(srcTmp);
+        scenes.push_back(scene);
+        if (endPtr) {
+            srcTmp = endPtr + 1;
+            srcTmp = const_cast<char*>(skipWhiteSpace(srcTmp));
+        }
+
+        float min = strtof(srcTmp, &endPtr);
+        minValues.push_back(min);
+        if (endPtr == nullptr || *endPtr != ',') {
+            LOGE("Malformed ET range in exposure time range configuration");
+            free(srcDup);
+            return UNKNOWN_ERROR;
+        }
+        srcTmp = endPtr + 1;
+        float max = strtof(srcTmp, &endPtr);
+        maxValues.push_back(max);
+
+        if (endPtr) {
+            srcTmp = endPtr + 1;
+            srcTmp = const_cast<char*>(skipWhiteSpace(srcTmp));
+        }
+    }
+    free(srcDup);
+    return OK;
+}
+
+void CameraParser::parseFormatElement(CameraParser *profiles, const char *name, const char **atts)
+{
+    LOGXML("@%s, name:%s", __func__, name);
+
+    McFormat fmt;
+    fmt.type = RESOLUTION_TARGET;
+
+    int idx = 0;
+    while (atts[idx]) {
+        const char* key = atts[idx];
+        const char* val = atts[idx + 1];
+        LOGXML("@%s, name:%s, atts[%d]:%s, atts[%d]:%s", __func__, name, idx, key, idx+1, val);
+        if (strcmp(key, "name") == 0) {
+            fmt.entityName = replaceStringInXml(profiles, val);
+            fmt.entity = profiles->mMC->getEntityIdByName(fmt.entityName.c_str());
+        } else if (strcmp(key, "pad") == 0) {
+            fmt.pad = strtoul(val, nullptr, 10);
+        } else if (strcmp(key, "stream") == 0) {
+            fmt.stream = strtoul(val, nullptr, 10);
+        } else if (strcmp(key, "type") == 0) {
+            if (strcmp(val, "RESOLUTION_MAX") == 0) {
+                fmt.type = RESOLUTION_MAX;
+            } else if (strcmp(val, "RESOLUTION_COMPOSE") == 0) {
+                fmt.type = RESOLUTION_COMPOSE;
+            } else if (strcmp(val, "RESOLUTION_CROP") == 0) {
+                fmt.type = RESOLUTION_CROP;
+            } else if (strcmp(val, "RESOLUTION_TARGET") == 0) {
+                fmt.type = RESOLUTION_TARGET;
+            } else {
+                LOGE("Parse format type failed. type = %s", val);
+                return;
+            }
+        } else if (strcmp(key, "width") == 0) {
+            fmt.width = strtoul(val, nullptr, 10);
+        } else if (strcmp(key, "height") == 0) {
+            fmt.height = strtoul(val, nullptr, 10);
+        } else if (strcmp(key, "format") == 0) {
+            fmt.pixelCode = CameraUtils::string2PixelCode(val);
+        }
+        idx += 2;
+    }
+
+    fmt.formatType = FC_FORMAT;
+    MediaCtlConf &mc = profiles->pCurrentCam->mMediaCtlConfs.back();
+    mc.formats.push_back(fmt);
+}
+
+void CameraParser::parseLinkElement(CameraParser *profiles, const char *name, const char **atts)
+{
+    McLink link;
+    MediaCtlConf &mc = profiles->pCurrentCam->mMediaCtlConfs.back();
+    LOGXML("@%s, name:%s", __func__, name);
+
+    int idx = 0;
+    while (atts[idx]) {
+        const char* key = atts[idx];
+        const char* val = atts[idx + 1];
+        LOGXML("@%s, name:%s, atts[%d]:%s, atts[%d]:%s", __func__, name, idx, key, idx+1, val);
+        if (strcmp(key, "srcName") == 0) {
+            link.srcEntityName = replaceStringInXml(profiles, val);
+            link.srcEntity = profiles->mMC->getEntityIdByName(link.srcEntityName.c_str());
+        } else if (strcmp(key, "srcPad") == 0) {
+            link.srcPad = strtoul(val, nullptr, 10);
+        } else if (strcmp(key, "sinkName") == 0) {
+            link.sinkEntityName = replaceStringInXml(profiles, val);
+            link.sinkEntity = profiles->mMC->getEntityIdByName(link.sinkEntityName.c_str());
+        } else if (strcmp(key, "sinkPad") == 0) {
+            link.sinkPad = strtoul(val, nullptr, 10);
+        } else if (strcmp(key, "enable") == 0) {
+            link.enable = strcmp(val, "true") == 0;
+        }
+
+        idx += 2;
+    }
+
+    mc.links.push_back(link);
+}
+
+void CameraParser::parseRouteElement(CameraParser *profiles, const char *name, const char **atts)
+{
+    McRoute route;
+    MediaCtlConf &mc = profiles->pCurrentCam->mMediaCtlConfs.back();
+    LOGXML("@%s, name:%s", __func__, name);
+    route.flag = MEDIA_LNK_FL_ENABLED;
+
+    int idx = 0;
+    while (atts[idx]) {
+        const char* key = atts[idx];
+        const char* val = atts[idx + 1];
+        LOGXML("@%s, name:%s, atts[%d]:%s, atts[%d]:%s", __func__, name, idx, key, idx+1, val);
+        if (strcmp(key, "name") == 0) {
+            route.entityName = replaceStringInXml(profiles, val);
+            route.entity = profiles->mMC->getEntityIdByName(route.entityName.c_str());
+        } else if (strcmp(key, "srcPad") == 0) {
+            route.srcPad = strtoul(val, nullptr, 10);
+        } else if (strcmp(key, "sinkPad") == 0) {
+            route.sinkPad = strtoul(val, nullptr, 10);
+        } else if (strcmp(key, "srcStream") == 0) {
+            route.srcStream = strtoul(val, nullptr, 10);
+        } else if (strcmp(key, "sinkStream") == 0) {
+            route.sinkStream = strtoul(val, nullptr, 10);
+        } else if (strcmp(key, "flag") == 0) {
+            route.flag = strtoul(val, nullptr, 10);
+        }
+        idx += 2;
+    }
+
+    mc.routes.push_back(route);
+}
+
+void CameraParser::parseVideoElement(CameraParser *profiles, const char * /*name*/, const char **atts)
+{
+   McVideoNode videoNode;
+   MediaCtlConf &mc = profiles->pCurrentCam->mMediaCtlConfs.back();
+
+   videoNode.name = replaceStringInXml(profiles, atts[1]);
+   videoNode.videoNodeType = GetNodeType(atts[3]);
+   LOGXML("@%s, name:%s, videoNodeType:%d", __func__, videoNode.name.c_str(), videoNode.videoNodeType);
+
+   mc.videoNodes.push_back(videoNode);
+}
+
+// MediaCtl output tag xml parsing code for the field like:
+// <output port="main" width="1920" height="1088" format="V4L2_PIX_FMT_YUYV420_V32"/>
+// <output port="second" width="3264" height="2448" format="V4L2_PIX_FMT_SGRBG12V32"/>
+void CameraParser::parseOutputElement(CameraParser *profiles, const char *name, const char **atts)
+{
+    LOGXML("@%s, name:%s", __func__, name);
+
+    McOutput output;
+
+    int idx = 0;
+    while (atts[idx]) {
+        const char* key = atts[idx];
+        const char* val = atts[idx + 1];
+        LOGXML("@%s, name:%s, atts[%d]:%s, atts[%d]:%s", __func__, name, idx, key, idx+1, val);
+        if (strcmp(key, "port") == 0) {
+            if (strcmp(val, "main") ==  0)
+                output.port = MAIN_PORT;
+            else if (strcmp(val, "second") ==  0)
+                output.port = SECOND_PORT;
+            else if (strcmp(val, "third") ==  0)
+                output.port = THIRD_PORT;
+            else if (strcmp(val, "forth") ==  0)
+                output.port = FORTH_PORT;
+            else
+                output.port = INVALID_PORT;
+        } else if (strcmp(key, "width") == 0) {
+            output.width = strtoul(val, nullptr, 10);
+        } else if (strcmp(key, "height") == 0) {
+            output.height = strtoul(val, nullptr, 10);
+        } else if (strcmp(key, "format") == 0) {
+            output.v4l2Format = CameraUtils::string2PixelCode(val);
+        }
+        idx += 2;
+    }
+
+    LOGXML("@%s, port:%d, output size:%dx%d, v4l2Format:%x", __func__, output.port,
+            output.width, output.height, output.v4l2Format);
+
+    MediaCtlConf &mc = profiles->pCurrentCam->mMediaCtlConfs.back();
+    mc.outputs.push_back(output);
+}
+
+void CameraParser::parseMultiExpRange(const char* src)
+{
+    ExpRange* range = nullptr;
+    MultiExpRange multiRange;
+    MultiExpRange* pCurrRange = nullptr;
+    pCurrentCam->mMultiExpRanges.clear();
+    static const int MULTI_EXPOSURE_TAG_SHS1 = 0;
+    static const int MULTI_EXPOSURE_TAG_RHS1 = 1;
+    static const int MULTI_EXPOSURE_TAG_SHS2 = 2;
+    static const int MULTI_EXPOSURE_TAG_RHS2 = 3;
+    static const int MULTI_EXPOSURE_TAG_SHS3 = 4;
+
+    string srcDup = src;
+    CheckError((srcDup.c_str() == nullptr), VOID_VALUE, "Create a copy of source string failed.");
+
+    const char* srcTmp = srcDup.c_str();
+    char* endPtr = nullptr;
+    int tag = -1;
+    while ((endPtr = (char *)strchr(srcTmp, ','))) {
+        *endPtr = 0;
+        if (strcmp(srcTmp, "SHS1") == 0) {
+            tag = MULTI_EXPOSURE_TAG_SHS1;
+        } else if (strcmp(srcTmp, "RHS1") == 0) {
+            tag = MULTI_EXPOSURE_TAG_RHS1;
+        } else if (strcmp(srcTmp, "SHS2") == 0) {
+            tag = MULTI_EXPOSURE_TAG_SHS2;
+        } else if (strcmp(srcTmp, "RHS2") == 0) {
+            tag = MULTI_EXPOSURE_TAG_RHS2;
+        } else if (strcmp(srcTmp, "SHS3") == 0) {
+            tag = MULTI_EXPOSURE_TAG_SHS3;
+        } else {
+            LOGE("Malformed tag for multi-exposure range configuration");
+            return;
+        }
+
+        if (endPtr) {
+            srcTmp = endPtr + 1;
+            srcTmp = const_cast<char*>(skipWhiteSpace(srcTmp));
+        }
+
+        CLEAR(multiRange);
+        multiRange.Resolution.width = strtol(srcTmp, &endPtr, 10);
+        CheckError((endPtr == nullptr || *endPtr != ','), VOID_VALUE, "Malformed resolution for multi-exposure range configuration");
+
+        srcTmp = endPtr + 1;
+        srcTmp = const_cast<char*>(skipWhiteSpace(srcTmp));
+        multiRange.Resolution.height = strtol(srcTmp, &endPtr, 10);
+        CheckError((endPtr == nullptr || *endPtr != ','), VOID_VALUE, "Malformed resolution for multi-exposure range configuration");
+
+        pCurrRange = nullptr;
+        for (unsigned int i = 0; i < pCurrentCam->mMultiExpRanges.size(); i++) {
+            if (pCurrentCam->mMultiExpRanges[i].Resolution.width == multiRange.Resolution.width &&
+                pCurrentCam->mMultiExpRanges[i].Resolution.height == multiRange.Resolution.height) {
+                pCurrRange = &(pCurrentCam->mMultiExpRanges[i]);
+                break;
+            }
+        }
+        if (pCurrRange) {
+            switch (tag) {
+                case MULTI_EXPOSURE_TAG_SHS1:
+                    range = &pCurrRange->SHS1;
+                    break;
+                case MULTI_EXPOSURE_TAG_RHS1:
+                    range = &pCurrRange->RHS1;
+                    break;
+                case MULTI_EXPOSURE_TAG_SHS2:
+                    range = &pCurrRange->SHS2;
+                    break;
+                case MULTI_EXPOSURE_TAG_RHS2:
+                    range = &pCurrRange->RHS2;
+                    break;
+                case MULTI_EXPOSURE_TAG_SHS3:
+                    range = &pCurrRange->SHS3;
+                    break;
+                default:
+                    LOGE("Wrong tag for multi-exposure range configuration");
+                    return;
+            }
+        } else {
+            switch (tag) {
+                case MULTI_EXPOSURE_TAG_SHS1:
+                    range = &multiRange.SHS1;
+                    break;
+                case MULTI_EXPOSURE_TAG_RHS1:
+                    range = &multiRange.RHS1;
+                    break;
+                case MULTI_EXPOSURE_TAG_SHS2:
+                    range = &multiRange.SHS2;
+                    break;
+                case MULTI_EXPOSURE_TAG_RHS2:
+                    range = &multiRange.RHS2;
+                    break;
+                case MULTI_EXPOSURE_TAG_SHS3:
+                    range = &multiRange.SHS3;
+                    break;
+                default:
+                    LOGE("Wrong tag for multi-exposure range configuration");
+                    return;
+            }
+        }
+
+        srcTmp = endPtr + 1;
+        srcTmp = const_cast<char*>(skipWhiteSpace(srcTmp));
+        range->min = strtol(srcTmp, &endPtr, 10);
+        CheckError((endPtr == nullptr || *endPtr != ','), VOID_VALUE, "Malformed range for multi-exposure range configuration");
+
+        srcTmp = endPtr + 1;
+        srcTmp = const_cast<char*>(skipWhiteSpace(srcTmp));
+        range->max = strtol(srcTmp, &endPtr, 10);
+        CheckError((endPtr == nullptr || *endPtr != ','), VOID_VALUE, "Malformed range for multi-exposure range configuration");
+
+        srcTmp = endPtr + 1;
+        srcTmp = const_cast<char*>(skipWhiteSpace(srcTmp));
+        range->step = strtol(srcTmp, &endPtr, 10);
+        CheckError((endPtr == nullptr || *endPtr != ','), VOID_VALUE, "Malformed range for multi-exposure range configuration");
+
+        srcTmp = endPtr + 1;
+        srcTmp = const_cast<char*>(skipWhiteSpace(srcTmp));
+        range->lowerBound = strtol(srcTmp, &endPtr, 10);
+        CheckError((endPtr == nullptr || *endPtr != ','), VOID_VALUE, "Malformed range for multi-exposure range configuration");
+
+        srcTmp = endPtr + 1;
+        srcTmp = const_cast<char*>(skipWhiteSpace(srcTmp));
+        range->upperBound = strtol(srcTmp, &endPtr, 10);
+
+        if (endPtr) {
+            srcTmp = endPtr + 1;
+            srcTmp = const_cast<char*>(skipWhiteSpace(srcTmp));
+        }
+
+        if (!pCurrRange) {
+            pCurrentCam->mMultiExpRanges.push_back(multiRange);
+        }
+    }
+}
+
+int CameraParser::parsePair(const char *str, int *first, int *second, char delim, char **endptr)
+{
+    // Find the first integer.
+    char *end;
+    int w = (int)strtol(str, &end, 10);
+    // If a delimeter does not immediately follow, give up.
+    if (*end != delim) {
+        LOGE("Cannot find delimeter (%c) in str=%s", delim, str);
+        return -1;
+    }
+
+    // Find the second integer, immediately after the delimeter.
+    int h = (int)strtol(end+1, &end, 10);
+
+    *first = w;
+    *second = h;
+
+    if (endptr) {
+        *endptr = end;
+    }
+
+    return 0;
+}
+
+void CameraParser::parseSizesList(const char *sizesStr, vector <camera_resolution_t> &sizes)
+{
+    if (sizesStr == 0) {
+        return;
+    }
+
+    char *sizeStartPtr = (char *)sizesStr;
+
+    while (true) {
+        camera_resolution_t r;
+        int success = parsePair(sizeStartPtr, &r.width, &r.height, 'x',
+                                 &sizeStartPtr);
+        if (success == -1 || (*sizeStartPtr != ',' && *sizeStartPtr != '\0')) {
+            LOGE("Picture sizes string \"%s\" contains invalid character.", sizesStr);
+            return;
+        }
+        if (r.width > 0 && r.height > 0)
+            sizes.push_back(r);
+
+        if (*sizeStartPtr == '\0') {
+            return;
+        }
+        sizeStartPtr++;
+    }
+}
+
+/*
+ * The pls output to user requirement mapping table
+ *
+ * first: user requirement, second: psl output
+ * eg: <pslOutputForRotation value="3264x2448@1200x1600"/>
+ */
+void CameraParser::parseOutputMap(const char *str, vector<UserToPslOutputMap> &outputMap)
+{
+    char *srcDup = strdup(str);
+    CheckError((srcDup == nullptr), VOID_VALUE, "Create a copy of source string failed.");
+
+    char *srcTmp = srcDup;
+    char *endPtr = nullptr;
+    do {
+        endPtr = (char *)strchr(srcTmp, ',');
+        if (endPtr) {
+            *endPtr = 0;
+        }
+        char *tmpPtr = (char *)strchr(srcTmp, '@');
+        if (tmpPtr) {
+            *tmpPtr = 0;
+        }
+
+        UserToPslOutputMap map;
+        parsePair(srcTmp, &(map.User).width, &(map.User).height, 'x');
+        if (tmpPtr) {
+            srcTmp = tmpPtr + 1;
+            srcTmp = (char*)skipWhiteSpace(srcTmp);
+        }
+        parsePair(srcTmp, &(map.Psl).width, &(map.Psl).height, 'x');
+        outputMap.push_back(map);
+
+        if (endPtr) {
+            srcTmp = endPtr + 1;
+            srcTmp = (char*)skipWhiteSpace(srcTmp);
+        }
+    } while (endPtr);
+
+    free(srcDup);
+}
+
+int CameraParser::getSupportedFormat(const char* str, vector <int>& supportedFormat)
+{
+    if (str == nullptr) {
+        LOGE("the str is nullptr");
+        return -1;
+    }
+
+    LOGXML("@%s, str:%s", __func__, str);
+    int sz = strlen(str);
+    char src[sz + 1];
+    MEMCPY_S(src, sz, str, sz);
+    src[sz] = '\0';
+    char* savePtr;
+    char* fmt = strtok_r(src, ",", &savePtr);
+    while (fmt) {
+        int actual = CameraUtils::string2PixelCode(fmt);
+        if (actual != -1) {
+            supportedFormat.push_back(actual);
+            LOGXML("@%s, add format:%d", __func__, actual);
+        }
+        fmt = strtok_r(nullptr, ",", &savePtr);
+    }
+
+    return 0;
+}
+
+/**
+ * This function will handle all the MediaCtlCfg related elements.
+ *
+ * It will be called in the function startElement
+ *
+ * \param profiles: the pointer of the CameraParser.
+ * \param name: the element's name.
+ * \param atts: the element's attribute.
+ */
+void CameraParser::handleMediaCtlCfg(CameraParser *profiles, const char *name, const char **atts)
+{
+    LOGXML("@%s, name:%s, atts[0]:%s, profiles->mCurrentSensor:%d", __func__, name, atts[0], profiles->mCurrentSensor);
+    if (strcmp(name, "MediaCtlConfig") == 0) {
+        parseMediaCtlConfigElement(profiles, name, atts);
+    } else if (strcmp(name, "link") == 0) {
+        parseLinkElement(profiles, name, atts);
+    } else if (strcmp(name, "route") == 0) {
+        parseRouteElement(profiles, name, atts);
+    } else if (strcmp(name, "control") == 0) {
+        parseControlElement(profiles, name, atts);
+    } else if (strcmp(name, "selection") == 0) {
+        parseSelectionElement(profiles, name, atts);
+    } else if (strcmp(name, "format") == 0) {
+        parseFormatElement(profiles, name, atts);
+    } else if (strcmp(name, "videonode") == 0) {
+        parseVideoElement(profiles, name, atts);
+    } else if (strcmp(name, "output") == 0) {
+        parseOutputElement(profiles, name, atts);
+    }
+}
+
+/**
+ * This function will handle all the StaticMetadata related elements.
+ *
+ * It will be called in the function startElement
+ *
+ * \param profiles: the pointer of the CameraParser.
+ * \param name: the element's name.
+ * \param atts: the element's attribute.
+ */
+void CameraParser::handleStaticMetaData(CameraParser *profiles, const char *name, const char **atts)
+{
+    LOGXML("@%s, name:%s, atts[0]:%s, profiles->mCurrentSensor:%d", __func__, name, atts[0], profiles->mCurrentSensor);
+    if (strcmp(name, "supportedStreamConfig") == 0) {
+        stream_array_t configsArray;
+        parseStreamConfig(atts[1], configsArray);
+        const int STREAM_MEMBER_NUM = sizeof(stream_t) / sizeof(int);
+        int dataSize = configsArray.size() * STREAM_MEMBER_NUM;
+        int configs[dataSize];
+        CLEAR(configs);
+        for (size_t i = 0; i < configsArray.size(); i++) {
+            LOGXML("@%s, stream config info: format=%s (%dx%d) field=%d type=%d", __func__,
+                    CameraUtils::format2string(configsArray[i].format).c_str(),
+                    configsArray[i].width, configsArray[i].height,
+                    configsArray[i].field, configsArray[i].streamType);
+            MEMCPY_S(&configs[i * STREAM_MEMBER_NUM], sizeof(stream_t),
+                     &configsArray[i], sizeof(stream_t));
+        }
+        mMetadata.update(INTEL_INFO_AVAILABLE_CONFIGURATIONS, configs, dataSize);
+    } else if (strcmp(name, "fpsRange") == 0) {
+        vector<double> rangeArray;
+        parseXmlConvertStrings(atts[1], rangeArray, atof);
+        float fpsRange[rangeArray.size()];
+        CLEAR(fpsRange);
+        for (size_t i = 0; i < rangeArray.size(); i++){
+            fpsRange[i] = static_cast<float>(rangeArray[i]);
+        }
+        LOGXML("@%s, supported fps range size: %zu", __func__, rangeArray.size());
+        mMetadata.update(CAMERA_AE_AVAILABLE_TARGET_FPS_RANGES, fpsRange, ARRAY_SIZE(fpsRange));
+    } else if (strcmp(name, "evRange") == 0) {
+        vector<int> rangeArray;
+        parseXmlConvertStrings(atts[1], rangeArray, atoi);
+
+        int evRange[rangeArray.size()];
+        CLEAR(evRange);
+        for (size_t i = 0; i < rangeArray.size(); i++) {
+            evRange[i] = rangeArray[i];
+        }
+        LOGXML("@%s, supported ev range size: %zu", __func__, rangeArray.size());
+        mMetadata.update(CAMERA_AE_COMPENSATION_RANGE, evRange, ARRAY_SIZE(evRange));
+    } else if (strcmp(name, "evStep") == 0) {
+        vector<int> rationalType;
+        int ret = parseXmlConvertStrings(atts[1], rationalType, atoi);
+        CheckError((ret != OK), VOID_VALUE, "Parse evStep failed");
+
+        icamera_metadata_rational_t evStep = {rationalType[0], rationalType[1]};
+        LOGXML("@%s, the numerator: %d, denominator: %d", __func__, evStep.numerator, evStep.denominator);
+        mMetadata.update(CAMERA_AE_COMPENSATION_STEP, &evStep, 1);
+    } else if (strcmp(name, "supportedFeatures") == 0) {
+        camera_features_list_t supportedFeatures;
+        parseSupportedFeatures(atts[1], supportedFeatures);
+        int numberOfFeatures = supportedFeatures.size();
+        uint8_t features[numberOfFeatures];
+        CLEAR(features);
+        for (int i = 0; i < numberOfFeatures; i++) {
+            features[i] = supportedFeatures[i];
+        }
+        mMetadata.update(INTEL_INFO_AVAILABLE_FEATURES, features, numberOfFeatures);
+    } else if (strcmp(name, "supportedAeExposureTimeRange") == 0) {
+        vector<int> scenes;
+        vector<float> minValues, maxValues;
+        int ret = parseSupportedAeParamRange(atts[1], scenes, minValues, maxValues);
+        CheckError((ret != OK), VOID_VALUE, "Parse AE eExposure time range failed");
+
+        const int MEMBER_COUNT = 3;
+        const int dataSize = scenes.size() * MEMBER_COUNT;
+        int rangeData[dataSize];
+        CLEAR(rangeData);
+
+        for (size_t i = 0; i < scenes.size(); i++) {
+            LOGXML("@%s, scene mode:%d supported exposure time range (%f-%f)", __func__,
+                    scenes[i], minValues[i], maxValues[i]);
+            rangeData[i * MEMBER_COUNT] = scenes[i];
+            rangeData[i * MEMBER_COUNT + 1] = (int)minValues[i];
+            rangeData[i * MEMBER_COUNT + 2] = (int)maxValues[i];
+        }
+        mMetadata.update(INTEL_INFO_AE_EXPOSURE_TIME_RANGE, rangeData, dataSize);
+    } else if (strcmp(name, "supportedAeGainRange") == 0) {
+        vector<int> scenes;
+        vector<float> minValues, maxValues;
+        int ret = parseSupportedAeParamRange(atts[1], scenes, minValues, maxValues);
+        CheckError((ret != OK), VOID_VALUE, "Parse AE gain range failed");
+
+        const int MEMBER_COUNT = 3;
+        const int dataSize = scenes.size() * MEMBER_COUNT;
+        int rangeData[dataSize];
+        CLEAR(rangeData);
+
+        for (size_t i = 0; i < scenes.size(); i++) {
+            LOGXML("@%s, scene mode:%d supported gain range (%f-%f)", __func__,
+                    scenes[i], minValues[i], maxValues[i]);
+            rangeData[i * MEMBER_COUNT] = scenes[i];
+            // Since we use int to store float, before storing it we multiply min and max by 100.
+            rangeData[i * MEMBER_COUNT + 1] = (int)(minValues[i] * 100);
+            rangeData[i * MEMBER_COUNT + 2] = (int)(maxValues[i] * 100);
+        }
+        mMetadata.update(INTEL_INFO_AE_GAIN_RANGE, rangeData, dataSize);
+    } else if (strcmp(name, "supportedVideoStabilizationModes") == 0) {
+        camera_video_stabilization_list_t supportedMode;
+        parseSupportedVideoStabilizationMode(atts[1], supportedMode);
+        uint8_t modes[supportedMode.size()];
+        CLEAR(modes);
+        for(size_t i = 0; i < supportedMode.size(); i++) {
+            modes[i] = supportedMode[i];
+        }
+        mMetadata.update(CAMERA_CONTROL_AVAILABLE_VIDEO_STABILIZATION_MODES, modes, supportedMode.size());
+    } else if (strcmp(name, "supportedAeMode") == 0) {
+        vector <camera_ae_mode_t> supportedAeMode;
+        parseSupportedAeMode(atts[1], supportedAeMode);
+        uint8_t aeModes[supportedAeMode.size()];
+        CLEAR(aeModes);
+        for (size_t i = 0; i < supportedAeMode.size(); i++) {
+            aeModes[i] = supportedAeMode[i];
+        }
+        mMetadata.update(CAMERA_AE_AVAILABLE_MODES, aeModes, supportedAeMode.size());
+    } else if (strcmp(name, "supportedAwbMode") == 0) {
+        vector <camera_awb_mode_t> supportedAwbMode;
+        parseXmlConvertStrings(atts[1], supportedAwbMode, CameraUtils::getAwbModeByName);
+        uint8_t awbModes[supportedAwbMode.size()];
+        CLEAR(awbModes);
+        for (size_t i = 0; i < supportedAwbMode.size(); i++) {
+            awbModes[i] = supportedAwbMode[i];
+        }
+        mMetadata.update(CAMERA_AWB_AVAILABLE_MODES, awbModes, supportedAwbMode.size());
+    } else if (strcmp(name, "supportedSceneMode") == 0) {
+        vector <camera_scene_mode_t> supportedSceneMode;
+        parseXmlConvertStrings(atts[1], supportedSceneMode, CameraUtils::getSceneModeByName);
+        uint8_t sceneModes[supportedSceneMode.size()];
+        CLEAR(sceneModes);
+        for (size_t i = 0; i < supportedSceneMode.size(); i++) {
+            sceneModes[i] = supportedSceneMode[i];
+        }
+        mMetadata.update(CAMERA_CONTROL_AVAILABLE_SCENE_MODES, sceneModes, supportedSceneMode.size());
+    } else if (strcmp(name, "supportedAfMode") == 0) {
+        vector <camera_af_mode_t> supportedAfMode;
+        parseSupportedAfMode(atts[1], supportedAfMode);
+        uint8_t afModes[supportedAfMode.size()];
+        CLEAR(afModes);
+        for (size_t i = 0; i < supportedAfMode.size(); i++) {
+            afModes[i] = supportedAfMode[i];
+        }
+        mMetadata.update(CAMERA_AF_AVAILABLE_MODES, afModes, supportedAfMode.size());
+    } else if (strcmp(name, "supportedAntibandingMode") == 0) {
+        vector <camera_antibanding_mode_t> supportedAntibandingMode;
+        parseSupportedAntibandingMode(atts[1], supportedAntibandingMode);
+        uint8_t antibandingModes[supportedAntibandingMode.size()];
+        CLEAR(antibandingModes);
+        for (size_t i = 0; i < supportedAntibandingMode.size(); i++) {
+            antibandingModes[i] = supportedAntibandingMode[i];
+        }
+        mMetadata.update(CAMERA_AE_AVAILABLE_ANTIBANDING_MODES, antibandingModes, supportedAntibandingMode.size());
+    } else if (strcmp(name, "sensorMountType") == 0) {
+        uint8_t mountType = WALL_MOUNTED;
+
+        if (strcmp(atts[1], "CEILING_MOUNTED") == 0)
+            mountType = CEILING_MOUNTED;
+
+        mMetadata.update(INTEL_INFO_SENSOR_MOUNT_TYPE, &mountType, 1);
+        LOGXML("@%s, sensor mount type: %d", __func__, mountType);
+    } else if (strcmp(name, "StaticMetadata") != 0) { // Make sure it doesn't reach the end of StaticMetadata.
+        handleGenericStaticMetaData(name, atts[1]);
+    }
+}
+
+/**
+ * \brief Parses string for generic static metadata and save them.
+ *
+ * \param name: the element's name.
+ * \param src: the element's value, only include data and separator 'x' or ','.
+ */
+void CameraParser::handleGenericStaticMetaData(const char *name, const char *src)
+{
+    uint32_t tag =
+            (strcmp(name, "ae.lockAvailable") == 0)              ? CAMERA_AE_LOCK_AVAILABLE
+          : (strcmp(name, "awb.lockAvailable") == 0)             ? CAMERA_AWB_LOCK_AVAILABLE
+          : (strcmp(name, "control.availableModes") == 0)        ? CAMERA_CONTROL_AVAILABLE_MODES
+          : (strcmp(name, "control.availableSceneModes") == 0)   ? CAMERA_CONTROL_AVAILABLE_SCENE_MODES
+          : (strcmp(name, "control.maxRegions") == 0)            ? CAMERA_CONTROL_MAX_REGIONS
+          : (strcmp(name, "statistics.info.availableFaceDetectModes") == 0) ? CAMERA_STATISTICS_INFO_AVAILABLE_FACE_DETECT_MODES
+          : (strcmp(name, "statistics.info.maxFaceCount") == 0)  ? CAMERA_STATISTICS_INFO_MAX_FACE_COUNT
+          : (strcmp(name, "sensor.info.activeArraySize") == 0)   ? CAMERA_SENSOR_INFO_ACTIVE_ARRAY_SIZE
+          : (strcmp(name, "sensor.info.pixelArraySize") == 0)    ? CAMERA_SENSOR_INFO_PIXEL_ARRAY_SIZE
+          : (strcmp(name, "sensor.info.physicalSize") == 0)      ? CAMERA_SENSOR_INFO_PHYSICAL_SIZE
+          : (strcmp(name, "sensor.info.sensitivityRange") == 0)  ? CAMERA_SENSOR_INFO_SENSITIVITY_RANGE
+          : (strcmp(name, "sensor.info.exposureTimeRange") == 0) ? CAMERA_SENSOR_INFO_EXPOSURE_TIME_RANGE
+          : (strcmp(name, "sensor.info.colorFilterArrangement") == 0) ? CAMERA_SENSOR_INFO_COLOR_FILTER_ARRANGEMENT
+          : (strcmp(name, "sensor.availableTestPatternModes") == 0) ? CAMERA_SENSOR_AVAILABLE_TEST_PATTERN_MODES
+          : (strcmp(name, "sensor.orientation") == 0)            ? CAMERA_SENSOR_ORIENTATION
+          : (strcmp(name, "sensor.opaqueRawSize") == 0)          ? CAMERA_SENSOR_OPAQUE_RAW_SIZE
+          : (strcmp(name, "shading.availableModes") == 0)        ? CAMERA_SHADING_AVAILABLE_MODES
+          : (strcmp(name, "lens.facing") == 0)                   ? CAMERA_LENS_FACING
+          : (strcmp(name, "lens.info.availableApertures") == 0)  ? CAMERA_LENS_INFO_AVAILABLE_APERTURES
+          : (strcmp(name, "lens.info.availableFilterDensities") == 0) ? CAMERA_LENS_INFO_AVAILABLE_FILTER_DENSITIES
+          : (strcmp(name, "lens.info.availableFocalLengths") == 0) ? CAMERA_LENS_INFO_AVAILABLE_FOCAL_LENGTHS
+          : (strcmp(name, "lens.info.availableOpticalStabilization") == 0) ? CAMERA_LENS_INFO_AVAILABLE_OPTICAL_STABILIZATION
+          : (strcmp(name, "lens.info.hyperfocalDistance") == 0)  ? CAMERA_LENS_INFO_HYPERFOCAL_DISTANCE
+          : (strcmp(name, "lens.info.minimumFocusDistance") == 0) ? CAMERA_LENS_INFO_MINIMUM_FOCUS_DISTANCE
+          : (strcmp(name, "lens.info.shadingMapSize") == 0)      ? CAMERA_LENS_INFO_SHADING_MAP_SIZE
+          : (strcmp(name, "lens.info.focusDistanceCalibration") == 0) ? CAMERA_LENS_INFO_FOCUS_DISTANCE_CALIBRATION
+          : (strcmp(name, "request.maxNumOutputStreams") == 0)   ? CAMERA_REQUEST_MAX_NUM_OUTPUT_STREAMS
+          : (strcmp(name, "request.maxNumInputStreams") == 0)    ? CAMERA_REQUEST_MAX_NUM_INPUT_STREAMS
+          : (strcmp(name, "request.pipelineMaxDepth") == 0)      ? CAMERA_REQUEST_PIPELINE_MAX_DEPTH
+          : (strcmp(name, "request.availableCapabilities") == 0) ? CAMERA_REQUEST_AVAILABLE_CAPABILITIES
+          : (strcmp(name, "scaler.availableInputOutputFormatsMap") == 0) ? CAMERA_SCALER_AVAILABLE_INPUT_OUTPUT_FORMATS_MAP
+          : (strcmp(name, "scaler.availableStreamConfigurations") == 0)  ? CAMERA_SCALER_AVAILABLE_STREAM_CONFIGURATIONS
+          : (strcmp(name, "scaler.availableMinFrameDurations") == 0)     ? CAMERA_SCALER_AVAILABLE_MIN_FRAME_DURATIONS
+          : (strcmp(name, "scaler.availableStallDurations") == 0)        ? CAMERA_SCALER_AVAILABLE_STALL_DURATIONS
+          : (strcmp(name, "reprocess.maxCaptureStall") == 0)     ? CAMERA_REPROCESS_MAX_CAPTURE_STALL
+          : (strcmp(name, "jpeg.maxSize") == 0)                  ? CAMERA_JPEG_MAX_SIZE
+          : (strcmp(name, "jpeg.availableThumbnailSizes") == 0)  ? CAMERA_JPEG_AVAILABLE_THUMBNAIL_SIZES
+          : (strcmp(name, "edge.availableEdgeModes") == 0)       ? CAMERA_EDGE_AVAILABLE_EDGE_MODES
+          : (strcmp(name, "hotPixel.availableHotPixelModes") == 0) ? CAMERA_HOT_PIXEL_AVAILABLE_HOT_PIXEL_MODES
+          : (strcmp(name, "noiseReduction.availableNoiseReductionModes") == 0) ? CAMERA_NOISE_REDUCTION_AVAILABLE_NOISE_REDUCTION_MODES
+          : (strcmp(name, "tonemap.maxCurvePoints") == 0)        ? CAMERA_TONEMAP_MAX_CURVE_POINTS
+          : (strcmp(name, "tonemap.availableToneMapModes") == 0) ? CAMERA_TONEMAP_AVAILABLE_TONE_MAP_MODES
+          : (strcmp(name, "info.supportedHardwareLevel") == 0)   ? CAMERA_INFO_SUPPORTED_HARDWARE_LEVEL
+          : (strcmp(name, "sync.maxLatency") == 0)               ? CAMERA_SYNC_MAX_LATENCY
+          : -1;
+    int tagType = get_icamera_metadata_tag_type(tag);
+    if (tagType == -1) {
+        LOGW("Unsupported metadata %s", name);
+        return;
+    }
+
+    union {
+        uint8_t* u8;
+        int32_t* i32;
+        int64_t* i64;
+        float*   f;
+        double*  d;
+        icamera_metadata_rational_t* r;
+    } data;
+    data.u8 = (unsigned char *)mMetadataCache;
+
+    int index = 0;
+    int maxIndex = mMetadataCacheSize / sizeof(double); // worst case
+    char * endPtr = nullptr;
+    do {
+        switch (tagType) {
+        case ICAMERA_TYPE_BYTE:
+            data.u8[index]= (char)strtol(src, &endPtr, 10);
+            LOGXML(" - %d -", data.u8[index]);
+            break;
+        case ICAMERA_TYPE_INT32:
+        case ICAMERA_TYPE_RATIONAL:
+            data.i32[index]= strtol(src, &endPtr, 10);
+            LOGXML(" - %d -", data.i32[index]);
+            break;
+        case ICAMERA_TYPE_INT64:
+            data.i64[index]= strtol(src, &endPtr, 10);
+            LOGXML(" - %ld -", data.i64[index]);
+            break;
+        case ICAMERA_TYPE_FLOAT:
+            data.f[index]= strtof(src, &endPtr);
+            LOGXML(" - %8.3f -", data.f[index]);
+            break;
+        case ICAMERA_TYPE_DOUBLE:
+            data.d[index]= strtof(src, &endPtr);
+            LOGXML(" - %8.3f -", data.d[index]);
+            break;
+        }
+        index++;
+
+        if (endPtr != nullptr && (*endPtr == 'x' || *endPtr == ',')) {
+            src = endPtr + 1;
+        } else {
+            break;
+        }
+    } while (index < maxIndex);
+
+    switch (tagType) {
+    case ICAMERA_TYPE_BYTE:
+        mMetadata.update(tag, data.u8, index);
+        break;
+    case ICAMERA_TYPE_INT32:
+        mMetadata.update(tag, data.i32, index);
+        break;
+    case ICAMERA_TYPE_INT64:
+        mMetadata.update(tag, data.i64, index);
+        break;
+    case ICAMERA_TYPE_FLOAT:
+        mMetadata.update(tag, data.f, index);
+        break;
+    case ICAMERA_TYPE_DOUBLE:
+        mMetadata.update(tag, data.d, index);
+        break;
+    case ICAMERA_TYPE_RATIONAL:
+        mMetadata.update(tag, data.r, index / 2);
+        break;
+    }
+}
+
+/**
+ * the callback function of the libexpat for handling of one element start
+ *
+ * When it comes to the start of one element. This function will be called.
+ *
+ * \param userData: the pointer we set by the function XML_SetUserData.
+ * \param name: the element's name.
+ */
+void CameraParser::startParseElement(void *userData, const char *name, const char **atts)
+{
+    CameraParser *profiles = reinterpret_cast<CameraParser*>(userData);
+
+    if (profiles->mCurrentDataField == FIELD_INVALID) {
+        profiles->checkField(profiles, name, atts);
+        return;
+    }
+
+    switch (profiles->mCurrentDataField) {
+        case FIELD_SENSOR:
+            if (strcmp(name, "MediaCtlConfig") == 0) {
+                profiles->mInMediaCtlCfg = true;
+                LOGXML("@%s %s, mInMediaCtlCfg is set to true", __func__, name);
+            } else if (strcmp(name, "StaticMetadata") == 0) {
+                profiles->mInStaticMetadata = true;
+                LOGXML("@%s %s, mInStaticMetadata is set to true", __func__, name);
+            }
+
+            if (profiles->mInMediaCtlCfg) {
+                // The MediaCtlCfg belongs to the sensor segments
+                profiles->handleMediaCtlCfg(profiles, name, atts);
+            } else if (profiles->mInStaticMetadata) {
+                // The StaticMetadata belongs to the sensor segments
+                profiles->handleStaticMetaData(profiles, name, atts);
+            } else {
+                profiles->handleSensor(profiles, name, atts);
+            }
+            break;
+        case FIELD_COMMON:
+            profiles->handleCommon(profiles, name, atts);
+            break;
+        default:
+            LOGE("@%s, line:%d, go to default handling", __func__, __LINE__);
+            break;
+    }
+}
+
+/**
+ * the callback function of the libexpat for handling of one element end
+ *
+ * When it comes to the end of one element. This function will be called.
+ *
+ * \param userData: the pointer we set by the function XML_SetUserData.
+ * \param name: the element's name.
+ */
+void CameraParser::endParseElement(void *userData, const char *name)
+{
+    LOGXML("@%s %s", __func__, name);
+
+    CameraParser *profiles = reinterpret_cast<CameraParser*>(userData);
+
+    if (strcmp(name, "Sensor") == 0) {
+        profiles->mCurrentDataField = FIELD_INVALID;
+        if (profiles->pCurrentCam) {
+            LOGXML("@%s: Add camera id %d (%s)", __func__, profiles->mCurrentSensor,
+                   profiles->pCurrentCam->sensorName.c_str());
+            if (profiles->pCurrentCam->mLensName.empty() &&
+                profiles->pCurrentCam->sensorName.find("-wf") != string::npos) {
+                int ret = profiles->mMC->getLensName(&profiles->pCurrentCam->mLensName);
+                if (ret != OK) {
+                    LOGXML("@%s, Failed to getLensName", __func__);
+                }
+            }
+            // Merge the content of mMetadata into mCapability.
+            ParameterHelper::merge(profiles->mMetadata, &profiles->pCurrentCam->mCapability);
+            profiles->mMetadata.clear();
+
+            // For non-extended camera, it should be in order by mCurrentSensor
+            profiles->mStaticCfg->mCameras.insert(profiles->mStaticCfg->mCameras.begin() +
+                                               profiles->mCurrentSensor, *(profiles->pCurrentCam));
+
+            delete profiles->pCurrentCam;
+            profiles->pCurrentCam = nullptr;
+        }
+    }
+
+    if (strcmp(name, "MediaCtlConfig") == 0) {
+        LOGXML("@%s %s, mInMediaCtlCfg is set to false", __func__, name);
+        profiles->mInMediaCtlCfg = false;
+    }
+
+    if (strcmp(name, "StaticMetadata") == 0) {
+        LOGXML("@%s %s, mInStaticMetadata is set to false", __func__, name);
+        profiles->mInStaticMetadata = false;
+    }
+
+    if (strcmp(name, "Common") == 0)
+        profiles->mCurrentDataField = FIELD_INVALID;
+}
+
+/**
+* Get available sensors.
+*
+* The function will read libcamhal_profile.xml, and parse out all of sensors.
+* Then those sensors will be checked if it exists in mediaEntity, if it exists,
+* we put it in availableSensors.
+* In libcamhal_profile.xml it should have the following requirements:
+* 1. <availableSensors value="ov8856-wf-2,ov2740-uf-0,ov2740-wf-2"/>
+*     The value is "'camera name'-wf/uf-'CSI port number'".
+*     For example: camera name is "ov8856". Sensor's sink entity name is
+*      "Intel IPU6 CSI-2 2" and it is word facing. The value is ov8856-wf-2.
+* 2. <platform value="IPU6"/> the platform value must be uppercase letter.
+*
+*/
+std::vector<std::string> CameraParser::getAvailableSensors(const std::string &ipuName,
+                                                        const std::vector<std::string> &sensorsList)
+{
+    LOGXML("@%s, ipuName:%s", __func__, ipuName.c_str());
+
+    /* if the string doesn't contain -wf- or -uf-, it needn't be parsed */
+    if ((sensorsList[0].find("-wf-") == string::npos) &&
+        (sensorsList[0].find("-uf-") == string::npos)) {
+        return sensorsList;
+    }
+
+    // sensor's sink entity name prefix:Intel IPU6 CSI-2 2
+    std::string sensorSinkName = "Intel ";
+    sensorSinkName.append(ipuName);
+    sensorSinkName.append(" CSI-2 ");
+
+    std::vector<string> availableSensors;
+    for (auto& sensor : sensorsList) {
+        std::string srcSensor = sensor;
+        std::string portNum = srcSensor.substr(srcSensor.find_last_of('-') + 1);
+        std::string sensorSinkNameTmp = sensorSinkName;
+        sensorSinkNameTmp.append(portNum);
+        std::string sensorName = srcSensor.substr(0, srcSensor.find_first_of('-'));
+
+        bool ret = mMC->checkAvailableSensor(sensorName, sensorSinkNameTmp);
+        if (ret) {
+            std::string sensorNameTmp = srcSensor.substr(0, srcSensor.find_last_of('-'));
+            availableSensors.push_back(sensorNameTmp);
+            mAvailableSensor[sensorNameTmp] = sensorSinkNameTmp;
+            LOGXML("@%s, The availabel sensor name:%s, sensorSinkNameTmp:%s",
+                   __func__, sensorNameTmp.c_str(), sensorSinkNameTmp.c_str());
+        }
+    }
+
+    return availableSensors;
+}
+
+/**
+ * Get camera configuration from xml file
+ *
+ * The function will read the xml configuration file firstly.
+ * Then it will parse out the camera settings.
+ * The camera setting is stored inside this CameraParser class.
+ *
+ */
+void CameraParser::getProfileDataFromXmlFile(void)
+{
+    LOGXML("@%s", __func__);
+
+    // Get common data from libcamhal_profile.xml
+    int ret = getDataFromXmlFile(LIBCAMHAL_PROFILE_NAME);
+    CheckError(ret != OK, VOID_VALUE, "Failed to get libcamhal profile data frome %s", LIBCAMHAL_PROFILE_NAME);
+
+    // According to sensor name to get sensor data
+    LOGXML("The kinds of sensor is %zu", mStaticCfg->mCommonConfig.availableSensors.size());
+    vector<string> allSensors = getAvailableSensors(mStaticCfg->mCommonConfig.ipuName,
+                                                    mStaticCfg->mCommonConfig.availableSensors);
+
+    if (allSensors.size() == 0) {
+        LOGW("The style of libcamhal_profile is too old, please switch it as soon as possible !!!");
+        return;
+    }
+
+    for (auto sensor : allSensors) {
+        string sensorName = "sensors/";
+        sensorName.append(sensor);
+        sensorName.append(".xml");
+        int ret = getDataFromXmlFile(sensorName);
+        CheckError(ret != OK, VOID_VALUE, "Failed to get sensor profile data frome %s", sensorName.c_str());
+    }
+}
+
+void CameraParser::dumpSensorInfo(void)
+{
+    LOGXML("@%s, line%d, for sensors settings==================", __func__, __LINE__);
+    LOGXML("@%s, line%d, sensor number:%d", __func__, __LINE__, getSensorNum());
+    for (unsigned i = 0; i < getSensorNum(); i++) {
+        LOGXML("@%s, line%d, i:%d", __func__, __LINE__, i);
+        LOGXML("@%s, line%d, mCameras[%d].sensorName:%s", __func__, __LINE__, i, mStaticCfg->mCameras[i].sensorName.c_str());
+        LOGXML("@%s, line%d, mCameras[%d].mISysFourcc:%d", __func__, __LINE__, i, mStaticCfg->mCameras[i].mISysFourcc);
+
+        stream_array_t supportedConfigs;
+        mStaticCfg->mCameras[i].mCapability.getSupportedStreamConfig(supportedConfigs);
+        for (size_t j = 0; j < supportedConfigs.size(); j++) {
+            LOGXML("@%s, line%d, mCameras[%d]: format:%d size(%dx%d) field:%d", __func__, __LINE__,
+                i, supportedConfigs[j].format, supportedConfigs[j].width,
+                supportedConfigs[j].height, supportedConfigs[j].field);
+        }
+
+        for (unsigned j = 0; j < mStaticCfg->mCameras[i].mSupportedISysFormat.size(); j++) {
+            LOGXML("@%s, line%d, mCameras[%d].mSupportedISysFormat:%d", __func__, __LINE__, i, mStaticCfg->mCameras[i].mSupportedISysFormat[j]);
+        }
+
+        // dump the media controller mapping table for supportedStreamConfig
+        LOGXML("The media controller mapping table size: %zu", mStaticCfg->mCameras[i].mStreamToMcMap.size());
+        for (auto& pool : mStaticCfg->mCameras[i].mStreamToMcMap) {
+            int mcId = pool.first;
+            stream_array_t &mcMapVector = pool.second;
+            LOGXML("mcId: %d, the supportedStreamConfig vector size: %zu", mcId, mcMapVector.size());
+        }
+
+        // dump the media controller information
+        LOGXML("============Format Configuration==================");
+        for (unsigned j = 0; j < mStaticCfg->mCameras[i].mMediaCtlConfs.size(); j++) {
+            const MediaCtlConf* mc = &mStaticCfg->mCameras[i].mMediaCtlConfs[j];
+            for (unsigned k = 0; k < mc->links.size(); k++) {
+                const McLink* link = &mc->links[k];
+                LOGXML("       link src %s [%d:%d] ==> %s [%d:%d] enable %d", link->srcEntityName.c_str(), link->srcEntity, link->srcPad, link->sinkEntityName.c_str(), link->sinkEntity, link->sinkPad, link->enable);
+            }
+            for (unsigned k = 0; k < mc->ctls.size(); k++) {
+                const McCtl* ctl = &mc->ctls[k];
+                LOGXML("       Ctl %s [%d] cmd %s [0x%08x] value %d", ctl->entityName.c_str(), ctl->entity, ctl->ctlName.c_str(), ctl->ctlCmd, ctl->ctlValue);
+            }
+            for (unsigned k = 0; k < mc->formats.size(); k++) {
+                const McFormat* format = &mc->formats[k];
+                if (format->formatType == FC_FORMAT)
+                    LOGXML("       format %s [%d:%d] [%dx%d] %s", format->entityName.c_str(), format->entity, format->pad, format->width, format->height, CameraUtils::pixelCode2String(format->pixelCode));
+                else if (format->formatType == FC_SELECTION)
+                    LOGXML("       select %s [%d:%d] selCmd: %d [%d, %d] [%dx%d]", format->entityName.c_str(), format->entity, format->pad, format->selCmd, format->top, format->left, format->width, format->height);
+            }
+        }
+        LOGXML("============End of Format Configuration===========");
+    }
+
+    LOGXML("@%s, line%d, for common settings==================", __func__, __LINE__);
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/platformdata/CameraParser.h b/camera/hal/intel/ipu6/src/platformdata/CameraParser.h
new file mode 100644
index 000000000000..4a0d42cf6f0e
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/CameraParser.h
@@ -0,0 +1,127 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ *\file CameraParser.h
+ *
+ * parser for the camera xml configuration file
+ *
+ * This file calls the libexpat ditectly. The libexpat is one xml parser.
+ * It will parse the camera configuration out firstly.
+ * Then other module can call the methods of it to get the real configuration.
+ *
+ */
+
+#pragma once
+
+#include <unordered_map>
+#include "PlatformData.h"
+#include "ParserBase.h"
+#include "CameraMetadata.h"
+
+namespace icamera {
+
+/**
+ * \class CameraParser
+ *
+ * This class is used to parse the camera configuration file.
+ * The configuration file is xml format.
+ * This class will use the expat lib to do the xml parser.
+ */
+class CameraParser : public ParserBase {
+public:
+    CameraParser(MediaControl *mc, PlatformData::StaticCfg *cfg);
+    ~CameraParser();
+
+    unsigned getSensorNum(void) {return mSensorNum;};
+
+private:
+    DISALLOW_COPY_AND_ASSIGN(CameraParser);
+
+private:
+    PlatformData::StaticCfg *mStaticCfg;
+
+    enum DataField {
+        FIELD_INVALID = 0,
+        FIELD_SENSOR,
+        FIELD_COMMON
+    } mCurrentDataField;
+    int mSensorNum;
+    int mCurrentSensor;
+    std::string mI2CBus;
+    std::string mCsiPort;
+    std::unordered_map<std::string, std::string> mAvailableSensor;
+    PlatformData::StaticCfg::CameraInfo *pCurrentCam;
+    bool mInMediaCtlCfg;
+    bool mInStaticMetadata;
+    MediaControl* mMC;
+    CameraMetadata mMetadata;
+
+    long* mMetadataCache;
+    static const int mMetadataCacheSize = 4096;
+
+    void startParseElement(void *userData, const char *name, const char **atts);
+    void endParseElement(void *userData, const char *name);
+
+    static void parseSizesList(const char *sizesStr, std::vector <camera_resolution_t> &sizes);
+    static int getSupportedFormat(const char* str, std::vector <int>& supportedFormat);
+    static int parsePair(const char *str, int *first, int *second, char delim, char **endptr = nullptr);
+
+    std::vector<std::string> getAvailableSensors(const std::string &ipuName,
+                                                 const std::vector<std::string> &sensorsList);
+    void getProfileDataFromXmlFile(void);
+    void getSensorDataFromXmlFile(void);
+    void checkField(CameraParser *profiles, const char *name, const char **atts);
+
+    void handleSensor(CameraParser *profiles, const char *name, const char **atts);
+    void handleCommon(CameraParser *profiles, const char *name, const char **atts);
+
+    void parseStreamConfig(const char* src, stream_array_t& configs);
+    void parseSupportedFeatures(const char* src, camera_features_list_t& features);
+    void parseSupportedIspControls(const char* src, std::vector<uint32_t>& features);
+    int parseSupportedVideoStabilizationMode(const char* str, camera_video_stabilization_list_t &supportedModes);
+    int parseSupportedAeMode(const char* str, std::vector <camera_ae_mode_t> &supportedModes);
+    int parseSupportedAfMode(const char* str, std::vector <camera_af_mode_t> &supportedModes);
+    int parseSupportedAntibandingMode(const char* str, std::vector <camera_antibanding_mode_t> &supportedModes);
+    int parseSupportedAeParamRange(const char* src, std::vector<int>& scenes,
+                                   std::vector<float>& minValues, std::vector<float>& maxValues);
+
+// parse the media controller configuration in xml, the MediaControl MUST be run before the parser to run.
+    void handleMediaCtlCfg(CameraParser *profiles, const char *name, const char **atts);
+    void handleStaticMetaData(CameraParser *profiles, const char *name, const char **atts);
+    void handleGenericStaticMetaData(const char *name, const char *src);
+    void parseMediaCtlConfigElement(CameraParser *profiles, const char *name, const char **atts);
+    void storeMcMappForConfig(int mcId, stream_t streamCfg);
+    void parseLinkElement(CameraParser *profiles, const char *name, const char **atts);
+    void parseRouteElement(CameraParser *profiles, const char *name, const char **atts);
+    void parseControlElement(CameraParser *profiles, const char *name, const char **atts);
+    void parseSelectionElement(CameraParser *profiles, const char *name, const char **atts);
+    void parseFormatElement(CameraParser *profiles, const char *name, const char **atts);
+    void parseVideoElement(CameraParser *profiles, const char *name, const char **atts);
+    void parseOutputElement(CameraParser *profiles, const char *name, const char **atts);
+    void parseMultiExpRange(const char* src);
+
+    int parseSupportedTuningConfig(const char *str, std::vector <TuningConfig> &config);
+    int parseLardTags(const char *str, std::vector <LardTagConfig> &lardTags);
+
+    void dumpSensorInfo(void);
+
+    void parseOutputMap(const char *str, std::vector<UserToPslOutputMap> &outputMap);
+
+    std::string replaceStringInXml(CameraParser *profiles, const char *value);
+};
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/platformdata/CameraTypes.h b/camera/hal/intel/ipu6/src/platformdata/CameraTypes.h
new file mode 100644
index 000000000000..61119ac701f9
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/CameraTypes.h
@@ -0,0 +1,228 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <string>
+#include <vector>
+#include <map>
+#include "Parameters.h"
+
+namespace icamera {
+
+/**
+ * Use to link buffer producers and consumers
+ */
+enum Port {
+    MAIN_PORT = 0,
+    SECOND_PORT,
+    THIRD_PORT,
+    FORTH_PORT,
+    INVALID_PORT
+};
+
+enum {
+    FACING_BACK = 0,
+    FACING_FRONT = 1,
+};
+
+enum {
+    ORIENTATION_0 = 0,
+    ORIENTATION_90 = 90,
+    ORIENTATION_180 = 180,
+    ORIENTATION_270 = 270,
+};
+
+enum {
+    LENS_VCM_HW = 0,
+    LENS_NONE_HW
+};
+
+enum {
+    AUTO_SWITCH_FULL = 0,
+    AUTO_SWITCH_PSYS
+};
+
+enum {
+    SENSOR_EXPOSURE_SINGLE = 0,        /* sensor is single exposure */
+    SENSOR_FIX_EXPOSURE_RATIO,         /* Fix exposure ratio between long and short exposure */
+    SENSOR_RELATIVE_MULTI_EXPOSURES,   /* AE output exposures are converted to Shutter and
+                                          Readout time, then set to sensor driver */
+    SENSOR_MULTI_EXPOSURES,            /* Multi-exposures are set to sensor driver directly */
+    SENSOR_DUAL_EXPOSURES_DCG_AND_VS   /* Dual-exposure and multiple gains, i.e. DCG + VS */
+};
+
+enum {
+    SENSOR_GAIN_NONE = 0,
+    SENSOR_MULTI_DG_AND_CONVERTION_AG,  /* Multi-DigitalGain and convertion AnalogGain are set
+                                           to sensor driver */
+    ISP_DG_AND_SENSOR_DIRECT_AG,        /* All digital gain is passed to ISP */
+    SENSOR_MULTI_DG_AND_DIRECT_AG       /* Multi analog and digital gains, i.e. DCG */
+};
+
+/**
+ * This definition is used to distinguish different camera running mode, like video or still.
+ */
+typedef enum {
+    TUNING_MODE_VIDEO,
+    TUNING_MODE_VIDEO_ULL,
+    TUNING_MODE_VIDEO_CUSTOM_AIC,
+    TUNING_MODE_VIDEO_LL,
+    TUNING_MODE_VIDEO_REAR_VIEW,
+    TUNING_MODE_VIDEO_HITCH_VIEW,
+    TUNING_MODE_STILL_CAPTURE,
+    TUNING_MODE_MAX
+} TuningMode;
+
+/*
+ * The mapping algorithm for sensor digital gain
+ */
+typedef enum {
+    SENSOR_DG_TYPE_NONE,
+    SENSOR_DG_TYPE_X,           //linear relationship, gain = n*value (value: register value, n: ratio)
+    SENSOR_DG_TYPE_2_X,         //exponential relationship, gain = 2 ^ value (value: register value)
+} SensorDgType;
+
+typedef enum {
+    MORPH_TABLE = 0,
+    IMG_TRANS
+} DvsType;
+
+// Note AUTO is not real config mode in the HAL.
+typedef camera_stream_configuration_mode_t ConfigMode;
+
+typedef struct TuningConfig {
+    ConfigMode configMode;                 /*!< configMode is internal usage to select AIQ and
+                                                Pipeline. AUTO is not real config mode. */
+    TuningMode tuningMode;                 /*!< tuningMode is used to define user cases,
+                                                like video or still. */
+    std::string aiqbName;                       /*!< special aiqb name corresponding with TuningMode */
+} TuningConfig;
+
+typedef struct {
+    /*!< tuningMode is used to define user cases, like video or still. */
+    TuningMode tuningMode;
+    unsigned int cmcTag;
+    unsigned int aiqTag;
+    unsigned int ispTag;
+    unsigned int othersTag;
+} LardTagConfig;
+
+typedef struct {
+    uint32_t horizontal_crop_offset;
+    uint32_t vertical_crop_offset;
+    uint32_t cropped_image_width;
+    uint32_t cropped_image_height;
+    uint32_t horizontal_scaling_numerator;
+    uint32_t horizontal_scaling_denominator;
+    uint32_t vertical_scaling_numerator;
+    uint32_t vertical_scaling_denominator;
+} SensorFrameParams;
+
+enum ExecutorNotifyPolicy {
+    POLICY_FRAME_FIRST = 0,
+    POLICY_STATS_FIRST,
+    POLICY_INVALID,
+};
+
+struct ExecutorPolicy {
+    std::string exeName;
+    ExecutorNotifyPolicy notifyPolicy;
+    std::vector<std::string> pgList;
+    std::vector<int> opModeList;
+    std::vector<int> cyclicFeedbackRoutineList;
+    std::vector<int> cyclicFeedbackDelayList;
+    ExecutorPolicy() : notifyPolicy(POLICY_FRAME_FIRST) {}
+};
+
+struct ExecutorDepth {
+    std::vector<std::string> bundledExecutors;
+    std::vector<int> depths;
+};
+
+struct PolicyConfig {
+    int graphId;
+    std::string policyDescription;
+    std::vector<ExecutorPolicy> pipeExecutorVec;
+    std::vector<std::string> exclusivePgs;
+    std::vector<ExecutorDepth> bundledExecutorDepths;
+    bool enableBundleInSdv;
+
+    PolicyConfig() { graphId = -1; enableBundleInSdv = true; }
+};
+
+struct CommonConfig {
+    float xmlVersion;
+    std::string ipuName;
+    std::vector<std::string> availableSensors;
+    bool isGpuTnrEnabled;
+    int cameraNumber;
+
+    CommonConfig() { xmlVersion = 1.0; isGpuTnrEnabled = false; cameraNumber = 0;}
+};
+
+struct OBSetting {
+    ConfigMode configMode;
+    int top;
+    int left;
+    int sectionHeight;
+    int interleaveStep;
+};
+
+struct ExpRange {
+    int min;
+    int max;
+    int step;
+    int lowerBound;
+    int upperBound;
+};
+
+/**
+ * Multi exposure range information
+*/
+struct MultiExpRange {
+    camera_resolution_t Resolution;
+    ExpRange SHS1;
+    ExpRange RHS1;
+    ExpRange SHS2;
+    ExpRange RHS2;
+    ExpRange SHS3;
+};
+
+struct UserToPslOutputMap {
+    camera_resolution_t User;
+    camera_resolution_t Psl;
+};
+
+struct FrameInfo {
+    FrameInfo() {}
+    int mWidth = 0;
+    int mHeight = 0;
+    int mFormat = 0;
+    int mStride = 0;
+    int mBpp = 0;
+};
+typedef std::map<Port, FrameInfo> FrameInfoPortMap;
+
+/**
+ * Indicate if the graph setting for video and still is coupled or dispersed
+ */
+typedef enum {
+    COUPLED,
+    DISPERSED,
+} GraphSettingType;
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/platformdata/ParserBase.cpp b/camera/hal/intel/ipu6/src/platformdata/ParserBase.cpp
new file mode 100644
index 000000000000..92c3f76ed0d8
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/ParserBase.cpp
@@ -0,0 +1,165 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#define LOG_TAG "ParserBase"
+
+#include <memory>
+#include <expat.h>
+#include <string.h>
+
+#include "ParserBase.h"
+#include "iutils/CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+namespace icamera {
+
+ParserBase::ParserBase()
+{
+    LOGXML("@%s", __func__);
+}
+
+const char *ParserBase::skipWhiteSpace(const char *src)
+{
+    while (*src == '\n' || *src == '\t' || *src == ' ' || *src == '\v' || *src == '\r' || *src == '\f') {
+        src++;
+    }
+    return src;
+}
+
+int ParserBase::parseXmlParameterToChar(const char *str, unsigned char *table)
+{
+    CheckError(str == nullptr, -1, "@%s, str is nullptr", __func__);
+
+    int index = 0;
+    char *savePtr, *tablePtr;
+    int sz = strlen(str);
+    char src[sz + 1];
+    MEMCPY_S(src, sz, str, sz);
+    src[sz] = '\0';
+
+    tablePtr = strtok_r(src, ",", &savePtr);
+    while (tablePtr) {
+        table[index] = atoi(tablePtr);
+        if (savePtr != nullptr)
+            savePtr = const_cast<char *>(skipWhiteSpace(savePtr));
+        index++;
+        tablePtr = strtok_r(nullptr, ",", &savePtr);
+    }
+    return 0;
+}
+
+/* template function need the function ptr */
+std::string ParserBase::convertCharToString(const char *str)
+{
+    return str;
+}
+
+void ParserBase::startElement(void *userData, const char *name, const char **atts)
+{
+    ParserBaseCallBack *profiles = reinterpret_cast<ParserBaseCallBack*>(userData);
+    profiles->startParseElement(userData, name, atts);
+}
+
+void ParserBase::endElement(void *userData, const char *name)
+{
+    ParserBaseCallBack *profiles = reinterpret_cast<ParserBaseCallBack*>(userData);
+    profiles->endParseElement(userData, name);
+}
+
+int ParserBase::parseXmlFile(const std::string &xmlFile)
+{
+    int ret = UNKNOWN_ERROR;
+    int done;
+    FILE *fp = nullptr;
+    int bufSize = 4 * 1024;  // parse 4k data every time
+
+    CheckError(xmlFile.empty(), UNKNOWN_ERROR, "xmlFile is empty");
+
+    LOGXML("@%s, parsing profile: %s", __func__, xmlFile.c_str());
+
+    fp = ::fopen(xmlFile.c_str(), "r");
+    CheckError(nullptr == fp, UNKNOWN_ERROR, "@%s, line:%d, Can not open profile file %s in read mode, fp is nullptr",
+          __func__, __LINE__, xmlFile.c_str());
+
+    std::unique_ptr<char[]>pBuf(new char[bufSize]);
+    XML_Parser parser = ::XML_ParserCreate(nullptr);
+    if (nullptr == parser) {
+        LOGE("@%s, line:%d, parser is nullptr", __func__, __LINE__);
+        goto exit;
+    }
+
+    ::XML_SetUserData(parser, this);
+    ::XML_SetElementHandler(parser, startElement, endElement);
+
+    do {
+        int len = (int)::fread(pBuf.get(), 1, bufSize, fp);
+        if (!len) {
+            if (ferror(fp)) {
+                clearerr(fp);
+                goto exit;
+            }
+        }
+        done = len < bufSize;
+        if (XML_Parse(parser, (const char *)pBuf.get(), len, done) == XML_STATUS_ERROR) {
+            LOGE("@%s, line:%d, XML_Parse error", __func__, __LINE__);
+            goto exit;
+        }
+    } while (!done);
+    ret = OK;
+
+exit:
+    if (parser)
+        ::XML_ParserFree(parser);
+    if (fp)
+    ::fclose(fp);
+
+    return ret;
+}
+
+void ParserBase::getAvaliableXmlFile(const std::vector<const char *> &avaliableXmlFiles,
+                                     std::string &xmlFile)
+{
+    struct stat st;
+    for (auto xml : avaliableXmlFiles) {
+        int ret = stat(xml, &st);
+        if (ret == 0) {
+            xmlFile = xml;
+            return;
+        }
+    }
+}
+
+int ParserBase::getDataFromXmlFile(std::string fileName)
+{
+    LOGXML("@%s", __func__);
+    CheckError(fileName.size() == 0, UNKNOWN_ERROR, "file name is null");
+
+    std::string curFolderFileName = std::string("./") + fileName;
+    std::string sysFolderFileName = PlatformData::getCameraCfgPath() + fileName;
+    const std::vector <const char *> profiles = {
+        curFolderFileName.c_str(),
+        sysFolderFileName.c_str()
+    };
+
+    std::string chosenXmlFile;
+    getAvaliableXmlFile(profiles, chosenXmlFile);
+    CheckError(chosenXmlFile.empty(), UNKNOWN_ERROR, "%s is not found in: %s or %s",
+          fileName.c_str(), curFolderFileName.c_str(), sysFolderFileName.c_str());
+
+    return parseXmlFile(chosenXmlFile);
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/platformdata/ParserBase.h b/camera/hal/intel/ipu6/src/platformdata/ParserBase.h
new file mode 100644
index 000000000000..c6ccaff7dd38
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/ParserBase.h
@@ -0,0 +1,102 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ *\file ParserBase.h
+ *
+ * parser for the camera xml configuration file, this is a basic class.
+ *
+ * This file calls the libexpat ditectly. The libexpat is one xml parser.
+ * It will parse the camera configuration out firstly.
+ * Then other module can call the methods of it to get the real configuration.
+ *
+ */
+
+#pragma once
+
+#include "PlatformData.h"
+
+namespace icamera {
+
+class ParserBaseCallBack {
+public:
+    virtual ~ParserBaseCallBack() {}
+    virtual void startParseElement(void *userData, const char *name, const char **atts){};
+    virtual void endParseElement(void *userData, const char *name){};
+};
+
+class ParserBase : public ParserBaseCallBack {
+public:
+    ParserBase();
+    virtual ~ParserBase() {}
+
+private:
+    // prevent copy constructor and assignment operator
+    DISALLOW_COPY_AND_ASSIGN(ParserBase);
+
+protected:
+    /**
+     * Get an avaliable xml file
+     *
+     * Find the first avaliable xml file.
+     *
+     * \param[in] const vector<char *>& allAvaliableXmlFiles: all avaliable xml files list.
+     * \param[out] string& xmlFile: to store a avaliable xml file
+     */
+    void getAvaliableXmlFile(const std::vector<const char *> &avaliableXmlFiles,
+                             std::string &xmlFile);
+
+    /**
+     * Get camera configuration from xml file
+     *
+     * The function will read the xml configuration file firstly.
+     * Then it will parse out the camera settings.
+     * The camera setting is stored inside this CameraProfiles class.
+     */
+    int getDataFromXmlFile(std::string fileName);
+
+    int parseXmlFile(const std::string &xmlFile);
+    const char* skipWhiteSpace(const char *src);
+    int parseXmlParameterToChar(const char *str, unsigned char *table);
+
+    static void startElement(void *userData, const char *name, const char **atts);
+    static void endElement(void *userData, const char *name);
+    static std::string convertCharToString(const char *str);
+
+    template<typename T>
+    int parseXmlConvertStrings(const char *str, std::vector<T> &vectorT,
+                               T (*parseXmlString)(const char *)) {
+        CheckError(str == nullptr || parseXmlString == nullptr, -1, "@%s, input parameter is nullptr", __func__);
+
+        int sz = strlen(str);
+        char src[sz + 1];
+        MEMCPY_S(src, sz, str, sz);
+        src[sz] = '\0';
+
+        char *savePtr = nullptr;
+        char *cfgName = strtok_r(src, ",", &savePtr);
+        while(cfgName) {
+            vectorT.push_back(parseXmlString(cfgName));
+            if (savePtr != nullptr)
+                savePtr = const_cast<char*>(skipWhiteSpace(savePtr));
+            cfgName = strtok_r(nullptr, ",", &savePtr);
+        }
+
+        return 0;
+    }
+};
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/platformdata/PlatformData.cpp b/camera/hal/intel/ipu6/src/platformdata/PlatformData.cpp
new file mode 100644
index 000000000000..b42f436b218d
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/PlatformData.cpp
@@ -0,0 +1,1369 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "PlatformData"
+
+#include <sys/sysinfo.h>
+#include <math.h>
+#include <memory>
+
+#include "iutils/CameraLog.h"
+
+#include "PlatformData.h"
+#include "CameraParser.h"
+#include "PolicyParser.h"
+#include "ParameterHelper.h"
+
+#include "gc/GraphConfigManager.h"
+
+using std::string;
+using std::vector;
+
+namespace icamera {
+PlatformData *PlatformData::sInstance = nullptr;
+Mutex  PlatformData::sLock;
+
+PlatformData* PlatformData::getInstance()
+{
+    AutoMutex lock(sLock);
+    if (sInstance == nullptr) {
+        sInstance = new PlatformData();
+    }
+
+    return sInstance;
+}
+
+void PlatformData::releaseInstance()
+{
+    AutoMutex lock(sLock);
+    LOG1("@%s", __func__);
+
+    if (sInstance) {
+        delete sInstance;
+        sInstance = nullptr;
+    }
+}
+
+PlatformData::PlatformData()
+{
+    LOG1("@%s", __func__);
+    MediaControl *mc = MediaControl::getInstance();
+    mc->initEntities();
+
+    CameraParser CameraParser(mc, &mStaticCfg);
+    PolicyParser PolicyParser(&mStaticCfg);
+}
+
+PlatformData::~PlatformData() {
+    LOG1("@%s", __func__);
+
+    releaseGraphConfigNodes();
+
+    MediaControl::getInstance()->clearEntities();
+    MediaControl::releaseInstance();
+
+    for (size_t i = 0; i < mAiqInitData.size(); i++) {
+        delete mAiqInitData[i];
+    }
+
+    mAiqInitData.clear();
+}
+
+int PlatformData::init() {
+    LOG2("@%s", __func__);
+
+    getInstance()->parseGraphFromXmlFile();
+
+    StaticCfg *staticCfg = &(getInstance()->mStaticCfg);
+    for (size_t i = 0; i < staticCfg->mCameras.size(); i++) {
+        AiqInitData* aiqInitData =
+            new AiqInitData(staticCfg->mCameras[i].sensorName,
+                            getCameraCfgPath(),
+                            staticCfg->mCameras[i].mSupportedTuningConfig,
+                            staticCfg->mCameras[i].mLardTagsConfig,
+                            staticCfg->mCameras[i].mNvmDirectory,
+                            staticCfg->mCameras[i].mMaxNvmDataSize);
+        getInstance()->mAiqInitData.push_back(aiqInitData);
+    }
+
+    return OK;
+}
+
+/**
+ * Read graph descriptor and settings from configuration files.
+ *
+ * The resulting graphs represend all possible graphs for given sensor, and
+ * they are stored in capinfo structure.
+ */
+void PlatformData::parseGraphFromXmlFile() {
+    std::shared_ptr<GraphConfig> graphConfig = std::make_shared<GraphConfig>();
+
+    // Assuming that PSL section from profiles is already parsed, and number
+    // of cameras is known.
+    graphConfig->addCustomKeyMap();
+    for (size_t i = 0; i < getInstance()->mStaticCfg.mCameras.size(); ++i) {
+        const string &fileName = getInstance()->mStaticCfg.mCameras[i].mGraphSettingsFile;
+        if (fileName.empty()) {
+            continue;
+        }
+
+        LOGXML("Using graph setting file:%s for camera:%zu", fileName.c_str(), i);
+        int ret  = graphConfig->parse(i, fileName.c_str());
+        CheckError(ret != OK, VOID_VALUE, "Could not read graph config file for camera %zu", i);
+    }
+}
+
+void PlatformData::releaseGraphConfigNodes()
+{
+    std::shared_ptr<GraphConfig> graphConfig = std::make_shared<GraphConfig>();
+    graphConfig->releaseGraphNodes();
+    for (uint8_t cameraId = 0; cameraId < mStaticCfg.mCameras.size(); cameraId++) {
+        IGraphConfigManager::releaseInstance(cameraId);
+    }
+}
+
+const char* PlatformData::getSensorName(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].sensorName.c_str();
+}
+
+const char* PlatformData::getSensorDescription(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].sensorDescription.c_str();
+}
+
+const char* PlatformData::getLensName(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mLensName.c_str();
+}
+
+int PlatformData::getLensHwType(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mLensHwType;
+}
+
+int PlatformData::getDVSType(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mDVSType;
+}
+
+bool PlatformData::getISYSCompression(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mISYSCompression;
+}
+
+bool PlatformData::getPSACompression(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mPSACompression;
+}
+
+bool PlatformData::getOFSCompression(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mOFSCompression;
+}
+
+int PlatformData::getCITMaxMargin(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mCITMaxMargin;
+}
+
+bool PlatformData::isEnableAIQ(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mEnableAIQ;
+}
+
+bool PlatformData::isEnableLtmThread(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mEnableLtmThread;
+}
+
+bool PlatformData::isFaceAeEnabled(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mFaceAeEnabled;
+}
+
+int PlatformData::faceEngineRunningInterval(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mFaceEngineRunningInterval;
+}
+
+int PlatformData::faceEngineRunningIntervalNoFace(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mFaceEngineRunningIntervalNoFace;
+}
+
+bool PlatformData::isFaceEngineSyncRunning(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mFaceEngineRunningSync;
+}
+
+bool PlatformData::isDvsSupported(int cameraId)
+{
+    camera_video_stabilization_list_t videoStabilizationList;
+    Parameters* param = &getInstance()->mStaticCfg.mCameras[cameraId].mCapability;
+    param->getSupportedVideoStabilizationMode(videoStabilizationList);
+
+    bool supported = false;
+    for (auto it : videoStabilizationList) {
+        if (it == VIDEO_STABILIZATION_MODE_ON) {
+            supported = true;
+        }
+    }
+
+    LOG2("@%s, dvs supported:%d", __func__, supported);
+    return supported;
+}
+
+bool PlatformData::psysAlignWithSof(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mPsysAlignWithSof;
+}
+
+bool PlatformData::psysBundleWithAic(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mPsysBundleWithAic;
+}
+
+bool PlatformData::swProcessingAlignWithIsp(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mSwProcessingAlignWithIsp;
+}
+
+bool PlatformData::isUsingSensorDigitalGain(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mUseSensorDigitalGain;
+}
+
+bool PlatformData::isUsingIspDigitalGain(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mUseIspDigitalGain;
+}
+
+bool PlatformData::isNeedToPreRegisterBuffer(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mNeedPreRegisterBuffers;
+}
+
+int PlatformData::getAutoSwitchType(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mAutoSwitchType;
+}
+
+bool PlatformData::isEnableFrameSyncCheck(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mFrameSyncCheckEnabled;
+}
+
+bool PlatformData::isEnableDefog(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mEnableLtmDefog;
+}
+
+int PlatformData::getExposureNum(int cameraId, bool multiExposure)
+{
+    if (multiExposure) {
+        return getInstance()->mStaticCfg.mCameras[cameraId].mSensorExposureNum;
+    }
+
+    int exposureNum = 1;
+
+    return exposureNum;
+}
+
+bool PlatformData::isLtmEnabled(int cameraId)
+{
+
+    return getInstance()->mStaticCfg.mCameras[cameraId].mLtmEnabled;
+}
+
+int PlatformData::getSensorExposureType(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mSensorExposureType;
+}
+
+int PlatformData::getSensorGainType(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mSensorGainType;
+}
+
+bool PlatformData::isSkipFrameOnSTR2MMIOErr(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mSkipFrameV4L2Error;
+}
+
+unsigned int PlatformData::getInitialSkipFrame(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mInitialSkipFrame;
+}
+
+unsigned int PlatformData::getMaxRawDataNum(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mMaxRawDataNum;
+}
+
+bool PlatformData::getTopBottomReverse(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mTopBottomReverse;
+}
+
+bool PlatformData::isPsysContinueStats(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mPsysContinueStats;
+}
+
+unsigned int PlatformData::getPreferredBufQSize(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mPreferredBufQSize;
+}
+
+unsigned int PlatformData::getPipeSwitchDelayFrame(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mPipeSwitchDelayFrame;
+}
+
+int PlatformData::getLtmGainLag(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mLtmGainLag;
+}
+
+int PlatformData::getMaxSensorDigitalGain(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mMaxSensorDigitalGain;
+}
+
+SensorDgType PlatformData::sensorDigitalGainType(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mSensorDgType;
+}
+
+int PlatformData::getExposureLag(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mExposureLag;
+}
+
+int PlatformData::getGainLag(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mGainLag;
+}
+
+PolicyConfig* PlatformData::getExecutorPolicyConfig(int graphId)
+{
+    size_t i = 0;
+    PlatformData::StaticCfg *cfg = &getInstance()->mStaticCfg;
+
+    for (i = 0; i < cfg->mPolicyConfig.size(); i++) {
+        if (graphId == cfg->mPolicyConfig[i].graphId) {
+            return &(cfg->mPolicyConfig[i]);
+        }
+    }
+
+    LOGW("Couldn't find the executor policy for graphId(%d), please check xml file", graphId);
+    return nullptr;
+}
+
+int PlatformData::numberOfCameras()
+{
+    return getInstance()->mStaticCfg.mCameras.size();
+}
+
+int PlatformData::getXmlCameraNumber()
+{
+    return getInstance()->mStaticCfg.mCommonConfig.cameraNumber;
+}
+
+MediaCtlConf *PlatformData::getMediaCtlConf(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mCurrentMcConf;
+}
+
+int PlatformData::getCameraInfo(int cameraId, camera_info_t& info)
+{
+    // TODO correct the version info
+    info.device_version = 1;
+    info.facing = getInstance()->mStaticCfg.mCameras[cameraId].mFacing;
+    info.orientation= getInstance()->mStaticCfg.mCameras[cameraId].mOrientation;
+    info.name = getSensorName(cameraId);
+    info.description = getSensorDescription(cameraId);
+    info.capability = &getInstance()->mStaticCfg.mCameras[cameraId].mCapability;
+    return OK;
+}
+
+bool PlatformData::isFeatureSupported(int cameraId, camera_features feature)
+{
+    camera_features_list_t features;
+    getInstance()->mStaticCfg.mCameras[cameraId].mCapability.getSupportedFeatures(features);
+
+    if (features.empty()) {
+        return false;
+    }
+    for (auto& item : features) {
+        if (item == feature) {
+            return true;
+        }
+    }
+    return false;
+}
+
+bool PlatformData::isSupportedStream(int cameraId, const stream_t& conf)
+{
+    int width = conf.width;
+    int height = conf.height;
+    int format = conf.format;
+    int field = conf.field;
+
+    stream_array_t availableConfigs;
+    getInstance()->mStaticCfg.mCameras[cameraId].mCapability.getSupportedStreamConfig(availableConfigs);
+    bool sameConfigFound = false;
+    for (auto const& config : availableConfigs) {
+        if (config.format == format && config.field == field
+                && config.width == width && config.height == height) {
+            sameConfigFound = true;
+            break;
+        }
+    }
+
+    return sameConfigFound;
+}
+
+void PlatformData::getSupportedISysSizes(int cameraId, vector <camera_resolution_t>& resolutions)
+{
+    resolutions = getInstance()->mStaticCfg.mCameras[cameraId].mSupportedISysSizes;
+}
+
+bool PlatformData::getSupportedISysFormats(int cameraId, vector <int>& formats)
+{
+    formats = getInstance()->mStaticCfg.mCameras[cameraId].mSupportedISysFormat;
+
+    return true;
+}
+
+int PlatformData::getISysFormat(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mISysFourcc;
+}
+
+/**
+ * The ISYS format is determined by the steps below:
+ * 1. Try to use the specified format in media control config if it exists.
+ * 2. If the given format is supported by ISYS, then use it.
+ * 3. Use the first supported format if still could not find an appropriate one.
+ */
+void PlatformData::selectISysFormat(int cameraId, int format)
+{
+    MediaCtlConf *mc = getMediaCtlConf(cameraId);
+    if (mc != nullptr && mc->format != -1) {
+        getInstance()->mStaticCfg.mCameras[cameraId].mISysFourcc = mc->format;
+    } else if (isISysSupportedFormat(cameraId, format)) {
+        getInstance()->mStaticCfg.mCameras[cameraId].mISysFourcc = format;
+    } else {
+        // Set the first one in support list to default Isys output.
+        vector <int> supportedFormat =
+            getInstance()->mStaticCfg.mCameras[cameraId].mSupportedISysFormat;
+        getInstance()->mStaticCfg.mCameras[cameraId].mISysFourcc = supportedFormat[0];
+    }
+}
+
+/**
+ * The media control config is determined by the steps below:
+ * 1. Check if can get one from the given MC ID.
+ * 2. And then, try to use ConfigMode to find matched one.
+ * 3. Use stream config to get a corresponding mc id, and then get the config by id.
+ * 4. Return nullptr if still could not find an appropriate one.
+ */
+void PlatformData::selectMcConf(int cameraId, stream_t stream, ConfigMode mode, int mcId)
+{
+    if (!isIsysEnabled(cameraId)) return;
+
+    const StaticCfg::CameraInfo& pCam = getInstance()->mStaticCfg.mCameras[cameraId];
+
+    MediaCtlConf* mcConfig = getMcConfByMcId(pCam, mcId);
+    if (!mcConfig) {
+        mcConfig = getMcConfByConfigMode(pCam, stream, mode);
+    }
+
+    if (!mcConfig) {
+        mcConfig = getMcConfByStream(pCam, stream);
+    }
+
+    getInstance()->mStaticCfg.mCameras[cameraId].mCurrentMcConf = mcConfig;
+
+    if (!mcConfig) {
+        LOGE("No matching McConf: cameraId %d, configMode %d, mcId %d", cameraId, mode, mcId);
+    }
+}
+
+/*
+ * Find the MediaCtlConf based on the given MC id.
+ */
+MediaCtlConf* PlatformData::getMcConfByMcId(const StaticCfg::CameraInfo& cameraInfo, int mcId)
+{
+    if (mcId == -1) {
+        return nullptr;
+    }
+
+    for (auto& mc : cameraInfo.mMediaCtlConfs) {
+        if (mcId == mc.mcId) {
+            return (MediaCtlConf*)&mc;
+        }
+    }
+
+    return nullptr;
+}
+
+/*
+ * Find the MediaCtlConf based on MC id in mStreamToMcMap.
+ */
+MediaCtlConf* PlatformData::getMcConfByStream(const StaticCfg::CameraInfo& cameraInfo,
+                                              const stream_t& stream)
+{
+    int mcId = -1;
+    for (auto& table : cameraInfo.mStreamToMcMap) {
+        for(auto& config : table.second) {
+            if (config.format == stream.format && config.field == stream.field
+                    && config.width == stream.width && config.height == stream.height) {
+                mcId = table.first;
+                break;
+            }
+        }
+        if (mcId != -1) {
+            break;
+        }
+    }
+
+    return getMcConfByMcId(cameraInfo, mcId);
+}
+
+/*
+ * Find the MediaCtlConf based on operation mode and stream info.
+ */
+MediaCtlConf* PlatformData::getMcConfByConfigMode(const StaticCfg::CameraInfo& cameraInfo,
+                                                  const stream_t& stream, ConfigMode mode)
+{
+    for (auto& mc : cameraInfo.mMediaCtlConfs) {
+        for (auto& cfgMode : mc.configMode) {
+            if (mode != cfgMode) continue;
+
+            int outputWidth = mc.outputWidth;
+            int outputHeight = mc.outputHeight;
+            int stride = CameraUtils::getStride(mc.format, mc.outputWidth);
+            bool sameStride = (stride == CameraUtils::getStride(mc.format, stream.width));
+            /*
+             * outputWidth and outputHeight is 0 means the ISYS output size
+             * is dynamic, we don't need to check if it matches with stream config.
+             */
+            if ((outputWidth == 0 && outputHeight == 0 ) ||
+                ((stream.width == outputWidth || sameStride)
+                && stream.height == outputHeight)) {
+                return (MediaCtlConf*)&mc;
+            }
+        }
+    }
+
+    return nullptr;
+}
+
+/*
+ * Check if video node is enabled via camera Id and video node type.
+ */
+bool PlatformData::isVideoNodeEnabled(int cameraId, VideoNodeType type) {
+    MediaCtlConf *mc = getMediaCtlConf(cameraId);
+    if (!mc) return false;
+
+    for(auto const& nd : mc->videoNodes) {
+        if (type == nd.videoNodeType) {
+            return true;
+        }
+    }
+    return false;
+}
+
+bool PlatformData::isISysSupportedFormat(int cameraId, int format)
+{
+    vector <int> supportedFormat;
+    getSupportedISysFormats(cameraId, supportedFormat);
+
+    for (auto const fmt : supportedFormat) {
+        if (format == fmt)
+            return true;
+    }
+    return false;
+}
+
+bool PlatformData::isISysSupportedResolution(int cameraId, camera_resolution_t resolution)
+{
+    vector <camera_resolution_t> res;
+    getSupportedISysSizes(cameraId, res);
+
+    for (auto const& size : res) {
+        if (resolution.width == size.width && resolution.height== size.height)
+            return true;
+    }
+
+    return false;
+}
+
+int PlatformData::getISysRawFormat(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mISysRawFormat;
+}
+
+stream_t PlatformData::getISysOutputByPort(int cameraId, Port port)
+{
+    stream_t config;
+    CLEAR(config);
+
+    MediaCtlConf *mc = PlatformData::getMediaCtlConf(cameraId);
+    CheckError(!mc, config, "Invalid media control config.");
+
+    for (const auto& output : mc->outputs) {
+        if (output.port == port) {
+            config.format  = output.v4l2Format;
+            config.width   = output.width;
+            config.height  = output.height;
+            break;
+        }
+    }
+
+    return config;
+}
+
+bool PlatformData::isAiqdEnabled(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mEnableAiqd;
+}
+
+int PlatformData::getFormatByDevName(int cameraId, const string& devName, McFormat& format)
+{
+    MediaCtlConf *mc = getMediaCtlConf(cameraId);
+    CheckError(!mc, BAD_VALUE, "getMediaCtlConf returns nullptr, cameraId:%d", cameraId);
+
+    for (auto &fmt : mc->formats) {
+        if (fmt.formatType == FC_FORMAT && devName == fmt.entityName) {
+            format = fmt;
+            return OK;
+        }
+    }
+
+    LOGE("Failed to find DevName for cameraId: %d, devname: %s", cameraId, devName.c_str());
+    return BAD_VALUE;
+}
+
+int PlatformData::getVideoNodeNameByType(int cameraId, VideoNodeType videoNodeType, string& videoNodeName)
+{
+    MediaCtlConf *mc = getMediaCtlConf(cameraId);
+    CheckError(!mc, BAD_VALUE, "getMediaCtlConf returns nullptr, cameraId:%d", cameraId);
+
+    for(auto const& nd : mc->videoNodes) {
+        if (videoNodeType == nd.videoNodeType) {
+            videoNodeName = nd.name;
+            return OK;
+        }
+    }
+
+    LOGE("failed to find video note name for cameraId: %d", cameraId);
+    return BAD_VALUE;
+}
+
+int PlatformData::getDevNameByType(int cameraId, VideoNodeType videoNodeType, string& devName)
+{
+    if (!isIsysEnabled(cameraId)) return OK;
+
+    MediaCtlConf *mc = getMediaCtlConf(cameraId);
+    bool isSubDev = false;
+
+    switch (videoNodeType) {
+        case VIDEO_PIXEL_ARRAY:
+        case VIDEO_PIXEL_BINNER:
+        case VIDEO_PIXEL_SCALER:
+        {
+            isSubDev = true;
+            // For sensor subdevices are fixed and sensor HW may be initialized before configure,
+            // the first MediaCtlConf is used to find sensor subdevice name.
+            PlatformData::StaticCfg::CameraInfo *pCam = &getInstance()->mStaticCfg.mCameras[cameraId];
+            mc = &pCam->mMediaCtlConfs[0];
+            break;
+        }
+        case VIDEO_ISYS_RECEIVER_BACKEND:
+        case VIDEO_ISYS_RECEIVER:
+        {
+            isSubDev = true;
+            break;
+        }
+        default:
+            break;
+    }
+
+    CheckError(!mc, NAME_NOT_FOUND, "failed to get MediaCtlConf, videoNodeType %d", videoNodeType);
+
+    for(auto& nd : mc->videoNodes) {
+        if (videoNodeType == nd.videoNodeType) {
+            string tmpDevName;
+            CameraUtils::getDeviceName(nd.name.c_str(), tmpDevName, isSubDev);
+            if (!tmpDevName.empty()) {
+                devName = tmpDevName;
+                LOG2("@%s, Found DevName. cameraId: %d, get video node: %s, devname: %s",
+                      __func__, cameraId, nd.name.c_str(), devName.c_str());
+                return OK;
+            } else {
+                // Use default device name if cannot find it
+                if (isSubDev)
+                    devName = "/dev/v4l-subdev1";
+                else
+                    devName = "/dev/video5";
+                LOGE("Failed to find DevName for cameraId: %d, get video node: %s, devname: %s",
+                      cameraId, nd.name.c_str(), devName.c_str());
+                return NAME_NOT_FOUND;
+            }
+        }
+    }
+
+    LOG1("Failed to find devname for cameraId: %d, use default setting instead", cameraId);
+    return NAME_NOT_FOUND;
+}
+
+/**
+ * The ISYS best resolution is determined by the steps below:
+ * 1. If the resolution is specified in MediaCtlConf, then use it.
+ * 2. Try to find the exact matched one in ISYS supported resolutions.
+ * 3. Try to find the same ratio resolution.
+ * 4. If still couldn't get one, then use the biggest one.
+ */
+camera_resolution_t PlatformData::getISysBestResolution(int cameraId, int width,
+                                                        int height, int field)
+{
+    LOG1("@%s, width:%d, height:%d", __func__, width, height);
+
+    // Skip for interlace, we only support by-pass in interlaced mode
+    if (field == V4L2_FIELD_ALTERNATE) {
+        return {width, height};
+    }
+
+    MediaCtlConf *mc = getMediaCtlConf(cameraId);
+    // The isys output size is fixed if outputWidth/outputHeight != 0
+    // So we use it to as the ISYS resolution.
+    if (mc != nullptr && mc->outputWidth != 0 && mc->outputHeight != 0) {
+        return {mc->outputWidth, mc->outputHeight};
+    }
+
+    const float RATIO_TOLERANCE = 0.05f; // Supported aspect ratios that are within RATIO_TOLERANCE
+    const float kTargetRatio = (float)width / height;
+
+    vector <camera_resolution_t> res;
+    // The supported resolutions are saved in res with ascending order(small -> bigger)
+    getSupportedISysSizes(cameraId, res);
+
+    // Try to find out the same resolution in the supported isys resolution list
+    // if it couldn't find out the same one, then use the bigger one which is the same ratio
+    for (auto const& size : res) {
+        if (width <= size.width && height <= size.height &&
+            fabs((float)size.width/size.height - kTargetRatio) < RATIO_TOLERANCE) {
+            LOG1("@%s: Found the best ISYS resoltoution (%d)x(%d)", __func__,
+                 size.width, size.height);
+            return {size.width, size.height};
+        }
+    }
+
+    // If it still couldn't find one, then use the biggest one in the supported list.
+    LOG1("@%s: ISYS resolution not found, used the biggest one: (%d)x(%d)",
+         __func__, res.back().width, res.back().height);
+    return {res.back().width, res.back().height};
+}
+
+bool PlatformData::isIsysEnabled(int cameraId)
+{
+    if (getInstance()->mStaticCfg.mCameras[cameraId].mMediaCtlConfs.empty()) {
+        return false;
+    }
+    return true;
+}
+
+int PlatformData::calculateFrameParams(int cameraId, SensorFrameParams& sensorFrameParams)
+{
+    if (!isIsysEnabled(cameraId)) {
+        LOG2("%s, no mc, just use default from xml", __func__);
+        vector <camera_resolution_t> res;
+        getSupportedISysSizes(cameraId, res);
+
+        CheckError(res.empty(), BAD_VALUE, "Supported ISYS resolutions are not configured.");
+        sensorFrameParams = {0, 0, static_cast<uint32_t>(res[0].width),
+                             static_cast<uint32_t>(res[0].height), 1, 1, 1, 1};
+
+        return OK;
+    }
+
+    CLEAR(sensorFrameParams);
+
+    uint32_t width = 0;
+    uint32_t horizontalOffset = 0;
+    uint32_t horizontalBinNum = 1;
+    uint32_t horizontalBinDenom = 1;
+    uint32_t horizontalBin = 1;
+
+    uint32_t height = 0;
+    uint32_t verticalOffset = 0;
+    uint32_t verticalBinNum = 1;
+    uint32_t verticalBinDenom = 1;
+    uint32_t verticalBin = 1;
+
+    /**
+     * For this function, it may be called without configuring stream
+     * in some UT cases, the mc is nullptr at this moment. So we need to
+     * get one default mc to calculate frame params.
+     */
+    MediaCtlConf *mc = PlatformData::getMediaCtlConf(cameraId);
+    if (mc == nullptr) {
+        PlatformData::StaticCfg::CameraInfo *pCam = &getInstance()->mStaticCfg.mCameras[cameraId];
+        mc = &pCam->mMediaCtlConfs[0];
+    }
+
+    bool pixArraySizeFound = false;
+    for (auto const& current : mc->formats) {
+        if (!pixArraySizeFound && current.width > 0 && current.height > 0) {
+            width = current.width;
+            height = current.height;
+            pixArraySizeFound = true;
+            LOG2("%s: active pixel array H=%d, W=%d", __func__, height, width);
+            //Setup initial sensor frame params.
+            sensorFrameParams.horizontal_crop_offset += horizontalOffset;
+            sensorFrameParams.vertical_crop_offset += verticalOffset;
+            sensorFrameParams.cropped_image_width = width;
+            sensorFrameParams.cropped_image_height = height;
+            sensorFrameParams.horizontal_scaling_numerator = horizontalBinNum;
+            sensorFrameParams.horizontal_scaling_denominator = horizontalBinDenom;
+            sensorFrameParams.vertical_scaling_numerator = verticalBinNum;
+            sensorFrameParams.vertical_scaling_denominator = verticalBinDenom;
+        }
+
+        if (current.formatType != FC_SELECTION) {
+            continue;
+        }
+
+        if (current.selCmd == V4L2_SEL_TGT_CROP) {
+
+            width = current.width * horizontalBin;
+            horizontalOffset = current.left * horizontalBin;
+            height = current.height * verticalBin;
+            verticalOffset = current.top * verticalBin;
+
+            LOG2("%s: crop (binning factor: hor/vert:%d,%d)"
+                  , __func__, horizontalBin, verticalBin);
+
+            LOG2("%s: crop left = %d, top = %d, width = %d height = %d",
+                  __func__, horizontalOffset, verticalOffset, width, height);
+
+        } else if (current.selCmd == V4L2_SEL_TGT_COMPOSE) {
+            if (width == 0 || height == 0) {
+                LOGE("Invalid XML configuration, no pixel array width/height when handling compose, skip.");
+                return BAD_VALUE;
+            }
+            if (current.width == 0 || current.height == 0) {
+                LOGW("%s: Invalid XML configuration for TGT_COMPOSE,"
+                     "0 value detected in width or height", __func__);
+                return BAD_VALUE;
+            } else {
+                LOG2("%s: Compose width %d/%d, height %d/%d", __func__, width, current.width,
+                    height, current.height);
+                // the scale factor should be float, so multiple numerator and denominator
+                // with coefficient to indicate float factor
+                const int SCALE_FACTOR_COEF = 10;
+                horizontalBin = width / current.width;
+                horizontalBinNum = width * SCALE_FACTOR_COEF / current.width;
+                horizontalBinDenom = SCALE_FACTOR_COEF;
+                verticalBin = height / current.height;
+                verticalBinNum = height * SCALE_FACTOR_COEF / current.height;
+                verticalBinDenom = SCALE_FACTOR_COEF;
+            }
+
+            LOG2("%s: COMPOSE horizontal bin factor=%d, (%d/%d)",
+                  __func__, horizontalBin, horizontalBinNum, horizontalBinDenom);
+            LOG2("%s: COMPOSE vertical bin factor=%d, (%d/%d)",
+                  __func__, verticalBin, verticalBinNum, verticalBinDenom);
+        } else {
+            LOGW("%s: Target for selection is not CROP neither COMPOSE!", __func__);
+            continue;
+        }
+
+        sensorFrameParams.horizontal_crop_offset += horizontalOffset;
+        sensorFrameParams.vertical_crop_offset += verticalOffset;
+        sensorFrameParams.cropped_image_width = width;
+        sensorFrameParams.cropped_image_height = height;
+        sensorFrameParams.horizontal_scaling_numerator = horizontalBinNum;
+        sensorFrameParams.horizontal_scaling_denominator = horizontalBinDenom;
+        sensorFrameParams.vertical_scaling_numerator = verticalBinNum;
+        sensorFrameParams.vertical_scaling_denominator = verticalBinDenom;
+    }
+
+    return OK;
+
+}
+
+void PlatformData::getSupportedTuningConfig(int cameraId, vector <TuningConfig> &configs)
+{
+    configs = getInstance()->mStaticCfg.mCameras[cameraId].mSupportedTuningConfig;
+}
+
+bool PlatformData::usePsys(int cameraId, int format)
+{
+    if (getInstance()->mStaticCfg.mCameras[cameraId].mSupportedTuningConfig.empty()) {
+        LOG1("@%s, the tuning config in xml does not exist", __func__);
+        return false;
+    }
+
+    if (getInstance()->mStaticCfg.mCameras[cameraId].mPSysFormat.empty()) {
+        LOG1("@%s, the psys supported format does not exist", __func__);
+        return false;
+    }
+
+    for (auto &psys_fmt : getInstance()->mStaticCfg.mCameras[cameraId].mPSysFormat) {
+        if (format == psys_fmt)
+            return true;
+    }
+
+    LOGW("%s, No matched format found, but expected format:%s", __func__,
+        CameraUtils::pixelCode2String(format));
+
+    return false;
+}
+
+int PlatformData::getConfigModesByOperationMode(int cameraId, uint32_t operationMode, vector <ConfigMode> &configModes)
+{
+    if (operationMode == CAMERA_STREAM_CONFIGURATION_MODE_END) {
+        LOG2("%s: operationMode was invalid operation mode", __func__);
+        return INVALID_OPERATION;
+    }
+
+    CheckError(getInstance()->mStaticCfg.mCameras[cameraId].mSupportedTuningConfig.empty(), INVALID_OPERATION,
+          "@%s, the tuning config in xml does not exist", __func__);
+
+    if (operationMode == CAMERA_STREAM_CONFIGURATION_MODE_AUTO) {
+        if (getInstance()->mStaticCfg.mCameras[cameraId].mConfigModesForAuto.empty()) {
+            // Use the first config mode as default for auto
+            configModes.push_back(getInstance()->mStaticCfg.mCameras[cameraId].mSupportedTuningConfig[0].configMode);
+            LOG2("%s: add config mode %d for operation mode %d", __func__, configModes[0], operationMode);
+        } else {
+            configModes = getInstance()->mStaticCfg.mCameras[cameraId].mConfigModesForAuto;
+        }
+    } else {
+        for (auto &cfg : getInstance()->mStaticCfg.mCameras[cameraId].mSupportedTuningConfig) {
+            if (operationMode == (uint32_t)cfg.configMode) {
+                configModes.push_back(cfg.configMode);
+                LOG2("%s: add config mode %d for operation mode %d", __func__, cfg.configMode, operationMode);
+            }
+        }
+    }
+
+    if (configModes.size() > 0) return OK;
+    LOGW("%s, configure number %zu, operationMode %x, cameraId %d", __func__,
+            configModes.size(), operationMode, cameraId);
+    return INVALID_OPERATION;
+}
+
+int PlatformData::getTuningModeByConfigMode(int cameraId, ConfigMode configMode,
+                                            TuningMode& tuningMode)
+{
+    CheckError(getInstance()->mStaticCfg.mCameras[cameraId].mSupportedTuningConfig.empty(),
+          INVALID_OPERATION, "the tuning config in xml does not exist");
+
+    for (auto &cfg : getInstance()->mStaticCfg.mCameras[cameraId].mSupportedTuningConfig) {
+        LOG2("%s, tuningMode %d, configMode %x", __func__, cfg.tuningMode, cfg.configMode);
+        if (cfg.configMode == configMode) {
+            tuningMode = cfg.tuningMode;
+            return OK;
+        }
+    }
+
+    LOGW("%s, configMode %x, cameraId %d, no tuningModes", __func__, configMode, cameraId);
+    return INVALID_OPERATION;
+}
+
+int PlatformData::getTuningConfigByConfigMode(int cameraId, ConfigMode mode, TuningConfig &config)
+{
+    CheckError(getInstance()->mStaticCfg.mCameras[cameraId].mSupportedTuningConfig.empty(), INVALID_OPERATION,
+          "@%s, the tuning config in xml does not exist.", __func__);
+
+    for (auto &cfg : getInstance()->mStaticCfg.mCameras[cameraId].mSupportedTuningConfig) {
+        if (cfg.configMode == mode) {
+            config = cfg;
+            return OK;
+        }
+    }
+
+    LOGW("%s, configMode %x, cameraId %d, no TuningConfig", __func__, mode, cameraId);
+    return INVALID_OPERATION;
+}
+
+int PlatformData::getStreamIdByConfigMode(int cameraId, ConfigMode configMode)
+{
+    std::map<int, int> modeMap = getInstance()->mStaticCfg.mCameras[cameraId].mConfigModeToStreamId;
+    return modeMap.find(configMode) == modeMap.end() ? -1 : modeMap[configMode];
+}
+
+int PlatformData::getMaxRequestsInflight(int cameraId)
+{
+    int inflight = getInstance()->mStaticCfg.mCameras[cameraId].mMaxRequestsInflight;
+    if (inflight <= 0) {
+        inflight = isEnableAIQ(cameraId) ? 4 : MAX_BUFFER_COUNT;
+    }
+
+    return inflight;
+}
+
+bool PlatformData::getGraphConfigNodes(int cameraId)
+{
+    return !(getInstance()->mStaticCfg.mCameras[cameraId].mGraphSettingsFile.empty());
+}
+
+GraphSettingType PlatformData::getGraphSettingsType(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mGraphSettingsType;
+}
+
+camera_yuv_color_range_mode_t PlatformData::getYuvColorRangeMode(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mYuvColorRangeMode;
+}
+
+ia_binary_data* PlatformData::getAiqd(int cameraId, TuningMode mode)
+{
+    CheckError(cameraId >= static_cast<int>(getInstance()->mAiqInitData.size()), nullptr,
+               "@%s, bad cameraId:%d", __func__, cameraId);
+
+    AiqInitData* aiqInitData = getInstance()->mAiqInitData[cameraId];
+    return aiqInitData->getAiqd(mode);
+}
+
+void PlatformData::saveAiqd(int cameraId, TuningMode tuningMode, const ia_binary_data& data)
+{
+    CheckError(cameraId >= static_cast<int>(getInstance()->mAiqInitData.size()), VOID_VALUE,
+               "@%s, bad cameraId:%d", __func__, cameraId);
+
+    AiqInitData* aiqInitData = getInstance()->mAiqInitData[cameraId];
+    aiqInitData->saveAiqd(tuningMode, data);
+}
+
+// load cpf when tuning file (.aiqb) is available
+int PlatformData::getCpfAndCmc(int cameraId,
+                               ia_binary_data* ispData,
+                               ia_binary_data* aiqData,
+                               ia_binary_data* otherData,
+                               uintptr_t* cmcHandle,
+                               TuningMode mode,
+                               ia_cmc_t** cmcData)
+{
+    CheckError(cameraId >= static_cast<int>(getInstance()->mAiqInitData.size()) ||
+               cameraId >= MAX_CAMERA_NUMBER, BAD_VALUE, "@%s, bad cameraId:%d",
+               __func__, cameraId);
+    CheckError(getInstance()->mStaticCfg.mCameras[cameraId].mSupportedTuningConfig.empty(),
+               INVALID_OPERATION, "@%s, the tuning config in xml does not exist", __func__);
+
+    AiqInitData* aiqInitData = getInstance()->mAiqInitData[cameraId];
+    return aiqInitData->getCpfAndCmc(ispData, aiqData, otherData, cmcHandle, mode, cmcData);
+}
+
+bool PlatformData::isCSIBackEndCapture(int cameraId)
+{
+    bool isCsiBECapture = false;
+    MediaCtlConf *mc = getMediaCtlConf(cameraId);
+    CheckError(!mc, false, "getMediaCtlConf returns nullptr, cameraId:%d", cameraId);
+
+    for(const auto& node : mc->videoNodes) {
+        if (node.videoNodeType == VIDEO_GENERIC &&
+                (node.name.find("BE capture") != string::npos ||
+                 node.name.find("BE SOC capture") != string::npos)) {
+            isCsiBECapture = true;
+            break;
+        }
+    }
+
+    return isCsiBECapture;
+}
+
+bool PlatformData::isCSIFrontEndCapture(int cameraId)
+{
+    bool isCsiFeCapture = false;
+    MediaCtlConf *mc = getMediaCtlConf(cameraId);
+    CheckError(!mc, false, "getMediaCtlConf returns nullptr, cameraId:%d", cameraId);
+
+    for(const auto& node : mc->videoNodes) {
+        if (node.videoNodeType == VIDEO_GENERIC &&
+                (node.name.find("CSI-2") != string::npos ||
+                 node.name.find("TPG") != string::npos)) {
+            isCsiFeCapture = true;
+            break;
+        }
+    }
+    return isCsiFeCapture;
+}
+
+bool PlatformData::isTPGReceiver(int cameraId)
+{
+    bool isTPGCapture = false;
+    MediaCtlConf *mc = getMediaCtlConf(cameraId);
+    CheckError(!mc, false, "getMediaCtlConf returns nullptr, cameraId:%d", cameraId);
+
+    for(const auto& node : mc->videoNodes) {
+        if (node.videoNodeType == VIDEO_ISYS_RECEIVER &&
+                (node.name.find("TPG") != string::npos)) {
+            isTPGCapture = true;
+            break;
+        }
+    }
+    return isTPGCapture;
+}
+
+int PlatformData::getSupportAeExposureTimeRange(int cameraId, camera_scene_mode_t sceneMode,
+                                                camera_range_t& etRange)
+{
+    vector<camera_ae_exposure_time_range_t> ranges;
+    getInstance()->mStaticCfg.mCameras[cameraId].mCapability.getSupportedAeExposureTimeRange(ranges);
+
+    if (ranges.empty())
+        return NAME_NOT_FOUND;
+
+    for (auto& item : ranges) {
+        if (item.scene_mode == sceneMode) {
+            etRange = item.et_range;
+            return OK;
+        }
+    }
+    return NAME_NOT_FOUND;
+}
+
+int PlatformData::getSupportAeGainRange(int cameraId, camera_scene_mode_t sceneMode,
+                                        camera_range_t& gainRange)
+{
+    vector<camera_ae_gain_range_t> ranges;
+    getInstance()->mStaticCfg.mCameras[cameraId].mCapability.getSupportedAeGainRange(ranges);
+
+    if(ranges.empty()) {
+        return NAME_NOT_FOUND;
+    }
+
+    for (auto& item : ranges) {
+        if (item.scene_mode == sceneMode) {
+            gainRange = item.gain_range;
+            return OK;
+        }
+    }
+    return NAME_NOT_FOUND;
+}
+
+bool PlatformData::isUsingCrlModule(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mUseCrlModule;
+}
+
+vector<MultiExpRange> PlatformData::getMultiExpRanges(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mMultiExpRanges;
+}
+
+camera_resolution_t *PlatformData::getPslOutputForRotation(int width, int height, int cameraId)
+{
+    CheckError(getInstance()->mStaticCfg.mCameras[cameraId].mOutputMap.empty(), nullptr,
+          "@%s, cameraId: %d, there isn't pslOutputMapForRotation field in xml.", __func__, cameraId);
+
+    vector<UserToPslOutputMap> &outputMap = getInstance()->mStaticCfg.mCameras[cameraId].mOutputMap;
+    for (auto & map : outputMap) {
+        if (width == map.User.width && height == map.User.height) {
+            LOG2("cameraId: %d, find the psl output resoltion(%d, %d) for %dx%d",
+                  cameraId, map.Psl.width, map.Psl.height, map.User.width, map.User.height);
+            return &map.Psl;
+        }
+    }
+
+    return nullptr;
+}
+
+bool PlatformData::isTestPatternSupported(int cameraId)
+{
+    return !getInstance()->mStaticCfg.mCameras[cameraId].mTestPatternMap.empty();
+}
+
+int32_t PlatformData::getSensorTestPattern(int cameraId, int32_t mode)
+{
+    CheckError(getInstance()->mStaticCfg.mCameras[cameraId].mTestPatternMap.empty(), -1,
+          "@%s, cameraId: %d, mTestPatternMap is empty!", __func__, cameraId);
+    auto testPatternMap = getInstance()->mStaticCfg.mCameras[cameraId].mTestPatternMap;
+
+    if (testPatternMap.find(mode) == testPatternMap.end()) {
+        LOGW("Test pattern %d wasn't found in configuration file, return -1", mode);
+        return -1;
+    }
+    return testPatternMap[mode];
+}
+
+ia_binary_data *PlatformData::getNvm(int cameraId)
+{
+    CheckError(cameraId >= static_cast<int>(getInstance()->mAiqInitData.size()), nullptr,
+               "@%s, bad cameraId:%d", __func__, cameraId);
+
+    return getInstance()->mAiqInitData[cameraId]->getNvm();
+}
+
+camera_coordinate_system_t PlatformData::getActivePixelArray(int cameraId)
+{
+    camera_coordinate_system_t arraySize;
+    CLEAR(arraySize);
+
+    getInstance()->mStaticCfg.mCameras[cameraId].mCapability.getSensorActiveArraySize(arraySize);
+
+    return {arraySize.left, arraySize.top, arraySize.right, arraySize.bottom};
+}
+
+string PlatformData::getCameraCfgPath()
+{
+    char* p = getenv("CAMERA_CFG_PATH");
+
+    return p? string(p) : string(CAMERA_DEFAULT_CFG_PATH);
+}
+
+string PlatformData::getGraphDescFilePath()
+{
+    return PlatformData::getCameraCfgPath() + string(CAMERA_GRAPH_DESCRIPTOR_FILE);
+}
+
+string PlatformData::getGraphSettingFilePath()
+{
+    return PlatformData::getCameraCfgPath() + string(CAMERA_GRAPH_SETTINGS_DIR);
+}
+
+int PlatformData::getSensorDigitalGain(int cameraId, float realDigitalGain)
+{
+    int sensorDg = 0;
+    int maxSensorDg = PlatformData::getMaxSensorDigitalGain(cameraId);
+
+    if (PlatformData::sensorDigitalGainType(cameraId) == SENSOR_DG_TYPE_2_X) {
+        int index = 0;
+        while (pow(2, index) <= realDigitalGain) {
+            sensorDg = index;
+            index++;
+        }
+        sensorDg = CLIP(sensorDg, maxSensorDg, 0);
+    } else {
+        LOGE("%s, don't support the sensor digital gain type: %d",
+                __func__, PlatformData::sensorDigitalGainType(cameraId));
+    }
+
+    return sensorDg;
+}
+
+float PlatformData::getIspDigitalGain(int cameraId, float realDigitalGain)
+{
+    float ispDg = 1.0f;
+    int sensorDg = getSensorDigitalGain(cameraId, realDigitalGain);
+
+    if (PlatformData::sensorDigitalGainType(cameraId) == SENSOR_DG_TYPE_2_X) {
+        ispDg = realDigitalGain / pow(2, sensorDg);
+        ispDg = CLIP(ispDg, ispDg, 1.0);
+    } else {
+        LOGE("%s, don't support the sensor digital gain type: %d",
+                __func__, PlatformData::sensorDigitalGainType(cameraId));
+    }
+
+    return ispDg;
+}
+
+int PlatformData::saveMakernoteData(int cameraId, camera_makernote_mode_t makernoteMode,
+                                    int64_t sequence)
+{
+    CheckError(cameraId >= static_cast<int>(getInstance()->mAiqInitData.size()), BAD_VALUE,
+               "@%s, bad cameraId:%d", __func__, cameraId);
+
+    return getInstance()->mAiqInitData[cameraId]->saveMakernoteData(makernoteMode, sequence);
+}
+
+void* PlatformData::getMknHandle(int cameraId)
+{
+    CheckError(cameraId >= static_cast<int>(getInstance()->mAiqInitData.size()), nullptr,
+               "@%s, bad cameraId:%d", __func__, cameraId);
+
+    return getInstance()->mAiqInitData[cameraId]->getMknHandle();
+}
+
+void PlatformData::updateMakernoteTimeStamp(int cameraId, int64_t sequence, uint64_t timestamp)
+{
+    CheckError(cameraId >= static_cast<int>(getInstance()->mAiqInitData.size()), VOID_VALUE,
+               "@%s, bad cameraId:%d", __func__, cameraId);
+
+    return getInstance()->mAiqInitData[cameraId]->updateMakernoteTimeStamp(sequence, timestamp);
+}
+
+void  PlatformData::acquireMakernoteData(int cameraId, uint64_t timestamp, Parameters *param)
+{
+    CheckError(cameraId >= static_cast<int>(getInstance()->mAiqInitData.size()), VOID_VALUE,
+               "@%s, bad cameraId:%d", __func__, cameraId);
+
+    return getInstance()->mAiqInitData[cameraId]->acquireMakernoteData(timestamp, param);
+}
+
+int PlatformData::getScalerInfo(int cameraId, int32_t streamId,
+                                float *scalerWidth, float *scalerHeight)
+{
+    if (getInstance()->mStaticCfg.mCameras[cameraId].mScalerInfo.empty()) {
+        *scalerWidth = 1.0;
+        *scalerHeight = 1.0;
+        return OK;
+    }
+
+    for (auto &scalerInfo : getInstance()->mStaticCfg.mCameras[cameraId].mScalerInfo) {
+        LOG2("%s, streamId %d, scalerWidth %f, scalerHeight %f", __func__, scalerInfo.streamId,
+             scalerInfo.scalerWidth, scalerInfo.scalerHeight);
+        if (scalerInfo.streamId == streamId) {
+            *scalerWidth = scalerInfo.scalerWidth;
+            *scalerHeight = scalerInfo.scalerHeight;
+            break;
+        }
+    }
+
+    return OK;
+}
+
+void  PlatformData::setScalerInfo(int cameraId, std::vector<IGraphType::ScalerInfo> scalerInfo)
+{
+    for (auto &scalerInfoInput : scalerInfo) {
+        bool flag = false;
+        for (auto &scalerInfoTmp : getInstance()->mStaticCfg.mCameras[cameraId].mScalerInfo) {
+            if (scalerInfoInput.streamId == scalerInfoTmp.streamId) {
+                scalerInfoTmp.scalerWidth = scalerInfoInput.scalerWidth;
+                scalerInfoTmp.scalerHeight = scalerInfoInput.scalerHeight;
+                flag = true;
+                break;
+            }
+        }
+        if (!flag) {
+            getInstance()->mStaticCfg.mCameras[cameraId].mScalerInfo.push_back(scalerInfoInput);
+        }
+    }
+}
+
+bool PlatformData::isGpuTnrEnabled()
+{
+    return getInstance()->mStaticCfg.mCommonConfig.isGpuTnrEnabled;
+}
+
+int PlatformData::getVideoStreamNum(int cameraId)
+{
+    return getInstance()->mStaticCfg.mCameras[cameraId].mVideoStreamNum;
+}
+
+bool PlatformData::isUsingGpuAlgo()
+{
+    bool enabled = false;
+    enabled |= isGpuTnrEnabled();
+    return enabled;
+}
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/platformdata/PlatformData.h b/camera/hal/intel/ipu6/src/platformdata/PlatformData.h
new file mode 100644
index 000000000000..a40f2c0a4d4e
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/PlatformData.h
@@ -0,0 +1,1159 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <limits.h>
+#include <v4l2_device.h>
+
+#include <vector>
+#include <string>
+#include <map>
+#include <unordered_map>
+
+#include "ICamera.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+#include "CameraTypes.h"
+#include "Parameters.h"
+#include "AiqInitData.h"
+#include "MediaControl.h"
+#include "IGraphConfig.h"
+
+namespace icamera {
+
+#define RESOLUTION_1_3MP_WIDTH  1280
+#define RESOLUTION_1_3MP_HEIGHT 960
+#define RESOLUTION_1080P_WIDTH  1920
+#define RESOLUTION_1080P_HEIGHT 1080
+#define RESOLUTION_720P_WIDTH   1280
+#define RESOLUTION_720P_HEIGHT  720
+#define RESOLUTION_VGA_WIDTH    640
+#define RESOLUTION_VGA_HEIGHT   480
+
+#define MAX_BUFFER_COUNT (10)
+#define MAX_STREAM_NUMBER   5
+#define DEFAULT_VIDEO_STREAM_NUM 2
+#define MAX_WEIGHT_GRID_SIDE_LEN 1024
+
+#define FACE_ENGINE_DEFAULT_RUNNING_INTERVAL 1
+
+/* Max number of the RAW buffer number is 32.
+ * Max number size of the pipeline depth is 6.
+ * Max setting count should be larger than raw buffer number + pipeline depth.
+ */
+#define MAX_SETTING_COUNT 40
+#define CAMERA_PORT_NAME "CSI-2"
+
+#define MAX_CAMERA_NUMBER 2
+#define CAMERA_CACHE_DIR "/var/cache/camera/"
+#define CAMERA_DEFAULT_CFG_PATH "/etc/camera/"
+#define CAMERA_GRAPH_DESCRIPTOR_FILE "gcss/graph_descriptor.xml"
+#define CAMERA_GRAPH_SETTINGS_DIR "gcss/"
+
+class GraphConfigNodes;
+class PlatformData {
+private:
+    //Prevent to create multiple instances
+    PlatformData();
+    ~PlatformData();
+
+public:
+    class StaticCfg {
+    public:
+        StaticCfg() {
+            mCameras.clear();
+        };
+        ~StaticCfg() {}; // not release resource by design
+
+        /**
+         * Camera feature info that is specific to camera id
+         */
+        class CameraInfo {
+        public:
+            CameraInfo() :
+                sensorName(""),
+                sensorDescription("unset"),
+                mLensName(""),
+                mLensHwType(LENS_NONE_HW),
+                mAutoSwitchType(AUTO_SWITCH_PSYS),
+                mLtmEnabled(false),
+                mSensorExposureNum(2),
+                mSensorExposureType(SENSOR_EXPOSURE_SINGLE),
+                mSensorGainType(SENSOR_GAIN_NONE),
+                mLensCloseCode(0),
+                mEnableAIQ(false),
+                mSkipFrameV4L2Error(false),
+                mCITMaxMargin(0),
+                mYuvColorRangeMode(CAMERA_FULL_MODE_YUV_COLOR_RANGE),
+                mInitialSkipFrame(0),
+                mMaxRawDataNum(MAX_BUFFER_COUNT),
+                mTopBottomReverse(false),
+                mPsysContinueStats(false),
+                mMaxRequestsInflight(0),
+                mPreferredBufQSize(MAX_BUFFER_COUNT),
+                mPipeSwitchDelayFrame(0),
+                mExposureLag(MAX_BUFFER_COUNT),
+                mGainLag(0),
+                mLtmGainLag(0),
+                mEnableLtmThread(false),
+                mEnableLtmDefog(false),
+                mMaxSensorDigitalGain(0),
+                mSensorDgType(SENSOR_DG_TYPE_NONE),
+                mISysFourcc(V4L2_PIX_FMT_SGRBG8),
+                mISysRawFormat(V4L2_PIX_FMT_SGRBG10),
+                mUseCrlModule(true),
+                mFacing(FACING_BACK),
+                mOrientation(ORIENTATION_0),
+                mUseSensorDigitalGain(false),
+                mUseIspDigitalGain(false),
+                mNeedPreRegisterBuffers(false),
+                mFrameSyncCheckEnabled(false),
+                mEnableAiqd(false),
+                mCurrentMcConf(nullptr),
+                mGraphSettingsType(COUPLED),
+                mDVSType(MORPH_TABLE),
+                mISYSCompression(false),
+                mPSACompression(false),
+                mOFSCompression(false),
+                mFaceAeEnabled(false),
+                mFaceEngineRunningInterval(FACE_ENGINE_DEFAULT_RUNNING_INTERVAL),
+                mFaceEngineRunningIntervalNoFace(FACE_ENGINE_DEFAULT_RUNNING_INTERVAL),
+                mFaceEngineRunningSync(false),
+                mPsysAlignWithSof(false),
+                mPsysBundleWithAic(false),
+                mSwProcessingAlignWithIsp(false),
+                mMaxNvmDataSize(0),
+                mVideoStreamNum(DEFAULT_VIDEO_STREAM_NUM)
+            {
+            }
+
+            std::vector <MediaCtlConf> mMediaCtlConfs;
+
+            std::string sensorName;
+            std::string sensorDescription;
+            std::string mLensName;
+            int mLensHwType;
+            int mAutoSwitchType;
+            bool mLtmEnabled;
+            int mSensorExposureNum;
+            int mSensorExposureType;
+            int mSensorGainType;
+            int mLensCloseCode;
+            bool mEnableAIQ;
+            bool mSkipFrameV4L2Error;
+            int mCITMaxMargin;
+            camera_yuv_color_range_mode_t mYuvColorRangeMode;
+            unsigned int mInitialSkipFrame;
+            unsigned int mMaxRawDataNum;
+            bool mTopBottomReverse;
+            bool mPsysContinueStats;
+            int mMaxRequestsInflight;
+            unsigned int mPreferredBufQSize;
+            unsigned int mPipeSwitchDelayFrame;
+            int mExposureLag;
+            int mGainLag;
+            int mLtmGainLag;
+            bool mEnableLtmThread;
+            bool mEnableLtmDefog;
+            int mMaxSensorDigitalGain;
+            SensorDgType mSensorDgType;
+            std::string mCustomAicLibraryName;
+            std::string mCustom3ALibraryName;
+            std::vector <camera_resolution_t> mSupportedISysSizes; // ascending order request
+            std::vector <int> mSupportedISysFormat;
+            int mISysFourcc; // the isys output format
+            int mISysRawFormat; // the isys raw format if scale enabled
+
+            std::vector <int> mPSysFormat; // the psys output format
+            std::vector <TuningConfig> mSupportedTuningConfig;
+            std::vector <LardTagConfig> mLardTagsConfig;
+            std::vector <ConfigMode> mConfigModesForAuto;
+
+            bool mUseCrlModule;
+            int mFacing;
+            int mOrientation;
+            bool mUseSensorDigitalGain;
+            bool mUseIspDigitalGain;
+            bool mNeedPreRegisterBuffers;
+            bool mFrameSyncCheckEnabled;
+            bool mEnableAiqd;
+            MediaCtlConf *mCurrentMcConf;
+            std::map<int, stream_array_t> mStreamToMcMap;
+            Parameters mCapability;
+
+            std::string mGraphSettingsFile;
+            GraphSettingType mGraphSettingsType;
+            std::vector <MultiExpRange> mMultiExpRanges;
+            std::vector <uint32_t> mSupportedIspControlFeatures;
+            int mDVSType;
+            bool mISYSCompression;
+            bool mPSACompression;
+            bool mOFSCompression;
+            bool mFaceAeEnabled;
+            int mFaceEngineRunningInterval;
+            int mFaceEngineRunningIntervalNoFace;
+            int mFaceEngineRunningSync;
+            bool mPsysAlignWithSof;
+            bool mPsysBundleWithAic;
+            bool mSwProcessingAlignWithIsp;
+
+            /* key: camera_test_pattern_mode_t, value: sensor test pattern mode */
+            std::unordered_map<int32_t, int32_t> mTestPatternMap;
+
+            // This is for binding stream id to ConfigMode, since the stream id from kernel list of
+            // a PG might be incorrect. To be removed after stream id mismatch issue fixed.
+            std::map<int, int> mConfigModeToStreamId;
+            std::vector<UserToPslOutputMap> mOutputMap;
+            int mMaxNvmDataSize;
+            std::string mNvmDirectory;
+            std::vector<IGraphType::ScalerInfo> mScalerInfo;
+            int mVideoStreamNum;
+        };
+
+        std::vector<CameraInfo> mCameras;
+        std::vector<PolicyConfig> mPolicyConfig;
+        CommonConfig mCommonConfig;
+    };
+private:
+    StaticCfg mStaticCfg;
+
+    std::vector<AiqInitData*> mAiqInitData;
+private:
+    /**
+     * Get access to the platform singleton.
+     *
+     * Note: this is implemented in PlatformFactory.cpp
+     */
+    static PlatformData* sInstance;
+    static Mutex sLock;
+    static PlatformData* getInstance();
+
+    /**
+     * Parse graph descriptor and settings from configuration files.
+     */
+    void parseGraphFromXmlFile();
+
+    /**
+     * Release GraphConfigNodes in StaticCfg::CameraInfo
+     */
+    void releaseGraphConfigNodes();
+
+    /**
+     * Get MediaCtlConf via MC ID.
+     */
+    static MediaCtlConf* getMcConfByMcId(const StaticCfg::CameraInfo& cameraInfo, int mcId);
+
+    /**
+     * Get MediaCtlConf via stream config.
+     */
+    static MediaCtlConf* getMcConfByStream(const StaticCfg::CameraInfo& cameraInfo,
+                                           const stream_t& stream);
+
+    /**
+     * Get MediaCtlConf via ConfigMode.
+     */
+    static MediaCtlConf* getMcConfByConfigMode(const StaticCfg::CameraInfo& cameraInfo,
+                                               const stream_t& stream, ConfigMode mode);
+
+    /**
+     * Check if video node is enabled via camera Id and video node type.
+     */
+    static bool isVideoNodeEnabled(int cameraId, VideoNodeType type);
+
+public:
+     /**
+      * releaseInstance
+      * This function must be called when the hal is destroyed.
+      */
+    static void releaseInstance();
+
+    /**
+     * init PlatformData
+     *
+     * \return OK if init PlatformData successfully, otherwise return ERROR.
+     */
+    static int init();
+
+    /**
+     * get the camera numbers
+     *
+     * \return int: the camera numbers
+     */
+    static int numberOfCameras();
+
+    /**
+     * get the camera number in xml
+     *
+     * \return int: the camera numbers in xml
+     */
+    static int getXmlCameraNumber();
+
+    /**
+     * get the sensor name
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return char*: the sensor name string.
+     */
+    static const char* getSensorName(int cameraId);
+
+    /**
+     * get the sensor description
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return const char*: the sensor descrition string.
+     */
+    static const char* getSensorDescription(int cameraId);
+
+    /**
+     * get the Lens name
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return char*: the lens name string.
+     */
+    static const char* getLensName(int cameraId);
+
+    /**
+     * get the Lens HW type
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return int: the Lens HW type
+     */
+    static int getLensHwType(int cameraId);
+
+    /**
+     * get the DVS type
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return int: the DVS type
+     */
+    static int getDVSType(int cameraId);
+
+    /**
+     * get the ISYS compression flag
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if ISYS compression is enabled
+     */
+    static bool getISYSCompression(int cameraId);
+
+    /**
+     * get the PSA compression flag
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if PSA compression is enabled
+     */
+    static bool getPSACompression(int cameraId);
+
+    /**
+     * get the OFS compression flag
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if OFS compression is enabled
+     */
+    static bool getOFSCompression(int cameraId);
+
+    /**
+     * get the max coarse integration time margin
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return int: the value of max coarse integration time margin.
+     */
+    static int getCITMaxMargin(int cameraId);
+
+    /**
+     * Check AIQ is enabled or not
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if AIQ is enabled or not.
+     */
+    static bool isEnableAIQ(int cameraId);
+
+    /**
+     * Check if sensor digital gain is used or not
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if sensor gain is used or not.
+     */
+    static bool isUsingSensorDigitalGain(int cameraId);
+
+    /**
+     * Check if using isp digital gain or not
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if using isp gain or not.
+     */
+    static bool isUsingIspDigitalGain(int cameraId);
+
+    /**
+     * Check if need to pre-register buffers or not
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if pre-register buffers or not.
+     */
+    static bool isNeedToPreRegisterBuffer(int cameraId);
+
+    /**
+     * Get auto switch type
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the value of auto switch type
+     */
+    static int getAutoSwitchType(int cameraId);
+
+    /**
+     * Check Defog(LTM) is enabled or not
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if Defog is enabled or not.
+     */
+    static bool isEnableDefog(int cameraId);
+
+    /**
+     * Check Frame Sync is enabled or not
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if Frame Sync is enabled or not.
+     */
+    static bool isEnableFrameSyncCheck(int cameraId);
+
+    /**
+     * Get exposure number
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param multiExposure: true or false
+     * \return the value of exposure number according to different cases
+     */
+    static int getExposureNum(int cameraId, bool multiExposure);
+
+    /**
+     * Check LTM is enabled or not
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if LTM is enabled or not.
+     */
+    static bool isLtmEnabled(int cameraId);
+
+    /**
+     * Get sensor exposure type
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the value of sensor exposure type
+     */
+    static int getSensorExposureType(int cameraId);
+
+    /**
+     * Get sensor gain type
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the value of sensor gain type
+     */
+    static int getSensorGainType(int cameraId);
+
+    /**
+     * Get sensor's initial skip frame number
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the value of initial skip frame number
+     */
+    static unsigned int getInitialSkipFrame(int cameraId);
+
+    /**
+     * Get max raw data number
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the value of max raw data number
+     */
+    static unsigned int getMaxRawDataNum(int cameraId);
+
+     /**
+     * Get sensor's top bottom filed reverse option
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the value of top bottom filed reverse value
+     */
+    static bool getTopBottomReverse(int cameraId);
+
+    /*
+     * Check if Psys continuous stats is needed or not.
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if it is needed
+     */
+    static bool isPsysContinueStats(int cameraId);
+
+    /**
+     * Get preferred buffer queue size
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the value of preferred buffer queue size
+     */
+    static unsigned int getPreferredBufQSize(int cameraId);
+
+    /**
+     * Get pipe switch delay frame
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the value of delay frame
+     */
+    static unsigned int getPipeSwitchDelayFrame(int cameraId);
+
+    /**
+     * Get Ltm Gain lag
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the value of LTM gain lag
+     */
+    static int getLtmGainLag(int cameraId);
+
+    /**
+     * Check ltm thread is enabled or not
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if ltm thread is enabled or not.
+     */
+    static bool isEnableLtmThread(int cameraId);
+
+    /**
+     * Check face detection is enabled or not
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if face detection is enabled or not.
+     */
+    static bool isFaceAeEnabled(int cameraId);
+
+    /**
+     * get face engine's running interval
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the face engine running interval value.
+     */
+    static int faceEngineRunningInterval(int cameraId);
+
+    /**
+     * get face engine's running interval when face is not found
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the face engine running interval value when face is not found.
+     */
+    static int faceEngineRunningIntervalNoFace(int cameraId);
+
+    /**
+     * Check face detection runs  synchronously or not
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if face detection runs synchronously or not.
+     */
+    static bool isFaceEngineSyncRunning(int cameraId);
+
+    /**
+     * get dvs supported status
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return true: dvs supported; false: dvs not supported.
+     */
+    static bool isDvsSupported(int cameraId);
+
+    /**
+     * Check psys align with sof is enabled or not
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if psys align with sof is enabled or not.
+     */
+    static bool psysAlignWithSof(int cameraId);
+
+    /**
+     * Check running psys bundle with aic is enabled or not
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if running psys bundle with aic is enabled or not.
+     */
+    static bool psysBundleWithAic(int cameraId);
+
+    /**
+     * Check software processing align with isp is enabled or not
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if software processing align with isp is enabled or not.
+     */
+    static bool swProcessingAlignWithIsp(int cameraId);
+
+    /**
+     * Get the max digital gain of sensor
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the value of max digital gain
+     */
+    static int getMaxSensorDigitalGain(int cameraId);
+
+    /**
+     * Get sensor digital gain type
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the sensor digital gain type
+     */
+    static SensorDgType sensorDigitalGainType(int cameraId);
+
+    /**
+     * Get sensor's exposure lag
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the value of exposure lag
+     */
+    static int getExposureLag(int cameraId);
+
+    /**
+     * Get sensor's gain lag
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the value of gain lag
+     */
+    static int getGainLag(int cameraId);
+
+    /**
+     * Get the executor policy config.
+     *
+     * \param[in] graphId: the graph id
+     *
+     * \return PolicyConfig* object if found, otherwise return nullptr.
+     */
+    static PolicyConfig* getExecutorPolicyConfig(int graphId);
+
+    /**
+     * According to stream info to select MC
+     * this function will compare the format/resolutions/interlace to find the MediaCtlConf
+     * and then store it into cameraInfo.
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param stream: the stream info
+     * \param mode: the stream operation mode
+     */
+    static void selectMcConf(int cameraId, stream_t stream, ConfigMode mode, int mcId);
+
+    /**
+     * to get the current MediaCtlConf
+     * after the media controller has been analyzed, the media controller information will be stored in the mMediaCtlConfs.
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return MediaCtlConf*, if it doens't find one, this function will return nullptr.
+     */
+    static MediaCtlConf *getMediaCtlConf(int cameraId);
+
+    /**
+     * \brief Fill camera info and capability according to given camera id
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param[out] camera_info_t info
+     *
+     * \return 0 if succeed, other value indicates failed.
+     */
+    static int getCameraInfo(int cameraId, camera_info_t& info);
+
+    /**
+     * \brief Check if the camera_features feature is supported
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param[in] camera_features feature
+     *
+     * \return true if supported, otherwise return false.
+     */
+    static bool isFeatureSupported(int cameraId, camera_features feature);
+
+    /**
+     * \brief Check if the given stream config is supported
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param[in] stream_t conf
+     *
+     * \return true if supported, otherwise return false.
+     */
+    static bool isSupportedStream(int cameraId, const stream_t& conf);
+
+    /**
+     * get the isys supported size list
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param sizes: the function will fill the isys supported size list to the sizes
+     */
+    static void getSupportedISysSizes(int cameraId, std::vector <camera_resolution_t>& resolutions);
+
+    /**
+     * get the isys supported format list
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param formats: the function will fill the isys supported format list to the formats
+     * \return true if success, return false if it fails.
+     */
+    static bool getSupportedISysFormats(int cameraId, std::vector <int>& formats);
+
+    /**
+     * Format for the ISYS output
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the format for the isys output
+     */
+    static int getISysFormat(int cameraId);
+
+    /**
+     * Set ISYS output format
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param format: the isys output format
+     */
+    static void selectISysFormat(int cameraId, int format);
+
+    /**
+     * If ISYS supported format.
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     */
+    static bool isISysSupportedFormat(int cameraId, int format);
+
+    /**
+     * if the resolution is supported by Isys
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param resolution: the requested resolution
+     * \return true if the resolution is supported by isys, otherwise false
+     */
+    static bool isISysSupportedResolution(int cameraId, camera_resolution_t resolution);
+
+    /**
+     * Check if the frame needs to be skipped when STR2MMIO error occurs
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return if corrupted frame needs to be skipped or not.
+     */
+    static bool isSkipFrameOnSTR2MMIOErr(int cameraId);
+
+    /**
+     * Format for the ISYS RAW output
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the RAW format if isys scale enabled
+     */
+    static int getISysRawFormat(int cameraId);
+
+    /**
+     * Get the config of the ISYS output per port
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the config of the ISYS output for the given port.
+     */
+    static stream_t getISysOutputByPort(int cameraId, Port port);
+
+    /**
+     * get the format by device name
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param devName: device name
+     * \format: return param for format
+     * \return the status
+     */
+    static int getFormatByDevName(int cameraId, const std::string& devName, McFormat& format);
+
+    /**
+     * get the video node name
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param videoNodeType: value of enum VideoNodeType
+     * \param videoNodeName: return param for the video node name
+     * \return the status
+     */
+    static int getVideoNodeNameByType(int cameraId, VideoNodeType videoNodeType,
+                                      std::string& videoNodeName);
+
+    /**
+     * get the hardware device name
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param videoNodeType: value of enum VideoNodeType
+     * \param devName: return param for the device name
+     * \return the status
+     */
+    static int getDevNameByType(int cameraId, VideoNodeType videoNodeType, std::string& devName);
+
+    /**
+     * Check if ISYS is enabled or not
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return true if ISYS is enabled, otherwise return false
+     */
+    static bool isIsysEnabled(int cameraId);
+
+    static int calculateFrameParams(int cameraId, SensorFrameParams& sensorFrameParams);
+
+    /**
+     * Get the optmized resolutions that supported by input system
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param width:    The width of the request frame
+     * \param height:   The height of the request frame
+     * \param field:    The field of the request frame
+     *
+     * \return camera_resolution_t: The optimized resolution that used to configure the ISYS.
+     */
+    static camera_resolution_t getISysBestResolution(int cameraId, int width, int height, int field);
+
+    /**
+     * to get if it support the format
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param format:the format will be checked if the psys supports.
+     * \return true or false for the psys could be used or not.
+     */
+    static bool usePsys(int cameraId, int format);
+
+    /**
+     * to get supported psys dag config
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param configs: the function will fill supported psys dag config list to the configs
+     */
+    static void getSupportedTuningConfig(int cameraId, std::vector <TuningConfig> &configs);
+
+    /**
+     * to get the ConfigMode by operation Mode
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param operationMode: the operation mode
+     * \param configModes: the function will fill available ConfigMode for this operation mode
+     * \return OK if get ConfigMode, otherwise return INVALID_OPERATION
+     */
+    static int getConfigModesByOperationMode(int cameraId, uint32_t operationMode,
+                                             std::vector <ConfigMode> &configModes);
+
+    /**
+     * to get the TuningMode by Config Mode
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param configMode: type of ConfigMode (except auto)
+     * \param tuningMode: return related TuningMode
+     * \return OK if get TuningMode, otherwise return INVALID_OPERATION
+     */
+    static int getTuningModeByConfigMode(int cameraId, ConfigMode configMode, TuningMode& tuningMode);
+
+    /**
+     * to get tuning config by ConfigMode
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param mode: ConfigMode
+     * \param config: return related TuningConfig
+     * \return OK if get TuningConfig, otherwise return INVALID_OPERATION
+     */
+    static int getTuningConfigByConfigMode(int cameraId, ConfigMode mode, TuningConfig &config);
+
+    /*
+     * Get stream id by the given configMode
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param configMode: ConfigMode
+     * \return the stream id if succeeds, otherwise return -1.
+     */
+    static int getStreamIdByConfigMode(int cameraId, ConfigMode configMode);
+
+    /*
+     * Get the max requests number in flight
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the max requests number in flight
+     */
+    static int getMaxRequestsInflight(int cameraId);
+
+    /**
+     * get yuv color range mode
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the correponding camera_yuv_color_range_mode_t.
+     */
+    static camera_yuv_color_range_mode_t getYuvColorRangeMode(int cameraId);
+
+    /**
+     * Get aiqd
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param tuningMode: mode
+     * \return ia_binary_data
+     */
+    static ia_binary_data* getAiqd(int cameraId, TuningMode mode);
+
+    /**
+     * Save aiqd
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param tuningMode: mode
+     * \param ia_binary_data: data
+     */
+    static void saveAiqd(int cameraId, TuningMode tuningMode, const ia_binary_data& data);
+
+    /**
+     * Get cpf and cmc
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param ispData: isp data in cpf
+     * \param aiqData:  aiq data in cpf
+     * \param otherData: other data in cpf
+     * \param cmcHandle: cmc handle
+     * \param mode: tuning mode
+     * \param cmcData: cmd data
+     * \return OK if it is successful.
+     */
+    static int getCpfAndCmc(int cameraId,
+                            ia_binary_data* ispData,
+                            ia_binary_data* aiqData,
+                            ia_binary_data* otherData,
+                            uintptr_t* cmcHandle,
+                            TuningMode mode = TUNING_MODE_VIDEO,
+                            ia_cmc_t** cmcData = nullptr);
+
+    /**
+     * If dynamic graph config enabled
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return true if use graphConfig file.
+     */
+    static bool getGraphConfigNodes(int cameraId);
+
+    /**
+     * to get the type of graph settings
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the graph settings type: COUPLED or DISPERSED.
+     */
+    static GraphSettingType getGraphSettingsType(int cameraId);
+
+    /**
+     * if ISYS CSI Back End capture enabled
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return true if the current sensor is CSI Back End capture or not
+     */
+    static bool isCSIBackEndCapture(int cameraId);
+
+    /**
+     * if ISYS CSI Front End capture enabled
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return true if the current sensor is CSI Front End capture or not
+     */
+    static bool isCSIFrontEndCapture(int cameraId);
+
+    /**
+     * if AIQD enabled
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return true if AIQD is enabled or not
+     */
+    static bool isAiqdEnabled(int cameraId);
+
+    /**
+     * if image from tpg
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return true if frame is from tpg or not
+     */
+    static bool isTPGReceiver(int cameraId);
+
+    static int getSupportAeExposureTimeRange(int cameraId, camera_scene_mode_t sceneMode,
+                                             camera_range_t& etRange);
+    static int getSupportAeGainRange(int cameraId, camera_scene_mode_t sceneMode,
+                                     camera_range_t& gainRange);
+
+    /**
+     * if CrlModule is used
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return true if CrlModule driver is used, otherwise return false
+     */
+    static bool isUsingCrlModule(int cameraId);
+
+    /**
+     * to get the MultiExpRange of CameraInfo
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return the MultiExpRange for current camera id.
+     */
+    static std::vector<MultiExpRange> getMultiExpRanges(int cameraId);
+
+    /**
+     * Get the psl output resolution
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param width:    The width of user requirement
+     * \param height:   The height of user requirement
+     * \return the psl output resolution if provides it in xml file, otherwise return nullptr.
+     */
+    static camera_resolution_t *getPslOutputForRotation(int width, int height, int cameraId);
+
+    /**
+     * Check if test pattern is supported or not
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return true if mTestPatternMap is defined, otherwise return false.
+     */
+    static bool isTestPatternSupported(int cameraId);
+
+    /**
+     * get sensor test pattern
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param mode: camera_test_pattern_mode_t
+     * \return corresponding sensor test pattern if provided in xml file, otherwise return -1.
+     */
+    static int32_t getSensorTestPattern(int cameraId, int32_t mode);
+
+    /**
+     * Get the nvm
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return ia_binary_data
+     */
+    static ia_binary_data* getNvm(int cameraId);
+
+    /**
+    * Get sensor active array size
+    *
+    * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+    * \return the value of camera_coordinate_system_t.
+    */
+    static camera_coordinate_system_t getActivePixelArray(int cameraId);
+
+    /**
+    * Get camera cfg path from environment variable
+    *
+    * \param void
+    * \return the value of camera cfg path.
+    */
+    static std::string getCameraCfgPath();
+
+    /**
+    * Get camera graph descriptor file path
+    *
+    * \param void
+    * \return the value of camera graph descriptor file path.
+    */
+    static std::string getGraphDescFilePath();
+
+    /**
+    * Get camera graph setting file path.
+    *
+    * \param void
+    * \return the value of camera graph setting file path.
+    */
+    static std::string getGraphSettingFilePath();
+
+    /*
+     * Get sensor value for the digital gain.
+     *
+     * Since the calculation formula may be different between sensors,
+     * so we need to get this value based on sensor digital gain type.
+     * For imx274, the magnification = 2^x (x is the register value).
+     *
+     * Need to specify the sensorDgType, maxSensorDg and useIspDigitalGain in xml.
+     */
+    static int getSensorDigitalGain(int cameraId, float realDigitalGain);
+
+    /*
+     * Get the isp gain
+     *
+     * Separate real digital to sensorDg and ispDg, and the ispDg >= 1
+     */
+    static float getIspDigitalGain(int cameraId, float realDigitalGain);
+
+    /**
+     * \brief Save Makernote by ia_mkn_trg mode
+     *
+     * \param[in] cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param[in] camera_makernote_mode_t: MAKERNOTE_MODE_JPEG is corresponding
+     *           to ia_mkn_trg_section_1 for Normal Jpeg capture;
+     *           MAKERNOTE_MODE_RAW is corresponding to ia_mkn_trg_section_2
+     *           for Raw image capture.
+     * \param[in] int64_t sequence: the sequence in latest AiqResult
+     *
+     * \return OK if get Makernote successfully, otherwise return ERROR.
+     */
+    static int saveMakernoteData(int cameraId, camera_makernote_mode_t makernoteMode,
+                                 int64_t sequence);
+
+    /**
+     * \brief Get ia_mkn (Makernote) handle.
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     */
+    static ia_mkn *getMknHandle(int cameraId);
+
+    /**
+     * \brief Update Makernote timestamp.
+     *
+     * \param[in] cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param[in] sequence: the sequence in frame buffer;
+     * \param[in] timestamp: the frame timestamp corresponding sequence
+     *
+     */
+    static void updateMakernoteTimeStamp(int cameraId, int64_t sequence, uint64_t timestamp);
+
+    /**
+     * \brief acquire Makernote data.
+     *
+     * \param[in] cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param[in] sequence: acquire MakerNote per timestamp
+     * \param[out] param: Makernote data will be saved in Parameters as metadata.
+     *
+     */
+    static void acquireMakernoteData(int cameraId, uint64_t timestamp, Parameters *param);
+
+    /*
+     * Get the scaler info
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param streamId: hal stream id
+     * \param sclscalerWidth and scalerHeight : return related scaler info
+     * \return OK.
+     */
+    static int getScalerInfo(int cameraId, int32_t streamId,
+                             float *scalerWidth, float *scalerHeight);
+
+    /*
+     * Set the scaler info
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \param scalerInfo related IGraphType::ScalerInfo
+     */
+    static void setScalerInfo(int cameraId, std::vector<IGraphType::ScalerInfo> scalerInfo);
+
+     /**
+     * Check gpu tnr is enabled or not
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return true if tnr is enabled.
+     */
+     static bool isGpuTnrEnabled();
+
+     /**
+     * get the video stream number supported
+     *
+     * \param cameraId: [0, MAX_CAMERA_NUMBER - 1]
+     * \return HAL video stream number.
+     */
+     static int getVideoStreamNum(int cameraId);
+
+     /**
+     * Check should connect gpu algo or not
+     * should connect gpu algo service if any gpu algorithm is used
+     * \return true if should connect gpu algo.
+     */
+     static bool isUsingGpuAlgo();
+};
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/platformdata/PolicyParser.cpp b/camera/hal/intel/ipu6/src/platformdata/PolicyParser.cpp
new file mode 100644
index 000000000000..f799f21a8ec8
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/PolicyParser.cpp
@@ -0,0 +1,226 @@
+/*
+ * Copyright (C) 2017-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#define LOG_TAG "PolicyParser"
+
+#include <string.h>
+#include <expat.h>
+
+#include "iutils/CameraLog.h"
+
+#include "PolicyParser.h"
+
+namespace icamera {
+#define PSYS_POLICY_FILE_NAME "psys_policy_profiles.xml"
+PolicyParser::PolicyParser(PlatformData::StaticCfg *cfg) :
+    mStaticCfg(cfg),
+    mCurrentDataField(FIELD_INVALID),
+    pCurrentConf(nullptr) {
+    LOGXML("@%s", __func__);
+    CheckError(!mStaticCfg, VOID_VALUE, "@%s, cfg parameter is wrong", __func__);
+    mStaticCfg->mPolicyConfig.clear();
+
+    int ret = getDataFromXmlFile(PSYS_POLICY_FILE_NAME);
+    CheckError(ret != OK, VOID_VALUE,
+               "Failed to get policy profiles data frome %s", PSYS_POLICY_FILE_NAME);
+}
+
+/**
+ * This function will check which field that the parser parses to.
+ *
+ * The field is set to 2 types.
+ * FIELD_INVALID FIELD_GRAPH
+ *
+ * \param profiles: the pointer of the PolicyParser.
+ * \param name: the element's name.
+ * \param atts: the element's attribute.
+ */
+void PolicyParser::checkField(PolicyParser *profiles, const char *name, const char **atts)
+{
+    LOGXML("@%s, name:%s", __func__, name);
+    if (strcmp(name, "PsysPolicyConfig") == 0) {
+        profiles->mCurrentDataField = FIELD_INVALID;
+        return;
+    } else if (strcmp(name, "graph") == 0) {
+        profiles->pCurrentConf = new PolicyConfig;
+
+        int idx = 0;
+        while (atts[idx]) {
+            const char* key = atts[idx];
+            const char* val = atts[idx + 1];
+            LOGXML("@%s, name:%s, atts[%d]:%s, atts[%d]:%s", __func__, name, idx, key, idx+1, val);
+            if (strcmp(key, "id") == 0) {
+                profiles->pCurrentConf->graphId = atoi(val);
+            } else if (strcmp(key, "description") == 0) {
+                profiles->pCurrentConf->policyDescription = val;
+            }
+            idx += 2;
+        }
+        profiles->mCurrentDataField = FIELD_GRAPH;
+        return;
+    }
+
+    LOGE("@%s, name:%s, atts[0]:%s, xml format wrong", __func__, name, atts[0]);
+    return;
+}
+
+void PolicyParser::handlePipeExecutor(PolicyParser *profiles, const char *name, const char **atts)
+{
+    int idx = 0;
+    ExecutorPolicy policy;
+
+    while (atts[idx]) {
+        const char *key = atts[idx];
+        LOGXML("%s: name: %s, value: %s", __func__, atts[idx], atts[idx + 1]);
+        if (strcmp(key, "name") == 0) {
+            policy.exeName = atts[idx + 1];
+        } else if (strcmp(key, "pgs") == 0) {
+            parseXmlConvertStrings(atts[idx + 1], policy.pgList, convertCharToString);
+        } else if (strcmp(key, "op_modes") == 0) {
+            parseXmlConvertStrings(atts[idx + 1], policy.opModeList, atoi);
+        } else if (strcmp(key, "notify_policy") == 0) {
+            int notifyPolicy = std::stoi(atts[idx + 1]);
+            if (notifyPolicy >= 0 && notifyPolicy < POLICY_INVALID) {
+                policy.notifyPolicy = (ExecutorNotifyPolicy)notifyPolicy;
+            } else {
+                LOGW("Invalid notify policy value: %d", notifyPolicy);
+            }
+        } else if (strcmp(key, "cyclic_feedback_routine") == 0) {
+            parseXmlConvertStrings(atts[idx + 1], policy.cyclicFeedbackRoutineList, atoi);
+        } else if (strcmp(key, "cyclic_feedback_delay") == 0) {
+            parseXmlConvertStrings(atts[idx + 1], policy.cyclicFeedbackDelayList, atoi);
+        } else {
+            LOGW("Invalid policy attribute: %s", key);
+        }
+        idx += 2;
+    }
+
+    LOGXML("@%s, name:%s, atts[0]:%s", __func__, name, atts[0]);
+    profiles->pCurrentConf->pipeExecutorVec.push_back(policy);
+}
+
+void PolicyParser::handleExclusivePGs(PolicyParser *profiles, const char *name, const char **atts)
+{
+    int idx = 0;
+    LOGXML("%s: name: %s, value: %s", __func__, atts[idx], atts[idx + 1]);
+    const char *key = atts[idx];
+    if (strcmp(key, "pgs") == 0) {
+        parseXmlConvertStrings(atts[idx + 1], profiles->pCurrentConf->exclusivePgs, convertCharToString);
+    } else {
+        LOGE("Invalid policy attribute %s in exclusive label.", key);
+    }
+}
+
+void PolicyParser::handleBundles(PolicyParser *profiles, const char *name, const char **atts)
+{
+    int idx = 0;
+    LOGXML("%s: name: %s, value: %s", __func__, atts[idx], atts[idx + 1]);
+    const char *key = atts[idx];
+
+    CheckError(strcmp(key, "executors") != 0, VOID_VALUE, "Invalid policy attribute %s in bundle label.", key);
+
+    // The structure of a bundle looks like: "proc:0,post:1" which uses ',' to split
+    // different executors' names, and uses ':' to specify the executor's depth.
+    std::vector<std::string> bundledExecutors;
+    std::vector<int> depths;
+    std::vector<std::string> executors = CameraUtils::splitString(atts[idx + 1], ',');
+
+    for (const auto & item : executors) {
+        std::vector<std::string> executorDepth = CameraUtils::splitString(item.c_str(), ':');
+        CheckError(executorDepth.size() != 2, VOID_VALUE, "Invalid executor-depth mapping.");
+
+        bundledExecutors.push_back(executorDepth[0]);
+        depths.push_back(std::stoi(executorDepth[1]));
+    }
+
+    ExecutorDepth executorDepth = {bundledExecutors, depths};
+    profiles->pCurrentConf->bundledExecutorDepths.push_back(executorDepth);
+}
+
+/**
+ * This function will handle all the sensor related elements.
+ *
+ * It will be called in the function startElement
+ *
+ * \param profiles: the pointer of the CameraParser.
+ * \param name: the element's name.
+ * \param atts: the element's attribute.
+ */
+void PolicyParser::handlePolicyConfig(PolicyParser *profiles, const char *name, const char **atts)
+{
+    LOGXML("@%s, name:%s, atts[0]:%s", __func__, name, atts[0]);
+    if (strcmp(name, "pipe_executor") == 0) {
+        handlePipeExecutor(profiles, name, atts);
+    } else if (strcmp(name, "exclusive") == 0) {
+        handleExclusivePGs(profiles, name, atts);
+    } else if (strcmp(name, "bundle") == 0) {
+        handleBundles(profiles, name, atts);
+    } else if (strcmp(name, "enableBundleInSdv") == 0) {
+        profiles->pCurrentConf->enableBundleInSdv = (strcmp(atts[1], "true") == 0) ? true : false;
+        LOGXML("%s: enableBundleInSdv: %s", __func__, atts[1]);
+    }
+}
+
+/**
+ * the callback function of the libexpat for handling of one element start
+ *
+ * When it comes to the start of one element. This function will be called.
+ *
+ * \param userData: the pointer we set by the function XML_SetUserData.
+ * \param name: the element's name.
+ */
+void PolicyParser::startParseElement(void *userData, const char *name, const char **atts)
+{
+    PolicyParser *profiles = reinterpret_cast<PolicyParser*>(userData);
+
+    if (profiles->mCurrentDataField == FIELD_INVALID) {
+        profiles->checkField(profiles, name, atts);
+        return;
+    }
+
+    switch (profiles->mCurrentDataField) {
+        case FIELD_GRAPH:
+            profiles->handlePolicyConfig(profiles, name, atts);
+            break;
+        default:
+            LOGE("@%s, line:%d, go to default handling", __func__, __LINE__);
+            break;
+    }
+}
+
+/**
+ * the callback function of the libexpat for handling of one element end
+ *
+ * When it comes to the end of one element. This function will be called.
+ *
+ * \param userData: the pointer we set by the function XML_SetUserData.
+ * \param name: the element's name.
+ */
+void PolicyParser::endParseElement(void *userData, const char *name)
+{
+    LOGXML("@%s %s", __func__, name);
+
+    PolicyParser *profiles = reinterpret_cast<PolicyParser*>(userData);
+
+    if (strcmp(name, "graph") == 0) {
+        LOGXML("@%s, add policyConf, graphId: %d", __func__, profiles->pCurrentConf->graphId);
+        profiles->mStaticCfg->mPolicyConfig.push_back(*(profiles->pCurrentConf));
+        delete profiles->pCurrentConf;
+        profiles->pCurrentConf = nullptr;
+        profiles->mCurrentDataField = FIELD_INVALID;
+    }
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/platformdata/PolicyParser.h b/camera/hal/intel/ipu6/src/platformdata/PolicyParser.h
new file mode 100644
index 000000000000..ca5b13e26a83
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/PolicyParser.h
@@ -0,0 +1,73 @@
+/*
+ * Copyright (C) 2018-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ *\File PolicyParser.h
+ *
+ * parser for the policy xml configuration file
+ *
+ * This file calls the libexpat ditectly. The libexpat is one xml parser.
+ * It will parse the camera configuration out firstly.
+ * Then other module can call the methods of it to get the real configuration.
+ */
+
+#pragma once
+
+#include "iutils/Utils.h"
+
+#include "CameraTypes.h"
+#include "PlatformData.h"
+#include "ParserBase.h"
+
+namespace icamera {
+
+/**
+ * \class PolicyParser
+ *
+ * This class is used to parse the policy configuration file.
+ * The configuration file is xml format.
+ * This class will use the expat lib to do the xml parser.
+ */
+class PolicyParser : public ParserBase {
+public:
+    PolicyParser(PlatformData::StaticCfg *cfg);
+    ~PolicyParser(){}
+
+    void startParseElement(void *userData, const char *name, const char **atts);
+    void endParseElement(void *userData, const char *name);
+
+private:
+    // prevent copy constructor and assignment operator
+    DISALLOW_COPY_AND_ASSIGN(PolicyParser);
+
+private:
+    void checkField(PolicyParser *profiles, const char *name, const char **atts);
+    void handlePolicyConfig(PolicyParser *profiles, const char *name, const char **atts);
+    void handlePipeExecutor(PolicyParser *profiles, const char *name, const char **atts);
+    void handleExclusivePGs(PolicyParser *profiles, const char *name, const char **atts);
+    void handleBundles(PolicyParser *profiles, const char *name, const char **atts);
+
+private:
+    PlatformData::StaticCfg *mStaticCfg;
+
+    enum DataField {
+        FIELD_INVALID = 0,
+        FIELD_GRAPH,
+    } mCurrentDataField;
+    PolicyConfig *pCurrentConf;
+};
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/platformdata/gc/FormatUtils.cpp b/camera/hal/intel/ipu6/src/platformdata/gc/FormatUtils.cpp
new file mode 100644
index 000000000000..76ba798a7d44
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/gc/FormatUtils.cpp
@@ -0,0 +1,338 @@
+/*
+ * Copyright (C) 2016-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "FormatUtils"
+
+#include <stdint.h>
+#include <math.h>
+#include <linux/v4l2-mediabus.h>
+#include <linux/ipu-isys.h>
+#include "ia_cipf/ia_cipf_types.h"
+#include "FormatUtils.h"
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+using std::string;
+using namespace icamera;
+
+/**
+ * Utilities to query information about V4L2 types in graph config
+ */
+
+namespace graphconfig {
+namespace utils {
+
+enum FormatType {
+    FORMAT_RAW,
+    FORMAT_RAW_VEC,
+    FORMAT_YUV,
+    FORMAT_YUV_VEC,
+    FORMAT_RGB,
+    FORMAT_MBUS_BAYER,
+    FORMAT_MBUS_YUV,
+    FORMAT_JPEG,
+    FORMAT_FOURCC
+};
+
+struct FormatInfo {
+    int32_t pixelCode;  // OS specific pixel code, in this case V4L2 or Media bus
+    int32_t commonPixelCode;  // Common pixel code used by CIPF and GCSS in settings
+    string fullName;
+    string shortName;
+    int32_t bpp;
+    FormatType type;
+};
+
+/**
+ * gFormatMapping
+ *
+ * Table for mapping OS agnostic formats defined in CIPF and OS specific ones
+ * (in this case V4L2, or media bus).
+ * The table also helps provide textual representation and bits per pixel.
+ * CIPF does not define most of the formats, only the ones it needs, that is why
+ * most of the entries have 0 on the common pixel format.
+ * Conversely there are some new formats introduce by CIPF that do not have
+ * V4L2 representation.
+ */
+static const FormatInfo gFormatMapping[] = {
+    { V4L2_PIX_FMT_SBGGR8, 0, "V4L2_PIX_FMT_SBGGR8", "BGGR8", 8, FORMAT_RAW },
+    { V4L2_PIX_FMT_SGBRG8, 0, "V4L2_PIX_FMT_SGBRG8", "GBRG8", 8, FORMAT_RAW },
+    { V4L2_PIX_FMT_SRGGB8, 0, "V4L2_PIX_FMT_SRGGB8", "RGGB8", 8, FORMAT_RAW },
+
+    { V4L2_PIX_FMT_SGRBG8, ia_cipf_frame_fourcc_grbg, "V4L2_PIX_FMT_SGRBG8", "GRBG8", 8, FORMAT_RAW },
+    { V4L2_PIX_FMT_SBGGR10, ia_cipf_frame_fourcc_bg10, "V4L2_PIX_FMT_SBGGR10", "BGGR10", 16, FORMAT_RAW },
+    { V4L2_PIX_FMT_SGBRG10, ia_cipf_frame_fourcc_gb10, "V4L2_PIX_FMT_SGBRG10", "GBRG10", 16, FORMAT_RAW },
+    { V4L2_PIX_FMT_SGRBG10, ia_cipf_frame_fourcc_ba10, "V4L2_PIX_FMT_SGRBG10", "GRBG10", 16, FORMAT_RAW },
+    { V4L2_PIX_FMT_SRGGB10, ia_cipf_frame_fourcc_rg10, "V4L2_PIX_FMT_SRGGB10", "RGGB10", 16, FORMAT_RAW },
+    // align to xos definition and css format: IA_CSS_DATA_FORMAT_BAYER_GRBG
+    // it's different with ia_cipf_frame_fourcc_ba10 which align to css format:
+    // IA_CSS_DATA_FORMAT_RAW
+    { V4L2_PIX_FMT_SGRBG10, ia_cipf_frame_fourcc_gr10, "V4L2_PIX_FMT_SGRBG10", "GRBG10", 16, FORMAT_RAW },
+    { 0, css_fourcc_grbg_12_li, "css_fourcc_grbg_12_li", "CSL6", 15, FORMAT_RAW},
+    { V4L2_PIX_FMT_SGRBG12, ia_cipf_frame_fourcc_ba12, "V4L2_PIX_FMT_SGRBG12", "GRBG12", 16, FORMAT_RAW },
+
+    { V4L2_PIX_FMT_NV12, ia_cipf_frame_fourcc_nv12, "V4L2_PIX_FMT_NV12", "NV12", 12, FORMAT_YUV },
+    { V4L2_PIX_FMT_NV21, ia_cipf_frame_fourcc_nv21, "V4L2_PIX_FMT_NV21", "NV21", 12, FORMAT_YUV },
+    { V4L2_PIX_FMT_UYVY, ia_cipf_frame_fourcc_uyvy, "V4L2_PIX_FMT_UYVY", "UYVY", 16, FORMAT_YUV },
+
+    { V4L2_PIX_FMT_YUV420, ia_cipf_frame_fourcc_iyuv, "YUV420_8_PL", "YUV420", 12, FORMAT_YUV },
+    // Packed formats No V4L2 equivalent exists
+    // Normal YUV420 planar but with each sample of 12bits stored in 16bits
+    { 0, ia_cipf_frame_fourcc_i420, "YUV420-12-16p", "YUV420", 24, FORMAT_YUV },
+    // Normal YUV420 planar but with each sample of 12-bit DMA-packed
+    // (42 pixels and 8 bits of padding in a 64-byte DMA word)
+    { 0, css_fourcc_yuv420_12_p64, "YUV420-12-64p", "YUV420", 18, FORMAT_YUV },
+    //  Normal YUV420 planar but with each sample of 10-bit DMA-packed
+    // (51 pixels and 2 bits of padding in a 64-byte DMA word)
+    { 0, css_fourcc_yuv420_10_p64, "YUV420-10-64p", "YUV420", 15, FORMAT_YUV },
+
+    { V4L2_MBUS_FMT_SBGGR10_1X10, ia_cipf_frame_fourcc_bg10, "V4L2_MBUS_FMT_SBGGR10_1X10", "SBGGR10_1X10", 10, FORMAT_MBUS_BAYER },
+    { V4L2_MBUS_FMT_SGBRG10_1X10, ia_cipf_frame_fourcc_gb10, "V4L2_MBUS_FMT_SGBRG10_1X10", "SGBRG10_1X10", 10, FORMAT_MBUS_BAYER },
+    { V4L2_MBUS_FMT_SGRBG10_1X10, ia_cipf_frame_fourcc_ba10, "V4L2_MBUS_FMT_SGRBG10_1X10", "SGRBG10_1X10", 10, FORMAT_MBUS_BAYER },
+    { V4L2_MBUS_FMT_SRGGB10_1X10, ia_cipf_frame_fourcc_rg10, "V4L2_MBUS_FMT_SRGGB10_1X10", "SRGGB10_1X10", 10, FORMAT_MBUS_BAYER },
+    { V4L2_MBUS_FMT_UYVY8_1X16, ia_cipf_frame_fourcc_uyvy, "V4L2_MBUS_FMT_UYVY8_1X16", "UYVY8_1X16", 16, FORMAT_MBUS_YUV },
+    { V4L2_PIX_FMT_SBGGR12, 0, "V4L2_PIX_FMT_SBGGR12", "BGGR12", 16, FORMAT_RAW },
+    { V4L2_PIX_FMT_SGBRG12, 0, "V4L2_PIX_FMT_SGBRG12", "GBRG12", 16, FORMAT_RAW },
+    { V4L2_PIX_FMT_SRGGB12, 0, "V4L2_PIX_FMT_SRGGB12", "RGGB12", 16, FORMAT_RAW },
+
+    { V4L2_PIX_FMT_SBGGR10P, 0, "V4L2_PIX_FMT_SBGGR10P", "BGGR10P", 10, FORMAT_RAW },
+    { V4L2_PIX_FMT_SGBRG10P, 0, "V4L2_PIX_FMT_SGBRG10P", "GBRG10P", 10, FORMAT_RAW },
+    { V4L2_PIX_FMT_SGRBG10P, 0, "V4L2_PIX_FMT_SGRBG10P", "GRBG10P", 10, FORMAT_RAW },
+    { V4L2_PIX_FMT_SRGGB10P, 0, "V4L2_PIX_FMT_SRGGB10P", "RGGB10P", 10, FORMAT_RAW },
+
+    { V4L2_PIX_FMT_NV16, 0, "V4L2_PIX_FMT_NV16", "NV16", 16, FORMAT_YUV },
+    { V4L2_PIX_FMT_YUYV, 0, "V4L2_PIX_FMT_YUYV", "YUYV", 16, FORMAT_YUV },
+    { V4L2_PIX_FMT_YVU420, 0, "V4L2_PIX_FMT_YVU420", "YVU420", 12, FORMAT_YUV },
+    { V4L2_PIX_FMT_YUV422P, 0, "V4L2_PIX_FMT_YUV422P", "YUV422P", 16, FORMAT_YUV },
+
+    { V4L2_PIX_FMT_BGR24, 0, "V4L2_PIX_FMT_BGR24", "BGR24", 24, FORMAT_RGB },
+    { V4L2_PIX_FMT_BGR32, 0, "V4L2_PIX_FMT_BGR32", "BGR32", 32, FORMAT_RGB },
+    { V4L2_PIX_FMT_XBGR32, 0, "V4L2_PIX_FMT_XBGR32", "XBGR32", 32, FORMAT_RGB },
+    { V4L2_PIX_FMT_XRGB32, 0, "V4L2_PIX_FMT_XRGB32", "XRGB32", 32, FORMAT_RGB },
+    { V4L2_PIX_FMT_RGB565, 0, "V4L2_PIX_FMT_RGB565", "RGB565", 16, FORMAT_RGB },
+
+    { V4L2_PIX_FMT_JPEG, 0, "V4L2_PIX_FMT_JPEG", "JPEG", 0, FORMAT_JPEG },
+
+    { V4L2_MBUS_FMT_SBGGR12_1X12, 0, "V4L2_MBUS_FMT_SBGGR12_1X12", "SBGGR12_1X12", 12, FORMAT_MBUS_BAYER },
+    { V4L2_MBUS_FMT_SGBRG12_1X12, 0, "V4L2_MBUS_FMT_SGBRG12_1X12", "SGBRG12_1X12", 12, FORMAT_MBUS_BAYER },
+    { V4L2_MBUS_FMT_SGRBG12_1X12, 0, "V4L2_MBUS_FMT_SGRBG12_1X12", "SGRBG12_1X12", 12, FORMAT_MBUS_BAYER },
+    { V4L2_MBUS_FMT_SRGGB12_1X12, 0, "V4L2_MBUS_FMT_SRGGB12_1X12", "SRGGB12_1X12", 12, FORMAT_MBUS_BAYER },
+
+    { V4L2_MBUS_FMT_SBGGR8_1X8, 0, "V4L2_MBUS_FMT_SBGGR8_1X8", "SBGGR8_1X8", 8, FORMAT_MBUS_BAYER },
+    { V4L2_MBUS_FMT_SGBRG8_1X8, 0, "V4L2_MBUS_FMT_SGBRG8_1X8", "SGBRG8_1X8", 8, FORMAT_MBUS_BAYER },
+    { V4L2_MBUS_FMT_SGRBG8_1X8, 0, "V4L2_MBUS_FMT_SGRBG8_1X8", "SGRBG8_1X8", 8, FORMAT_MBUS_BAYER },
+    { V4L2_MBUS_FMT_SRGGB8_1X8, 0, "V4L2_MBUS_FMT_SRGGB8_1X8", "SRGGB8_1X8", 8, FORMAT_MBUS_BAYER },
+
+    { V4L2_MBUS_FMT_YUYV8_1X16, 0, "V4L2_MBUS_FMT_YUYV8_1X16", "YUYV8_1X16", 16, FORMAT_MBUS_YUV },
+    { V4L2_MBUS_FMT_UYVY8_2X8, 0, "V4L2_MBUS_FMT_UYVY8_2X8","UYVY8_2X8", 8, FORMAT_MBUS_YUV},
+
+};
+
+const string pixelCode2String(int32_t code)
+{
+
+    for (size_t i = 0; i < ARRAY_SIZE(gFormatMapping); i++) {
+        if (gFormatMapping[i].pixelCode == code) {
+            return gFormatMapping[i].fullName;
+        }
+    }
+    for (size_t i = 0; i < ARRAY_SIZE(gFormatMapping); i++) {
+        if (gFormatMapping[i].commonPixelCode == code) {
+            return gFormatMapping[i].fullName;
+        }
+    }
+
+    LOGE("Invalid Pixel Format: 0x%x", code);
+    return "INVALID FORMAT";
+}
+
+int32_t string2PixelCode(const string &code)
+{
+    if (code.empty()) {
+        LOGE("Invalid Pixel Format: %s", code.c_str());
+        return -1;
+    }
+
+    for (size_t i = 0; i < ARRAY_SIZE(gFormatMapping); i++) {
+        if (gFormatMapping[i].fullName == code) {
+            return gFormatMapping[i].pixelCode;
+        }
+    }
+
+    LOGE("Invalid Pixel Format: %s", code.c_str());
+    return -1;
+}
+
+const string format2string(int32_t format)
+{
+    for (size_t i = 0; i < ARRAY_SIZE(gFormatMapping); i++) {
+        if (gFormatMapping[i].pixelCode == format) {
+            return gFormatMapping[i].shortName;
+        }
+    }
+
+    LOGW("Not in our format list :%x", format);
+    return "INVALID-FORMAT";
+}
+
+bool isPlanarFormat(int32_t format)
+{
+    return (format == V4L2_PIX_FMT_NV12 ||
+            format == V4L2_PIX_FMT_NV21 ||
+            format == V4L2_PIX_FMT_YUV420 ||
+            format == V4L2_PIX_FMT_YVU420 ||
+            format == V4L2_PIX_FMT_YUV422P);
+}
+
+bool isRaw(int32_t format)
+{
+    int32_t size = ARRAY_SIZE(gFormatMapping);
+    for (int32_t i = 0; i < size; i++) {
+        if (gFormatMapping[i].pixelCode == format) {
+            // Both normal raw and vector raw treated as raw here.
+            return gFormatMapping[i].type == FORMAT_RAW_VEC || gFormatMapping[i].type == FORMAT_RAW;
+        }
+    }
+
+    return false;
+}
+
+bool isVectorizedRaw(int32_t format)
+{
+    for (size_t i = 0; i < ARRAY_SIZE(gFormatMapping); i++) {
+        if (gFormatMapping[i].pixelCode == format) {
+            return gFormatMapping[i].type == FORMAT_RAW_VEC;
+        }
+    }
+
+    return false;
+}
+
+/**
+ * Calculate bytes per line(bpl) based on fourcc format.
+ *
+ * \param[in] format 4CC code in OS specific format
+ * \return bpl bytes per line
+ */
+int32_t getBpl(int32_t format, int32_t width)
+{
+    int32_t bpl = 0;
+    switch (format) {
+        case css_fourcc_yuv420_12_p64:      // YUV
+            /*
+             * Align based on UV planes, which have half the strides compared to y plane.
+             * 42 whole pixels in each 64 byte word, rest 8 bits per word is padding.
+             * The total bpl is double the UV-plane strides.
+             */
+            bpl = ceil(((double)width / 2) / 42) * 64 * 2;
+            break;
+        case css_fourcc_yyuv420_v32:        // Y032
+            bpl = width * 6;
+            break;
+        case css_fourcc_grbg_12_li:            // CSL6
+            bpl = width * 4;
+            break;
+        case css_fourcc_grbg_10_v32:        // BV0G
+        case ia_cipf_frame_fourcc_i420:     // V420
+        case css_fourcc_raw_interleaved:    // BV0K
+        case ia_cipf_frame_fourcc_ba10:     // BA10
+        case ia_cipf_frame_fourcc_gr10:     // GR10
+        case ia_cipf_frame_fourcc_ba12:
+        case css_fourcc_p010:               //YUV 10bit serial
+        case css_fourcc_p010_lsb:
+        case css_fourcc_p010_msb_tile_y:
+        case css_fourcc_p010_msb_cile_y:
+            bpl = width * 2;
+            break;
+        case ia_cipf_frame_fourcc_nv12:     // NV12
+        case ia_cipf_frame_fourcc_grbg:     // GRBG
+            bpl = width;
+            break;
+        default:
+            bpl = width;
+            LOGW("bpl defaulting to width for format:%s", CameraUtils::format2string(format).c_str());
+            break;
+    }
+    return bpl;
+}
+
+/**
+ *  Retrieve the bits per pixel  from the OS specific pixel code.
+ *  This is ususally used for buffer allocation calculations
+ *
+ *  \param [in] format 4CC code in OS specific format
+ *  \return bits per pixel
+ */
+int32_t getBpp(int32_t format)
+{
+    for (size_t i = 0; i < ARRAY_SIZE(gFormatMapping); i++) {
+        if (gFormatMapping[i].pixelCode == format) {
+            return gFormatMapping[i].bpp;
+        }
+    }
+
+    LOGE("There is no bpp supplied for format %s",
+            pixelCode2String(format).c_str());
+    return -1;
+}
+
+/**
+ *  Retrieve the bits per pixel from the common pixel code format (CIPF)
+ *  This is usually used for buffer allocation calculations
+ *
+ *  \param [in] format 4CC code in Common format
+ *  \return bits per pixel
+ */
+int32_t getBppFromCommon(int32_t format)
+{
+    for (size_t i = 0; i < ARRAY_SIZE(gFormatMapping); i++) {
+        if (gFormatMapping[i].commonPixelCode == format) {
+            return gFormatMapping[i].bpp;
+        }
+    }
+
+    LOGE("There is no bpp supplied for format %s",
+            pixelCode2String(format).c_str());
+    return -1;
+}
+
+int32_t getNumOfPlanes(int32_t format)
+{
+    switch(format) {
+        case V4L2_PIX_FMT_NV12:
+        case V4L2_PIX_FMT_SGRBG8:
+        case V4L2_FMT_IPU_ISYS_META:
+            return 1;
+        //Add more when needed...
+        default:
+            return 1;
+    }
+}
+
+int32_t getV4L2Format(const int32_t commonPixelFormat)
+{
+    for (size_t i = 0; i < ARRAY_SIZE(gFormatMapping); i++) {
+        if (gFormatMapping[i].commonPixelCode == commonPixelFormat)
+            return gFormatMapping[i].pixelCode;
+    }
+
+    LOGE("Failed to find any V4L2 format with format %s",
+            pixelCode2String(commonPixelFormat).c_str());
+    return -1;
+}
+
+} // namespace utils
+} // namespace graphconfig
diff --git a/camera/hal/intel/ipu6/src/platformdata/gc/FormatUtils.h b/camera/hal/intel/ipu6/src/platformdata/gc/FormatUtils.h
new file mode 100644
index 000000000000..d6f3a03ad9dd
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/gc/FormatUtils.h
@@ -0,0 +1,34 @@
+/*
+ * Copyright (C) 2016-2019 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <string>
+
+namespace graphconfig {
+namespace utils {
+
+int32_t getV4L2Format(const int32_t commonPixelFormat);
+const std::string format2string(int32_t format);
+bool isPlanarFormat(int32_t v4l2Format);
+bool isRaw(int32_t format);
+bool isVectorizedRaw(int32_t format);
+int32_t getBpl(int32_t format, int32_t width);
+int32_t getBpp(int32_t format);
+int32_t getBppFromCommon(int32_t format);
+
+}  // namespace utils
+}  // namespace graphconfig
diff --git a/camera/hal/intel/ipu6/src/platformdata/gc/GraphConfig.cpp b/camera/hal/intel/ipu6/src/platformdata/gc/GraphConfig.cpp
new file mode 100644
index 000000000000..1fb4ed262100
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/gc/GraphConfig.cpp
@@ -0,0 +1,199 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "GraphConfig"
+
+#include "src/platformdata/gc/GraphConfig.h"
+
+#include "PlatformData.h"
+#include "iutils/CameraLog.h"
+
+using std::vector;
+using std::map;
+using std::string;
+
+namespace icamera {
+
+GraphConfig::GraphConfig(int32_t camId, ConfigMode mode) :
+        mCameraId(camId) {
+    mGraphConfigImpl = std::unique_ptr<GraphConfigImpl>(new GraphConfigImpl(camId, mode,
+                                                        PlatformData::getGraphSettingsType(camId)));
+}
+
+GraphConfig::GraphConfig() :
+        mCameraId(-1) {
+    mGraphConfigImpl = std::unique_ptr<GraphConfigImpl>(new GraphConfigImpl());
+}
+
+GraphConfig::~GraphConfig() {
+}
+
+void GraphConfig::addCustomKeyMap() {
+    mGraphConfigImpl->addCustomKeyMap();
+}
+
+status_t GraphConfig::parse(int cameraId, const char *settingsXmlFile) {
+    string graphDescFile = PlatformData::getGraphDescFilePath();
+    string settingsFile = PlatformData::getGraphSettingFilePath() + settingsXmlFile;
+    return mGraphConfigImpl->parse(cameraId, graphDescFile.c_str(), settingsFile.c_str());
+}
+
+void GraphConfig::releaseGraphNodes() {
+    mGraphConfigImpl->releaseGraphNodes();
+}
+
+status_t GraphConfig::configStreams(const vector<HalStream*> &activeStreams) {
+    LOG1("@%s", __func__);
+
+    int ret = mGraphConfigImpl->configStreams(activeStreams);
+    CheckError(ret != OK, UNKNOWN_ERROR, "%s, Failed to config streams", __func__);
+
+    ret = mGraphConfigImpl->getGraphConfigData(&mGraphData);
+    CheckError(ret != OK, UNKNOWN_ERROR, "%s, Failed to get the static graph config data", __func__);
+
+    return OK;
+}
+
+status_t GraphConfig::getGdcKernelSetting(uint32_t *kernelId,
+                                          ia_isp_bxt_resolution_info_t *resolution) {
+    LOG1("@%s", __func__);
+    CheckError(!kernelId || !resolution, UNKNOWN_ERROR, "kernelId or resolution is nullptr");
+
+    if ((mGraphData.gdcReso.input_width == 0) || (mGraphData.gdcReso.input_height == 0) ||
+        (mGraphData.gdcReso.output_width == 0) || (mGraphData.gdcReso.output_height == 0)) {
+        LOG2("%s, Failed to get gdc InReso: w: %d, h: %d; OutReso: w: %d, h: %d; ", __func__,
+             mGraphData.gdcReso.input_width, mGraphData.gdcReso.input_height,
+             mGraphData.gdcReso.output_width, mGraphData.gdcReso.output_height);
+        return NO_ENTRY;
+    }
+
+    *kernelId = mGraphData.gdcKernelId;
+    *resolution = mGraphData.gdcReso;
+
+    return OK;
+}
+
+status_t GraphConfig::graphGetStreamIds(vector<int32_t> &streamIds) {
+    LOG1("@%s", __func__);
+    CheckError(mGraphData.streamIds.empty(), UNKNOWN_ERROR, "%s, The streamIds vector is empty", __func__);
+
+    streamIds = mGraphData.streamIds;
+    return OK;
+}
+
+int GraphConfig::getStreamIdByPgName(string pgName) {
+    LOG1("@%s", __func__);
+    CheckError(mGraphData.pgInfo.empty(), -1, "%s, The pgInfo vector is empty", __func__);
+
+    for (auto &info : mGraphData.pgInfo) {
+        if (info.pgName == pgName) {
+            return info.streamId;
+        }
+    }
+
+    LOGE("%s, Failed to get stream id for pgName: %s", __func__, pgName.c_str());
+    return -1;
+}
+
+int GraphConfig::getPgIdByPgName(string pgName) {
+    LOG1("@%s", __func__);
+    CheckError(mGraphData.pgInfo.empty(), -1, "%s, The pgInfo vector is empty", __func__);
+
+    for (auto &info : mGraphData.pgInfo) {
+        if (info.pgName == pgName) {
+            return info.pgId;
+        }
+    }
+
+    LOGE("%s, Failed to get pg id for pgName: %s", __func__, pgName.c_str());
+    return -1;
+}
+
+ia_isp_bxt_program_group* GraphConfig::getProgramGroup(int32_t streamId) {
+    LOG1("@%s", __func__);
+    CheckError(mGraphData.programGroup.empty(), nullptr, "%s, The programGroup vector is empty", __func__);
+
+    for (auto &info : mGraphData.programGroup) {
+        if (info.streamId == streamId && info.pgPtr != nullptr) {
+            return info.pgPtr;
+        }
+    }
+
+    LOGE("%s, Failed to get programGroup for streamId", __func__, streamId);
+    return nullptr;
+}
+
+status_t GraphConfig::getMBRData(int32_t streamId, ia_isp_bxt_gdc_limits *data) {
+    LOG1("@%s", __func__);
+    for (auto &info : mGraphData.mbrInfo) {
+        if (streamId == info.streamId) {
+            data = &info.data;
+            return OK;
+        }
+    }
+
+    return BAD_VALUE;
+}
+
+status_t GraphConfig::getPgNames(vector<string>* pgNames) {
+    LOG1("@%s", __func__);
+    CheckError(mGraphData.pgNames.empty(), UNKNOWN_ERROR, "%s, The pgNames vector is empty", __func__);
+
+    *pgNames = mGraphData.pgNames;
+    return OK;
+}
+
+status_t GraphConfig::getPgRbmValue(string pgName, IGraphType::StageAttr *stageAttr) {
+    LOG1("@%s", __func__);
+    CheckError(mGraphData.pgInfo.empty(), UNKNOWN_ERROR, "%s, The pgInfo vector is empty", __func__);
+
+    for (auto &info : mGraphData.pgInfo) {
+        if (info.pgName == pgName && info.rbmValue.rbm != nullptr) {
+            *stageAttr = info.rbmValue;
+            return OK;
+        }
+    }
+
+    return BAD_VALUE;
+}
+
+int GraphConfig::getProgramGroup(string pgName, ia_isp_bxt_program_group* programGroupForPG) {
+    LOG1("@%s", __func__);
+    return mGraphConfigImpl->getProgramGroup(pgName, programGroupForPG);
+}
+
+status_t GraphConfig::pipelineGetConnections(const vector<string>& pgList,
+                                             vector<IGraphType::PipelineConnection> *confVector) {
+    LOG1("@%s", __func__);
+    CheckError(!confVector, UNKNOWN_ERROR, "%s, The confVector is nullptr", __func__);
+
+    status_t ret;
+    std::vector<IGraphType::ScalerInfo> scalerInfo;
+
+    ret = mGraphConfigImpl->pipelineGetConnections(pgList, &scalerInfo, confVector);
+    CheckError(ret != OK, ret, "%s, Failed to pipelineGetConnections", __func__);
+
+    CheckError(mCameraId == -1, UNKNOWN_ERROR, "%s: mCameraId is -1", __func__);
+    PlatformData::setScalerInfo(mCameraId, scalerInfo);
+    return OK;
+}
+
+status_t GraphConfig::getPgIdForKernel(const uint32_t streamId, const int32_t kernelId, int32_t *pgId) {
+    LOG1("@%s", __func__);
+    CheckError(!pgId, UNKNOWN_ERROR, "%s, the pgId is nullptr", __func__);
+    return mGraphConfigImpl->getPgIdForKernel(streamId, kernelId, pgId);
+}
+}  // icamera
diff --git a/camera/hal/intel/ipu6/src/platformdata/gc/GraphConfig.h b/camera/hal/intel/ipu6/src/platformdata/gc/GraphConfig.h
new file mode 100644
index 000000000000..03c306f5683d
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/gc/GraphConfig.h
@@ -0,0 +1,88 @@
+/*
+ * Copyright (C) 2019-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <memory>
+#include <utility>
+#include <vector>
+#include "HalStream.h"
+#include "iutils/Utils.h"
+#include "iutils/Errors.h"
+
+#ifdef ENABLE_SANDBOXING
+#include "modules/sandboxing/client/GraphConfigImpl.h"
+#else
+#include "modules/algowrapper/graph/GraphConfigImpl.h"
+#endif
+
+namespace icamera {
+
+/**
+ * \class GraphConfig
+ *
+ * \brief This is a wrapper of GraphConfigImpl class and it provides the
+ * public APIs to get the graph config data.
+ *
+ * It maintains one static area and GraphConfigImpl object, user get graph
+ * config data from the local structure or GraphConfigImpl object through
+ * the public APIs
+ */
+class GraphConfig : public IGraphConfig {
+public:
+    GraphConfig();
+    GraphConfig(int32_t camId, ConfigMode mode);
+    virtual ~GraphConfig();
+
+    void addCustomKeyMap();
+    status_t parse(int cameraId, const char *settingsXmlFile);
+    void releaseGraphNodes();
+
+    // These public methods called by GraphConfigManager
+    status_t configStreams(const std::vector<HalStream*> &activeStreams);
+    int getSelectedMcId() { return mGraphData.mcId; }
+    virtual int getGraphId(void) { return mGraphData.graphId; }
+    virtual void getCSIOutputResolution(camera_resolution_t &reso) { reso = mGraphData.csiReso; }
+
+    virtual status_t getGdcKernelSetting(uint32_t *kernelId,
+                                         ia_isp_bxt_resolution_info_t *resolution);
+    virtual status_t graphGetStreamIds(std::vector<int32_t> &streamIds);
+    virtual int getStreamIdByPgName(std::string pgName);
+    virtual int getPgIdByPgName(std::string pgName);
+    virtual ia_isp_bxt_program_group *getProgramGroup(int32_t streamId);
+    virtual status_t getPgRbmValue(std::string pgName, IGraphType::StageAttr *stageAttr);
+    virtual status_t getMBRData(int32_t streamId, ia_isp_bxt_gdc_limits *data);
+    virtual status_t getPgNames(std::vector<std::string>* pgNames);
+
+    virtual int getProgramGroup(std::string pgName,
+                                ia_isp_bxt_program_group* programGroupForPG);
+    virtual status_t getPgIdForKernel(const uint32_t streamId,
+                                      const int32_t kernelId, int32_t *pgId);
+
+    virtual status_t pipelineGetConnections(
+                         const std::vector<std::string> &pgList,
+                         std::vector<IGraphType::PipelineConnection> *confVector);
+private:
+    // Disable copy constructor and assignment operator
+    DISALLOW_COPY_AND_ASSIGN(GraphConfig);
+
+private:
+    int32_t mCameraId;
+    IGraphType::GraphConfigData mGraphData;
+    std::unique_ptr<GraphConfigImpl> mGraphConfigImpl;
+};
+
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/platformdata/gc/GraphConfigManager.cpp b/camera/hal/intel/ipu6/src/platformdata/gc/GraphConfigManager.cpp
new file mode 100644
index 000000000000..3b792e0f8600
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/gc/GraphConfigManager.cpp
@@ -0,0 +1,179 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "GraphConfigManager"
+
+#include "src/platformdata/gc/GraphConfigManager.h"
+
+#include "iutils/Utils.h"
+#include "iutils/CameraLog.h"
+#include "PlatformData.h"
+
+using std::vector;
+using std::map;
+
+namespace icamera {
+GraphConfigManager::GraphConfigManager(int32_t cameraId) :
+    mGcConfigured(false),
+    mCameraId(cameraId),
+    mMcId(-1)
+{
+}
+
+GraphConfigManager::~GraphConfigManager()
+{
+    mGraphConfigMap.clear();
+    mGcConfigured = false;
+    releaseHalStream();
+}
+
+void GraphConfigManager::releaseHalStream()
+{
+    for(auto &halStream : mHalStreamVec) {
+        delete halStream;
+    }
+    mHalStreamVec.clear();
+}
+
+/*
+ * Get the useCase from the stream and operationMode.
+ */
+StreamUseCase GraphConfigManager::getUseCaseFromStream(ConfigMode configMode, const stream_t &stream)
+{
+    if (configMode == CAMERA_STREAM_CONFIGURATION_MODE_STILL_CAPTURE ||
+            stream.usage == CAMERA_STREAM_STILL_CAPTURE)
+        return USE_CASE_STILL_CAPTURE;
+
+    return USE_CASE_PREVIEW;
+}
+
+/**
+ * Initialize the state of the GraphConfigManager after parsing the stream
+ * configuration.
+ * Perform the first level query to find a subset of settings that fulfill the
+ * constrains from the stream configuration.
+ *
+ * \param[in] streamList: all the streams info.
+ */
+status_t GraphConfigManager::configStreams(const stream_config_t *streamList)
+{
+    HAL_TRACE_CALL(CAMERA_DEBUG_LOG_LEVEL1);
+    CheckError(!streamList, BAD_VALUE, "%s: Null streamList configured", __func__);
+
+    vector <ConfigMode> configModes;
+    int ret = PlatformData::getConfigModesByOperationMode(mCameraId, streamList->operation_mode, configModes);
+    CheckError(ret != OK, ret, "%s, get ConfigMode failed %d", __func__, ret);
+
+    // Convert the stream_t to HalStream
+    // Use the stream list with descending order to find graph settings.
+    releaseHalStream();
+    for (int i = 0; i < streamList->num_streams; i++) {
+        // Don't handle input stream or opaque RAW stream when configure graph configuration.
+        if (streamList->streams[i].streamType == CAMERA_STREAM_INPUT ||
+            streamList->streams[i].usage == CAMERA_STREAM_OPAQUE_RAW) continue;
+
+        bool stored = false;
+        StreamUseCase useCase = getUseCaseFromStream(configModes[0], streamList->streams[i]);
+        streamProps props = {
+            static_cast<uint32_t>(streamList->streams[i].width),
+            static_cast<uint32_t>(streamList->streams[i].height),
+            streamList->streams[i].format,
+            streamList->streams[i].id,
+            useCase,
+        };
+        HalStream* halStream = new HalStream(props, static_cast<void*>(&streamList->streams[i]));
+        CheckError(!halStream, UNKNOWN_ERROR, "Failed to create hal stream");
+
+        for (size_t j = 0; j < mHalStreamVec.size(); j++) {
+            if (halStream->width() * halStream->height() > mHalStreamVec[j]->width() * mHalStreamVec[j]->height()) {
+                stored = true;
+                mHalStreamVec.insert((mHalStreamVec.begin() + j), halStream);
+                break;
+            }
+        }
+        if (!stored)
+            mHalStreamVec.push_back(halStream);
+    }
+
+    //debug
+    dumpStreamConfig();
+    mGraphConfigMap.clear();
+    mMcId = -1;
+
+    for (auto mode : configModes) {
+        LOG1("Mapping the operationMode %d to ConfigMode %d", streamList->operation_mode, mode);
+
+        std::shared_ptr<GraphConfig> graphConfig = std::make_shared<GraphConfig>(mCameraId, mode);
+        ret = graphConfig->configStreams(mHalStreamVec);
+        CheckWarning(ret != OK, ret, "%s, Failed to configure graph: real ConfigMode %x", __func__, mode);
+
+        int id = graphConfig->getSelectedMcId();
+        CheckError((id != -1 && mMcId != -1 && mMcId != id), UNKNOWN_ERROR,
+                    "Not support two different MC ID at same time:(%d/%d)", mMcId, id);
+        mMcId = id;
+        LOGG("%s: Add graph setting for op_mode %d", __func__, mode);
+        mGraphConfigMap[mode] = graphConfig;
+    }
+
+    mGcConfigured = true;
+    return OK;
+}
+
+std::shared_ptr<IGraphConfig> GraphConfigManager::getGraphConfig(ConfigMode configMode)
+{
+    for (auto& gc : mGraphConfigMap) {
+        if (gc.first == configMode) {
+            LOGG("%s: found graph config for mode %d", __func__, configMode);
+            return gc.second;
+        }
+    }
+
+    return nullptr;
+}
+
+void GraphConfigManager::dumpStreamConfig()
+{
+    for (size_t i = 0; i < mHalStreamVec.size(); i++) {
+        LOG1("stream[%zu] %dx%d, fmt %s", i,
+             mHalStreamVec[i]->width(), mHalStreamVec[i]->height(),
+             CameraUtils::pixelCode2String(mHalStreamVec[i]->format()));
+    }
+}
+
+map<int, IGraphConfigManager*> IGraphConfigManager::sInstances;
+Mutex IGraphConfigManager::sLock;
+
+IGraphConfigManager* IGraphConfigManager::getInstance(int cameraId)
+{
+    AutoMutex lock(sLock);
+    if (sInstances.find(cameraId) != sInstances.end()) {
+        return sInstances[cameraId];
+    }
+
+    sInstances[cameraId] = new GraphConfigManager(cameraId);
+    return sInstances[cameraId];
+}
+
+void IGraphConfigManager::releaseInstance(int cameraId)
+{
+    AutoMutex lock(sLock);
+    if (sInstances.find(cameraId) != sInstances.end()) {
+        IGraphConfigManager* gcManager = sInstances[cameraId];
+        sInstances.erase(cameraId);
+        delete gcManager;
+    }
+}
+}  // icamera
diff --git a/camera/hal/intel/ipu6/src/platformdata/gc/GraphConfigManager.h b/camera/hal/intel/ipu6/src/platformdata/gc/GraphConfigManager.h
new file mode 100644
index 000000000000..68b5066ed200
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/gc/GraphConfigManager.h
@@ -0,0 +1,85 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <gcss.h>
+
+#include <memory>
+#include <utility>
+#include <vector>
+
+#include "iutils/Errors.h"
+#include "iutils/Thread.h"
+#include "GraphConfig.h"
+#include "IGraphConfigManager.h"
+
+namespace icamera {
+
+/**
+ * \class GraphConfigManager
+ *
+ * Class to wrap over parsing and executing queries on graph settings.
+ * GraphConfigManager owns the interface towards GCSS and provides convenience
+ * for HAL to execute queries and it generates GraphConfig objects as results.
+ *
+ * GraphConfigManager also provides static method for parsing graph descriptor
+ * and graph settings from XML files and filtering that data based on sensor.
+ * The \class GraphConfigmanager::Nodes object is stored in CameraCapInfo and
+ * is used when instantiating GCM.
+ *
+ * At camera open, GraphConfigManager object is created.
+ * At stream config time the state of GraphConfig manager changes with the
+ * result of the first query. This is the possible subset of graph settings that
+ * can fulfill the requirements of requested streams.
+ * At this point, there may be more than one options, but
+ * GCM can always return some default settings.
+ *
+ * Per each request, GraphConfigManager creates GraphConfig objects based
+ * on request content. These objects are owned by GCM in a pool, and passed
+ * around HAL via shared pointers.
+ */
+class GraphConfigManager: public IGraphConfigManager
+{
+public:
+    explicit GraphConfigManager(int32_t cameraId);
+    virtual ~GraphConfigManager();
+
+    // Public APIs in IGraphConfigManager
+    virtual status_t configStreams(const stream_config_t *streamList);
+    virtual std::shared_ptr<IGraphConfig> getGraphConfig(ConfigMode configMode);
+    virtual int getSelectedMcId() { LOGG("%s: %d", __func__, mMcId); return mMcId; }
+    virtual bool isGcConfigured(void) { LOGG("%s: %d", __func__, mGcConfigured); return mGcConfigured; }
+
+private:
+    // Disable copy constructor and assignment operator
+    DISALLOW_COPY_AND_ASSIGN(GraphConfigManager);
+
+    StreamUseCase getUseCaseFromStream(ConfigMode configMode, const stream_t &stream);
+    void releaseHalStream();
+
+    // Debuging helpers
+    void dumpStreamConfig();
+private:
+
+    bool mGcConfigured;
+    int32_t mCameraId;
+    std::map<ConfigMode, std::shared_ptr<GraphConfig> > mGraphConfigMap;
+    std::vector<HalStream*> mHalStreamVec;
+    int mMcId;
+};
+
+} // icamera
diff --git a/camera/hal/intel/ipu6/src/platformdata/gc/GraphUtils.cpp b/camera/hal/intel/ipu6/src/platformdata/gc/GraphUtils.cpp
new file mode 100644
index 000000000000..7f822e6b20c6
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/gc/GraphUtils.cpp
@@ -0,0 +1,112 @@
+/*
+ * Copyright (C) 2018-2019 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#define LOG_TAG "GraphUtils"
+
+#include "GraphUtils.h"
+#include "iutils/CameraLog.h"
+#include "iutils/Utils.h"
+
+#define psys_2600_pg_uid(id) ia_fourcc(((id & 0xFF00) >> 8),id,'G','0')
+#define psys_2600_pg_id_from_uid(uid) ((uid & 0xFFFF0000) >> 16)
+#define psys_2600_term_idx_from_uid(uid) ((uid & 0x0000FFFF) - 1)
+
+using namespace std;
+
+namespace icamera {
+
+void GraphUtils::dumpConnections(const std::vector<IGraphType::PipelineConnection>& connections)
+{
+    if (!Log::isDebugLevelEnable(CAMERA_DEBUG_LOG_GRAPH)) {
+        return;
+    }
+
+    LOGG("Graph connections:");
+    for (auto& conn : connections) {
+
+        LOGG("Format settings: enabled === %d ===, terminalIdx %d, width %d, height %d, fourcc %s, bpl %d, bpp %d",
+                    conn.portFormatSettings.enabled,
+                    conn.portFormatSettings.terminalId,
+                    conn.portFormatSettings.width, conn.portFormatSettings.height,
+                    CameraUtils::fourcc2String(conn.portFormatSettings.fourcc).c_str(),
+                    conn.portFormatSettings.bpl, conn.portFormatSettings.bpp);
+
+        LOGG("Connection config: sourceStage %d(%d), sourceTerminal %d(%d), sourceIteration %d, " \
+                    "sinkStage %d(%d), sinkTerminal %d(%d), sinkIteration %d, connectionType %d",
+                    psys_2600_pg_id_from_uid(conn.connectionConfig.mSourceStage),
+                    conn.connectionConfig.mSourceStage,
+                    conn.connectionConfig.mSourceTerminal - conn.connectionConfig.mSourceStage -1,
+                    conn.connectionConfig.mSourceTerminal,
+                    conn.connectionConfig.mSourceIteration,
+                    psys_2600_pg_id_from_uid(conn.connectionConfig.mSinkStage),
+                    conn.connectionConfig.mSinkStage,
+                    conn.connectionConfig.mSinkTerminal - conn.connectionConfig.mSinkStage -1,
+                    conn.connectionConfig.mSinkTerminal,
+                    conn.connectionConfig.mSinkIteration,
+                    conn.connectionConfig.mConnectionType);
+
+        LOGG("Edge port: %d", conn.hasEdgePort);
+    }
+
+    return;
+}
+
+void GraphUtils::dumpKernelInfo(const ia_isp_bxt_program_group& programGroup)
+{
+    if (!Log::isDebugLevelEnable(CAMERA_DEBUG_LOG_GRAPH)) {
+        return;
+    }
+
+    LOGG("Kernel info: count %d, opMode %d", programGroup.kernel_count, programGroup.operation_mode);
+
+    for(unsigned int i = 0; i< programGroup.kernel_count; i++) {
+
+        const ia_isp_bxt_run_kernels_t& curRunKernel = programGroup.run_kernels[i];
+
+        LOGG("uid %d, streamId: %d, enabled %d", curRunKernel.kernel_uuid, curRunKernel.stream_id,
+                    curRunKernel.enable);
+
+        if (programGroup.run_kernels[i].resolution_info) {
+            LOGG("Resolution: inputWidth %d, inputHeight %d, inputCrop %d %d %d %d," \
+                       "outputWidth %d, outputHeight %d, outputCrop %d %d %d %d,",
+                       curRunKernel.resolution_info->input_width, curRunKernel.resolution_info->input_height,
+                       curRunKernel.resolution_info->input_crop.left, curRunKernel.resolution_info->input_crop.top,
+                       curRunKernel.resolution_info->input_crop.right, curRunKernel.resolution_info->input_crop.bottom,
+                       curRunKernel.resolution_info->output_width, curRunKernel.resolution_info->output_height,
+                       curRunKernel.resolution_info->output_crop.left, curRunKernel.resolution_info->output_crop.top,
+                       curRunKernel.resolution_info->output_crop.right, curRunKernel.resolution_info->output_crop.bottom);
+        }
+
+        if (programGroup.run_kernels[i].resolution_history) {
+            LOGG("Resolution history: inputWidth %d, inputHeight %d, inputCrop %d %d %d %d," \
+                       "outputWidth %d, outputHeight %d, outputCrop %d %d %d %d,",
+                       curRunKernel.resolution_history->input_width, curRunKernel.resolution_history->input_height,
+                       curRunKernel.resolution_history->input_crop.left, curRunKernel.resolution_history->input_crop.top,
+                       curRunKernel.resolution_history->input_crop.right, curRunKernel.resolution_history->input_crop.bottom,
+                       curRunKernel.resolution_history->output_width, curRunKernel.resolution_history->output_height,
+                       curRunKernel.resolution_history->output_crop.left, curRunKernel.resolution_history->output_crop.top,
+                       curRunKernel.resolution_history->output_crop.right, curRunKernel.resolution_history->output_crop.bottom);
+
+        }
+
+        LOGG("metadata %d %d %d %d, bppInfo: %d %d, outputCount %d",
+                   curRunKernel.metadata[0], curRunKernel.metadata[1], curRunKernel.metadata[2], curRunKernel.metadata[3],
+                   curRunKernel.bpp_info.input_bpp, curRunKernel.bpp_info.output_bpp,
+                   curRunKernel.output_count);
+    }
+
+    return;
+}
+}
diff --git a/camera/hal/intel/ipu6/src/platformdata/gc/GraphUtils.h b/camera/hal/intel/ipu6/src/platformdata/gc/GraphUtils.h
new file mode 100644
index 000000000000..86686c576e1c
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/gc/GraphUtils.h
@@ -0,0 +1,27 @@
+/*
+ * Copyright (C) 2018-2019 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+#include "IGraphConfig.h"
+#include "IGraphConfigManager.h"
+#include "ia_isp_bxt_types.h"
+
+namespace icamera {
+namespace GraphUtils {
+    void dumpConnections(const std::vector<IGraphType::PipelineConnection>& connections);
+    void dumpKernelInfo(const ia_isp_bxt_program_group& programGroup);
+};
+}
diff --git a/camera/hal/intel/ipu6/src/platformdata/gc/HalStream.h b/camera/hal/intel/ipu6/src/platformdata/gc/HalStream.h
new file mode 100644
index 000000000000..139125a46552
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/gc/HalStream.h
@@ -0,0 +1,74 @@
+/*
+ * Copyright (C) 2016-2019 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+namespace icamera {
+
+// Temporary solution
+enum StreamUseCase {
+    USE_CASE_COMMON = 0,
+    USE_CASE_PREVIEW = 1 << 0,        // For HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED
+    USE_CASE_VIDEO = 1 << 1,          // For HAL_PIXEL_FORMAT_IMPLEMENTATION_DEFINED
+    USE_CASE_STILL_CAPTURE = 1 << 2,  // For HAL_PIXEL_FORMAT_BLOB/HAL_PIXEL_FORMAT_YCbCr_420_888
+    USE_CASE_RAW = 1 << 3,            // For HAL_PIXEL_FORMAT_RAW16/HAL_PIXEL_FORMAT_RAW_OPAQUE
+    USE_CASE_ZSL = 1 << 4,            // For ZSL stream
+    USE_CASE_INPUT = 1 << 5,           // For input stream
+};
+
+struct streamProps {
+    uint32_t width;
+    uint32_t height;
+    int format;
+    int streamId;
+    StreamUseCase useCase;
+};
+
+class HalStream
+{
+ public:
+    HalStream(struct streamProps &props, void *priv):
+        mWidth(props.width),
+        mHeight(props.height),
+        mFormat(props.format),
+        mStreamId(props.streamId),
+        mUseCase(props.useCase)
+    {
+        maxBuffers = 0;
+        mPrivate = priv;
+    }
+
+    ~HalStream() { }
+
+    uint32_t width() const { return mWidth; }
+    uint32_t height() const { return mHeight; }
+    int format() const { return mFormat; }
+    int streamId() const { return mStreamId; }
+    StreamUseCase useCase() const { return mUseCase; }
+    void *priv() { return mPrivate; }
+
+ public:
+    uint32_t mWidth;
+    uint32_t mHeight;
+    int mFormat;  // TODO: use v4l2 definition
+    int mStreamId;
+    StreamUseCase mUseCase;
+
+    int maxBuffers;
+    void *mPrivate;
+};
+
+} /* namespace icamera */
diff --git a/camera/hal/intel/ipu6/src/platformdata/gc/IGraphConfig.h b/camera/hal/intel/ipu6/src/platformdata/gc/IGraphConfig.h
new file mode 100644
index 000000000000..8771706dc447
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/gc/IGraphConfig.h
@@ -0,0 +1,203 @@
+/*
+ * Copyright (C) 2018-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+#include <string>
+#include "HalStream.h"
+#include "Parameters.h"
+#include "iutils/CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+
+#include <gcss.h>
+#include <gcss_aic_utils.h>
+#include "ia_isp_bxt_types.h"
+#include "ia_view_types.h"
+
+typedef uint32_t ia_uid;
+
+namespace GCSS {
+    class GraphConfigNode;
+    class GraphQueryManager;
+    class ItemUID;
+}
+
+typedef GCSS::GraphConfigNode Node;
+typedef std::vector<Node*> NodesPtrVector;
+
+namespace icamera {
+
+/**
+ * Stream id associated with still capture.
+ */
+static const int32_t STILL_STREAM_ID = 60000;
+/**
+ * Stream id associated with video stream.
+ */
+#ifdef TNR7_CM
+static const int32_t VIDEO_STREAM_ID = 60006;
+#else
+static const int32_t VIDEO_STREAM_ID = 60001;
+#endif
+
+namespace IGraphType {
+class ConnectionConfig {
+ public:
+    ConnectionConfig(): mSourceStage(0),
+                        mSourceTerminal(0),
+                        mSourceIteration(0),
+                        mSinkStage(0),
+                        mSinkTerminal(0),
+                        mSinkIteration(0),
+                        mConnectionType(0) {}
+
+    ConnectionConfig(ia_uid sourceStage,
+                     ia_uid sourceTerminal,
+                     ia_uid sourceIteration,
+                     ia_uid sinkStage,
+                     ia_uid sinkTerminal,
+                     ia_uid sinkIteration,
+                     int connectionType):
+                         mSourceStage(sourceStage),
+                         mSourceTerminal(sourceTerminal),
+                         mSourceIteration(sourceIteration),
+                         mSinkStage(sinkStage),
+                         mSinkTerminal(sinkTerminal),
+                         mSinkIteration(sinkIteration),
+                         mConnectionType(connectionType) {}
+    void dump() {
+        LOG1("connection src 0x%x (0x%x) sink 0x%x(0x%x)",
+             mSourceStage, mSourceTerminal, mSinkStage, mSinkTerminal);
+    }
+
+    ia_uid mSourceStage;
+    ia_uid mSourceTerminal;
+    ia_uid mSourceIteration;
+    ia_uid mSinkStage;
+    ia_uid mSinkTerminal;
+    ia_uid mSinkIteration;
+    int mConnectionType;
+};
+
+/**
+* \struct PortFormatSettings
+* Format settings for a port in the graph
+*/
+struct PortFormatSettings {
+    int32_t      enabled;
+    uint32_t     terminalId; /**< Unique terminal id (is a fourcc code) */
+    int32_t      width;    /**< Width of the frame in pixels */
+    int32_t      height;   /**< Height of the frame in lines */
+    int32_t      fourcc;   /**< Frame format */
+    int32_t      bpl;      /**< Bytes per line*/
+    int32_t      bpp;      /**< Bits per pixel */
+};
+
+/**
+ * \struct PipelineConnection
+ * Group port format, connection, stream, edge port for
+ * pipeline configuration
+ */
+struct PipelineConnection {
+    PipelineConnection() : stream(nullptr), hasEdgePort(false) { CLEAR(portFormatSettings); }
+    PortFormatSettings portFormatSettings;
+    ConnectionConfig connectionConfig;
+    HalStream *stream;
+    bool hasEdgePort;
+};
+
+struct StageAttr{
+    void *rbm;
+    uint32_t rbm_bytes;
+    StageAttr() : rbm(nullptr), rbm_bytes(0) {}
+};
+
+enum terminal_connection_type {
+    connection_type_push, /* data is pushed by source stage execute */
+    connection_type_pull  /* data is pulled by sink stage execute */
+};
+
+struct PgInfo {
+    PgInfo() : pgId(-1), streamId(-1) {}
+    std::string pgName;
+    int pgId;
+    int streamId;
+    StageAttr rbmValue;
+};
+
+struct MbrInfo {
+    MbrInfo() { streamId = -1; CLEAR(data); }
+    int streamId;
+    ia_isp_bxt_gdc_limits data;
+};
+
+struct ProgramGroupInfo {
+    ProgramGroupInfo() { streamId = -1; pgPtr = nullptr; }
+    int streamId;
+    ia_isp_bxt_program_group *pgPtr;
+};
+
+struct GraphConfigData {
+    int mcId;
+    int graphId;
+    uint32_t gdcKernelId;
+    camera_resolution_t csiReso;
+    ia_isp_bxt_resolution_info_t gdcReso;
+    std::vector<int32_t> streamIds;
+    std::vector<PgInfo> pgInfo;
+    std::vector<MbrInfo> mbrInfo;
+    std::vector<std::string> pgNames;
+    std::vector<ProgramGroupInfo> programGroup;
+    GraphConfigData() : mcId(-1),
+                        graphId(-1),
+                        gdcKernelId(-1) {
+        CLEAR(csiReso);
+        CLEAR(gdcReso);
+    }
+};
+
+struct ScalerInfo {
+    int32_t streamId;
+    float scalerWidth;
+    float scalerHeight;
+};
+}  // namespace IGraphType
+
+class IGraphConfig {
+public:
+    virtual ~IGraphConfig() = default;
+
+    virtual void getCSIOutputResolution(camera_resolution_t &reso) = 0;
+    virtual status_t getGdcKernelSetting(uint32_t *kernelId,
+                                         ia_isp_bxt_resolution_info_t *resolution) = 0;
+    virtual status_t graphGetStreamIds(std::vector<int32_t> &streamIds) = 0;
+    virtual int getGraphId(void) = 0;
+    virtual int getStreamIdByPgName(std::string pgName) = 0;
+    virtual int getPgIdByPgName(std::string pgName) = 0;
+    virtual ia_isp_bxt_program_group *getProgramGroup(int32_t streamId) = 0;
+    virtual int getProgramGroup(std::string pgName,
+                                ia_isp_bxt_program_group* programGroupForPG) {return OK;}
+    virtual status_t getMBRData(int32_t streamId, ia_isp_bxt_gdc_limits *data) = 0;
+    virtual status_t getPgRbmValue(std::string pgName,
+                                   IGraphType::StageAttr *stageAttr) {return OK;}
+    virtual status_t getPgIdForKernel(const uint32_t streamIds,
+                                      const int32_t kernelId, int32_t *pgId) {return OK;}
+    virtual status_t getPgNames(std::vector<std::string>* pgNames) = 0;
+    virtual status_t pipelineGetConnections(
+                         const std::vector<std::string> &pgList,
+                         std::vector<IGraphType::PipelineConnection> *confVector) = 0;
+};
+}
diff --git a/camera/hal/intel/ipu6/src/platformdata/gc/IGraphConfigManager.h b/camera/hal/intel/ipu6/src/platformdata/gc/IGraphConfigManager.h
new file mode 100644
index 000000000000..fa55c57ab2d1
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/gc/IGraphConfigManager.h
@@ -0,0 +1,43 @@
+/*
+ * Copyright (C) 2018-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+#pragma once
+
+#include <memory>
+#include <map>
+
+#include "CameraTypes.h"
+#include "IGraphConfig.h"
+#include "Parameters.h"
+#include "iutils/Thread.h"
+
+namespace icamera {
+class IGraphConfigManager {
+public:
+    virtual ~IGraphConfigManager() = default;
+
+    virtual int configStreams(const stream_config_t* streams) = 0;
+    virtual int getSelectedMcId() = 0;
+    virtual std::shared_ptr<IGraphConfig> getGraphConfig(ConfigMode configMode) = 0;
+    virtual bool isGcConfigured(void) = 0;
+    static void releaseInstance(int cameraId);
+    static IGraphConfigManager* getInstance(int cameraId);
+
+private:
+    // Guard for singleton instance creation.
+    static Mutex sLock;
+    static std::map<int, IGraphConfigManager*> sInstances;
+};
+}
diff --git a/camera/hal/intel/ipu6/src/platformdata/gc/custom_gcss_keys.h b/camera/hal/intel/ipu6/src/platformdata/gc/custom_gcss_keys.h
new file mode 100644
index 000000000000..624d750c868d
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/platformdata/gc/custom_gcss_keys.h
@@ -0,0 +1,50 @@
+/*
+ * Copyright (C) 2016-2018 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+GCSS_KEY (BAYER_ORDER, bayer_order)
+GCSS_KEY (GAIN, analogue_gain)
+GCSS_KEY (CSI_BE, csi_be)
+GCSS_KEY (CSI_BE_SOC, csi_be_soc)
+GCSS_KEY (CSI_PORT, csi_port)
+GCSS_KEY (EXPOSURE, exposure)
+GCSS_KEY (FLL, min_fll)
+GCSS_KEY (INTERLACED, interlaced)
+GCSS_KEY (HFLIP, hflip)
+GCSS_KEY (LINK_FREQ, link_freq)
+GCSS_KEY (LLP, min_llp)
+GCSS_KEY (SENSOR_MODE, sensor_mode)
+GCSS_KEY (PIXEL_ARRAY, pixel_array)
+GCSS_KEY (BINNER, binner)
+GCSS_KEY (SCALER, scaler)
+GCSS_KEY (BINNING_H_FACTOR, h_factor)
+GCSS_KEY (BINNING_V_FACTOR, v_factor)
+GCSS_KEY (SCALING_FACTOR_NUM, num_factor)
+GCSS_KEY (SCALING_FACTOR_DENOM, denom_factor)
+GCSS_KEY (PORT_0, port_0)
+GCSS_KEY (SCALER_PAD, scaler_pad)
+GCSS_KEY (TARGET, target)
+GCSS_KEY (SENSOR_NAME, sensor_name)
+GCSS_KEY (TPG, tpg)
+GCSS_KEY (STILL_POST_GDC, still_post_gdc)
+GCSS_KEY (VFLIP, vflip)
+GCSS_KEY (VIDEO0, video0)
+GCSS_KEY (VIDEO1, video1)
+GCSS_KEY (VIDEO2, video2)
+GCSS_KEY (STILL0, still0)
+GCSS_KEY (STILL1, still1)
+GCSS_KEY (STILL2, still2)
+GCSS_KEY (RAW, raw)
+GCSS_KEY (OP_MODE, op_mode)
+GCSS_KEY (MC_ID, mc_id)
diff --git a/camera/hal/intel/ipu6/src/v4l2/MediaControl.cpp b/camera/hal/intel/ipu6/src/v4l2/MediaControl.cpp
new file mode 100644
index 000000000000..0b530c394e90
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/v4l2/MediaControl.cpp
@@ -0,0 +1,1116 @@
+/*
+ * Copyright (C) 2011 The Android Open Source Project
+ * Copyright (C) 2015-2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "MediaControl"
+
+#include <stack>
+#include <linux/v4l2-mediabus.h>
+#include <linux/videodev2.h>
+
+#include "iutils/CameraLog.h"
+#include "iutils/Errors.h"
+#include "iutils/Utils.h"
+#include "V4l2DeviceFactory.h"
+#include "MediaControl.h"
+#include "Parameters.h"
+#include "SysCall.h"
+#include "PlatformData.h"
+
+using std::string;
+using std::vector;
+
+namespace icamera {
+
+struct MediaLink {
+    MediaPad *source;
+    MediaPad *sink;
+    MediaLink *twin;
+    uint32_t flags;
+    uint32_t padding[3];
+};
+
+struct MediaPad {
+    MediaEntity *entity;
+    uint32_t index;
+    uint32_t flags;
+    uint32_t padding[3];
+};
+
+struct MediaEntity {
+    media_entity_desc info;
+    MediaPad *pads;
+    MediaLink *links;
+    unsigned int maxLinks;
+    unsigned int numLinks;
+
+    char devname[32];
+};
+
+MediaControl *MediaControl::sInstance = nullptr;
+Mutex MediaControl::sLock;
+
+/*static*/ MediaControl*
+MediaControl::getInstance()
+{
+    LOG1("%s", __func__);
+    AutoMutex lock(sLock);
+    if (!sInstance) {
+        sInstance = new MediaControl(MEDIACTLDEVNAME);
+    }
+    return sInstance;
+}
+
+void MediaControl::releaseInstance()
+{
+    LOG1("%s", __func__);
+    AutoMutex lock(sLock);
+    delete sInstance;
+    sInstance = nullptr;
+}
+
+MediaControl::MediaControl(const char *devName) :
+    mDevName(devName)
+{
+    LOG1("@%s device: %s", __func__, devName);
+}
+
+MediaControl::~MediaControl()
+{
+    LOG1("@%s", __func__);
+}
+
+int MediaControl::initEntities()
+{
+    LOG1("@%s", __func__);
+
+    mEntities.reserve(100);
+
+    int ret = enumInfo();
+    if (ret != 0) {
+        LOGE("Enum Info failed.");
+        return -1;
+    }
+
+    return 0;
+}
+
+void MediaControl::clearEntities()
+{
+    LOG1("@%s", __func__);
+
+    auto entity = mEntities.begin();
+    while (entity != mEntities.end()) {
+        delete [] entity->pads;
+        entity->pads = nullptr;
+        delete [] entity->links;
+        entity->links = nullptr;
+        entity = mEntities.erase(entity);
+    }
+}
+
+MediaEntity *MediaControl::getEntityByName(const char *name)
+{
+    CheckError(!name, nullptr, "Invalid Entity name");
+
+    for (auto &entity : mEntities) {
+        if (strcmp(name, entity.info.name) == 0) {
+            return &entity;
+        }
+    }
+
+    return nullptr;
+}
+
+int MediaControl::getEntityIdByName(const char *name)
+{
+    MediaEntity *entity = getEntityByName(name);
+    if (!entity) {
+        return -1;
+    }
+
+    return entity->info.id;
+}
+
+int MediaControl::resetAllLinks()
+{
+    int ret;
+
+    LOG1("@%s", __func__);
+
+    for (auto &entity : mEntities) {
+
+        for (uint32_t j = 0; j < entity.numLinks; j++) {
+            MediaLink *link = &entity.links[j];
+
+            if (link->flags & MEDIA_LNK_FL_IMMUTABLE ||
+                    link->source->entity->info.id != entity.info.id) {
+                continue;
+            }
+            ret = setupLink(link->source, link->sink,
+                    link->flags & ~MEDIA_LNK_FL_ENABLED);
+
+            if (ret < 0)
+                return ret;
+        }
+    }
+
+    return 0;
+}
+
+int MediaControl::setupLink(MediaPad *source, MediaPad *sink, uint32_t flags)
+{
+    MediaLink *link = nullptr;
+    media_link_desc ulink;
+    uint32_t i;
+    int ret = 0;
+    LOG1("@%s", __func__);
+
+    SysCall *sc = SysCall::getInstance();
+
+    int fd = openDevice();
+    if (fd < 0)
+        goto done;
+
+    for (i = 0; i < source->entity->numLinks; i++) {
+        link = &source->entity->links[i];
+
+        if (link->source->entity == source->entity &&
+                link->source->index == source->index &&
+                link->sink->entity == sink->entity &&
+                link->sink->index == sink->index)
+            break;
+    }
+
+    if (i == source->entity->numLinks) {
+        LOGE("%s: Link not found", __func__);
+        ret = -ENOENT;
+        goto done;
+    }
+
+    /* source pad */
+    memset(&ulink, 0, sizeof(media_link_desc));
+    ulink.source.entity = source->entity->info.id;
+    ulink.source.index = source->index;
+    ulink.source.flags = MEDIA_PAD_FL_SOURCE;
+
+    /* sink pad */
+    ulink.sink.entity = sink->entity->info.id;
+    ulink.sink.index = sink->index;
+    ulink.sink.flags = MEDIA_PAD_FL_SINK;
+
+    if (link)
+        ulink.flags = flags | (link->flags & MEDIA_LNK_FL_IMMUTABLE);
+
+    if (Log::isDumpMediaInfo())
+        dumpLinkDesc(&ulink, 1);
+
+    ret = sc->ioctl(fd, MEDIA_IOC_SETUP_LINK, &ulink);
+    if (ret == -1) {
+        ret = -errno;
+        LOGE( "%s: Unable to setup link (%s)",
+                __func__, strerror(errno));
+        goto done;
+    }
+
+    if (link) {
+        link->flags = ulink.flags;
+        link->twin->flags = ulink.flags;
+    }
+
+    ret = 0;
+
+done:
+    closeDevice(fd);
+    return ret;
+}
+
+int MediaControl::setupLink(uint32_t srcEntity, uint32_t srcPad,
+                            uint32_t sinkEntity, uint32_t sinkPad, bool enable)
+{
+    LOG1("@%s srcEntity %d srcPad %d sinkEntity %d sinkPad %d enable %d",
+            __func__, srcEntity, srcPad, sinkEntity, sinkPad, enable);
+
+    for (auto &entity : mEntities) {
+        for (uint32_t j = 0; j < entity.numLinks; j++) {
+            MediaLink *link = &entity.links[j];
+
+            if ((link->source->entity->info.id == srcEntity)
+                && (link->source->index == srcPad)
+                && (link->sink->entity->info.id == sinkEntity)
+                && (link->sink->index == sinkPad)) {
+
+                if (enable)
+                    link->flags |= MEDIA_LNK_FL_ENABLED;
+                else
+                    link->flags &= ~MEDIA_LNK_FL_ENABLED;
+
+                return setupLink(link->source, link->sink, link->flags);
+            }
+        }
+    }
+
+    return -1;
+}
+
+int MediaControl::openDevice()
+{
+    int fd;
+    LOG1("@%s %s", __func__, mDevName.c_str());
+
+    SysCall *sc = SysCall::getInstance();
+
+    fd = sc->open(mDevName.c_str(), O_RDWR);
+    if (fd < 0) {
+        LOGE("%s: Error open media device %s: %s", __func__,
+             mDevName.c_str(), strerror(errno));
+        return UNKNOWN_ERROR;
+    }
+
+    return fd;
+}
+
+void MediaControl::closeDevice(int fd)
+{
+    LOG1("@%s", __func__);
+
+    if (fd < 0)
+        return ;
+
+    SysCall *sc = SysCall::getInstance();
+
+    if (sc->close(fd) < 0) {
+        LOGE("%s: Error close media device %s: %s", __func__,
+             mDevName.c_str(), strerror(errno));
+    }
+}
+
+void MediaControl::dumpInfo(media_device_info& devInfo)
+{
+    LOGD("Media controller API version %u.%u.%u\n\n",
+         (devInfo.media_version << 16) & 0xff,
+         (devInfo.media_version << 8) & 0xff,
+         (devInfo.media_version << 0) & 0xff);
+
+    LOGD("Media device information\n"
+         "------------------------\n"
+         "driver          %s\n"
+         "model           %s\n"
+         "serial          %s\n"
+         "bus info        %s\n"
+         "hw revision     0x%x\n"
+         "driver version  %u.%u.%u\n\n",
+         devInfo.driver, devInfo.model,
+         devInfo.serial, devInfo.bus_info,
+         devInfo.hw_revision,
+         (devInfo.driver_version << 16) & 0xff,
+         (devInfo.driver_version << 8) & 0xff,
+         (devInfo.driver_version << 0) & 0xff);
+
+    for (uint32_t i = 0; i < sizeof(devInfo.reserved)/sizeof(uint32_t); i++)
+         LOG2("reserved[%u] %d", i, devInfo.reserved[i]);
+}
+
+int MediaControl::enumInfo()
+{
+    int ret;
+    int fd = -1;
+    media_device_info info;
+    LOG1("@%s", __func__);
+
+    SysCall *sc = SysCall::getInstance();
+
+    if (mEntities.size() > 0)
+        return 0;
+
+    fd = openDevice();
+    if (fd < 0) {
+        LOGE("Open device failed.");
+        return fd;
+    }
+
+    ret = sc->ioctl(fd, MEDIA_IOC_DEVICE_INFO, &info);
+    if (ret < 0) {
+        LOGE("%s: Unable to retrieve media device information for device %s (%s)",__func__, mDevName.c_str(), strerror(errno));
+        goto done;
+    }
+
+    if (Log::isDumpMediaInfo())
+        dumpInfo(info);
+
+    ret = enumEntities(fd, info);
+    if (ret < 0) {
+        LOGE("%s: Unable to enumerate entities for device %s", __func__, mDevName.c_str());
+        goto done;
+    }
+
+    LOG2("Found %lu entities", mEntities.size());
+    LOG2("Enumerating pads and links");
+
+    ret = enumLinks(fd);
+    if (ret < 0) {
+        LOGE("%s: Unable to enumerate pads and linksfor device %s", __func__, mDevName.c_str());
+        goto done;
+    }
+
+    ret = 0;
+
+done:
+    closeDevice(fd);
+    return ret;
+}
+
+void MediaControl::dumpEntityDesc(media_entity_desc& desc, media_device_info& devInfo)
+{
+    LOGD("id %d", desc.id);
+    LOGD("name %s", desc.name);
+    LOGD("type 0x%x", desc.type);
+    LOGD("revision %d", desc.revision);
+    LOGD("flags %d", desc.flags);
+    LOGD("group_id %d", desc.group_id);
+    LOGD("pads %d", desc.pads);
+    LOGD("links %u", desc.links);
+
+    for (uint32_t i = 0; i < sizeof(desc.reserved)/sizeof(uint32_t); i++)
+        LOGD("reserved[%u] %d", i, devInfo.reserved[i]);
+}
+
+int MediaControl::enumEntities(int fd, media_device_info& devInfo)
+{
+    MediaEntity entity;
+    uint32_t id;
+    int ret;
+    LOG1("@%s", __func__);
+    SysCall *sc = SysCall::getInstance();
+
+    for (id = 0, ret = 0; ; id = entity.info.id) {
+        memset(&entity, 0, sizeof(MediaEntity));
+        entity.info.id = id | MEDIA_ENT_ID_FLAG_NEXT;
+
+        ret = sc->ioctl(fd, MEDIA_IOC_ENUM_ENTITIES, &entity.info);
+        if (ret < 0) {
+            ret = errno != EINVAL ? -errno : 0;
+            break;
+        }
+
+        if (Log::isDumpMediaInfo())
+            dumpEntityDesc(entity.info, devInfo);
+
+        /* Number of links (for outbound links) plus number of pads (for
+         * inbound links) is a good safe initial estimate of the total
+         * number of links.
+         */
+        entity.maxLinks = entity.info.pads + entity.info.links;
+
+        entity.pads = new MediaPad[entity.info.pads];
+        entity.links = new MediaLink[entity.maxLinks];
+        getDevnameFromSysfs(&entity);
+        mEntities.push_back(entity);
+
+        /* Note: carefully to move the follow setting. It must be behind of
+         * push_back to mEntities:
+         * 1. if entity is not pushed back to mEntities, getEntityById will
+         * return NULL.
+         * 2. we can't set entity.pads[i].entity to &entity direct. Because,
+         * entity is stack variable, its scope is just this function.
+         */
+        for (uint32_t i = 0; i < entity.info.pads; ++i) {
+            entity.pads[i].entity = getEntityById(entity.info.id);
+        }
+    }
+
+    return ret;
+}
+
+int MediaControl::getDevnameFromSysfs(MediaEntity *entity)
+{
+    char sysName[MAX_SYS_NAME] = {'\0'};
+    char target[MAX_TARGET_NAME] = {'\0'};
+    int ret;
+
+    if (!entity) {
+        LOGE("entity is null.");
+        return -EINVAL;
+    }
+
+    ret = snprintf(sysName, MAX_SYS_NAME, "/sys/dev/char/%u:%u",
+                   entity->info.v4l.major, entity->info.v4l.minor);
+    if (ret <= 0) {
+        LOGE("create sysName failed ret %d.", ret);
+        return -EINVAL;
+    }
+
+    ret = readlink(sysName, target, MAX_TARGET_NAME);
+    if (ret <= 0) {
+        LOGE("readlink sysName %s failed ret %d.", sysName, ret);
+        return -EINVAL;
+    }
+
+    char *d = strrchr(target, '/');
+    if (!d) {
+        LOGE("target is invalid %s.", target);
+        return -EINVAL;
+    }
+    d++; /* skip '/' */
+
+    char *t = strstr(d, "dvb");
+    if (t && t == d) {
+        t = strchr(t, '.');
+        if (!t) {
+            LOGE("target is invalid %s.", target);
+            return -EINVAL;
+        }
+        *t = '/';
+        d +=3; /* skip "dvb" */
+        snprintf(entity->devname, sizeof(entity->devname), "/dev/dvb/adapter%s", d);
+    } else {
+        snprintf(entity->devname, sizeof(entity->devname), "/dev/%s", d);
+    }
+
+    return 0;
+}
+
+void MediaControl::dumpPadDesc(media_pad_desc *pads, const int padsCount, const char *name)
+{
+    for (int i = 0; i < padsCount; i++) {
+        LOGD("Dump %s Pad desc %d", name == nullptr? "": name, i);
+        LOGD("entity: %d", pads[i].entity);
+        LOGD("index: %d", pads[i].index);
+        LOGD("flags: %d", pads[i].flags);
+        LOGD("reserved[0]: %d", pads[i].reserved[0]);
+        LOGD("reserved[1]: %d", pads[i].reserved[1]);
+    }
+}
+
+void MediaControl::dumpLinkDesc(media_link_desc *links, const int linksCount)
+{
+    for (int i = 0; i < linksCount; i++) {
+        LOG2("Dump Link desc %d", i);
+        MediaEntity *sourceEntity = getEntityById(links[i].source.entity);
+        MediaEntity *sinkEntity = getEntityById(links[i].sink.entity);
+
+        dumpPadDesc(&links[i].source, 1, sourceEntity->info.name);
+        dumpPadDesc(&links[i].sink, 1, sinkEntity->info.name);
+        LOGD("flags: %d", links[i].flags);
+        LOGD("reserved[0]: %d", links[i].reserved[0]);
+        LOGD("reserved[1]: %d", links[i].reserved[1]);
+    }
+}
+
+int MediaControl::enumLinks(int fd)
+{
+    int ret = 0;
+    LOG1("@%s", __func__);
+
+    SysCall *sc = SysCall::getInstance();
+
+    for (auto &entity : mEntities) {
+        media_links_enum links;
+        uint32_t i;
+
+        links.entity = entity.info.id;
+        links.pads = new media_pad_desc[entity.info.pads];
+        links.links = new media_link_desc[entity.info.links];
+
+        if (sc->ioctl(fd, MEDIA_IOC_ENUM_LINKS, &links) < 0) {
+            ret = -errno;
+            LOG2("%s: Unable to enumerate pads and links (%s).", __func__, strerror(errno));
+            delete [] links.pads;
+            delete [] links.links;
+            return ret;
+        }
+
+        if (Log::isDumpMediaInfo()) {
+            LOG2("entity %d", links.entity);
+            dumpPadDesc(links.pads, entity.info.pads);
+            dumpLinkDesc(links.links, entity.info.links);
+        }
+
+        for (i = 0; i < entity.info.pads; ++i) {
+            entity.pads[i].entity = getEntityById(entity.info.id);
+            entity.pads[i].index = links.pads[i].index;
+            entity.pads[i].flags = links.pads[i].flags;
+        }
+
+        for (i = 0; i < entity.info.links; ++i) {
+            media_link_desc *link = &links.links[i];
+            MediaLink *fwdlink;
+            MediaLink *backlink;
+            MediaEntity *source;
+            MediaEntity *sink;
+
+            source = getEntityById(link->source.entity);
+            sink = getEntityById(link->sink.entity);
+
+            if (source == nullptr || sink == nullptr) {
+                LOG2("WARNING entity %u link %u src %u/%u to %u/%u is invalid!",
+                        entity.info.id, i, link->source.entity,
+                        link->source.index,
+                        link->sink.entity,
+                        link->sink.index);
+                ret = -EINVAL;
+            } else {
+                fwdlink = entityAddLink(source);
+                if (fwdlink) {
+                    fwdlink->source = &source->pads[link->source.index];
+                    fwdlink->sink = &sink->pads[link->sink.index];
+                    fwdlink->flags = link->flags;
+                }
+
+                backlink = entityAddLink(sink);
+                if (backlink) {
+                    backlink->source = &source->pads[link->source.index];
+                    backlink->sink = &sink->pads[link->sink.index];
+                    backlink->flags = link->flags;
+                }
+
+                if (fwdlink)
+                    fwdlink->twin = backlink;
+                if (backlink)
+                    backlink->twin = fwdlink;
+            }
+        }
+
+        delete [] links.pads;
+        delete [] links.links;
+    }
+
+    return ret;
+}
+
+MediaLink *MediaControl::entityAddLink(MediaEntity *entity)
+{
+    if (entity->numLinks >= entity->maxLinks) {
+        uint32_t maxLinks = entity->maxLinks * 2;
+        MediaLink* links = new MediaLink[maxLinks];
+
+        MEMCPY_S(links, sizeof(MediaLink) * maxLinks, entity->links,
+                 sizeof(MediaLink) * entity->maxLinks);
+        delete [] entity->links;
+
+        for (uint32_t i = 0; i < entity->numLinks; ++i) {
+            links[i].twin->twin = &links[i];
+        }
+
+        entity->maxLinks = maxLinks;
+        entity->links = links;
+    }
+
+    return &entity->links[entity->numLinks++];
+}
+
+MediaEntity *MediaControl::getEntityById(uint32_t id)
+{
+    bool next = id & MEDIA_ENT_ID_FLAG_NEXT;
+
+    id &= ~MEDIA_ENT_ID_FLAG_NEXT;
+
+    for (uint32_t i = 0; i < mEntities.size(); i++) {
+        if ((mEntities[i].info.id == id && !next) ||
+                (mEntities[0].info.id > id && next)) {
+            return &mEntities[i];
+        }
+
+    }
+
+    return nullptr;
+}
+
+const char *MediaControl::entitySubtype2String(unsigned type)
+{
+    static const char *nodeTypes[] = {
+        "Unknown",
+        "V4L",
+        "FB",
+        "ALSA",
+        "DVB",
+    };
+    static const char *subdevTypes[] = {
+        "Unknown",
+        "Sensor",
+        "Flash",
+        "Lens",
+    };
+
+    uint32_t subtype = type & MEDIA_ENT_SUBTYPE_MASK;
+
+    switch (type & MEDIA_ENT_TYPE_MASK) {
+    case MEDIA_ENT_T_DEVNODE:
+        if (subtype >= ARRAY_SIZE(nodeTypes))
+            subtype = 0;
+        return nodeTypes[subtype];
+
+    case MEDIA_ENT_T_V4L2_SUBDEV:
+        if (subtype >= ARRAY_SIZE(subdevTypes))
+            subtype = 0;
+        return subdevTypes[subtype];
+    default:
+        return nodeTypes[0];
+    }
+}
+
+const char *MediaControl::padType2String(unsigned flag)
+{
+    static const struct {
+        __u32 flag;
+        const char *name;
+    } flags[] = {
+        { MEDIA_PAD_FL_SINK, "Sink" },
+        { MEDIA_PAD_FL_SOURCE, "Source" },
+    };
+
+    uint32_t i;
+
+    for (i = 0; i < ARRAY_SIZE(flags); i++) {
+        if (flags[i].flag & flag)
+            return flags[i].name;
+    }
+
+    return "Unknown";
+}
+
+int MediaControl::setMediaMcCtl(int cameraId, vector <McCtl> ctls)
+{
+    for (auto &ctl : ctls) {
+        MediaEntity *entity = getEntityById(ctl.entity);
+        V4L2Subdevice* subDev = V4l2DeviceFactory::getSubDev(cameraId, entity->devname);
+        int ret = subDev->SetControl(ctl.ctlCmd, ctl.ctlValue);
+        LOG2("set Ctl %s [%d] cmd %s [0x%08x] value %d", ctl.entityName.c_str(), ctl.entity,
+                ctl.ctlName.c_str(), ctl.ctlCmd, ctl.ctlValue);
+        CheckError(ret != OK, ret, "set Ctl %s [%d] cmd %s [0x%08x] value %d failed.",
+                ctl.entityName.c_str(), ctl.entity, ctl.ctlName.c_str(), ctl.ctlCmd, ctl.ctlValue);
+    }
+    return 0;
+}
+
+int MediaControl::setMediaMcLink(vector <McLink> links)
+{
+    for (auto &link : links) {
+        LOG2("setup Link %s [%d:%d] ==> %s [%dx%d] enable %d.",
+              link.srcEntityName.c_str(), link.srcEntity, link.srcPad, link.sinkEntityName.c_str(),
+              link.sinkEntity, link.sinkPad, link.enable);
+        int ret = setupLink(link.srcEntity, link.srcPad, link.sinkEntity, link.sinkPad, link.enable);
+        if (ret < 0) {
+            LOGE("setup Link %s [%d:%d] ==> %s [%dx%d] enable %d failed.",
+                link.srcEntityName.c_str(), link.srcEntity, link.srcPad, link.sinkEntityName.c_str(),
+                link.sinkEntity, link.sinkPad, link.enable);
+            return ret;
+        }
+    }
+    return 0;
+}
+
+int MediaControl::setFormat(int cameraId, const McFormat *format, int targetWidth, int targetHeight, int field)
+{
+    PERF_CAMERA_ATRACE();
+    int ret;
+    v4l2_mbus_framefmt mbusfmt;
+    MediaEntity *entity = getEntityById(format->entity);
+    if (entity == nullptr) {
+        LOGE("@%s, get entity fail for calling getEntityById", __func__);
+        return BAD_VALUE;
+    }
+
+    MediaPad *pad = &entity->pads[format->pad];
+    V4L2Subdevice* subDev = V4l2DeviceFactory::getSubDev(cameraId, entity->devname);
+    LOG1("@%s, targetWidth:%d, targetHeight:%d", __func__, targetWidth, targetHeight);
+    LOG2("SENSORCTRLINFO: width=%d", targetWidth);
+    LOG2("SENSORCTRLINFO: height=%d", targetHeight);
+    LOG2("SENSORCTRLINFO: code=0x%x", format->pixelCode);
+
+    CLEAR(mbusfmt);
+    if (format->width != 0 && format->height != 0) {
+        mbusfmt.width  = format->width;
+        mbusfmt.height = format->height;
+    } else if (format->type == RESOLUTION_TARGET) {
+        mbusfmt.width  = targetWidth;
+        mbusfmt.height = targetHeight;
+    }
+    mbusfmt.field = field;
+
+    if (format->pixelCode) {
+        mbusfmt.code = format->pixelCode;
+    } else {
+        mbusfmt.code = CameraUtils::getMBusFormat(cameraId, PlatformData::getISysFormat(cameraId));
+    }
+    LOG2("set format %s [%d:%d] [%dx%d] [%dx%d] %s ", format->entityName.c_str(),
+            format->entity, format->pad, mbusfmt.width, mbusfmt.height,
+            targetWidth, targetHeight, CameraUtils::pixelCode2String(mbusfmt.code));
+
+    struct v4l2_subdev_format fmt = {};
+    fmt.pad = format->pad;
+    fmt.which = V4L2_SUBDEV_FORMAT_ACTIVE;
+    fmt.format = mbusfmt;
+    ret = subDev->SetFormat(fmt);
+    CheckError(ret < 0, BAD_VALUE, "set format %s [%d:%d] [%dx%d] %s failed.",
+            format->entityName.c_str(), format->entity, format->pad, format->width, format->height,
+            CameraUtils::pixelCode2String(format->pixelCode));
+
+    mbusfmt = fmt.format;
+
+    /* If the pad is an output pad, automatically set the same format on
+     * the remote subdev input pads, if any.
+     */
+    if (pad->flags & MEDIA_PAD_FL_SOURCE) {
+        for (unsigned int i = 0; i < pad->entity->numLinks; ++i) {
+            MediaLink *link = &pad->entity->links[i];
+
+            if (!(link->flags & MEDIA_LNK_FL_ENABLED))
+                continue;
+
+            if (link->source == pad && link->sink->entity->info.type == MEDIA_ENT_T_V4L2_SUBDEV) {
+                auto subDev = V4l2DeviceFactory::getSubDev(cameraId, link->sink->entity->devname);
+
+                struct v4l2_subdev_format tmt = {};
+                tmt.format = mbusfmt;
+                tmt.pad = link->sink->index;
+                tmt.which = V4L2_SUBDEV_FORMAT_ACTIVE;
+                subDev->SetFormat(tmt);
+            }
+        }
+    }
+
+    return 0;
+}
+
+int MediaControl::setSelection(int cameraId,
+        const McFormat *format, int targetWidth, int targetHeight)
+{
+    PERF_CAMERA_ATRACE();
+    int ret = OK;
+
+    MediaEntity *entity = getEntityById(format->entity);
+    V4L2Subdevice* subDev = V4l2DeviceFactory::getSubDev(cameraId, entity->devname);
+    LOG1("@%s, cameraId:%d, targetWidth:%d, targetHeight:%d", __func__, cameraId, targetWidth, targetHeight);
+
+    if (format->top != -1 && format->left != -1 && format->width != 0 && format->height != 0) {
+        struct v4l2_subdev_selection selection = {};
+        selection.pad = format->pad;
+        selection.which = V4L2_SUBDEV_FORMAT_ACTIVE;
+        selection.target = format->selCmd;
+        selection.flags = 0;
+        selection.r.top = format->top;
+        selection.r.left = format->left;
+        selection.r.width = format->width;
+        selection.r.height = format->height;
+
+        ret = subDev->SetSelection(selection);
+    } else if (format->selCmd == V4L2_SEL_TGT_CROP || format->selCmd == V4L2_SEL_TGT_COMPOSE) {
+        LOG2("@%s, line:%d, targetWidth:%d, targetHeight:%d", __func__, __LINE__, targetWidth, targetHeight);
+
+        struct v4l2_subdev_selection selection = {};
+        selection.pad = format->pad;
+        selection.which = V4L2_SUBDEV_FORMAT_ACTIVE;
+        selection.target = format->selCmd;
+        selection.flags = 0;
+        selection.r.top = 0;
+        selection.r.left = 0;
+        selection.r.width = targetWidth;
+        selection.r.height = targetHeight;
+
+        ret = subDev->SetSelection(selection);
+    } else {
+        ret = BAD_VALUE;
+    }
+
+    CheckError(ret < 0, BAD_VALUE, "set selection %s [%d:%d] selCmd: %d [%d, %d] [%dx%d] failed",
+            format->entityName.c_str(), format->entity, format->pad, format->selCmd,
+            format->top, format->left, format->width, format->height);
+
+    return OK;
+}
+
+int MediaControl::mediaCtlSetup(int cameraId, MediaCtlConf *mc, int width, int height, int field)
+{
+    LOG1("%s, cameraId:%d", __func__, cameraId);
+    /* Setup controls in format Configuration */
+    int ret = setMediaMcCtl(cameraId, mc->ctls);
+    CheckError(ret != OK, ret, "set MediaCtlConf McCtl failed: ret=%d", ret);
+
+    /* Set format & selection in format Configuration */
+    for (auto &fmt : mc->formats) {
+        if (fmt.formatType == FC_FORMAT) {
+            setFormat(cameraId, &fmt, width, height, field);
+        } else if (fmt.formatType == FC_SELECTION) {
+            setSelection(cameraId, &fmt, width, height);
+        }
+    }
+
+    /* Set link in format Configuration */
+    ret = setMediaMcLink(mc->links);
+    CheckError(ret != OK, ret, "set MediaCtlConf McLink failed: ret = %d", ret);
+
+    dumpEntityTopology();
+
+    return OK;
+}
+
+int MediaControl::getVCMI2CAddr(const char* vcmName, string* vcmI2CAddr) {
+    CheckError(!vcmI2CAddr, BAD_VALUE, "vcmI2CAddr is nullptr");
+    CheckError(!vcmName, BAD_VALUE, "vcmName is nullptr");
+
+    for (auto &entity : mEntities) {
+        if (strncmp(entity.info.name, vcmName, strlen(vcmName)) == 0) {
+            *vcmI2CAddr = entity.info.name;
+            LOG2("%s, vcm addr name %s", __func__, entity.info.name);
+            return OK;
+        }
+    }
+
+    return NAME_NOT_FOUND;
+}
+
+void MediaControl::mediaCtlClear(int cameraId, MediaCtlConf *mc)
+{
+    LOG1("%s, cameraId:%d", __func__, cameraId);
+
+}
+
+// This function must be called after enumEntities().
+int MediaControl::getLensName(string *lensName)
+{
+    LOG1("@%s", __func__);
+    CheckError(!lensName, UNKNOWN_ERROR, "lensName is nullptr");
+
+    for (auto &entity : mEntities) {
+        if (entity.info.type == MEDIA_ENT_T_V4L2_SUBDEV_LENS) {
+           *lensName = entity.info.name;
+           return OK;
+        }
+    }
+
+    return UNKNOWN_ERROR;
+}
+
+// This function must be called after enumEntities().
+bool MediaControl::checkAvailableSensor(const std::string &sensorEntityName,
+                                        const std::string &sinkEntityName)
+{
+    LOG1("@%s, sensorEntityName:%s, sinkEntityName:%s", __func__,
+         sensorEntityName.c_str(), sinkEntityName.c_str());
+
+    std::string sensorEntityNameTmp = sensorEntityName;
+    sensorEntityNameTmp.append(" ");
+    size_t nameLen = sensorEntityNameTmp.length();
+    for (auto &entity : mEntities) {
+        int linksCount = entity.info.links;
+        MediaLink *links = entity.links;
+        for (int i = 0; i < linksCount; i++) {
+            if (strcmp(links[i].sink->entity->info.name, sinkEntityName.c_str()) == 0) {
+                char *entityName = entity.info.name;
+                if (strncmp(entityName, sensorEntityNameTmp.c_str(), nameLen) == 0) {
+                    return true;
+                }
+            }
+        }
+    }
+
+    return false;
+}
+
+// This function must be called after enumEntities().
+int MediaControl::getI2CBusAddress(const string &sensorEntityName, const string &sinkEntityName, string *i2cBus)
+{
+    LOG1("@%s, sensorEntityName:%s, sinkEntityName:%s", __func__, sensorEntityName.c_str(), sinkEntityName.c_str());
+    CheckError(!i2cBus, UNKNOWN_ERROR, "i2cBus is nullptr");
+
+    for (auto &entity : mEntities) {
+        int linksCount = entity.info.links;
+        MediaLink *links = entity.links;
+        char *entityName = nullptr;
+        size_t sensorEntityNameLen = sensorEntityName.length();
+        for (int i = 0; i < linksCount; i++) {
+            if (strcmp(links[i].sink->entity->info.name, sinkEntityName.c_str()) == 0) {
+                entityName = entity.info.name;
+                break;
+            }
+        }
+
+        // entityName example: "imx319 10-0010", sensorEntityName example: "imx319"
+        if (entityName && (strlen(entityName) > (sensorEntityNameLen + 1))) {
+            *i2cBus = entityName + sensorEntityNameLen + 1;
+            LOG2("i2cBus is %s", i2cBus->c_str());
+            return OK;
+        }
+    }
+
+    return UNKNOWN_ERROR;
+}
+
+void MediaControl::dumpTopologyDot()
+{
+    printf("digraph board {\n");
+    printf("\trankdir=TB\n");
+
+    for (auto &entity : mEntities) {
+        const media_entity_desc *info = &entity.info;
+        const char *devname = (entity.devname[0] ? entity.devname : nullptr);
+        uint32_t numLinks = entity.numLinks;
+        uint32_t npads;
+        UNUSED(npads);
+
+        switch (info->type & MEDIA_ENT_TYPE_MASK) {
+        case MEDIA_ENT_T_DEVNODE:
+            // Although printf actually can print NULL pointer, but make check
+            // to make KW happy.
+            if (devname)
+                printf("\tn%08x [label=\"%s\\n%s\", shape=box, style=filled, "
+                    "fillcolor=yellow]\n",
+                    info->id, info->name, devname);
+            break;
+
+        case MEDIA_ENT_T_V4L2_SUBDEV:
+            printf("\tn%08x [label=\"{{", info->id);
+
+            for (int i = 0, npads = 0; i < info->pads; ++i) {
+                MediaPad *pad = entity.pads + i;
+
+                if (!(pad->flags & MEDIA_PAD_FL_SINK))
+                    continue;
+
+                printf("%s<port%d> %d", npads ? " | " : "", i, i);
+                npads++;
+            }
+
+            printf("} | %s", info->name);
+            if (devname)
+                printf("\\n%s", devname);
+            printf(" | {");
+
+            for (int i = 0, npads = 0; i < info->pads; ++i) {
+                MediaPad *pad = entity.pads + i;
+
+                if (!(pad->flags & MEDIA_PAD_FL_SOURCE))
+                    continue;
+
+                printf("%s<port%d> %d", npads ? " | " : "", i, i);
+                npads++;
+            }
+
+            printf("}}\", shape=Mrecord, style=filled, fillcolor=green]\n");
+            break;
+
+        default:
+            continue;
+        }
+
+        for (uint32_t i = 0; i < numLinks; i++) {
+             MediaLink *link = entity.links + i;
+             MediaPad *source = link->source;
+             MediaPad *sink = link->sink;
+
+            /*Only print the forward links of the entity*/
+            if (source->entity != &entity)
+                continue;
+
+            printf("\tn%08x", source->entity->info.id);
+            if ((source->entity->info.type & MEDIA_ENT_TYPE_MASK) == MEDIA_ENT_T_V4L2_SUBDEV)
+                printf(":port%u", source->index);
+            printf(" -> ");
+            printf("n%08x", sink->entity->info.id);
+            if ((sink->entity->info.type & MEDIA_ENT_TYPE_MASK) == MEDIA_ENT_T_V4L2_SUBDEV)
+                printf(":port%u", sink->index);
+
+            if (link->flags & MEDIA_LNK_FL_IMMUTABLE)
+                printf(" [style=bold]");
+            else if (!(link->flags & MEDIA_LNK_FL_ENABLED))
+                printf(" [style=dashed]");
+            printf("\n");
+        }
+    }
+
+    printf("}\n");
+}
+
+void MediaControl::dumpTopologyText()
+{
+    static const struct {
+        __u32 flag;
+        const char *name;
+    } link_flags[] = {
+        { MEDIA_LNK_FL_ENABLED, "ENABLED" },
+        { MEDIA_LNK_FL_IMMUTABLE, "IMMUTABLE" },
+        { MEDIA_LNK_FL_DYNAMIC, "DYNAMIC" },
+    };
+
+    printf("Device topology\n");
+
+    for (auto &entity : mEntities) {
+        const media_entity_desc *info = &entity.info;
+        const char *devname = (entity.devname[0] ? entity.devname : nullptr);
+        uint32_t numLinks = entity.numLinks;
+
+        uint32_t padding = printf("- entity %u: ", info->id);
+        printf("%s (%u pad%s, %u link%s)\n", info->name,
+            info->pads, info->pads > 1 ? "s" : "",
+            numLinks, numLinks > 1 ? "s" : "");
+        printf("%*ctype %s subtype %s flags %x\n", padding, ' ',
+            padType2String(info->type),
+            entitySubtype2String(info->type),
+            info->flags);
+        if (devname)
+            printf("%*cdevice node name %s\n", padding, ' ', devname);
+
+        for (int i = 0; i < info->pads; i++) {
+            MediaPad *pad = entity.pads + i;
+
+            printf("\tpad%d: %s\n", i, padType2String(pad->flags));
+
+            /*
+             *if ((info->type & MEDIA_ENT_TYPE_MASK) == MEDIA_ENT_T_V4L2_SUBDEV)
+             *v4l2_subdev_print_format(entity, i, V4L2_SUBDEV_FORMAT_ACTIVE);
+             */
+            for (uint32_t j = 0; j < numLinks; j++) {
+                MediaLink *link = entity.links + j;
+                MediaPad *source = link->source;
+                MediaPad *sink = link->sink;
+                bool first = true;
+
+                if (source->entity == &entity && source->index == j)
+                    printf("\t\t-> \"%s\":%u [", sink->entity->info.name,
+                       sink->index);
+                else if (sink->entity == &entity && sink->index == j)
+                    printf("\t\t<- \"%s\":%u [", source->entity->info.name,
+                       source->index);
+                else
+                    continue;
+
+                for (uint32_t k = 0; k < ARRAY_SIZE(link_flags); k++) {
+                    if (!(link->flags & link_flags[k].flag))
+                        continue;
+                    if (!first)
+                        printf(",");
+                    printf("%s", link_flags[k].name);
+                    first = false;
+                }
+
+                printf("]\n");
+            }
+        }
+        printf("\n");
+    }
+}
+
+void MediaControl::dumpEntityTopology(bool dot)
+{
+    if (Log::isDumpMediaTopo()) {
+        if (dot)
+            dumpTopologyDot();
+        else
+            dumpTopologyText();
+    }
+}
+} // namespace icamera
diff --git a/camera/hal/intel/ipu6/src/v4l2/MediaControl.h b/camera/hal/intel/ipu6/src/v4l2/MediaControl.h
new file mode 100644
index 000000000000..bf2850731f08
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/v4l2/MediaControl.h
@@ -0,0 +1,302 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <vector>
+#include <string>
+
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <unistd.h>
+#include <fcntl.h>
+#include <sys/ioctl.h>
+#include <string.h>
+#include <stdlib.h>
+#include <stdio.h>
+#include <errno.h>
+#include <linux/v4l2-subdev.h>
+#include <linux/media.h>
+#include <expat.h>
+
+#include <v4l2_device.h>
+
+#include "CameraTypes.h"
+#include "iutils/Thread.h"
+
+#include "NodeInfo.h"
+
+using namespace std;
+namespace icamera {
+
+struct MediaEntity;
+struct MediaPad;
+struct MediaLink;
+
+#define MEDIACTLDEVNAME "/dev/media0"
+
+enum {
+    FC_FORMAT = 0,
+    FC_SELECTION = 1,
+};
+
+enum ResolutionType {
+    RESOLUTION_MAX = 0,
+    RESOLUTION_COMPOSE,
+    RESOLUTION_CROP,
+    RESOLUTION_TARGET,
+
+};
+
+struct McFormat {
+    int entity;
+    int pad;
+    int stream;
+    int formatType;
+    int selCmd;
+    int top;
+    int left;
+    int width;
+    int height;
+    enum ResolutionType type;
+    std::string entityName;
+    unsigned int pixelCode;
+    McFormat() {
+        entity = 0;
+        pad = 0;
+        stream = 0;
+        formatType = 0;
+        selCmd = 0;
+        top = 0;
+        left = 0;
+        width = 0;
+        height = 0;
+        type = RESOLUTION_MAX;
+        pixelCode = 0;}
+};
+
+struct McOutput {
+    Port port;
+    unsigned int v4l2Format;
+    int width;
+    int height;
+    McOutput() { port = INVALID_PORT; v4l2Format = 0; width = 0; height = 0; }
+};
+
+struct McCtl {
+    int entity;
+    int ctlCmd;
+    int ctlValue;
+    std::string ctlName;
+    std::string entityName;
+    McCtl() { entity = 0; ctlCmd = 0; ctlValue= 0; }
+};
+
+struct McLink {
+    int srcEntity;
+    int srcPad;
+    int sinkEntity;
+    int sinkPad;
+    bool enable;
+    std::string srcEntityName;
+    std::string sinkEntityName;
+    McLink() { srcEntity = 0; srcPad = 0; sinkEntity = 0; sinkPad = 0; enable = false; }
+};
+
+struct McRoute {
+    int entity;
+    uint32_t sinkPad;
+    uint32_t sinkStream;
+    uint32_t srcPad;
+    uint32_t srcStream;
+    uint32_t flag;
+    std::string entityName;
+    McRoute() { entity = 0; sinkPad = 0; srcPad = 0; sinkStream = 0; srcStream = 0; flag = 0; entityName.clear(); }
+};
+
+struct McVideoNode {
+    std::string name;
+    VideoNodeType videoNodeType;
+    McVideoNode() { videoNodeType = VIDEO_GENERIC; }
+};
+
+struct MediaCtlConf {
+    std::vector <McCtl> ctls;
+    std::vector <McLink> links;
+    std::vector <McRoute> routes;
+    std::vector <McFormat> formats;
+    std::vector <McOutput> outputs;
+    std::vector <McVideoNode> videoNodes;
+    int mcId;
+    int outputWidth;
+    int outputHeight;
+    std::vector <ConfigMode> configMode;
+    int format;
+    /*
+     * The outputWidth or outputHeight is 0 if there isn't this setting
+     * in MediaCtlConf. It means the isys output size is dynamic, and
+     * we don't use stream size to select MC.
+     */
+    MediaCtlConf() {
+        mcId = -1;
+        outputWidth = 0;
+        outputHeight = 0;
+        format = -1;
+    }
+};
+
+/**
+ * \class MediaController
+ *
+ * This class is used for discovering and configuring the internal topology
+ * of a media device. Devices are modelled as an oriented graph of building
+ * blocks called media entities. The media entities are connected to each other
+ * through pads.
+ *
+ * Each media entity corresponds to a V4L2 subdevice. This class is also used
+ * for configuring the V4L2 subdevices.
+ */
+
+class MediaControl {
+public:
+    /**
+     * \brief Get the singleton instance of MediaControl
+     */
+    static MediaControl* getInstance();
+
+    /**
+     * \brief Release the singleton instance of MediaControl.
+     */
+    static void releaseInstance();
+
+    /**
+     * \brief Enum entities and link, and reset all links
+     *
+     * \return 0 if succeed, other value indicates failed
+     */
+    int initEntities();
+
+    /**
+     * \brief Free all entities and links memory
+     */
+    void clearEntities();
+
+    /**
+     * \brief Get the entity by name
+     *
+     * \return entity id if succeed or -1 if error
+     */
+    int getEntityIdByName(const char *name);
+
+    /**
+     * \brief Get VCM I2C bus address
+     *
+     * \return 0 if succeed, other value indicates failed
+     */
+    int getVCMI2CAddr(const char* vcmName, std::string* vcmI2CAddr);
+
+    /**
+     * \brief Set up media controller pipe
+     *
+     * \param cameraId: the current camera id
+     * \param mc: the MediaCtlConf got from platform data
+     * \param sensorName: the sensor name get from platform data
+     * \param width: The width of the request frame
+     * \param height: The height of the request frame
+     * \param format: The format of the request frame
+     * \param field: The field of the request frame
+     *
+     * \return OK if succeed, other value indicates failed
+     */
+    int mediaCtlSetup(int cameraId, MediaCtlConf *mc, int width, int height, int field);
+
+    /**
+     * \brief Clear media controller pipe
+     *
+     *
+     * \param cameraId: the current camera id
+     * \param mc: the MediaCtlConf got from platform data
+     */
+    void mediaCtlClear(int cameraId, MediaCtlConf *mc);
+
+    int resetAllLinks();
+
+    int getLensName(string *lensName);
+    bool checkAvailableSensor(const std::string &sensorEntityName,
+                              const std::string &sinkEntityName);
+    /**
+     * Getting I2C bus address by the name of sensor entity and the name of sensor's sink entity.
+     *
+     * \sensorEntityName: the name of sensor entity.
+     * \sinkEntityName: the name of sensor's sink entity.
+     * \i2cBus: I2C bus address.
+     * \return OK if succeed, other value indicates failed
+     */
+    int getI2CBusAddress(const std::string &sensorEntityName, const std::string &sinkEntityName,
+                         std::string *i2cBus);
+private:
+    MediaControl& operator=(const MediaControl&);
+    MediaControl(const char *devName);
+    ~MediaControl();
+
+    int openDevice();
+    void closeDevice(int fd);
+
+    //enum MediaControl info.
+    int enumInfo();
+    int enumLinks(int fd);
+    int enumEntities(int fd, media_device_info& devInfo);
+
+    //get entity info.
+    int getDevnameFromSysfs(MediaEntity *entity);
+    MediaEntity *getEntityById(uint32_t id);
+    MediaEntity *getEntityByName(const char *name);
+
+    //set up entity link.
+
+    MediaLink *entityAddLink(MediaEntity *entity);
+    int setupLink(uint32_t srcEntity, uint32_t srcPad, uint32_t sinkEntity, uint32_t sinkPad, bool enable);
+    int setupLink(MediaPad *source, MediaPad *sink, uint32_t flags);
+
+    //set up MediaCtlConf info.
+    int setMediaMcCtl(int cameraId, std::vector <McCtl> ctls);
+    int setMediaMcLink(std::vector <McLink> links);
+    int setFormat(int cameraId, const McFormat *format,
+            int targetWidth, int targetHeight, int field);
+    int setSelection(int cameraId, const McFormat *format,
+            int targetWidth, int targetHeight);
+
+    /* Dump functions */
+    void dumpInfo(media_device_info& devInfo);
+    void dumpEntityDesc(media_entity_desc& desc, media_device_info& devInfo);
+    void dumpPadDesc(media_pad_desc *pads, const int padsCount = 1, const char *name = nullptr);
+    void dumpLinkDesc(media_link_desc *links, const int linksCount = 1);
+    const char *entitySubtype2String(unsigned type);
+    const char *padType2String(unsigned flag);
+    void dumpEntityTopology(bool dot = true);
+    void dumpTopologyDot();
+    void dumpTopologyText();
+
+private:
+    std::string mDevName;
+    std::vector <MediaEntity> mEntities;
+
+private:
+    static MediaControl *sInstance;
+    static Mutex sLock;
+};
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/v4l2/NodeInfo.cpp b/camera/hal/intel/ipu6/src/v4l2/NodeInfo.cpp
new file mode 100644
index 000000000000..2709861a757d
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/v4l2/NodeInfo.cpp
@@ -0,0 +1,53 @@
+/*
+ * Copyright (C) 2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#include "src/v4l2/NodeInfo.h"
+
+const VideoNodeInfo gVideoNodeInfos[] = {
+    { VIDEO_GENERIC,             "VIDEO_GENERIC",              "Generic" },
+    { VIDEO_GENERIC_MEDIUM_EXPO, "VIDEO_GENERIC_MEDIUM_EXPO",  "GenericMediumExpo" },
+    { VIDEO_GENERIC_SHORT_EXPO,  "VIDEO_GENERIC_SHORT_EXPO",   "GenericShortExpo" },
+
+    { VIDEO_PIXEL_ARRAY,         "VIDEO_PIXEL_ARRAY",          "PixelArray" },
+    { VIDEO_PIXEL_BINNER,        "VIDEO_PIXEL_BINNER",         "PixelBinner" },
+    { VIDEO_PIXEL_SCALER,        "VIDEO_PIXEL_SCALER",         "PixelScaler" },
+
+    { VIDEO_ISYS_RECEIVER,       "VIDEO_ISYS_RECEIVER",        "ISysReceiver" },
+    { VIDEO_ISYS_RECEIVER_BACKEND,  "VIDEO_ISYS_RECEIVER_BACKEND",  "CsiBE"},
+};
+
+const char* GetNodeName(VideoNodeType nodeType)
+{
+    int size = ARRAY_SIZE(gVideoNodeInfos);
+    for (int i = 0; i < size; i++) {
+        if (gVideoNodeInfos[i].type == nodeType) {
+            return gVideoNodeInfos[i].shortName;
+        }
+    }
+    return "InvalidNode";
+}
+
+VideoNodeType GetNodeType(const char* nodeName)
+{
+    int size = ARRAY_SIZE(gVideoNodeInfos);
+    for (int i = 0; i < size; i++) {
+        if (strcmp(gVideoNodeInfos[i].fullName, nodeName) == 0) {
+            return gVideoNodeInfos[i].type;
+        }
+    }
+
+    return VIDEO_GENERIC;
+}
diff --git a/camera/hal/intel/ipu6/src/v4l2/NodeInfo.h b/camera/hal/intel/ipu6/src/v4l2/NodeInfo.h
new file mode 100644
index 000000000000..c5905603a4c5
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/v4l2/NodeInfo.h
@@ -0,0 +1,50 @@
+/*
+ * Copyright (C) 2020 Intel Corporation
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include "iutils/Utils.h"
+
+enum VideoNodeType {
+    // video node device
+    VIDEO_GENERIC,
+    VIDEO_GENERIC_MEDIUM_EXPO,
+    VIDEO_GENERIC_SHORT_EXPO,
+
+    // sensor subdevice
+    VIDEO_PIXEL_ARRAY,
+    VIDEO_PIXEL_BINNER,
+    VIDEO_PIXEL_SCALER,
+
+    // ISP subdevice
+    VIDEO_ISYS_RECEIVER,
+    VIDEO_ISYS_RECEIVER_BACKEND,
+};
+
+struct VideoNodeInfo {
+    VideoNodeType type;
+    const char* fullName;
+    const char* shortName;
+};
+
+enum EncodeBufferType {
+    ENCODE_ISA_CONFIG  = 0,
+    ENCODE_STATS = 1,
+};
+
+extern const VideoNodeInfo gVideoNodeInfos[];
+extern const char* GetNodeName(VideoNodeType nodeType);
+extern VideoNodeType GetNodeType(const char* nodeName);
diff --git a/camera/hal/intel/ipu6/src/v4l2/SysCall.cpp b/camera/hal/intel/ipu6/src/v4l2/SysCall.cpp
new file mode 100644
index 000000000000..2f82704d143b
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/v4l2/SysCall.cpp
@@ -0,0 +1,212 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "SysCall"
+
+#include "iutils/CameraLog.h"
+#include "SysCall.h"
+
+namespace icamera {
+
+static int sCreatedCount = 0;
+bool SysCall::sIsInitialized = false;
+SysCall *SysCall::sInstance = nullptr;
+//Guard for singleton instance creation
+Mutex SysCall::sLock;
+
+/*static*/ SysCall*
+SysCall::getInstance()
+{
+    AutoMutex lock(sLock);
+    if (!sIsInitialized) {
+        //Use real sys call as default
+        sInstance = new SysCall();
+        sIsInitialized = true;
+    }
+    return sInstance;
+}
+
+void SysCall::updateInstance(SysCall* newSysCall)
+{
+    LOG1("%s", __func__);
+    AutoMutex lock(sLock);
+    if (sIsInitialized) {
+        sIsInitialized = false;
+    }
+    sInstance = newSysCall;
+    if (newSysCall != nullptr)
+        sIsInitialized = true;
+}
+
+SysCall::SysCall()
+{
+    sCreatedCount++;
+    LOG1("Syscall was created %d time", sCreatedCount);
+}
+
+SysCall::~SysCall()
+{
+    sCreatedCount--;
+    LOG1("Syscall was destructed %d time", sCreatedCount);
+}
+
+int SysCall::open(const char *pathname, int flags)
+{
+    return ::open(pathname, flags);
+}
+
+int SysCall::close(int fd)
+{
+    return ::close(fd);
+}
+
+void *SysCall::mmap(void *addr, size_t len, int prot, int flag, int filedes, off_t off)
+{
+    return ::mmap(addr, len, prot, flag, filedes, off);
+}
+
+int SysCall::munmap(void *addr, size_t len)
+{
+    return ::munmap(addr, len);
+}
+
+int SysCall::ioctl(int fd, int request, struct media_device_info *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+int SysCall::ioctl(int fd, int request, struct media_link_desc *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+int SysCall::ioctl(int fd, int request, struct media_links_enum *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+int SysCall::ioctl(int fd, int request, struct media_links_desc *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+int SysCall::ioctl(int fd, int request, struct media_entity_desc *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+int SysCall::ioctl(int fd, int request, struct v4l2_capability *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+int SysCall::ioctl(int fd, int request, v4l2_fmtdesc *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+int SysCall::ioctl(int fd, int request, enum v4l2_buf_type *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+int SysCall::ioctl(int fd, int request, struct v4l2_format *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+int SysCall::ioctl(int fd, int request, struct v4l2_requestbuffers *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+int SysCall::ioctl(int fd, int request, struct v4l2_buffers *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+int SysCall::ioctl(int fd, int request, struct v4l2_buffer *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+int SysCall::ioctl(int fd, int request, struct v4l2_subdev_format *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+int SysCall::ioctl(int fd, int request, struct v4l2_subdev_stream *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+int SysCall::ioctl(int fd, int request, struct v4l2_streamon_info *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+
+int SysCall::ioctl(int fd, int request, struct v4l2_ext_controls *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+
+int SysCall::ioctl(int fd, int request, struct v4l2_control *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+
+int SysCall::ioctl(int fd, int request, struct v4l2_queryctrl *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+
+int SysCall::ioctl(int fd, int request, struct v4l2_subdev_selection *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+
+int SysCall::ioctl(int fd, int request, struct v4l2_subdev_routing *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+
+int SysCall::ioctl(int fd, int request, struct v4l2_querymenu *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+
+int SysCall::ioctl(int fd, int request, struct v4l2_event_subscription *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+
+int SysCall::ioctl(int fd, int request, struct v4l2_event *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+
+int SysCall::ioctl(int fd, int request, struct v4l2_exportbuffer *arg)
+{
+    return ioctl(fd, request, (void *)arg);
+}
+
+int SysCall::ioctl(int fd, int request, void *arg)
+{
+    int ret = 0;
+    do {
+        ret = ::ioctl(fd, request, arg);
+    } while (-1 == ret && EINTR == errno);
+
+    return ret;
+}
+
+int SysCall::poll(struct pollfd *pfd, nfds_t nfds, int timeout)
+{
+    int ret = 0;
+    do {
+        ret = ::poll(pfd, nfds, timeout);
+    } while (-1 == ret && EINTR == errno);
+
+    return ret;
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/v4l2/SysCall.h b/camera/hal/intel/ipu6/src/v4l2/SysCall.h
new file mode 100644
index 000000000000..4ef9c058455d
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/v4l2/SysCall.h
@@ -0,0 +1,87 @@
+/*
+ * Copyright (C) 2015-2018 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <sys/types.h>
+#include <sys/stat.h>
+#include <fcntl.h>
+#include <errno.h>
+#include <poll.h>
+#include <unistd.h>
+#include <sys/ioctl.h>
+#include <sys/mman.h>
+#include <linux/media.h>
+#include <linux/videodev2.h>
+#include <linux/v4l2-subdev.h>
+
+#include "iutils/Thread.h"
+
+namespace icamera {
+
+class SysCall {
+protected:
+    SysCall();
+    virtual ~SysCall();
+
+public:
+    virtual int open(const char *pathname, int flags);
+    virtual int close(int fd);
+    virtual void *mmap(void *addr, size_t len, int prot, int flag, int filedes, off_t off);
+    virtual int munmap(void *addr, size_t len);
+
+    virtual int ioctl(int fd, int request, struct media_device_info *arg);
+    virtual int ioctl(int fd, int request, struct media_link_desc *arg);
+    virtual int ioctl(int fd, int request, struct media_links_enum *arg);
+    virtual int ioctl(int fd, int request, struct media_links_desc *arg);
+    virtual int ioctl(int fd, int request, struct media_entity_desc *arg);
+    virtual int ioctl(int fd, int request, struct v4l2_capability *arg);
+    virtual int ioctl(int fd, int request, v4l2_fmtdesc *arg);
+    virtual int ioctl(int fd, int request, enum v4l2_buf_type *arg);
+    virtual int ioctl(int fd, int request, struct v4l2_format *arg);
+    virtual int ioctl(int fd, int request, struct v4l2_requestbuffers *arg);
+    virtual int ioctl(int fd, int request, struct v4l2_buffers *arg);
+    virtual int ioctl(int fd, int request, struct v4l2_buffer *arg);
+    virtual int ioctl(int fd, int request, struct v4l2_subdev_format *arg);
+    virtual int ioctl(int fd, int request, struct v4l2_subdev_stream *arg);
+    virtual int ioctl(int fd, int request, struct v4l2_streamon_info *arg);
+    virtual int ioctl(int fd, int request, struct v4l2_ext_controls *arg);
+    virtual int ioctl(int fd, int request, struct v4l2_control *arg);
+    virtual int ioctl(int fd, int request, struct v4l2_queryctrl *arg);
+    virtual int ioctl(int fd, int request, struct v4l2_subdev_selection *arg);
+    virtual int ioctl(int fd, int request, struct v4l2_subdev_routing *arg);
+    virtual int ioctl(int fd, int request, struct v4l2_querymenu *arg);
+    virtual int ioctl(int fd, int request, struct v4l2_event_subscription *arg);
+    virtual int ioctl(int fd, int request, struct v4l2_event *arg);
+    virtual int ioctl(int fd, int request, struct v4l2_exportbuffer *arg);
+
+    virtual int poll(struct pollfd *pfd, nfds_t nfds, int timeout);
+
+private:
+        int ioctl(int fd, int request, void *arg);
+
+public:
+    static SysCall* getInstance();
+    static void updateInstance(SysCall* newSysCall);
+private:
+    SysCall&   operator=(const SysCall&);         //Don't call me
+
+    static bool sIsInitialized;
+    static SysCall *sInstance;
+    static Mutex sLock;
+};
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/v4l2/V4l2DeviceFactory.cpp b/camera/hal/intel/ipu6/src/v4l2/V4l2DeviceFactory.cpp
new file mode 100644
index 000000000000..90950d2d04c8
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/v4l2/V4l2DeviceFactory.cpp
@@ -0,0 +1,126 @@
+/*
+ * Copyright (C) 2015-2020 Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#define LOG_TAG "V4l2DeviceFactory"
+
+#include <fcntl.h>
+
+#include "iutils/CameraLog.h"
+
+#include "V4l2DeviceFactory.h"
+
+using namespace std;
+
+namespace icamera {
+
+map<int, V4l2DeviceFactory*> V4l2DeviceFactory::sInstances;
+Mutex V4l2DeviceFactory::sLock;
+
+V4l2DeviceFactory::V4l2DeviceFactory(int cameraId) : mCameraId(cameraId)
+{
+    LOG1("V4l2DeviceFactory created for id:%d", mCameraId);
+}
+
+V4l2DeviceFactory::~V4l2DeviceFactory()
+{
+    LOG1("V4l2DeviceFactory released for id:%d", mCameraId);
+}
+
+/**
+ * Create a static instance of V4l2DeviceFactory for cameraId.
+ * It should be called before any device is used.
+ */
+void V4l2DeviceFactory::createDeviceFactory(int cameraId)
+{
+    AutoMutex lock(sLock);
+    getInstance(cameraId);
+}
+
+/**
+ * Release the static instance of V4l2DeviceFactory for cameraId.
+ * All device related to the instance of of V4l2DeviceFactory will be release here as well
+ * After calling this function, all device could not be used anymore.
+ */
+void V4l2DeviceFactory::releaseDeviceFactory(int cameraId)
+{
+    AutoMutex lock(sLock);
+    V4l2DeviceFactory* factory = getInstance(cameraId);
+    sInstances.erase(cameraId);
+    factory->releaseSubDevices(cameraId);
+    delete factory;
+}
+
+/**
+ * Get an opened sub device
+ *
+ * The caller is supposed to get an opened sub device.
+ * If openSubDev failed, it just return non-opened instance,
+ * and using this instance to call its funtion will cause 'device not open' error,
+ *
+ * Return a not nullptr sub device pointer
+ */
+V4L2Subdevice* V4l2DeviceFactory::getSubDev(int cameraId, const string& devName)
+{
+    AutoMutex lock(sLock);
+    V4l2DeviceFactory* factory = getInstance(cameraId);
+    // If an existing sub device found, then just return it.
+
+    if (factory->mDevices.find(devName) != factory->mDevices.end()) {
+        return factory->mDevices[devName];
+    }
+    // Create a new sub device for devName, since it's not created before.
+    V4L2Subdevice* subdev = new V4L2Subdevice(devName);
+
+    // Make sure the caller always got an opened device.
+    subdev->Open(O_RDWR);
+    // Add the new allocated sub device into device map.
+    factory->mDevices[devName] = subdev;
+    return subdev;
+}
+
+/**
+ * Release all sub devices in device map
+ *
+ * It's a private function with no lock in it, must be called with lock protection.
+ *
+ * It MUST be called after all sub devices are not used anymore
+ */
+void V4l2DeviceFactory::releaseSubDevices(int  /*cameraId*/)
+{
+    for (auto it = mDevices.begin(); it != mDevices.end(); it++) {
+        V4L2Subdevice* subdev = it->second;
+        if (subdev) {
+            subdev->Close();
+            delete subdev;
+        }
+    }
+    mDevices.clear();
+}
+
+/**
+ * Private function with no lock in it, must be called with lock protection
+ */
+V4l2DeviceFactory* V4l2DeviceFactory::getInstance(int cameraId)
+{
+    if (sInstances.find(cameraId) != sInstances.end()) {
+        return sInstances[cameraId];
+    }
+
+    sInstances[cameraId] = new V4l2DeviceFactory(cameraId);
+    return sInstances[cameraId];
+}
+
+} //namespace icamera
diff --git a/camera/hal/intel/ipu6/src/v4l2/V4l2DeviceFactory.h b/camera/hal/intel/ipu6/src/v4l2/V4l2DeviceFactory.h
new file mode 100644
index 000000000000..115187d5fa37
--- /dev/null
+++ b/camera/hal/intel/ipu6/src/v4l2/V4l2DeviceFactory.h
@@ -0,0 +1,61 @@
+/*
+ * Copyright (C) 2015-2020 Intel Corporation.
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+#pragma once
+
+#include <v4l2_device.h>
+
+#include <map>
+#include <string>
+
+#include "iutils/Utils.h"
+#include "iutils/Thread.h"
+
+namespace icamera {
+
+/**
+ * Create all v4l2 devices, and provide users an opened device pointer,
+ * the users should not release any instance got from this factory,
+ * all devices will be released together by releaseDeviceFactory.
+ *
+ * Currently only sub device is supported.
+ *
+ * TODO: Next step, all v4l2 devices should be managed by this class
+ */
+class V4l2DeviceFactory {
+public:
+    static void createDeviceFactory(int cameraId);
+    static void releaseDeviceFactory(int cameraId);
+
+    static V4L2Subdevice* getSubDev(int cameraId, const std::string& devName);
+
+private:
+    V4l2DeviceFactory(int cameraId);
+    ~V4l2DeviceFactory();
+
+    static V4l2DeviceFactory* getInstance(int cameraId);
+    void releaseSubDevices(int cameraId);
+
+private:
+    static std::map<int, V4l2DeviceFactory*> sInstances;
+    //Guard for V4l2DeviceFactory public API access
+    static Mutex sLock;
+
+    int mCameraId;
+    std::map<std::string, V4L2Subdevice*> mDevices;
+};
+
+} //namespace icamera
-- 
2.17.1

